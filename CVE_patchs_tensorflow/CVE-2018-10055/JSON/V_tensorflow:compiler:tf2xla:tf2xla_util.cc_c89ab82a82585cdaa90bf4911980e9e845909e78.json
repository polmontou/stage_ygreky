"/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/compiler/tf2xla/tf2xla_util.h\"\n\n#include <queue>\n#include <set>\n#include <unordered_map>\n\n#include \"tensorflow/compiler/tf2xla/sharding_util.h\"\n#include \"tensorflow/compiler/tf2xla/tf2xla.pb.h\"\n#include \"tensorflow/compiler/xla/xla_data.pb.h\"\n#include \"tensorflow/core/framework/graph.pb.h\"\n#include \"tensorflow/core/framework/graph_def_util.h\"\n#include \"tensorflow/core/framework/node_def.pb.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/framework/tensor_shape.pb.h\"\n#include \"tensorflow/core/framework/versions.pb.h\"\n#include \"tensorflow/core/graph/tensor_id.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/lib/core/status.h\"\n#include \"tensorflow/core/lib/gtl/optional.h\"\n#include \"tensorflow/core/lib/strings/strcat.h\"\n\nnamespace tensorflow {\n\nnamespace {\n\nStatus ValidateTensorId(const tf2xla::TensorId& id) {\n  if (id.node_name().empty()) {\n    return errors::InvalidArgument(\"TensorId node_name must be non-empty\");\n  }\n  if (id.output_index() < 0) {\n    return errors::InvalidArgument(\"TensorId output_index must be positive\");\n  }\n  return Status::OK();\n}\n\nStatus CheckNameDuplicates(const string& kind, const string& name,\n                           std::set<string>* names) {\n  if (!name.empty()) {\n    if (!names->insert(name).second) {\n      return errors::InvalidArgument(\"duplicate \", kind, \" name: \", name);\n    }\n  }\n  return Status::OK();\n}\n\nStatus CheckFeedFetchNameConflicts(const string& kind,\n                                   const std::set<string>& names) {\n  // We don't allow the feeds or fetches to contain both \"foo\" and \"foo_data\",\n  // since that will cause a collision in codegen symbols.\n  for (const string& name : names) {\n    const string name_data(name + \"_data\");\n    if (names.find(name_data) != names.end()) {\n      return errors::InvalidArgument(\"conflicting \", kind, \" name: \", name,\n                                     \" and \", name_data);\n    }\n  }\n  return Status::OK();\n}\n\n}  // namespace\n\nStatus ValidateConfig(const tf2xla::Config& config) {\n  std::set<string> names;\n  for (const tf2xla::Feed& feed : config.feed()) {\n    TF_RETURN_IF_ERROR(ValidateTensorId(feed.id()));\n    TF_RETURN_IF_ERROR(TensorShape::IsValidShape(feed.shape()));\n    TF_RETURN_IF_ERROR(CheckNameDuplicates(\"feed\", feed.name(), &names));\n  }\n  TF_RETURN_IF_ERROR(CheckFeedFetchNameConflicts(\"feed\", names));\n  names.clear();\n  for (const tf2xla::Fetch& fetch : config.fetch()) {\n    TF_RETURN_IF_ERROR(ValidateTensorId(fetch.id()));\n    TF_RETURN_IF_ERROR(CheckNameDuplicates(\"fetch\", fetch.name(), &names));\n  }\n  TF_RETURN_IF_ERROR(CheckFeedFetchNameConflicts(\"fetch\", names));\n  if (config.fetch().empty()) {\n    return errors::InvalidArgument(\"fetches must be specified\");\n  }\n  return Status::OK();\n}\n\nStatus AddPlaceholdersForFeeds(\n    const tf2xla::Config& config, const OpRegistryInterface* op_registry,\n    std::unordered_map<string, string>* feed_remapping, GraphDef* graph_def) {\n  struct PlaceholderInfo {\n    const tf2xla::Feed* feed = nullptr;  // point to Feed in <config>.\n    string placeholder_name;\n    DataType data_type = DT_INVALID;\n  };\n\n  // Put each fed tensor into a map by name:port. A map is used for determinism\n  // when creating placeholders (genrules want deterministic output).\n  std::map<string, PlaceholderInfo> placeholder_info;\n  for (int i = 0; i < config.feed_size(); ++i) {\n    const tf2xla::Feed* feed = &config.feed(i);\n    const string name_port = TensorIdToString(feed->id());\n    PlaceholderInfo& info = placeholder_info[name_port];\n    info.feed = feed;\n    info.placeholder_name = strings::StrCat(\n        \"aot_feed_\", feed->id().output_index(), \"/\", feed->id().node_name());\n    (*feed_remapping)[name_port] = info.placeholder_name;\n  }\n\n  // Verify node exists and determine data type.\n  std::unordered_map<string, const NodeDef*> name_to_node;\n  for (int i = 0; i < graph_def->node_size(); ++i) {\n    name_to_node[graph_def->node(i).name()] = &graph_def->node(i);\n  }\n  for (auto it = placeholder_info.begin(); it != placeholder_info.end(); ++it) {\n    PlaceholderInfo& info = it->second;\n    const tf2xla::TensorId& feed_id = info.feed->id();\n\n    // Find the existing node and determine data type.\n    auto node_it = name_to_node.find(feed_id.node_name());\n    if (node_it == name_to_node.end()) {\n      return errors::NotFound(\"Can't find feed node: \",\n                              TensorIdToString(feed_id));\n    }\n    const NodeDef* existing = node_it->second;\n\n    if (info.feed->type() != DT_INVALID) {\n      info.data_type = info.feed->type();\n    } else {\n      // Build the node in order to infer its type.\n\n      // Must first add default attrs as well, so do this in a copied GraphDef.\n      GraphDef gd;\n      *gd.mutable_versions() = graph_def->versions();\n      *gd.add_node() = *existing;\n      TF_RETURN_IF_ERROR(\n          AddDefaultAttrsToGraphDef(&gd, *op_registry, 0 /*node_offset*/));\n\n      // Now build the node from the copied node def.\n      Graph g(op_registry);\n      g.set_versions(graph_def->versions());\n      Status status;\n      Node* feed_node = g.AddNode(gd.node(0), &status);\n      TF_RETURN_IF_ERROR(status);\n      info.data_type =\n          BaseType(feed_node->output_type(info.feed->id().output_index()));\n    }\n  }\n\n  // Create placeholders. Note that we could avoid creating a placeholder for\n  // feeds which are already placeholders, but we omit that to avoid more cases\n  // in this code.\n  for (auto it = placeholder_info.begin(); it != placeholder_info.end(); ++it) {\n    const PlaceholderInfo& info = it->second;\n    NodeDef* d = graph_def->add_node();\n    d->set_name(info.placeholder_name);\n    d->set_op(\"PlaceholderV2\");\n    auto& attr_map = *d->mutable_attr();\n    attr_map[\"dtype\"].set_type(info.data_type);\n    *attr_map[\"shape\"].mutable_shape() = info.feed->shape();\n  }\n\n  // Rewrite references to the fed tensors to refer to the placeholder.\n  for (int i = 0; i < graph_def->node_size(); ++i) {\n    NodeDef* node_def = graph_def->mutable_node(i);\n    for (int j = 0; j < node_def->input_size(); ++j) {\n      auto id = ParseTensorName(node_def->input(j));\n      auto it = placeholder_info.find(id.ToString());\n      if (it != placeholder_info.end()) {\n        node_def->set_input(j, it->second.placeholder_name);\n      }\n    }\n  }\n\n  return Status::OK();\n}\n\nStatus PruneGraphDefInto(const tf2xla::Config& config, const GraphDef& in,\n                         GraphDef* out) {\n  *out = in;\n  out->clear_node();\n\n  // Tensors needed for feeding.\n  std::set<std::pair<string, int>> feed_tensors;\n  for (const tf2xla::Feed& feed : config.feed()) {\n    feed_tensors.insert(\n        std::make_pair(feed.id().node_name(), feed.id().output_index()));\n  }\n\n  // Maps node name to reachability.\n  std::unordered_map<string, std::pair<bool, const NodeDef*>> node_by_name;\n  for (const NodeDef& node : in.node()) {\n    node_by_name[node.name()] = std::pair<bool, const NodeDef*>(false, &node);\n  }\n\n  // Traverse.\n  std::queue<string> name_queue;\n  for (int i = 0; i < config.fetch_size(); ++i) {\n    name_queue.push(config.fetch(i).id().node_name());\n  }\n  while (!name_queue.empty()) {\n    const string name = name_queue.front();\n    name_queue.pop();\n\n    auto find_it = node_by_name.find(name);\n    if (find_it == node_by_name.end()) {\n      return errors::InvalidArgument(\"While pruning graph, node \", name,\n                                     \" needed but not found in the graph.\");\n    }\n    auto& map_entry = find_it->second;\n    if (map_entry.first) {\n      continue;\n    }\n    map_entry.first = true;\n\n    // Push input nodes of the currently visited node to name_queue.\n    for (const string& in_edge : map_entry.second->input()) {\n      auto id = ParseTensorName(in_edge);\n      const string node_name = id.first.ToString();\n      if (feed_tensors.find(std::make_pair(node_name, id.second)) ==\n          feed_tensors.end()) {\n        name_queue.push(node_name);\n      } else {\n        // The input tensor is from an edge that is being fed. Therefore,\n        // we skip recursing down that edge, to avoid requiring nodes that\n        // may not be needed (note that the input node may still be added\n        // to name_queue later if one of its output edges is not being fed).\n      }\n    }\n  }\n\n  // Copy over, preserving order of original and only nodes that are reachable\n  // from the fetches.\n  out->mutable_node()->Reserve(in.node_size());\n  for (const NodeDef& node : in.node()) {\n    if (node_by_name[node.name()].first) {\n      *out->add_node() = node;\n    }\n  }\n  return Status::OK();\n}\n\nstring TensorIdToString(const tf2xla::TensorId& id) {\n  return strings::StrCat(id.node_name(), \":\", id.output_index());\n}\n\nStatus SetNodeShardingFromNeighbors(Node* n, bool out_edges) {\n  int core = -1;\n  const Node* matching_node = nullptr;\n  for (const Edge* edge : (out_edges ? n->out_edges() : n->in_edges())) {\n    if (edge->IsControlEdge()) continue;\n    const Node* possible_match = out_edges ? edge->dst() : edge->src();\n    TF_ASSIGN_OR_RETURN(\n        tensorflow::gtl::optional<xla::OpSharding> sharding,\n        ParseShardingFromDevice(\n            *possible_match,\n            /*num_cores_per_replica=*/std::numeric_limits<int32>::max()));\n    if (sharding.has_value()) {\n      TF_RET_CHECK(sharding.value().type() ==\n                   xla::OpSharding::Type::OpSharding_Type_MAXIMAL);\n      const int core_annotation = sharding.value().tile_assignment_devices(0);\n      if (core == -1 || core > core_annotation) {\n        core = core_annotation;\n        matching_node = possible_match;\n      }\n    }\n  }\n  if (matching_node != nullptr) {\n    n->set_assigned_device_name(matching_node->assigned_device_name());\n    n->set_requested_device(matching_node->requested_device());\n  }\n  return Status::OK();\n}\n\n}  // namespace tensorflow"