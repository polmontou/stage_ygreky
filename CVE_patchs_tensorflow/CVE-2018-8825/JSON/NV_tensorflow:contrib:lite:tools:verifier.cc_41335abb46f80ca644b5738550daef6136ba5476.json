"/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/contrib/lite/tools/verifier.h\"\n#include <climits>\n#include \"tensorflow/contrib/lite/schema/schema_generated.h\"\n#include \"tensorflow/contrib/lite/string_util.h\"\n#include \"tensorflow/contrib/lite/version.h\"\n\nnamespace tflite {\n\nnamespace {\n\n// Reports error message when the reporter is set.\nvoid ReportError(ErrorReporter* error_reporter, const char* format, ...) {\n  if (error_reporter) {\n    va_list args;\n    va_start(args, format);\n    error_reporter->Report(format, args);\n    va_end(args);\n  }\n}\n\n// Returns the int32_t value pointed by ptr.\nconst uint32_t* GetIntPtr(const char* ptr) {\n  return reinterpret_cast<const uint32_t*>(ptr);\n}\n\n// Verifies flatbuffer format of the model contents and returns the in-memory\n// model.\nconst Model* VerifyFlatbufferAndGetModel(const void* buf, size_t len) {\n  ::flatbuffers::Verifier verifier(static_cast<const uint8_t*>(buf), len);\n  if (VerifyModelBuffer(verifier)) {\n    return ::tflite::GetModel(buf);\n  } else {\n    return nullptr;\n  }\n}\n\nconst uint32_t kMaxNumString = UINT_MAX / sizeof(int32_t) - 2;\n\n// Verifies string tensor has legit buffer contents that follow the schema\n// defined in lite/string_util.h\nbool VerifyStringTensorBuffer(const Buffer& buffer,\n                              ErrorReporter* error_reporter) {\n  uint32_t buffer_size = buffer.data()->size();\n  const char* buffer_ptr = reinterpret_cast<const char*>(buffer.data()->data());\n\n  uint32_t num_strings = *GetIntPtr(buffer_ptr);\n  if (num_strings > kMaxNumString) {\n    ReportError(error_reporter,\n                \"String tensor has invalid num of string set: %d\", num_strings);\n    return false;\n  }\n  uint32_t header_offsets =\n      static_cast<uint32_t>(num_strings + 2) * sizeof(int32_t);\n\n  if (buffer_size < header_offsets) {\n    ReportError(error_reporter,\n                \"String tensor buffer requires at least %d bytes, but is \"\n                \"allocated with %d bytes\",\n                header_offsets, buffer_size);\n    return false;\n  }\n\n  uint32_t prev_ptr = header_offsets;\n  uint32_t offset = sizeof(int32_t);\n\n  if (*GetIntPtr(buffer_ptr + offset) != header_offsets) {\n    ReportError(error_reporter,\n                \"String tensor buffer initial offset must be: %d\",\n                header_offsets);\n    return false;\n  }\n  offset += sizeof(int32_t);\n  for (int i = 1; i <= num_strings; i++, offset += sizeof(int32_t)) {\n    int string_offset = *GetIntPtr(buffer_ptr + offset);\n    if (string_offset < prev_ptr || string_offset > buffer_size) {\n      ReportError(error_reporter, \"String tensor buffer is invalid: index %d\",\n                  i);\n      return false;\n    }\n  }\n  if (*GetIntPtr(buffer_ptr + offset - sizeof(int32_t)) != buffer_size) {\n    ReportError(error_reporter, \"String tensor buffer last offset must be %d\",\n                buffer_size);\n    return false;\n  }\n  return true;\n}\n\n// Verifies numeric tensor has legit buffer.\nbool VerifyNumericTensorBuffer(const Tensor& tensor, const Buffer& buffer,\n                               ErrorReporter* error_reporter) {\n  uint64_t bytes_required = 1;\n  for (int dim : *tensor.shape()) {\n    bytes_required *= dim;\n    if (bytes_required > UINT_MAX) {\n      ReportError(error_reporter, \"Tensor dimension overflow\");\n      return false;\n    }\n  }\n  switch (tensor.type()) {\n    case TensorType_FLOAT32:\n      bytes_required *= sizeof(float);\n      break;\n    case TensorType_INT32:\n      bytes_required *= sizeof(int32_t);\n      break;\n    case TensorType_UINT8:\n      bytes_required *= sizeof(uint8_t);\n      break;\n    case TensorType_INT64:\n      bytes_required *= sizeof(int64_t);\n      break;\n    case TensorType_FLOAT16:\n      // FALLTHROUGH_INTENDED;\n    default:\n      ReportError(error_reporter, \"Invalid tensor type: %d\", tensor.type());\n      return false;\n  }\n  if (bytes_required > UINT_MAX) {\n    ReportError(error_reporter, \"Tensor dimension overflow\");\n    return false;\n  }\n\n  if (bytes_required != buffer.data()->size()) {\n    ReportError(\n        error_reporter,\n        \"Tensor requires %d bytes, but is allocated with %d bytes buffer\",\n        bytes_required, buffer.data()->size());\n    return false;\n  }\n  return true;\n\n  // TODO(yichengfan): verify quantized tensors.\n}\n\nusing flatbuffers::Offset;\nusing flatbuffers::Vector;\n\nbool VerifyOperators(const Vector<Offset<Operator>>& operators,\n                     ErrorReporter* error_reporter) {\n  for (const auto& op : operators) {\n    if (!op->inputs()) {\n      ReportError(error_reporter, \"Missing 'inputs' for operator.\");\n      return false;\n    }\n    if (!op->outputs()) {\n      ReportError(error_reporter, \"Missing 'outputs' for operator.\");\n      return false;\n    }\n  }\n  return true;\n}\n\nbool VerifySubGraphs(const Model& model, ErrorReporter* error_reporter) {\n  if (!model.subgraphs()) {\n    ReportError(error_reporter, \"Missing 'subgraphs' section.\");\n    return false;\n  }\n  for (const auto& subgraph : *model.subgraphs()) {\n    if (!subgraph->operators()) {\n      ReportError(error_reporter, \"Missing 'operators' section in subgraph.\");\n      return false;\n    }\n\n    if (!VerifyOperators(*subgraph->operators(), error_reporter)) {\n      return false;\n    }\n  }\n  return true;\n}\n\n// Verifies tensors have valid properties and legit buffer if set.\nbool VerifyTensors(const Model& model, ErrorReporter* error_reporter) {\n  if (!model.subgraphs()) {\n    return true;\n  }\n  if (!model.buffers()) {\n    ReportError(error_reporter, \"Missing 'buffers' section.\");\n    return false;\n  }\n\n  for (const auto& subgraph : *model.subgraphs()) {\n    if (!subgraph->tensors()) {\n      continue;\n    }\n    for (const auto& tensor : *subgraph->tensors()) {\n      if (!tensor->buffer()) {\n        continue;\n      }\n      if (tensor->buffer() >= model.buffers()->size()) {\n        ReportError(error_reporter, \"Invalid tensor buffer index: %d\",\n                    tensor->buffer());\n        return false;\n      }\n      auto* buffer = model.buffers()->Get(tensor->buffer());\n      if (!buffer) {\n        ReportError(error_reporter, \"Tensor buffer %d not set\",\n                    tensor->buffer());\n        return false;\n      }\n\n      // Many transient tensors don't have data in the flatbuffer. Their\n      // buffers will be allocated by the interpreter at run-time.\n      if (buffer->data()) {\n        if (tensor->type() == TensorType_STRING) {\n          if (!VerifyStringTensorBuffer(*buffer, error_reporter)) {\n            return false;\n          }\n        } else {\n          if (!VerifyNumericTensorBuffer(*tensor, *buffer, error_reporter)) {\n            return false;\n          }\n        }\n      }\n    }\n  }\n  return true;\n}\n\nbool VerifyOps(const Model& model, const OpResolver& resolver,\n               ErrorReporter* error_reporter) {\n  if (!model.operator_codes()) {\n    return true;\n  }\n  for (const auto& opcode : *model.operator_codes()) {\n    if (opcode->builtin_code() < BuiltinOperator_MIN ||\n        opcode->builtin_code() > BuiltinOperator_MAX) {\n      ReportError(error_reporter, \"Operator id '%d' is out of range.\",\n                  opcode->builtin_code());\n      return false;\n    }\n\n    if (opcode->builtin_code() == BuiltinOperator_CUSTOM) {\n      if (!resolver.FindOp(opcode->custom_code()->c_str())) {\n        ReportError(error_reporter, \"Unsupported custom op: %s\",\n                    opcode->custom_code()->c_str());\n        return false;\n      }\n    } else {\n      if (!resolver.FindOp(opcode->builtin_code())) {\n        ReportError(error_reporter, \"Unsupported builtin op: %s\",\n                    EnumNameBuiltinOperator(opcode->builtin_code()));\n        return false;\n      }\n    }\n  }\n  return true;\n}\n\n}  // namespace\n\nbool Verify(const void* buf, size_t len, const OpResolver& resolver,\n            ErrorReporter* error_reporter) {\n  const Model* model = VerifyFlatbufferAndGetModel(buf, len);\n  if (model == nullptr) {\n    ReportError(error_reporter, \"Invalid flatbuffer format\");\n    return false;\n  }\n  if (model->version() != TFLITE_SCHEMA_VERSION) {\n    ReportError(error_reporter, \"Invalid model version %d\", model->version());\n    return false;\n  }\n  if (!VerifySubGraphs(*model, error_reporter)) {\n    return false;\n  }\n  if (!VerifyTensors(*model, error_reporter)) {\n    return false;\n  }\n  if (!VerifyOps(*model, resolver, error_reporter)) {\n    return false;\n  }\n  return true;\n}\n}  // namespace tflite"