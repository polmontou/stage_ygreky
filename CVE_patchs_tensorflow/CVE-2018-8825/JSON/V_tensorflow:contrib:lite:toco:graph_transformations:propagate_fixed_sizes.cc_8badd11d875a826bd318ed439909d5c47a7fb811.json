"/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#include <algorithm>\n#include <iterator>\n#include <memory>\n#include <string>\n#include <unordered_map>\n#include <vector>\n\n#include \"absl/strings/str_join.h\"\n#include \"tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h\"\n#include \"tensorflow/contrib/lite/toco/model.h\"\n#include \"tensorflow/contrib/lite/toco/tooling_util.h\"\n#include \"tensorflow/core/platform/logging.h\"\n\nnamespace toco {\n\nnamespace {\n\nvoid ComputeConvSizes(const Shape& input_shape, int output_depth, int kwidth,\n                      int kheight, int stride_width, int stride_height,\n                      int dilation_width_factor, int dilation_height_factor,\n                      PaddingType padding_type, Shape* output_shape,\n                      FixedPadding* fixed_padding) {\n  const int input_width = input_shape.dims(2);\n  const int input_height = input_shape.dims(1);\n  const int batch = input_shape.dims(0);\n\n  int dilated_kwidth = dilation_width_factor * (kwidth - 1) + 1;\n  int dilated_kheight = dilation_height_factor * (kheight - 1) + 1;\n\n  int output_height = 0;\n  int output_width = 0;\n  if (padding_type == PaddingType::kValid) {\n    output_height =\n        (input_height + stride_height - dilated_kheight) / stride_height;\n    output_width = (input_width + stride_width - dilated_kwidth) / stride_width;\n  } else if (padding_type == PaddingType::kSame) {\n    output_height = (input_height + stride_height - 1) / stride_height;\n    output_width = (input_width + stride_width - 1) / stride_width;\n  } else {\n    LOG(FATAL) << \"Only supporting SAME or VALID padding\";\n  }\n\n  fixed_padding->height = std::max(0, ((output_height - 1) * stride_height +\n                                       dilated_kheight - input_height) /\n                                          2);\n  fixed_padding->width = std::max(\n      0,\n      ((output_width - 1) * stride_width + dilated_kwidth - input_width) / 2);\n\n  // Actually had to debug a situation where those were negative due to bad\n  // propagation of placeholder -1 sizes in TensorFlowReshape.\n  CHECK_GT(output_width, 0);\n  CHECK_GT(output_height, 0);\n  output_shape->ReplaceDims({batch, output_height, output_width, output_depth});\n}\n\nvoid ComputeBinaryOperatorOutputSize(const Shape& input_shape_x,\n                                     const Shape& input_shape_y,\n                                     Array* output_array) {\n  // This matches the code in BroadcastBinaryOpShapeFn from tensorflow.\n  // It zips together the two input shapes and pads with 1 to make them the\n  // same length. For each dimension we broadcast if either dimension is 1 and\n  // otherwise expect them to match.\n  int rank_x = input_shape_x.dimensions_count();\n  int rank_y = input_shape_y.dimensions_count();\n  int rank_out = std::max(rank_x, rank_y);\n  std::vector<int>* dims_out = output_array->mutable_shape()->mutable_dims();\n  dims_out->clear();\n  dims_out->reserve(rank_out);\n  for (int i = 0; i < rank_out; ++i) {\n    int dim_x = i < (rank_out - rank_x)\n                    ? 1\n                    : input_shape_x.dims(i - (rank_out - rank_x));\n    bool dim_y_is_one = i < (rank_out - rank_y);\n    int dim_y = dim_y_is_one ? 1 : input_shape_y.dims(i - (rank_out - rank_y));\n    if (dim_x == -1 || dim_y == -1) {\n      // One or both dimensions is unknown.\n      QCHECK(false) << \"Shapes must be specified\";\n    } else if (dim_x == 1 || dim_y == 1) {\n      // Broadcast one dimension to the other that is 1.\n      if (dim_x == 1 && !dim_y_is_one) {\n        // Broadcast dim_y to dim_x (1).\n        dims_out->push_back(dim_y);\n      } else {\n        // Broadcast dim_x to dim_y (1).\n        DCHECK_EQ(dim_y, 1);\n        dims_out->push_back(dim_x);\n      }\n    } else {\n      // Expect the dimensions to match.\n      CHECK_EQ(dim_x, dim_y) << \"Dimensions must match\";\n      dims_out->push_back(dim_x);\n    }\n  }\n  CHECK(output_array->has_shape());\n}\n\nint GetOutputDepthFromWeights(const Model& model, const Operator& op) {\n  const string& weights_name = op.inputs[1];\n  const auto& weights_shape = model.GetArray(weights_name).shape();\n  if (op.type == OperatorType::kConv ||\n      op.type == OperatorType::kFullyConnected) {\n    return weights_shape.dims(0);\n  } else if (op.type == OperatorType::kDepthwiseConv) {\n    return weights_shape.dims(3);\n  } else {\n    LOG(FATAL) << \"Unhandled operator type\";\n  }\n}\n\nbool EnsureBiasVectorShape(Model* model, Operator* op) {\n  const string& weights_name = op->inputs[1];\n  const auto& weights_array = model->GetArray(weights_name);\n  // Yield until weights shape has been resolved.\n  if (!weights_array.has_shape()) {\n    return false;\n  }\n\n  if (op->inputs.size() < 3) {\n    return false;\n  }\n  auto& bias_array = model->GetArray(op->inputs[2]);\n  if (bias_array.has_shape()) {\n    return true;\n  }\n\n  const int output_depth = GetOutputDepthFromWeights(*model, *op);\n  bias_array.copy_shape(Shape({output_depth}));\n\n  auto& float_buffer = bias_array.GetMutableBuffer<ArrayDataType::kFloat>();\n  float_buffer.data.resize(output_depth, 0);\n\n  return true;\n}\n\nvoid ProcessConvOperator(Model* model, ConvOperator* op) {\n  if (!EnsureBiasVectorShape(model, op)) {\n    return;\n  }\n\n  const auto& input_array = model->GetArray(op->inputs[0]);\n  // Yield until input dims have been resolved.\n  if (!input_array.has_shape()) {\n    return;\n  }\n  const auto& input_shape = input_array.shape();\n  CHECK_EQ(input_shape.dimensions_count(), 4);\n\n  const auto& weights_array = model->GetArray(op->inputs[1]);\n  // Yield until weights dims have been resolved.\n  if (!weights_array.has_shape()) {\n    return;\n  }\n  const auto& weights_shape = weights_array.shape();\n  CHECK_EQ(weights_shape.dimensions_count(), 4);\n\n  auto& output_array = model->GetArray(op->outputs[0]);\n  const int output_depth = weights_shape.dims(0);\n  const int kheight = weights_shape.dims(1);\n  const int kwidth = weights_shape.dims(2);\n  ComputeConvSizes(input_shape, output_depth, kwidth, kheight, op->stride_width,\n                   op->stride_height, op->dilation_width_factor,\n                   op->dilation_height_factor, op->padding.type,\n                   output_array.mutable_shape(),\n                   &op->padding.GetOrCreateFixedPadding());\n  CHECK_EQ(output_array.shape().dimensions_count(), 4);\n\n  // Set im2col array dimensions if there is one.\n  if (op->outputs.size() == 2) {\n    const auto& output_shape = output_array.shape();\n    const int input_depth = weights_shape.dims(3);\n    auto& im2col_array = model->GetArray(op->outputs[1]);\n    im2col_array.copy_shape(Shape{output_shape.dims(0), output_shape.dims(1),\n                                  output_shape.dims(2),\n                                  input_depth * kheight * kwidth});\n  }\n}\n\nvoid ProcessTransposeConvOperator(Model* model, TransposeConvOperator* op) {\n  // TransposeConv is unique in that it is specifically given the output shape\n  // as a 1D array on it's 1st input. Theoretically then, resolving the output\n  // shape is as easy as waiting for this input to be resolved. However, we also\n  // have to calculate the padding which requires the weights shape. So, we\n  // might as well calculate the output shape and ensure it matches the\n  // specified one\n\n  // Check if we have already run.\n  auto& output_array = model->GetArray(op->outputs[0]);\n  if (output_array.has_shape()) {\n    return;\n  }\n\n  // SPECIFIED OUTPUT SHAPE\n  // The below is the specified, or prescribed output shape, _given_ to the\n  // operator as an input.\n  auto& specified_output_shape_array =\n      model->GetArray(op->inputs[TransposeConvOperator::OUTPUT_SHAPE]);\n  if (!specified_output_shape_array.has_shape() ||\n      !specified_output_shape_array.buffer) {\n    // Yield until the specified output shape is resolved as a constant\n    return;\n  }\n\n  CHECK(specified_output_shape_array.data_type == ArrayDataType::kInt32)\n      << \"TransposeConv input_dims must be int32\";\n\n  CHECK(specified_output_shape_array.shape().dimensions_count() == 1 &&\n        specified_output_shape_array.shape().dims(0) == 4)\n      << \"TransposeConv requires a 1D, 4 element array on it's 0th input \"\n         \"specifying the output shape. \\\"\"\n      << op->inputs[TransposeConvOperator::OUTPUT_SHAPE] << \"\\\" had shape \"\n      << toco::ShapeToString(specified_output_shape_array.shape());\n\n  // COMPUTE PADDING\n  // We require the weights shape to calculate padding.\n  const auto& weights_array =\n      model->GetArray(op->inputs[TransposeConvOperator::WEIGHTS]);\n  if (!weights_array.has_shape()) {\n    // Yield until weights dims have been resolved.\n    return;\n  }\n  const auto& weights_shape = weights_array.shape();\n  CHECK_EQ(weights_shape.dimensions_count(), 4)\n      << \"TransposeConv weights must have 4 input dimensions. Input weights \\\"\"\n      << op->inputs[TransposeConvOperator::WEIGHTS] << \"\\\" had shape \"\n      << toco::ShapeToString(weights_shape) << \".\";\n\n  CHECK(weights_shape.dims(0) == 1 && weights_shape.dims(3) == 1)\n      << \"TransposeConv weights dimensions must begin and end with 1. Input \"\n         \"weights \\\"\"\n      << op->inputs[TransposeConvOperator::WEIGHTS] << \"\\\" had shape \"\n      << toco::ShapeToString(weights_shape) << \".\";\n\n  // Compute padding\n  const int kheight = weights_shape.dims(1);\n  const int kwidth = weights_shape.dims(2);\n  op->padding.GetOrCreateFixedPadding();\n  if (op->padding.type == PaddingType::kValid) {\n    op->padding.fixed->height = 0;\n    op->padding.fixed->width = 0;\n  } else if (op->padding.type == PaddingType::kSame) {\n    op->padding.fixed->height = (kheight - 1) / 2;\n    op->padding.fixed->width = (kwidth - 1) / 2;\n  } else {\n    LOG(FATAL) << \"TransposeConv only supports SAME or VALID padding\";\n  }\n\n  // VALIDATE OUTPUT SHAPE\n  // Compute the output shape from the input and weights shapes to verify it\n  // agrees with the specified output shape.\n  const auto& input_array =\n      model->GetArray(op->inputs[TransposeConvOperator::DATA_INPUT]);\n  if (!input_array.has_shape()) {\n    // Yield until input dims have been resolved.\n    return;\n  }\n  const auto& input_shape = input_array.shape();\n  CHECK_EQ(input_shape.dimensions_count(), 4)\n      << \"TransposeConv input shape must have 4 dimensions. Input \\\"\"\n      << op->inputs[TransposeConvOperator::WEIGHTS] << \"\\\" had shape \"\n      << toco::ShapeToString(weights_shape) << \".\";\n\n  // Compute output shape\n  const int input_width = input_shape.dims(2);\n  const int input_height = input_shape.dims(1);\n  int output_height = op->stride_height * (input_height - 1);\n  int output_width = op->stride_width * (input_width - 1);\n  if (op->padding.type == PaddingType::kValid) {\n    output_height += kheight;\n    output_width += kwidth;\n  } else if (op->padding.type == PaddingType::kSame) {\n    output_height += 1;\n    output_width += 1;\n  }\n\n  CHECK(specified_output_shape_array.GetBuffer<ArrayDataType::kInt32>().data ==\n        std::vector<int32>({input_shape.dims(0), output_height, output_width,\n                            weights_shape.dims(3)}))\n      << \"Specified output shape: \" << ShapeToString(output_array.shape())\n      << \", does not agree with shape computed from input data and weights: [\"\n      << input_shape.dims(0) << \", \" << output_height << \", \" << output_width\n      << \", \" << weights_shape.dims(3) << \"].\";\n\n  // SUCCESS: Set the op's output shape according to the specified output shape.\n  *(output_array.mutable_shape()->mutable_dims()) =\n      specified_output_shape_array.GetBuffer<ArrayDataType::kInt32>().data;\n}\n\nvoid ProcessDepthwiseConvOperator(Model* model, DepthwiseConvOperator* op) {\n  if (!EnsureBiasVectorShape(model, op)) {\n    return;\n  }\n\n  const auto& input_array = model->GetArray(op->inputs[0]);\n  // Yield until input dims have been resolved.\n  if (!input_array.has_shape()) {\n    return;\n  }\n  const auto& input_shape = input_array.shape();\n  CHECK_EQ(input_shape.dimensions_count(), 4);\n\n  const auto& weights_array = model->GetArray(op->inputs[1]);\n  // Yield until weights dims have been resolved.\n  if (!weights_array.has_shape()) {\n    return;\n  }\n  const auto& weights_shape = weights_array.shape();\n  CHECK_EQ(weights_shape.dimensions_count(), 4);\n\n  const string& output_name = op->outputs[0];\n  const int input_depth = input_shape.dims(3);\n  const int output_depth = weights_shape.dims(3);\n  // TensorFlow doesn't define the depth_multiplier value on DepthwiseConv ops,\n  // instead it has to be inferred from the weights dims. However, once we are\n  // here, weights dims have already been converted to our own internal format,\n  // where the multiplier is no longer readily apparent. So instead we get it\n  // as the quotient of output and input depths. We only want to do that when\n  // depth_multiplier had the zero value: any other value should be checked\n  // as done by the next if() below.\n  if (!op->depth_multiplier) {\n    op->depth_multiplier = output_depth / input_depth;\n  }\n  QCHECK_EQ(output_depth, input_depth * op->depth_multiplier)\n      << \"input/output depths and depth_multiplier don't match\";\n\n  const int kheight = weights_shape.dims(1);\n  const int kwidth = weights_shape.dims(2);\n  ComputeConvSizes(input_shape, output_depth, kwidth, kheight, op->stride_width,\n                   op->stride_height, 1, 1, op->padding.type,\n                   model->GetArray(output_name).mutable_shape(),\n                   &op->padding.GetOrCreateFixedPadding());\n}\n\nvoid ProcessDepthToSpaceOperator(Model* model, DepthToSpaceOperator* op) {\n  const auto& input_array = model->GetArray(op->inputs[0]);\n  // Yield until input dims have been resolved.\n  if (!input_array.has_shape()) {\n    return;\n  }\n  const auto& input_shape = input_array.shape();\n  CHECK_EQ(input_shape.dimensions_count(), 4);\n\n  const string& output_name = op->outputs[0];\n  const int block_size = op->block_size;\n  CHECK_NE(block_size, 0) << \"Invalid block_size in \" << output_name;\n  const int batch = input_shape.dims(0);\n  const int height = input_shape.dims(1);\n  const int width = input_shape.dims(2);\n  const int depth = input_shape.dims(3);\n  QCHECK_EQ(depth % (block_size * block_size), 0);\n\n  model->GetArray(output_name)\n      .copy_shape(Shape({batch, height * block_size, width * block_size,\n                         depth / block_size / block_size}));\n}\n\nvoid ProcessSpaceToDepthOperator(Model* model, SpaceToDepthOperator* op) {\n  const auto& input_array = model->GetArray(op->inputs[0]);\n  // Yield until input dims have been resolved.\n  if (!input_array.has_shape()) {\n    return;\n  }\n  const auto& input_shape = input_array.shape();\n  CHECK_EQ(input_shape.dimensions_count(), 4);\n\n  const string& output_name = op->outputs[0];\n  const int block_size = op->block_size;\n  CHECK_NE(block_size, 0) << \"Invalid block_size in \" << output_name;\n  const int batch = input_shape.dims(0);\n  const int height = input_shape.dims(1);\n  const int width = input_shape.dims(2);\n  const int depth = input_shape.dims(3);\n  QCHECK_EQ(width % block_size, 0);\n  QCHECK_EQ(height % block_size, 0);\n\n  model->GetArray(output_name)\n      .copy_shape(Shape({batch, height / block_size, width / block_size,\n                         depth * block_size * block_size}));\n}\n\nvoid ProcessOpWithShapeInput(Model* model, Operator* op) {\n  CHECK_EQ(op->outputs.size(), 1);\n  auto& output_array = model->GetArray(op->outputs[0]);\n  if (output_array.has_shape()) {\n    // We have already run\n    return;\n  }\n\n  auto& dims_array = model->GetArray(op->inputs[0]);\n  if (!dims_array.has_shape()) {\n    // Yield until dims shape been resolved.\n    return;\n  }\n  if (!dims_array.buffer) {\n    // Yield until the dims are constant\n    return;\n  }\n  CHECK(dims_array.data_type == ArrayDataType::kInt32) << \"dims must be int32\";\n  CHECK_LE(RequiredBufferSizeForShape(dims_array.shape()), 4)\n      << \"dims vector can be no larger than 4 values\";\n\n  std::vector<int32> const& dims =\n      dims_array.GetBuffer<ArrayDataType::kInt32>().data;\n  *(output_array.mutable_shape()->mutable_dims()) = dims;\n}\n\nvoid ProcessFullyConnectedOperator(Model* model, FullyConnectedOperator* op) {\n  if (!EnsureBiasVectorShape(model, op)) {\n    return;\n  }\n\n  const auto& input_array = model->GetArray(op->inputs[0]);\n  // Yield until input dims have been resolved.\n  if (!input_array.has_shape()) {\n    return;\n  }\n  const auto& input_shape = input_array.shape();\n  CHECK_GE(input_shape.dimensions_count(), 1);\n\n  const auto& weights_array = model->GetArray(op->inputs[1]);\n  // Yield until weights dims have been resolved.\n  if (!weights_array.has_shape()) {\n    return;\n  }\n  const auto& weights_shape = weights_array.shape();\n\n  const int weights_output_depth = weights_shape.dims(0);\n  CHECK_EQ(weights_shape.dimensions_count(), 2);\n\n  const int input_overall_size = RequiredBufferSizeForShape(input_shape);\n  const int matmul_repeats = input_overall_size / weights_shape.dims(1);\n  CHECK_EQ(matmul_repeats * weights_shape.dims(1), input_overall_size);\n\n  auto& output_array = model->GetArray(op->outputs[0]);\n  output_array.copy_shape(Shape({matmul_repeats, weights_output_depth}));\n}\n\nvoid ProcessTensorFlowReshapeOperator(Model* model,\n                                      TensorFlowReshapeOperator* op) {\n  auto& output_array = model->GetArray(op->outputs[0]);\n  if (output_array.has_shape()) {\n    // We have already run\n    return;\n  }\n\n  const auto& input_array = model->GetArray(op->inputs[0]);\n  if (!input_array.has_shape()) {\n    // Yield until input dims have been resolved.\n    return;\n  }\n  const auto& input_shape = input_array.shape();\n\n  auto& shape_array = model->GetArray(op->inputs[1]);\n  if (!shape_array.has_shape()) {\n    // Yield until target_shape shape been resolved.\n    return;\n  }\n  if (!shape_array.buffer) {\n    // Yield until the target_shape is constant\n    return;\n  }\n  CHECK(shape_array.data_type == ArrayDataType::kInt32)\n      << \"Reshape dims must be int32\";\n\n  // shape_data is the raw array of ints describing the shape\n  // in the TensorFlow node. We intentionally make a copy here, rather than\n  // modify wildcards in-place below, because in some graphs, the same shape\n  // array with a wildcard may be referenced from multiple Reshape nodes, where\n  // the wildcard needs to resolved to distinct values.\n  std::vector<int32> shape_data =\n      shape_array.GetBuffer<ArrayDataType::kInt32>().data;\n  // The Reshape shape may have a wildcard dim, encoded as -1.\n  bool has_wildcard = false;\n  int wildcard_index = 0;\n  int product_non_wildcard_dims = 1;\n  for (int i = 0; i < shape_data.size(); i++) {\n    if (shape_data[i] == -1) {\n      CHECK(!has_wildcard);\n      has_wildcard = true;\n      wildcard_index = i;\n    } else {\n      product_non_wildcard_dims *= shape_data[i];\n    }\n  }\n  const int input_flat_size = RequiredBufferSizeForShape(input_shape);\n  if (has_wildcard) {\n    CHECK_GE(input_flat_size, product_non_wildcard_dims)\n        << \"Array not large enough to fill the requested dimensions for \"\n           \"Reshape op with output \\\"\"\n        << op->outputs[0] << \"\\\". Are your input shapes correct?\";\n    shape_data[wildcard_index] = input_flat_size / product_non_wildcard_dims;\n  }\n  auto& output_shape = *output_array.mutable_shape();\n  *output_shape.mutable_dims() = shape_data;\n  CHECK_EQ(input_flat_size, RequiredBufferSizeForShape(output_shape))\n      << \"Input cannot be reshaped to requested dimensions for Reshape op with \"\n         \"output \\\"\"\n      << op->outputs[0] << \"\\\". Are your input shapes correct?\";\n}\n\nvoid ProcessSimpleOperator(Model* model, Operator* op) {\n  const auto& input_array = model->GetArray(op->inputs[0]);\n  // Yield until input dims have been resolved.\n  if (!input_array.has_shape()) {\n    return;\n  }\n\n  const string& output_name = op->outputs[0];\n  auto& output_array = model->GetArray(output_name);\n  if (output_array.has_shape()) {\n    return;\n  }\n\n  output_array.copy_shape(input_array.shape());\n}\n\nvoid ProcessSimpleBinaryOperator(Model* model, Operator* op) {\n  CHECK_EQ(op->inputs.size(), 2);\n  const auto& input0_array = model->GetArray(op->inputs[0]);\n  const auto& input1_array = model->GetArray(op->inputs[1]);\n  // Yield until input dims have been resolved.\n  if (!input0_array.has_shape() || !input1_array.has_shape()) {\n    return;\n  }\n  const string& output_name = op->outputs[0];\n  auto& output_array = model->GetArray(output_name);\n  ComputeBinaryOperatorOutputSize(input0_array.shape(), input1_array.shape(),\n                                  &output_array);\n}\n\nvoid ProcessAddNOperator(Model* model, Operator* op) {\n  // Yield until all input dims have been resolved.\n  //\n  // TODO(myenik): Since AddN does not support broadcasting, maybe we could\n  // actually use this to improve shape propagation by propagating the shape of\n  // one input to all other inputs once it is resolved instead of just the\n  // output, since all inputs must be the same size and shape for a well-formed\n  // graph.\n  for (const auto& input : op->inputs) {\n    const auto& input_array = model->GetArray(input);\n    if (!input_array.has_shape()) {\n      return;\n    }\n  }\n\n  // AddN does not support broadcasting, all inputs must be the same shape, so\n  // we just take the first input shape and apply it to the output.\n  const auto& input0_array = model->GetArray(op->inputs[0]);\n  auto& output_array = model->GetArray(op->outputs[0]);\n  output_array.copy_shape(input0_array.shape());\n}\n\nbool KeepDims(const Operator& op) {\n  switch (op.type) {\n    case OperatorType::kTensorFlowMin:\n      return static_cast<const TensorFlowMinOperator&>(op).keep_dims;\n    case OperatorType::kTensorFlowMax:\n      return static_cast<const TensorFlowMaxOperator&>(op).keep_dims;\n    case OperatorType::kTensorFlowSum:\n      return static_cast<const TensorFlowSumOperator&>(op).keep_dims;\n    case OperatorType::kMean:\n      return static_cast<const MeanOperator&>(op).keep_dims;\n    default:\n      LOG(FATAL) << \"Not a reduction operator!\";\n      return false;\n  }\n}\n\nvoid ProcessTensorFlowReductionOperator(Model* model, Operator* op) {\n  CHECK_LE(op->inputs.size(), 2);\n  auto& output_array = model->GetArray(op->outputs[0]);\n  if (output_array.has_shape()) {\n    return;\n  }\n  const auto& input_array = model->GetArray(op->inputs[0]);\n  if (!input_array.has_shape()) {\n    return;\n  }\n  const auto& input_shape = input_array.shape();\n  const bool keep_dims = KeepDims(*op);\n  if (op->inputs.size() == 2) {\n    // There is a reduction_indices input.\n    const auto& reduction_array = model->GetArray(op->inputs[1]);\n    if (!reduction_array.buffer) {\n      return;\n    }\n    CHECK(reduction_array.buffer->type == ArrayDataType::kInt32);\n    const auto& reduction_array_vals =\n        reduction_array.GetBuffer<ArrayDataType::kInt32>().data;\n    auto& output_dims = *output_array.mutable_shape()->mutable_dims();\n    output_dims.clear();\n    for (int i = 0; i < input_shape.dimensions_count(); i++) {\n      bool is_reduction_dim = false;\n      for (int r : reduction_array_vals) {\n        if (i == r) {\n          is_reduction_dim = true;\n        }\n      }\n      if (!is_reduction_dim) {\n        output_dims.push_back(input_shape.dims(i));\n      } else if (keep_dims) {\n        output_dims.push_back(1);\n      }\n    }\n  } else {\n    // No reduction_indices means complete reduction to a single scalar.\n    if (keep_dims) {\n      output_array.copy_shape(input_shape);\n    } else {\n      output_array.copy_shape(Shape({}));\n    }\n  }\n}\n\nvoid ProcessSliceOperator(Model* model, SliceOperator* op) {\n  CHECK_EQ(op->inputs.size(), 3);\n  CHECK_EQ(op->outputs.size(), 1);\n\n  // Yield until the Slice params have been resolved.\n  if (op->begin.empty()) return;\n\n  // Yield until input dims have been resolved.\n  const auto& input_array = model->GetArray(op->inputs[0]);\n  if (!input_array.has_shape()) return;\n  const Shape& input_shape = input_array.shape();\n\n  auto& output_array = model->GetArray(op->outputs[0]);\n  if (output_array.has_shape()) return;\n\n  CHECK_EQ(input_shape.dims().size(), op->size.size());\n  CHECK_EQ(op->begin.size(), op->size.size());\n\n  std::vector<int> output_dims;\n  for (int i = 0; i < op->begin.size(); ++i) {\n    int size = op->size[i];\n    if (size == -1) {\n      size = input_array.shape().dims(i) - op->begin[i];\n    }\n    output_dims.push_back(size);\n  }\n\n  *output_array.mutable_shape()->mutable_dims() = output_dims;\n}\n\nvoid ProcessReorderAxesOperator(Model* model, ReorderAxesOperator* op) {\n  const string& input_name = op->inputs[0];\n  const auto& input_array = model->GetArray(input_name);\n  // Yield until input dims have been resolved.\n  if (!input_array.has_shape()) {\n    return;\n  }\n  const auto& input_shape = input_array.shape();\n  const string& output_name = op->outputs[0];\n  Shape* output_shape = model->GetArray(output_name).mutable_shape();\n  ShuffleDims(input_shape, op->input_axes_order, op->output_axes_order,\n              output_shape);\n}\n\nvoid ProcessConcatenationOperator(Model* model, ConcatenationOperator* op) {\n  // Yield until input dims have been resolved.\n  for (const auto& input_name : op->inputs) {\n    auto& input_array = model->GetArray(input_name);\n    if (!input_array.has_shape()) {\n      return;\n    }\n  }\n  auto& output_array = model->GetArray(op->outputs[0]);\n  // Use 0 input as basis for output dimensions.\n  const auto& first_input_array = model->GetArray(op->inputs[0]);\n  output_array.copy_shape(first_input_array.shape());\n  // Negative axis means the count starts at the back of the dims().\n  int axis = op->axis;\n  if (axis < 0) axis += first_input_array.shape().dims().size();\n  // Determine the concat size, and enfore that all inputs have\n  // the same dimensions count.\n  int concat_size = 0;\n  for (const auto& input_name : op->inputs) {\n    auto& input_array = model->GetArray(input_name);\n    CHECK(input_array.has_shape());\n    if (input_array.shape().dimensions_count() == 0) {\n      continue;\n    }\n    CHECK_EQ(input_array.shape().dimensions_count(),\n             output_array.shape().dimensions_count());\n    const std::vector<int>& input_dims = input_array.shape().dims();\n    CHECK_LT(axis, input_dims.size());\n    concat_size += input_dims[axis];\n  }\n  // Write out the concat_size on the output array shape.\n  auto& output_shape = *output_array.mutable_shape();\n  auto& output_dims = *output_shape.mutable_dims();\n  CHECK_LT(axis, output_shape.dimensions_count());\n  output_dims[axis] = concat_size;\n}\n\nvoid ProcessRangeOperator(Model* model, RangeOperator* op) {\n  CHECK_EQ(op->inputs.size(), 3);\n  const auto& start_array = model->GetArray(op->inputs[0]);\n  if (!start_array.has_shape()) {\n    // Yield until input dims have been resolved.\n    return;\n  }\n  const auto& limit_array = model->GetArray(op->inputs[1]);\n  if (!limit_array.has_shape()) {\n    return;\n  }\n  const auto& delta_array = model->GetArray(op->inputs[2]);\n  if (!delta_array.has_shape()) {\n    return;\n  }\n\n  if (!IsConstantParameterArray(*model, op->inputs[0])) {\n    // Yield until inputs are constant.\n    return;\n  }\n  if (!IsConstantParameterArray(*model, op->inputs[1])) {\n    return;\n  }\n  if (!IsConstantParameterArray(*model, op->inputs[2])) {\n    return;\n  }\n\n  CHECK(start_array.data_type == ArrayDataType::kInt32)\n      << \"Range op inputs must be int32.\";\n  CHECK(limit_array.data_type == ArrayDataType::kInt32)\n      << \"Range op inputs must be int32.\";\n  CHECK(delta_array.data_type == ArrayDataType::kInt32)\n      << \"Range op inputs must be int32.\";\n  CHECK_EQ(RequiredBufferSizeForShape(start_array.shape()), 1)\n      << \"Range op inputs must be scalar.\";\n  CHECK_EQ(RequiredBufferSizeForShape(limit_array.shape()), 1)\n      << \"Range op inputs must be scalar.\";\n  CHECK_EQ(RequiredBufferSizeForShape(delta_array.shape()), 1)\n      << \"Range op inputs must be scalar.\";\n  int size = floor((limit_array.GetBuffer<ArrayDataType::kInt32>().data[0] -\n                    start_array.GetBuffer<ArrayDataType::kInt32>().data[0]) /\n                   delta_array.GetBuffer<ArrayDataType::kInt32>().data[0]);\n\n  // Only set the output shape. Contents are set by ResolveConstantRange.\n  CHECK_EQ(op->outputs.size(), 1);\n  auto& output_array = model->GetArray(op->outputs[0]);\n  Shape* output_shape = output_array.mutable_shape();\n  output_shape->ReplaceDims({size});\n}\n\nvoid ProcessTensorFlowSplitOperator(Model* model, TensorFlowSplitOperator* op) {\n  CHECK_EQ(op->inputs.size(), 2);\n  const string& input_name = op->inputs[1];\n  const auto& input_array = model->GetArray(input_name);\n  // Yield until input dims have been resolved.\n  if (!input_array.has_shape()) {\n    return;\n  }\n  const Shape& input_shape = input_array.shape();\n\n  // Yield until axis is constant.\n  if (!IsConstantParameterArray(*model, op->inputs[0])) {\n    return;\n  }\n\n  const auto& axis_array = model->GetArray(op->inputs[0]);\n\n  // Yield until axis dims have been resolved.\n  if (!axis_array.has_shape()) {\n    return;\n  }\n\n  CHECK(axis_array.data_type == ArrayDataType::kInt32)\n      << \"Axis array must be int32.\";\n  CHECK_EQ(RequiredBufferSizeForShape(axis_array.shape()), 1)\n      << \"Axis array must be scalar.\";\n\n  int axis = axis_array.GetBuffer<ArrayDataType::kInt32>().data[0];\n  if (axis < 0) {\n    axis += input_shape.dimensions_count();\n  }\n\n  const int split_dim = input_shape.dims(axis);\n  CHECK_EQ(split_dim % op->num_split, 0);\n  const int split_depth = split_dim / op->num_split;\n\n  Shape output_shape = input_shape;\n  (*output_shape.mutable_dims())[axis] = split_depth;\n\n  CHECK_EQ(op->outputs.size(), op->num_split);\n  for (const auto& output : op->outputs) {\n    model->GetArray(output).copy_shape(output_shape);\n  }\n}\n\nvoid ProcessAveragePoolOperator(Model* model, AveragePoolOperator* op) {\n  const string& input_name = op->inputs[0];\n  const auto& input_array = model->GetArray(input_name);\n  // Yield until input dims have been resolved.\n  if (!input_array.has_shape()) {\n    return;\n  }\n  const auto& input_shape = input_array.shape();\n  CHECK_EQ(input_shape.dimensions_count(), 4);\n  const string& output_name = op->outputs[0];\n  const int output_depth = input_shape.dims(3);\n  ComputeConvSizes(input_shape, output_depth, op->kwidth, op->kheight,\n                   op->stride_width, op->stride_height, 1, 1, op->padding.type,\n                   model->GetArray(output_name).mutable_shape(),\n                   &op->padding.GetOrCreateFixedPadding());\n}\n\nvoid ProcessMaxPoolOperator(Model* model, MaxPoolOperator* op) {\n  const string& input_name = op->inputs[0];\n  const auto& input_array = model->GetArray(input_name);\n  // Yield until input dims have been resolved.\n  if (!input_array.has_shape()) {\n    return;\n  }\n  const auto& input_shape = input_array.shape();\n  CHECK_EQ(input_shape.dimensions_count(), 4);\n  const string& output_name = op->outputs[0];\n  const int output_depth = input_shape.dims(3);\n  ComputeConvSizes(input_shape, output_depth, op->kwidth, op->kheight,\n                   op->stride_width, op->stride_height, 1, 1, op->padding.type,\n                   model->GetArray(output_name).mutable_shape(),\n                   &op->padding.GetOrCreateFixedPadding());\n}\n\nvoid ProcessL2PoolOperator(Model* model, L2PoolOperator* op) {\n  const string& input_name = op->inputs[0];\n  const auto& input_array = model->GetArray(input_name);\n  // Yield until input dims have been resolved.\n  if (!input_array.has_shape()) {\n    return;\n  }\n  const auto& input_shape = input_array.shape();\n  if (input_shape.dimensions_count() < 4) {\n    LOG(FATAL) << \"missing dimensions for \" << input_name;\n  }\n  const string& output_name = op->outputs[0];\n  const int output_depth = input_shape.dims(3);\n  ComputeConvSizes(input_shape, output_depth, op->kwidth, op->kheight,\n                   op->stride_width, op->stride_height, 1, 1, op->padding.type,\n                   model->GetArray(output_name).mutable_shape(),\n                   &op->padding.GetOrCreateFixedPadding());\n}\n\nvoid ProcessResizeBilinearOperator(Model* model, ResizeBilinearOperator* op) {\n  CHECK_EQ(op->inputs.size(), 2);\n  CHECK_EQ(op->outputs.size(), 1);\n\n  if (!model->GetArray(op->inputs[0]).has_shape() ||\n      !model->GetArray(op->inputs[1]).has_shape()) {\n    return;\n  }\n  const auto& input_data_shape = model->GetArray(op->inputs[0]).shape();\n\n  const string& output_size_name = op->inputs[1];\n  const auto& output_size_array = model->GetArray(output_size_name);\n  CHECK(output_size_array.data_type == ArrayDataType::kInt32);\n  CHECK(output_size_array.has_shape());\n  const auto& output_size_shape = output_size_array.shape();\n  CHECK_EQ(output_size_shape.dimensions_count(), 1);\n  CHECK_EQ(output_size_shape.dims(0), 2);\n  if (!output_size_array.buffer) {\n    return;\n  }\n  std::vector<int32> output_shape =\n      output_size_array.GetBuffer<ArrayDataType::kInt32>().data;\n  model->GetArray(op->outputs[0])\n      .copy_shape(Shape({input_data_shape.dims(0), output_shape[0],\n                         output_shape[1], input_data_shape.dims(3)}));\n}\n\nvoid ProcessLstmCellOperator(Model* model, LstmCellOperator* op) {\n  // Only required for compact LstmCell with default NUM_INPUTS of inputs.\n  if (op->inputs.size() != LstmCellOperator::NUM_INPUTS) return;\n\n  const auto& input_array =\n      model->GetArray(op->inputs[LstmCellOperator::DATA_INPUT]);\n  // Yield until all input dims have been resolved.\n  if (!input_array.has_shape()) {\n    return;\n  }\n  const auto& input_shape = input_array.shape();\n  CHECK_GE(input_shape.dimensions_count(), 2);\n\n  const auto& prev_activ_array =\n      model->GetArray(op->inputs[LstmCellOperator::PREV_ACTIV_INPUT]);\n  // Yield until all input dims have been resolved.\n  if (!prev_activ_array.has_shape()) {\n    return;\n  }\n  const auto& prev_activ_shape = prev_activ_array.shape();\n  CHECK_GE(prev_activ_shape.dimensions_count(), 2);\n\n  const auto& weights_array =\n      model->GetArray(op->inputs[LstmCellOperator::WEIGHTS_INPUT]);\n  // Yield until weights dims have been resolved.\n  if (!weights_array.has_shape()) {\n    return;\n  }\n  const auto& weights_shape = weights_array.shape();\n  CHECK_EQ(weights_shape.dimensions_count(), 2);\n\n  const auto& bias_array =\n      model->GetArray(op->inputs[LstmCellOperator::BIASES_INPUT]);\n  // Yield until bias dims have been resolved.\n  if (!bias_array.has_shape()) {\n    return;\n  }\n  const auto& bias_shape = bias_array.shape();\n  CHECK_GE(bias_shape.dimensions_count(), 1);\n\n  const auto& prev_state_array =\n      model->GetArray(op->inputs[LstmCellOperator::PREV_STATE_INPUT]);\n  // Yield until all input dims have been resolved.\n  if (!prev_state_array.has_shape()) {\n    return;\n  }\n  const auto& prev_state_shape = prev_state_array.shape();\n  CHECK_GE(prev_state_shape.dimensions_count(), 2);\n\n  const int fc_output_depth = weights_shape.dims(0);\n  CHECK_EQ(fc_output_depth, bias_shape.dims(0));\n  CHECK_EQ(fc_output_depth % 4, 0);\n  const int depth = fc_output_depth / 4;\n\n  const int input_depth = input_shape.dims(input_shape.dimensions_count() - 1);\n  const int fc_input_depth = weights_shape.dims(1);\n  CHECK_EQ(input_depth + depth, fc_input_depth);\n  Shape output_shape(input_shape);\n  (*output_shape.mutable_dims())[output_shape.dimensions_count() - 1] = depth;\n\n  // Set output dimensions\n  model->GetArray(op->outputs[LstmCellOperator::STATE_OUTPUT])\n      .copy_shape(output_shape);\n  model->GetArray(op->outputs[LstmCellOperator::ACTIV_OUTPUT])\n      .copy_shape(output_shape);\n\n  Shape concat_temp_shape(input_shape);\n  (*concat_temp_shape\n        .mutable_dims())[concat_temp_shape.dimensions_count() - 1] =\n      fc_input_depth;\n  model->GetArray(op->outputs[LstmCellOperator::CONCAT_TEMP])\n      .copy_shape(concat_temp_shape);\n\n  Shape activ_temp_shape(input_shape);\n  (*activ_temp_shape.mutable_dims())[activ_temp_shape.dimensions_count() - 1] =\n      fc_output_depth;\n  model->GetArray(op->outputs[LstmCellOperator::ACTIV_TEMP])\n      .copy_shape(activ_temp_shape);\n}\n\nvoid ProcessSpaceToBatchNDOperator(Model* model, SpaceToBatchNDOperator* op) {\n  const auto& input_array = model->GetArray(op->inputs[0]);\n  // Yield until input dims have been resolved.\n  if (!input_array.has_shape()) {\n    return;\n  }\n  const auto& input_shape = input_array.shape();\n  // This method only handles input dimensions of 4.\n  if (input_shape.dimensions_count() != 4) {\n    return;\n  }\n  const auto input_height = input_shape.dims(1);\n  const auto input_width = input_shape.dims(2);\n\n  const auto& block_shape_array = model->GetArray(op->inputs[1]);\n  const auto& paddings_array = model->GetArray(op->inputs[2]);\n  const auto& block_shape_array_shape = block_shape_array.shape();\n  const auto& paddings_array_shape = paddings_array.shape();\n  QCHECK_EQ(block_shape_array_shape.dimensions_count(), 1);\n  QCHECK_EQ(paddings_array_shape.dimensions_count(), 2);\n\n  // We only support two dimensions.\n  QCHECK_EQ(block_shape_array_shape.dims(0), 2);\n  if (!block_shape_array.buffer) {\n    return;\n  }\n  QCHECK(block_shape_array.data_type == ArrayDataType::kInt32);\n  const auto& block_shape_data =\n      block_shape_array.GetBuffer<ArrayDataType::kInt32>().data;\n  auto block_height = block_shape_data[0];\n  auto block_width = block_shape_data[1];\n\n  QCHECK_EQ(paddings_array_shape.dims(0), 2);  // Number of block dimensions\n  QCHECK_EQ(paddings_array_shape.dims(1), 2);  // Two parameters per dimension.\n  if (!paddings_array.buffer) {\n    return;\n  }\n  QCHECK(paddings_array.data_type == ArrayDataType::kInt32);\n  const auto& paddings_data =\n      paddings_array.GetBuffer<ArrayDataType::kInt32>().data;\n  int height_with_paddings = input_height + paddings_data[0] + paddings_data[1];\n  int width_with_paddings = input_width + paddings_data[2] + paddings_data[3];\n  QCHECK_EQ(height_with_paddings % block_height, 0);\n  QCHECK_EQ(width_with_paddings % block_width, 0);\n  int output_height = height_with_paddings / block_height;\n  int output_width = width_with_paddings / block_width;\n\n  model->GetArray(op->outputs[0])\n      .copy_shape(Shape({input_shape.dims(0) * block_height * block_width,\n                         output_height, output_width, input_shape.dims(3)}));\n}\n\nvoid ProcessBatchToSpaceNDOperator(Model* model, BatchToSpaceNDOperator* op) {\n  const auto& input_array = model->GetArray(op->inputs[0]);\n  // Yield until input dims have been resolved.\n  if (!input_array.has_shape()) {\n    return;\n  }\n  const auto& input_shape = input_array.shape();\n  CHECK_EQ(input_shape.dimensions_count(), 4);\n  const auto input_height = input_shape.dims(1);\n  const auto input_width = input_shape.dims(2);\n\n  const auto& block_shape_array = model->GetArray(op->inputs[1]);\n  const auto& crops_array = model->GetArray(op->inputs[2]);\n  const auto& block_shape_array_shape = block_shape_array.shape();\n  const auto& crops_array_shape = crops_array.shape();\n  QCHECK_EQ(block_shape_array_shape.dimensions_count(), 1);\n  QCHECK_EQ(crops_array_shape.dimensions_count(), 2);\n\n  // We only support two dimensions.\n  QCHECK_EQ(block_shape_array_shape.dims(0), 2);\n  if (!block_shape_array.buffer) {\n    return;\n  }\n  QCHECK(block_shape_array.data_type == ArrayDataType::kInt32);\n  const auto& block_shape_data =\n      block_shape_array.GetBuffer<ArrayDataType::kInt32>().data;\n  auto block_height = block_shape_data[0];\n  auto block_width = block_shape_data[1];\n\n  QCHECK_EQ(crops_array_shape.dims(0), 2);  // Number of block dimensions\n  QCHECK_EQ(crops_array_shape.dims(1), 2);  // Two parameters per dimension.\n  if (!crops_array.buffer) {\n    return;\n  }\n  QCHECK(crops_array.data_type == ArrayDataType::kInt32);\n  const auto& crops_data = crops_array.GetBuffer<ArrayDataType::kInt32>().data;\n  // We don't support crops now.\n  QCHECK_EQ(crops_data[0], 0);\n  QCHECK_EQ(crops_data[1], 0);\n  QCHECK_EQ(crops_data[2], 0);\n  QCHECK_EQ(crops_data[3], 0);\n\n  QCHECK_EQ(input_shape.dims(0) % (block_height * block_width), 0);\n\n  int output_height = input_height * block_height;\n  int output_width = input_width * block_width;\n\n  model->GetArray(op->outputs[0])\n      .copy_shape(Shape({input_shape.dims(0) / (block_height * block_width),\n                         output_height, output_width, input_shape.dims(3)}));\n}\n\nvoid ProcessGatherOperator(Model* model, GatherOperator* op) {\n  const auto& input_array = model->GetArray(op->inputs[0]);\n  const auto& indices_array = model->GetArray(op->inputs[1]);\n  auto& output_array = model->GetArray(op->outputs[0]);\n\n  // Bail if we already know the output shape.\n  if (output_array.has_shape()) {\n    return;\n  }\n\n  // Yield until input dims have been resolved.\n  if (!input_array.has_shape() || !indices_array.has_shape()) {\n    return;\n  }\n\n  const auto& input_shape = input_array.shape();\n  const auto& indices_shape = indices_array.shape();\n  QCHECK_GE(input_shape.dimensions_count(), 1);\n  op->input_rank = input_shape.dimensions_count();\n\n  // We only support 1-D indices.\n  QCHECK_EQ(indices_shape.dimensions_count(), 1);\n\n  // Copy the input dimensions to the output except for dimension 0,\n  // where the dimension of indices_shape is used.\n  // TODO(mgubin): if axis != 0 this is not true, change when it's supported.\n  auto output_dims = output_array.mutable_shape()->mutable_dims();\n  output_dims->push_back(indices_shape.dims(0));\n  for (int dim = 1; dim < input_shape.dimensions_count(); dim++) {\n    output_dims->push_back(input_shape.dims(dim));\n  }\n}\n\nvoid ProcessTopkV2Operator(Model* model, TopKV2Operator* op) {\n  const auto& input_values = model->GetArray(op->inputs[0]);\n  const auto& input_k = model->GetArray(op->inputs[1]);\n  auto& output_indexes = model->GetArray(op->outputs[0]);\n  auto& output_values = model->GetArray(op->outputs[1]);\n\n  // Bail if we already know the output shape.\n  if (output_indexes.has_shape()) {\n    QCHECK(output_values.has_shape());\n    return;\n  }\n\n  // Yield until input dims have been resolved.\n  if (!input_values.has_shape()) {\n    return;\n  }\n\n  const auto& input_values_shape = input_values.shape();\n  auto output_indexes_dims = output_indexes.mutable_shape()->mutable_dims();\n  auto output_values_dims = output_values.mutable_shape()->mutable_dims();\n  for (int dim = 0; dim < input_values_shape.dimensions_count() - 1; dim++) {\n    output_indexes_dims->push_back(input_values_shape.dims(dim));\n    output_values_dims->push_back(input_values_shape.dims(dim));\n  }\n  // If the value is initialized, we can specify the last dimension, otherwise\n  // unknown.\n  if (input_k.buffer) {\n    const int32_t k_value = input_k.GetBuffer<ArrayDataType::kInt32>().data[0];\n    output_indexes_dims->push_back(k_value);\n    output_values_dims->push_back(k_value);\n\n  } else {\n    output_indexes_dims->push_back(0);\n    output_values_dims->push_back(0);\n  }\n}\n\nvoid ProcessPadOperator(Model* model, PadOperator* op) {\n  CHECK_EQ(op->inputs.size(), 2);\n  CHECK_EQ(op->outputs.size(), 1);\n\n  const auto& input_array = model->GetArray(op->inputs[0]);\n\n  // Yield until input dims have been resolved.\n  if (!input_array.has_shape()) return;\n\n  if (op->left_padding.empty()) return;\n  CHECK_EQ(op->left_padding.size(), op->right_padding.size());\n\n  auto& output_array = model->GetArray(op->outputs[0]);\n  if (output_array.has_shape()) return;\n\n  Shape output_shape = input_array.shape();\n  std::vector<int>& dims = *output_shape.mutable_dims();\n  CHECK_EQ(op->left_padding.size(), dims.size());\n\n  for (int i = 0; i < op->left_padding.size(); ++i) {\n    dims[i] += op->left_padding[i] + op->right_padding[i];\n  }\n\n  output_array.copy_shape(output_shape);\n}\n\nvoid ProcessRankOperator(Model* model, RankOperator* op) {\n  CHECK_GE(op->inputs.size(), 1);\n  CHECK_EQ(op->outputs.size(), 1);\n  auto& output_array = model->GetArray(op->outputs[0]);\n  if (output_array.has_shape()) {\n    // Shape already propagated\n    return;\n  }\n\n  const auto& input_array = model->GetArray(op->inputs[0]);\n  if (!input_array.has_shape()) {\n    // Yield until input dims have been resolved.\n    return;\n  }\n\n  // Only set the output shape. Array contents are set by\n  // ResolveConstantShapeOrRank.\n  Shape* output_shape = output_array.mutable_shape();\n  output_shape->ReplaceDims({});\n}\n\nvoid ProcessShapeOperator(Model* model, TensorFlowShapeOperator* op) {\n  CHECK_GE(op->inputs.size(), 1);\n  CHECK_EQ(op->outputs.size(), 1);\n  auto& output_array = model->GetArray(op->outputs[0]);\n  if (output_array.has_shape()) {\n    // Shape already propagated\n    return;\n  }\n\n  const auto& input_array = model->GetArray(op->inputs[0]);\n  if (!input_array.has_shape()) {\n    // Yield until input dims have been resolved.\n    return;\n  }\n\n  // Only set the output shape. Array contents are set by\n  // ResolveConstantShapeOrRank.\n  Shape* output_shape = output_array.mutable_shape();\n  output_shape->ReplaceDims({input_array.shape().dimensions_count()});\n}\n\nvoid ProcessStackOperator(Model* model, StackOperator* op) {\n  CHECK_GE(op->inputs.size(), 1);\n  CHECK_EQ(op->outputs.size(), 1);\n  auto& output_array = model->GetArray(op->outputs[0]);\n  if (output_array.has_shape()) {\n    // Shape already propagated\n    return;\n  }\n\n  std::unique_ptr<Shape> stacked_shape;\n  for (const auto& input : op->inputs) {\n    const auto& input_array = model->GetArray(input);\n    if (!input_array.has_shape()) {\n      // Yield until all input dims have been resolved.\n      return;\n    }\n\n    Shape shape = input_array.shape();\n    if (shape.dimensions_count() == 0) {\n      // Convert 0D scalars to 1D scalars of shape {1}.\n      shape.mutable_dims()->push_back(1);\n    }\n    if (!stacked_shape) {\n      stacked_shape.reset(new Shape(shape));\n    } else {\n      CHECK(*stacked_shape == shape) << \"All input arrays to Stack operators \"\n                                        \"must have the same shape. Input \\\"\"\n                                     << input << \"\\\" is different.\";\n    }\n  }\n\n  int axis = op->axis;\n  if (axis < 0) {\n    // Handle negative axis\n    axis += stacked_shape->dims().size() + 1;\n  }\n  stacked_shape->mutable_dims()->insert(\n      stacked_shape->mutable_dims()->begin() + axis, op->inputs.size());\n  output_array.copy_shape(*stacked_shape);\n}\n\nvoid ProcessStridedSliceOperator(Model* model, StridedSliceOperator* op) {\n  CHECK_GE(op->inputs.size(), 1);\n  CHECK_EQ(op->outputs.size(), 1);\n  auto& output_array = model->GetArray(op->outputs[0]);\n  if (output_array.has_shape()) {\n    // Shape already propagated\n    return;\n  }\n\n  if (op->start_indices.empty() || op->stop_indices.empty() ||\n      op->strides.empty()) {\n    // ResolveStridedSliceAttributes has not run yet.\n    return;\n  }\n\n  const auto& input_array = model->GetArray(op->inputs[0]);\n  if (!input_array.has_shape()) {\n    // Yield until input dims have been resolved.\n    return;\n  }\n\n  if (op->ellipsis_mask != 0) {\n    // Something like LOG_FIRST_N(WARNING, 10) would be prefferable to reduce\n    // log noise. However, the TensorFlow logging library does not appear to\n    // support this.\n    LOG(WARNING) << \"Skipping StridedSlice op with output \\\"\" << op->outputs[0]\n                 << \"\\\". ellipsis_mask is not supported (mask=\"\n                 << op->ellipsis_mask << \")\";\n    return;\n  }\n  if (op->new_axis_mask != 0) {\n    LOG(WARNING) << \"Skipping StridedSlice op with output \\\"\" << op->outputs[0]\n                 << \"\\\". new_axis_mask is not supported (mask=\"\n                 << op->new_axis_mask << \")\";\n    return;\n  }\n\n  int dim_count = input_array.shape().dimensions_count();\n  CHECK(op->start_indices.size() == dim_count)\n      << \": Incorrect number of start indices supplied to StridedSlice op with \"\n         \"output \\\"\"\n      << op->outputs[0] << \"\\\". Op requires \" << dim_count << \" start indices\";\n  CHECK(op->stop_indices.size() == dim_count)\n      << \": Incorrect number of stop indices supplied to StridedSlice op with \"\n         \"output \\\"\"\n      << op->outputs[0] << \"\\\". Op requires \" << dim_count << \" stop indices\";\n  CHECK(op->strides.size() == dim_count)\n      << \": Incorrect number of strides supplied to StridedSlice op with \"\n         \" output \\\"\"\n      << op->outputs[0] << \"\\\". Op requires \" << dim_count << \" strides\";\n\n  // Create output shape\n  std::vector<int>* dims = output_array.mutable_shape()->mutable_dims();\n\n  // Compute output shape\n  for (int i = 0; i < dim_count; ++i) {\n    const int mask = 1 << i;\n    int start = (op->begin_mask & mask) ? 0 : op->start_indices[i];\n    if (start < 0) {\n      // handle negative indices\n      start += input_array.shape().dims(i);\n    }\n    int stop = (op->end_mask & mask) ? input_array.shape().dims(i)\n                                     : op->stop_indices[i];\n    if (stop < 0) {\n      // handle negative indices\n      stop += input_array.shape().dims(i);\n    }\n\n    int dim_size = ceil((stop - start) / static_cast<float>(op->strides[i]));\n    dim_size = dim_size < 0 ? 0 : dim_size;\n    if (op->shrink_axis_mask & mask) {\n      CHECK_EQ(dim_size, 1) << \"Output size for an axis must compute to 1 when \"\n                               \"shrinking that axis\";\n    } else {\n      dims->push_back(dim_size);\n    }\n  }\n}\n\nvoid ProcessSqueezeOperator(Model* model, SqueezeOperator* op) {\n  CHECK_EQ(op->inputs.size(), 1);\n  CHECK_EQ(op->outputs.size(), 1);\n\n  const auto& input_array = model->GetArray(op->inputs[0]);\n\n  // Yield until input dims have been resolved.\n  if (!input_array.has_shape()) return;\n\n  auto& output_array = model->GetArray(op->outputs[0]);\n  if (output_array.has_shape()) return;\n\n  const std::vector<int>& input_dims = input_array.shape().dims();\n  std::vector<int> output_dims;\n\n  for (int i = 0; i < input_dims.size(); ++i) {\n    if (input_dims[i] != 1 ||\n        (!op->squeeze_dims.empty() &&\n         std::find(op->squeeze_dims.begin(), op->squeeze_dims.end(), i) ==\n             op->squeeze_dims.end())) {\n      output_dims.push_back(input_dims[i]);\n    }\n  }\n  *output_array.mutable_shape()->mutable_dims() = output_dims;\n}\n\nvoid ProcessSvdfOperator(Model* model, SvdfOperator* op) {\n  CHECK(op->inputs.size() == 3 || op->inputs.size() == 4);\n  const auto& input_array = model->GetArray(op->inputs[0]);\n  if (!input_array.has_shape()) return;\n\n  auto& weights_feature_array = model->GetArray(op->inputs[1]);\n  if (!weights_feature_array.has_shape()) return;\n\n  const auto& weights_time_array = model->GetArray(op->inputs[2]);\n  if (!weights_time_array.has_shape()) return;\n\n  const bool has_bias = (op->inputs.size() == 4);\n  if (has_bias) {\n    const auto& bias_array = model->GetArray(op->inputs[3]);\n    if (!bias_array.has_shape()) return;\n  }\n\n  const int batch_size = input_array.shape().dims()[0];\n  const int num_units = weights_feature_array.shape().dims()[0];\n  const int memory_size = weights_time_array.shape().dims()[1];\n\n  auto& state_array = model->GetArray(op->outputs[0]);\n  state_array.mutable_shape()->ReplaceDims(\n      {batch_size, memory_size * num_units});\n\n  auto& output_array = model->GetArray(op->outputs[1]);\n  output_array.mutable_shape()->ReplaceDims({batch_size, num_units});\n}\n\nvoid ProcessTransposeOperator(Model* model, TransposeOperator* op) {\n  auto& output_array = model->GetArray(op->outputs[0]);\n  if (output_array.has_shape()) {\n    // We have already run\n    return;\n  }\n\n  const auto& input_array = model->GetArray(op->inputs[0]);\n  if (!input_array.has_shape()) {\n    // Yield until input dims have been resolved.\n    return;\n  }\n  const auto& input_shape = input_array.shape();\n\n  auto& perm_array = model->GetArray(op->inputs[1]);\n  if (!perm_array.has_shape()) {\n    // Yield until permutation shape been resolved.\n    return;\n  }\n  if (!perm_array.buffer) {\n    // Yield until the permutation is constant\n    return;\n  }\n  CHECK(perm_array.data_type == ArrayDataType::kInt32)\n      << \"Transpose permutation input must be int32\";\n\n  std::vector<int32> const& perm =\n      perm_array.GetBuffer<ArrayDataType::kInt32>().data;\n  CHECK_EQ(perm.size(), input_shape.dimensions_count())\n      << \"Transpose permutation input \" << op->inputs[1]\n      << \" must be same length as input dimensions\";\n  std::vector<int>* output_dims = output_array.mutable_shape()->mutable_dims();\n  for (int i = 0; i < perm.size(); i++) {\n    int axis = perm[i];\n    CHECK_GE(axis, 0);\n    CHECK_LT(axis, input_shape.dimensions_count());\n    output_dims->push_back(input_shape.dims(axis));\n  }\n}\n\nvoid ProcessArgMaxOperator(Model* model, ArgMaxOperator* op) {\n  CHECK_EQ(op->inputs.size(), 2);\n  const auto& input_array = model->GetArray(op->inputs[0]);\n  // Yield until input dims have been resolved.\n  if (!input_array.has_shape()) {\n    return;\n  }\n\n  // The current ArgMax implementation only supports 4-dimensional inputs with\n  // the last dimension as the axis to perform ArgMax for.\n  const std::vector<int>& input_dims = input_array.shape().dims();\n  CHECK_EQ(input_dims.size(), 4);\n  std::vector<int> output_dims;\n\n  output_dims.reserve(input_dims.size() - 1);\n  for (int i = 0; i < input_dims.size() - 1; ++i) {\n    output_dims.push_back(input_dims[i]);\n  }\n  output_dims.push_back(1);\n  const string& output_name = op->outputs[0];\n  auto& output_array = model->GetArray(output_name);\n  if (output_array.has_shape()) {\n    return;\n  }\n  *output_array.mutable_shape()->mutable_dims() = output_dims;\n}\n\n}  // namespace\n\nbool PropagateFixedSizes::Run(Model* model, std::size_t op_index) {\n  auto it = model->operators.begin() + op_index;\n  auto* op = it->get();\n  std::unordered_map<string, std::vector<int>> old_output_dims;\n  for (const auto& output : op->outputs) {\n    if (model->GetArray(output).has_shape()) {\n      old_output_dims[output] = model->GetArray(output).shape().dims();\n    }\n  }\n\n  switch (op->type) {\n    case OperatorType::kBatchNormalization:\n    case OperatorType::kL2Normalization:\n    case OperatorType::kDequantize:\n    case OperatorType::kRelu:\n    case OperatorType::kRelu1:\n    case OperatorType::kRelu6:\n    case OperatorType::kPRelu:\n    case OperatorType::kSoftmax:\n    case OperatorType::kLogSoftmax:\n    case OperatorType::kLogistic:\n    case OperatorType::kTanh:\n    case OperatorType::kLocalResponseNormalization:\n    case OperatorType::kTensorFlowIdentity:\n    case OperatorType::kFakeQuant:\n    case OperatorType::kNeg:\n    case OperatorType::kTensorFlowRsqrt:\n    case OperatorType::kTensorFlowSqrt:\n    case OperatorType::kTensorFlowSquare:\n    case OperatorType::kTensorFlowAll:\n    case OperatorType::kTensorFlowAssert:\n    case OperatorType::kCast:\n    case OperatorType::kFloor:\n    case OperatorType::kExp:\n      ProcessSimpleOperator(model, op);\n      break;\n    case OperatorType::kGather:\n      ProcessGatherOperator(model, static_cast<GatherOperator*>(op));\n      break;\n    case OperatorType::kTopK_V2:\n      ProcessTopkV2Operator(model, static_cast<TopKV2Operator*>(op));\n      break;\n    case OperatorType::kAdd:\n    case OperatorType::kSub:\n    case OperatorType::kMul:\n    case OperatorType::kDiv:\n    case OperatorType::kFloorDiv:\n    case OperatorType::kFloorMod:\n    case OperatorType::kTensorFlowLess:\n    case OperatorType::kTensorFlowLessEqual:\n    case OperatorType::kTensorFlowGreater:\n    case OperatorType::kTensorFlowMaximum:\n    case OperatorType::kTensorFlowMinimum:\n    case OperatorType::kTensorFlowGreaterEqual:\n      ProcessSimpleBinaryOperator(model, op);\n      break;\n    case OperatorType::kAddN:\n      ProcessAddNOperator(model, op);\n      break;\n    case OperatorType::kConv:\n      ProcessConvOperator(model, static_cast<ConvOperator*>(op));\n      break;\n    case OperatorType::kTransposeConv:\n      ProcessTransposeConvOperator(model,\n                                   static_cast<TransposeConvOperator*>(op));\n      break;\n    case OperatorType::kDepthwiseConv:\n      ProcessDepthwiseConvOperator(model,\n                                   static_cast<DepthwiseConvOperator*>(op));\n      break;\n    case OperatorType::kDepthToSpace:\n      ProcessDepthToSpaceOperator(model,\n                                  static_cast<DepthToSpaceOperator*>(op));\n      break;\n    case OperatorType::kSpaceToDepth:\n      ProcessSpaceToDepthOperator(model,\n                                  static_cast<SpaceToDepthOperator*>(op));\n      break;\n    case OperatorType::kFill:\n      CHECK_EQ(op->inputs.size(), 2);\n      ProcessOpWithShapeInput(model, op);\n      break;\n    case OperatorType::kFullyConnected:\n      ProcessFullyConnectedOperator(model,\n                                    static_cast<FullyConnectedOperator*>(op));\n      break;\n    case OperatorType::kTensorFlowReshape:\n      ProcessTensorFlowReshapeOperator(\n          model, static_cast<TensorFlowReshapeOperator*>(op));\n      break;\n    case OperatorType::kAveragePool:\n      ProcessAveragePoolOperator(model, static_cast<AveragePoolOperator*>(op));\n      break;\n    case OperatorType::kMaxPool:\n      ProcessMaxPoolOperator(model, static_cast<MaxPoolOperator*>(op));\n      break;\n    case OperatorType::kL2Pool:\n      ProcessL2PoolOperator(model, static_cast<L2PoolOperator*>(op));\n      break;\n    case OperatorType::kTensorFlowMin:\n    case OperatorType::kTensorFlowMax:\n    case OperatorType::kTensorFlowSum:\n    case OperatorType::kMean:\n      ProcessTensorFlowReductionOperator(model, op);\n      break;\n\n    case OperatorType::kSlice:\n      ProcessSliceOperator(model, static_cast<SliceOperator*>(op));\n      break;\n\n    case OperatorType::kTensorFlowTile:\n      // We don't currently implement the propagation of fixed sizes through\n      // a TensorFlow Tile.\n      //\n      // Fortunately, we don't need to: so far, we have only dealt with Tile\n      // or Slice ops in subgraphs that are identified as L2Normalization.\n      // See IdentifyL2Normalization.\n      break;\n    case OperatorType::kTensorFlowSwitch:\n      // We can't know the sizes of the outputs until we have resolved the\n      // predicate, and once we have resolved the predicate, the whole\n      // Switch node will get resolved away.\n      // See ResolveTensorFlowSwitch.\n      break;\n    case OperatorType::kTensorFlowMerge:\n      // No need to bother resolving TensorFlow Merge ops: other graph\n      // transformations will remove them anyway.\n      // See ResolveTensorFlowMerge.\n      break;\n    case OperatorType::kTensorFlowSplit:\n      ProcessTensorFlowSplitOperator(model,\n                                     static_cast<TensorFlowSplitOperator*>(op));\n      break;\n    case OperatorType::kSqueeze:\n      ProcessSqueezeOperator(model, static_cast<SqueezeOperator*>(op));\n      break;\n    case OperatorType::kTensorFlowConcat:\n    case OperatorType::kTensorFlowConcatV2:\n      // Unimplemented, hopefully another graph transformation will\n      // drop it or rewrite it. Concretely, either ResolveTensorFlowConcat\n      // will resolve this node to a DepthConcatenation, or else we have\n      // a more general non-depth concatenation that will hopefully be dropped,\n      // or else at the moment we will abort.\n      break;\n    case OperatorType::kExpandDims:\n      // Yield until ExpandDims is converted to Reshape\n      break;\n    case OperatorType::kRange:\n      ProcessRangeOperator(model, static_cast<RangeOperator*>(op));\n      break;\n    case OperatorType::kRank:\n      ProcessRankOperator(model, static_cast<RankOperator*>(op));\n      break;\n    case OperatorType::kTensorFlowShape:\n      ProcessShapeOperator(model, static_cast<TensorFlowShapeOperator*>(op));\n      break;\n    case OperatorType::kStack:\n      ProcessStackOperator(model, static_cast<StackOperator*>(op));\n      break;\n    case OperatorType::kReorderAxes:\n      ProcessReorderAxesOperator(model, static_cast<ReorderAxesOperator*>(op));\n      break;\n    case OperatorType::kConcatenation:\n      ProcessConcatenationOperator(model,\n                                   static_cast<ConcatenationOperator*>(op));\n      break;\n    case OperatorType::kResizeBilinear:\n      ProcessResizeBilinearOperator(model,\n                                    static_cast<ResizeBilinearOperator*>(op));\n      break;\n    case OperatorType::kLstmCell:\n      ProcessLstmCellOperator(model, static_cast<LstmCellOperator*>(op));\n      break;\n    case OperatorType::kBatchMatMul:\n    case OperatorType::kTensorFlowMatMul:\n      // MatMul operators are converted to FullyConnected, after which their\n      // shapes are propagated.\n      break;\n    case OperatorType::kSpaceToBatchND:\n      ProcessSpaceToBatchNDOperator(model,\n                                    static_cast<SpaceToBatchNDOperator*>(op));\n      break;\n    case OperatorType::kBatchToSpaceND:\n      ProcessBatchToSpaceNDOperator(model,\n                                    static_cast<BatchToSpaceNDOperator*>(op));\n      break;\n    case OperatorType::kPad:\n      ProcessPadOperator(model, static_cast<PadOperator*>(op));\n      break;\n    case OperatorType::kStridedSlice:\n      ProcessStridedSliceOperator(model,\n                                  static_cast<StridedSliceOperator*>(op));\n      break;\n    case OperatorType::kArgMax:\n      ProcessArgMaxOperator(model, static_cast<ArgMaxOperator*>(op));\n      break;\n    case OperatorType::kTensorFlowUnsupported:\n      break;\n    case OperatorType::kSvdf:\n      ProcessSvdfOperator(model, static_cast<SvdfOperator*>(op));\n      break;\n    case OperatorType::kTranspose:\n      ProcessTransposeOperator(model, static_cast<TransposeOperator*>(op));\n      break;\n    case OperatorType::kDynamicPartition:\n    case OperatorType::kDynamicStitch:\n      // DynamicPartition/DynamicStitch are currently only supported for\n      // transforms that remove them, so we avoid propagating shapes through\n      // them and let things settle once they've been removed.\n      break;\n    case OperatorType::kRandomUniform:\n      CHECK_EQ(op->inputs.size(), 1);\n      ProcessOpWithShapeInput(model, op);\n      break;\n    default:\n      // Unimplemented, another graph transformation should drop it.\n      LOG(FATAL) << \"Unhandled operator type \" << OperatorTypeName(op->type);\n  }\n\n  // Return true if any output dim changed, false if none changed.\n  // Assumption: no transformation clears an output shape, they only add shapes.\n  for (const auto& output : op->outputs) {\n    if (model->GetArray(output).has_shape() &&\n        (old_output_dims[output] != model->GetArray(output).shape().dims())) {\n      AddMessageF(\"Set shape of %s to [%s]\", output,\n                  absl::StrJoin(model->GetArray(output).shape().dims(), \",\"));\n      return true;\n    }\n  }\n  return false;\n}\n\n}  // namespace toco"