"# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OiR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n# pylint: disable=g-long-lambda\n\"\"\"Tests for tensorflow.ops.control_flow_ops.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport math\nimport re\nimport sys\nimport time\n\nfrom absl.testing import parameterized\nimport numpy as np\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\n\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.python import tf2\nfrom tensorflow.python.client import device_lib\nfrom tensorflow.python.client import session\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.eager import function as eager_function\nfrom tensorflow.python.eager import wrap_function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import function\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_spec\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import control_flow_util\nfrom tensorflow.python.ops import data_flow_ops\nfrom tensorflow.python.ops import functional_ops\nfrom tensorflow.python.ops import gen_array_ops\nfrom tensorflow.python.ops import gen_control_flow_ops\nfrom tensorflow.python.ops import gen_data_flow_ops\nfrom tensorflow.python.ops import gen_logging_ops\nfrom tensorflow.python.ops import gen_state_ops\nfrom tensorflow.python.ops import gradient_checker_v2\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import linalg_ops\nfrom tensorflow.python.ops import logging_ops\nfrom tensorflow.python.ops import map_fn\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_grad  # pylint: disable=unused-import\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import resource_variable_ops\nfrom tensorflow.python.ops import script_ops\nfrom tensorflow.python.ops import sparse_ops\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import tensor_array_grad  # pylint: disable=unused-import\nfrom tensorflow.python.ops import tensor_array_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.ops import while_v2  # pylint: disable=unused-import\n# pylint: disable=unused-import\nfrom tensorflow.python.ops.ragged import ragged_factory_ops\nfrom tensorflow.python.ops.ragged import ragged_tensor\nimport tensorflow.python.ops.tensor_array_grad\n# pylint: enable=unused-import\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.training import adam\nfrom tensorflow.python.training import gradient_descent\nfrom tensorflow.python.util import nest\n\n\ndef check_consumers(graph):\n  \"\"\"Sanity check on the consumer list of the tensors.\"\"\"\n\n  consumer_count = {}\n  for op in graph.get_operations():\n    for v in op.inputs:\n      cnt = consumer_count.get(v, 0)\n      consumer_count[v] = cnt + 1\n  for k, v in consumer_count.items():\n    if len(k.consumers()) != v:\n      return False\n  return True\n\n\ndef all_fetchables():\n  tensor_names = []\n  graph = ops.get_default_graph()\n  for op in graph.get_operations():\n    for t in op.outputs:\n      if graph.is_fetchable(t):\n        tensor_names.append(t.name)\n  return tensor_names\n\n\ndef all_feedables():\n  feedable_tensors = []\n  graph = ops.get_default_graph()\n  for op in graph.get_operations():\n    for t in op.inputs:\n      if graph.is_feedable(t):\n        feedable_tensors.append(t)\n  return feedable_tensors\n\n\ndef opt_cfg(do_constant_folding=True):\n  return config_pb2.ConfigProto(\n      allow_soft_placement=True,\n      graph_options=config_pb2.GraphOptions(\n          optimizer_options=config_pb2.OptimizerOptions(\n              opt_level=config_pb2.OptimizerOptions.L1,\n              do_function_inlining=True,\n              do_constant_folding=do_constant_folding)))\n\n\ndef isum(s, maximum_iterations=None):\n  i = constant_op.constant(0, name=\"i\")\n  c = lambda i, s: math_ops.less(i, 10)\n  b = lambda i, s: [math_ops.add(i, 1), math_ops.add(i, s)]\n  _, r_s = control_flow_ops.while_loop(\n      c, b, [i, s], maximum_iterations=maximum_iterations)\n  return r_s\n\n\ndef enqueue_print_op(s):\n  \"\"\"Enqueues an op that prints a message to be captured in the test.\"\"\"\n  return logging_ops.print_v2(\"ControlFlowOpsTest: \" + s)\n\n\ndef filter_test_messages(s):\n  \"\"\"Returns a list of messages printed by enqueue_print_op.\"\"\"\n  prefix = \"ControlFlowOpsTest: \"\n  return [l[len(prefix):] for l in s.split(\"\\n\") if l.startswith(prefix)]\n\n\ndef tf_function_in_tf2(f):\n  if tf2.enabled():\n    # In TF1 do not wrap with tf.function so that we can test the v1 control\n    # flow code path.\n    return def_function.function(f)\n  return f\n\n\n@test_util.with_control_flow_v2\nclass ControlFlowTest(test.TestCase, parameterized.TestCase):\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testRefIdentity(self):\n    with self.cached_session():\n      v = variables.VariableV1(7)\n\n      v = control_flow_ops._Identity(v)\n      op = state_ops.assign(v, 9)\n      v2 = control_flow_ops.with_dependencies([op], v)\n\n      self.assertTrue(isinstance(v2, ops.Tensor))\n      self.evaluate(variables.global_variables_initializer())\n      self.assertEqual(9, self.evaluate(v2))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testRefEnter(self):\n    with self.cached_session():\n      v = variables.VariableV1(7)\n\n      enter_v = control_flow_ops._Enter(v, \"foo_1\", is_constant=True)\n      nine = constant_op.constant(9)\n      enter_nine = gen_control_flow_ops.enter(nine, \"foo_1\")\n      op = state_ops.assign(enter_v, enter_nine)\n      v2 = control_flow_ops.with_dependencies([op], enter_v)\n      v3 = control_flow_ops.exit(v2)\n      self.evaluate(variables.global_variables_initializer())\n      self.assertEqual(9, self.evaluate(v3))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testRefSwitch(self):\n    with self.cached_session():\n      v = variables.VariableV1(7)\n\n      p = constant_op.constant(True)\n      v1 = control_flow_ops._SwitchRefOrTensor(v._ref(), p)  # pylint: disable=protected-access\n      v2 = state_ops.assign(v1[1], 9)\n      self.evaluate(variables.global_variables_initializer())\n      self.assertEqual(9, self.evaluate(v2))\n\n  def testEnterMulExit(self):\n    with self.cached_session():\n      data = constant_op.constant([1, 2, 3, 4, 5, 6], name=\"data\")\n      enter_data = gen_control_flow_ops.enter(data, \"foo_1\", False)\n      five = constant_op.constant(5)\n      enter_five = gen_control_flow_ops.enter(five, \"foo_1\", False)\n      mul_op = math_ops.multiply(enter_data, enter_five)\n      exit_op = control_flow_ops.exit(mul_op)\n\n      result = self.evaluate(exit_op)\n    self.assertAllEqual(np.array([x * 5 for x in [1, 2, 3, 4, 5, 6]]), result)\n\n  @test_util.run_deprecated_v1\n  def testEnterShapePropagation(self):\n    with self.cached_session():\n      v = variables.Variable([0.0, 0.0], dtype=dtypes.float32)\n\n      # If is_constant=True, the shape information should be propagated.\n      enter_v_constant = gen_control_flow_ops.enter(\n          v, \"frame1\", is_constant=True)\n      self.assertEqual(enter_v_constant.shape, [2])\n\n      # Otherwise, the shape should be unknown.\n      enter_v_non_constant = gen_control_flow_ops.enter(\n          v, \"frame2\", is_constant=False)\n      self.assertEqual(enter_v_non_constant.shape, None)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testSwitchMergeIndexedSlices(self):\n    with self.cached_session():\n      values = constant_op.constant([1, 2, 3, 4, 5, 6])\n      indices = constant_op.constant([0, 2, 4, 6, 8, 10])\n      data = ops.IndexedSlices(values, indices)\n      pred = ops.convert_to_tensor(True)\n      switch_op = control_flow_ops.switch(data, pred)\n      merge_op = control_flow_ops.merge(switch_op)[0]\n\n      val = merge_op.values\n      ind = merge_op.indices\n    self.assertAllEqual(np.arange(1, 7), val)\n    self.assertAllEqual(np.arange(0, 12, 2), ind)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testSwitchDeadBranch(self):\n    with self.cached_session():\n      data = constant_op.constant([1, 2, 3, 4, 5, 6], name=\"data\")\n      ports = ops.convert_to_tensor(True, name=\"ports\")\n      switch_op = control_flow_ops.switch(data, ports)\n      dead_branch = array_ops.identity(switch_op[0])\n\n      with self.assertRaisesWithPredicateMatch(\n          errors_impl.InvalidArgumentError,\n          lambda e: \"Retval[0] does not have value\" in str(e)):\n        self.evaluate(dead_branch)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testSwitchMergeLess(self):\n    with self.cached_session():\n      data = constant_op.constant([1, 2, 3, 4, 5, 6], name=\"data\")\n      zero = ops.convert_to_tensor(0)\n      one = ops.convert_to_tensor(1)\n      less_op = math_ops.less(zero, one)\n      switch_op = control_flow_ops.switch(data, less_op)\n      merge_op = control_flow_ops.merge(switch_op)[0]\n\n      result = self.evaluate(merge_op)\n    self.assertAllEqual(np.arange(1, 7), result)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testSwitchMergeAddIdentity(self):\n    with self.cached_session():\n      data = constant_op.constant([1, 2, 3, 4, 5, 6], name=\"data\")\n      ports = ops.convert_to_tensor(False, name=\"ports\")\n      switch_op = control_flow_ops.switch(data, ports)\n      one = constant_op.constant(1)\n      add_op = math_ops.add(switch_op[0], one)\n      id_op = array_ops.identity(switch_op[1])\n      merge_op = control_flow_ops.merge([add_op, id_op])[0]\n\n      result = self.evaluate(merge_op)\n    self.assertAllEqual(np.array([x + 1 for x in [1, 2, 3, 4, 5, 6]]), result)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testSwitchMergeAddMul(self):\n    with self.cached_session():\n      data = constant_op.constant([1, 2, 3, 4, 5, 6], name=\"data\")\n      ports = ops.convert_to_tensor(True, name=\"ports\")\n      switch_op = control_flow_ops.switch(data, ports)\n      one = constant_op.constant(1)\n      add_op = math_ops.add(switch_op[0], one)\n      five = constant_op.constant(5)\n      mul_op = math_ops.multiply(switch_op[1], five)\n      merge_op = control_flow_ops.merge([add_op, mul_op])[0]\n\n      result = self.evaluate(merge_op)\n    self.assertAllEqual(np.array([x * 5 for x in [1, 2, 3, 4, 5, 6]]), result)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testLoop_false(self):\n    with self.cached_session():\n      false = ops.convert_to_tensor(False)\n      n = constant_op.constant(10)\n\n      enter_false = gen_control_flow_ops.enter(false, \"foo_1\", False)\n      enter_n = gen_control_flow_ops.enter(n, \"foo_1\", False)\n\n      merge_n = control_flow_ops.merge([enter_n, enter_n], name=\"merge_n\")[0]\n      switch_n = control_flow_ops.switch(merge_n, enter_false)\n      exit_n = control_flow_ops.exit(switch_n[0])\n      next_n = control_flow_ops.next_iteration(switch_n[0])\n      merge_n.op._update_input(1, next_n)\n\n      result = self.evaluate(exit_n)\n    self.assertAllEqual(10, result)\n\n  @test_util.run_deprecated_v1\n  def testLoop_1(self):\n    with self.cached_session():\n      zero = constant_op.constant(0)\n      one = constant_op.constant(1)\n      n = constant_op.constant(10)\n\n      enter_i = gen_control_flow_ops.enter(zero, \"foo\", False)\n      enter_one = gen_control_flow_ops.enter(one, \"foo\", True)\n      enter_n = gen_control_flow_ops.enter(n, \"foo\", True)\n\n      with ops.device(test.gpu_device_name()):\n        merge_i = control_flow_ops.merge([enter_i, enter_i])[0]\n\n      less_op = math_ops.less(merge_i, enter_n)\n      cond_op = control_flow_ops.loop_cond(less_op)\n      switch_i = control_flow_ops.switch(merge_i, cond_op)\n\n      add_i = math_ops.add(switch_i[1], enter_one)\n\n      next_i = control_flow_ops.next_iteration(add_i)\n      merge_i.op._update_input(1, next_i)\n\n      exit_i = control_flow_ops.exit(switch_i[0])\n      result = self.evaluate(exit_i)\n    self.assertAllEqual(10, result)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testLoop_2(self):\n    with self.cached_session():\n      zero = constant_op.constant(0)\n      one = constant_op.constant(1)\n      n = constant_op.constant(10)\n\n      enter_i = gen_control_flow_ops.enter(zero, \"foo\", False)\n      enter_one = gen_control_flow_ops.enter(one, \"foo\", True)\n      enter_n = gen_control_flow_ops.enter(n, \"foo\", True)\n\n      merge_i = control_flow_ops.merge([enter_i, enter_i])[0]\n\n      less_op = math_ops.less(merge_i, enter_n)\n      cond_op = control_flow_ops.loop_cond(less_op)\n      switch_i = control_flow_ops.switch(merge_i, cond_op)\n\n      add_i = math_ops.add(switch_i[1], enter_one)\n\n      with ops.device(test.gpu_device_name()):\n        next_i = control_flow_ops.next_iteration(add_i)\n      merge_i.op._update_input(1, next_i)\n\n      exit_i = control_flow_ops.exit(switch_i[0])\n      result = self.evaluate(exit_i)\n    self.assertAllEqual(10, result)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testDifferentFrame(self):\n    with self.cached_session():\n      data = array_ops.placeholder(dtypes.float32, shape=[])\n      enter_1 = gen_control_flow_ops.enter(data, \"foo_1\", False)\n      enter_2 = gen_control_flow_ops.enter(data, \"foo_2\", False)\n      res = math_ops.add(enter_1, enter_2)\n      with self.assertRaisesOpError(\"has inputs from different frames\"):\n        res.eval(feed_dict={data: 1.0})\n\n  @test_util.run_deprecated_v1\n  def testCondBool(self):\n    values = constant_op.constant(10)\n    fn1 = lambda: math_ops.add(values, 1)\n    fn2 = lambda: math_ops.subtract(values, 1)\n    with self.assertRaisesRegex(TypeError, \"must not be a Python bool\"):\n      _ = control_flow_ops.cond(False, fn1, fn2)\n\n  @test_util.run_deprecated_v1\n  def testCondInt(self):\n    p = array_ops.placeholder(dtypes.bool, shape=[])\n    v = constant_op.constant(10)\n    fn1 = lambda: math_ops.add(v, 1)\n    fn2 = lambda: math_ops.subtract(v, 1)\n    y = control_flow_ops.cond(p, fn1, fn2)\n    grad = gradients_impl.gradients(y, [v])\n    self.assertAllEqual([None], grad)\n\n  def testCondOutputShape(self):\n    x = constant_op.constant(1.0)\n    b = control_flow_ops.cond(\n        constant_op.constant(True), lambda: math_ops.square(x),\n        lambda: math_ops.subtract(x, 1.))\n    self.assertEqual(b.shape, tensor_shape.TensorShape([]))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testFetchable(self):\n    with self.cached_session() as sess:\n      x = array_ops.placeholder(dtypes.float32)\n      control_flow_ops.cond(\n          constant_op.constant(True), lambda: x + 2, lambda: x + 0)\n      graph = ops.get_default_graph()\n      for op in graph.get_operations():\n        for t in op.inputs:\n          if graph.is_fetchable(t.op):\n            sess.run(t, feed_dict={x: 3})\n          else:\n            with self.assertRaisesRegex(ValueError,\n                                        \"has been marked as not fetchable\"):\n              sess.run(t, feed_dict={x: 3})\n\n  @test_util.disable_control_flow_v2(\"Not relevant\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testFeedable(self):\n    with self.cached_session() as sess:\n      c = constant_op.constant(2)\n      i0 = constant_op.constant(0)\n      r = control_flow_ops.while_loop(lambda i: i < 1000,\n                                      lambda i: math_ops.square(c) + i, [i0])\n      self.assertEqual(1000, r.eval(feed_dict={i0: 0}))\n      feedable_tensors = all_feedables()\n      for t in feedable_tensors:\n        sess.run(r, feed_dict={t: 3})\n      graph = ops.get_default_graph()\n      for op in graph.get_operations():\n        for t in op.inputs:\n          if t not in feedable_tensors and t.dtype is dtypes.int32:\n            with self.assertRaisesRegex(ValueError, \"may not be fed\"):\n              sess.run(r, feed_dict={t: 3})\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCondIndexedSlices(self):\n    with self.cached_session():\n      values = constant_op.constant([10])\n      indices = constant_op.constant([0])\n      x = ops.IndexedSlices(values, indices)\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: ops.IndexedSlices(math_ops.add(x.values, 1), indices)\n      fn2 = lambda: ops.IndexedSlices(math_ops.subtract(x.values, 1), indices)\n      r = control_flow_ops.cond(pred, fn1, fn2)\n\n      val = r.values\n      ind = r.indices\n    self.assertAllEqual([11], val)\n    self.assertAllEqual([0], ind)\n\n  def testCondMismatchedIndexedSlices(self):\n    @def_function.function\n    def foo():\n      values = constant_op.constant([10])\n      indices = constant_op.constant([0])\n      x = ops.IndexedSlices(values, indices)\n      with self.assertRaisesRegex(TypeError,\n                                  \"Cannot reconcile tf.cond 0-th outputs\"):\n        control_flow_ops.cond(\n            constant_op.constant(True),\n            lambda: ops.IndexedSlices(math_ops.add(x.values, 1), indices),\n            lambda: math_ops.add(x.values, 1), indices)\n    foo()\n\n  def testCondSparseTensor(self):\n    values = constant_op.constant([2.0, 4.0], name=\"values\")\n    indices = constant_op.constant([[0], [3]],\n                                   dtype=dtypes.int64,\n                                   name=\"indices\")\n    shape = constant_op.constant([10], dtype=dtypes.int64, name=\"dense_shape\")\n    x = sparse_tensor.SparseTensor(indices, values, dense_shape=shape)\n    pred = math_ops.less(1, 2)\n    fn1 = lambda: sparse_tensor.SparseTensor(\n        indices + 1, x.values + 1, dense_shape=shape)\n    fn2 = lambda: sparse_tensor.SparseTensor(\n        indices, x.values - 1, dense_shape=shape)\n    r = control_flow_ops.cond(pred, fn1, fn2)\n    self.assertAllEqual([3.0, 5.0], r.values)\n    self.assertAllEqual([[1], [4]], r.indices)\n    self.assertAllEqual(r.values.get_shape(), (2,))\n\n  def testCondRaggedTensor(self):\n    rt = ragged_factory_ops.constant([[1, 2], [3], [4, 5, 6]])\n    pred = math_ops.less(1, 2)\n    fn1 = lambda: array_ops.concat([rt + 2, [[100]]], axis=0)\n    fn2 = lambda: rt[:2] - 2\n    result = control_flow_ops.cond(pred, fn1, fn2)\n    self.assertAllEqual([3, 4, 5, 6, 7, 8, 100], result.values)\n    self.assertAllEqual([0, 2, 3, 6, 7], result.row_splits)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCondResource(self):\n\n    with self.cached_session():\n      rv = resource_variable_ops.ResourceVariable(True)\n      self.evaluate(variables.global_variables_initializer())\n      t = ops.convert_to_tensor(1.0)\n\n      def case():\n        assign = resource_variable_ops.assign_variable_op(rv.handle, False)\n        with ops.control_dependencies([assign]):\n          return array_ops.identity(t)\n\n      self.assertEqual(\n          1.0, self.evaluate(control_flow_ops.cond(rv, case, lambda: t)))\n\n  @test_util.run_deprecated_v1\n  def testCondResourceGradShape(self):\n    rv1 = resource_variable_ops.ResourceVariable([1.0, 2.0])\n    rv2 = resource_variable_ops.ResourceVariable([3.0, 4.0])\n    pred = constant_op.constant(True)\n    result = control_flow_ops.cond(pred, lambda: rv1, lambda: rv2)\n    grads = gradients_impl.gradients(result, [rv1, rv2])\n    self.assertAllEqual(grads[0].shape.as_list(), [2])\n    self.assertAllEqual(grads[1].shape.as_list(), [2])\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCondWithTensorArrayGrad(self):\n    with self.cached_session() as sess:\n      with ops.device(test.gpu_device_name()):\n        pred = array_ops.placeholder(dtypes.bool, [])\n        x = constant_op.constant([1.0, 2.0, 3.0])\n        y = control_flow_ops.cond(\n            pred, lambda: map_fn.map_fn(lambda z: z * 2.0, x),\n            lambda: constant_op.constant([1.0, 1.0, 1.0]))\n        g = gradients_impl.gradients(y, x)[0]\n\n      self.assertAllEqual(sess.run(g, {pred: True}), [2.0, 2.0, 2.0])\n      self.assertAllEqual(sess.run(g, {pred: False}), [0.0, 0.0, 0.0])\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCondIndexedSlicesDifferentTypes(self):\n    with self.cached_session():\n      values = constant_op.constant([10])\n      i_32 = ops.convert_to_tensor([0], name=\"one\", dtype=dtypes.int32)\n      i_64 = ops.convert_to_tensor([0], name=\"one\", dtype=dtypes.int64)\n      x = ops.IndexedSlices(values, i_32)\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: ops.IndexedSlices(math_ops.add(x.values, 1), i_32)\n      fn2 = lambda: ops.IndexedSlices(math_ops.subtract(x.values, 1), i_64)\n      r = control_flow_ops.cond(pred, fn1, fn2)\n\n      val = r.values\n      ind = r.indices\n    self.assertAllEqual([11], val)\n    self.assertAllEqual([0], ind)\n    self.assertTrue(ind.dtype == np.int64)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCondColocation(self):\n    with self.session(use_gpu=True):\n      with ops.device(\"/cpu:0\"):\n        v = variables.Variable(7.0)\n\n      x = constant_op.constant(10.0)\n      pred = math_ops.less(1.0, 2.0)\n      fn1 = lambda: math_ops.add(v, 1.0)\n      fn2 = lambda: math_ops.subtract(x, 1.0)\n      r = control_flow_ops.cond(pred, fn1, fn2)\n\n      for op in x.graph.get_operations():\n        if op.name == \"cond/Add/Switch\":\n          self.assertDeviceEqual(op.device, \"/cpu:0\")\n\n  def _testCond_1(self, use_gpu):\n    with self.cached_session(use_gpu=use_gpu):\n      x = constant_op.constant(10)\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: math_ops.add(x, 1)\n      fn2 = lambda: math_ops.subtract(x, 1)\n      r = control_flow_ops.cond(pred, fn1, fn2)\n\n      result = self.evaluate(r)\n    self.assertAllEqual(11, result)\n\n  def testCond_1(self):\n\n    self._testCond_1(use_gpu=False)\n    # TODO(b/116526896): Enable GPU tests.\n    # self._testCond_1(use_gpu=True)\n\n  def testCond_2(self):\n\n    with self.cached_session():\n      x = constant_op.constant(10)\n      r = control_flow_ops.cond(\n          math_ops.less(1, 0), lambda: math_ops.add(x, 1),\n          lambda: math_ops.subtract(x, 1))\n      result = self.evaluate(r)\n    self.assertAllEqual(9, result)\n\n  def testCond_3(self):\n\n    with self.cached_session():\n      x = constant_op.constant(10)\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: math_ops.add(x, 1)\n      fn2 = lambda: math_ops.subtract(x, 1)\n      fn3 = lambda: math_ops.add(control_flow_ops.cond(pred, fn1, fn2), 1)\n      r = control_flow_ops.cond(pred, fn3, fn2)\n\n      result = self.evaluate(r)\n    self.assertAllEqual(12, result)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testCondPruning(self):\n    v1 = variables.Variable(7)\n    v2 = variables.Variable(7)\n    v3 = variables.Variable(7)\n\n    def f():\n      age = constant_op.constant(3)\n      max_age = constant_op.constant(2)\n      pred = math_ops.greater(age, max_age)\n      fn1 = lambda: [state_ops.assign(v1, 1).op, state_ops.assign(v2, 2).op]\n      fn2 = lambda: [state_ops.assign(v3, 3).op, constant_op.constant(10).op]\n      r = control_flow_ops.cond(pred, fn1, fn2)\n      self.assertEqual(len(r), 2)\n      return r[1]\n\n    f_defun = eager_function.defun(f)\n\n    if not context.executing_eagerly():\n      with self.cached_session():\n        self.evaluate(variables.global_variables_initializer())\n        result = self.evaluate(f())\n        self.assertEqual(True, result)\n        # Only second cond result was fetched, so v1 assign shouldn't run.\n        self.assertEqual(7, self.evaluate(v1))\n        self.assertEqual(2, self.evaluate(v2))\n        self.assertEqual(7, self.evaluate(v3))\n\n    result = f_defun()\n    self.assertEqual(True, self.evaluate(result))\n    # Both v1 and v2 branch assignments should be run in defun.\n    self.assertEqual(1, self.evaluate(v1))\n    self.assertEqual(2, self.evaluate(v2))\n    self.assertEqual(7, self.evaluate(v3))\n\n  def testCond_5(self):\n    with self.cached_session():\n      alive = constant_op.constant(True, name=\"alive\")\n      count = constant_op.constant(0, name=\"count\")\n\n      def body(i):\n        return control_flow_ops.cond(\n            alive, lambda: [math_ops.less(i, 3), math_ops.add(count, 1)],\n            lambda: [alive, count])\n\n      for i in range(10):\n        alive, count = body(i)\n      self.assertAllEqual(4, self.evaluate(count))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCond_6(self):\n    with self.cached_session():\n      v1 = variables.Variable([7])\n\n      age = constant_op.constant(3)\n      pred = math_ops.greater(age, 4)\n      fn1 = lambda: age\n      fn2 = lambda: v1\n      r = control_flow_ops.cond(pred, fn1, fn2)\n\n      self.evaluate(variables.global_variables_initializer())\n      result = self.evaluate(r)\n      self.assertAllEqual(np.array([7]), result)\n\n  def testCond_7(self):\n    with self.cached_session() as sess:\n      x = constant_op.constant(10)\n      y = constant_op.constant(200)\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: [math_ops.add(x, 1), math_ops.add(x, 2)]\n      fn2 = lambda: [y, y]\n      r = control_flow_ops.cond(pred, fn1, fn2)\n      self.assertAllEqual([11, 12], self.evaluate(r))\n\n  @parameterized.parameters(dtypes.float32, dtypes.float64)\n  @test_util.run_v1_only(\"Uses tf.gradients\")\n  def testCondResourceGrad(self, dtype):\n    init = constant_op.constant([7.], dtype=dtype)\n    v1 = variables.Variable(init)\n\n    age = constant_op.constant(3., dtype=dtype)\n    pred = math_ops.greater(age, 4.)\n    fn1 = lambda: age\n    fn2 = lambda: v1\n    r = control_flow_ops.cond(pred, fn1, fn2)\n\n    grad = gradients_impl.gradients(r, v1)[0]\n    self.evaluate(variables.global_variables_initializer())\n    self.assertAllEqual(grad, [1.])\n\n  @test_util.run_gpu_only\n  @test_util.run_deprecated_v1\n  def testCond_Device(self):\n    x = constant_op.constant(-10.)\n\n    # True branch function defined outside of device scope\n    def true_fn():\n      return math_ops.exp(x)\n\n    with ops.device(\"CPU:0\"):\n      r = control_flow_ops.cond(\n          constant_op.constant(True), true_fn, lambda: 0.)\n      self.assertIn(\"cpu\", r.device.lower())\n\n    with session.Session() as sess:\n      options = config_pb2.RunOptions(output_partition_graphs=True)\n      run_metadata = config_pb2.RunMetadata()\n      sess.run(r, options=options, run_metadata=run_metadata)\n      # We expect that everything runs on CPU, even if GPU is available.\n      self.assertEqual(len(run_metadata.partition_graphs), 1)\n\n  def _count_matching_switch_nodes_on_device(self, run_metadata, device_str):\n    # Returns the number of Switch nodes with type float32 placed on\n    # `device_str`.\n    device_graphs = [\n        g for g in run_metadata.partition_graphs\n        if device_str in g.node[0].device\n    ]\n    self.assertLen(device_graphs, 1)\n    switch_nodes = [\n        n for n in device_graphs[0].node if n.op == \"Switch\" and\n        n.attr[\"T\"].type == dtypes.float32.as_datatype_enum\n    ]\n    return len(switch_nodes)\n\n  @test_util.run_gpu_only\n  @test_util.run_deprecated_v1\n  def testCondSwitchColocatedWithInputWhenInputOnCPU(self):\n    x = array_ops.placeholder(dtypes.float32)\n\n    # `arg` is used in the cond then branch so a Switch node is created for it.\n    # We test that the Switch node gets placed on the same device as `arg`.\n    # We force `arg` to be on CPU here.\n    with ops.device(\"CPU:0\"):\n      arg = x + 10.\n\n    def true_fn():\n      with ops.device(\"CPU:0\"):\n        return arg + 1\n\n    r = control_flow_ops.cond(constant_op.constant(True), true_fn, lambda: 0.)\n\n    with session.Session() as sess:\n      run_metadata = config_pb2.RunMetadata()\n      options = config_pb2.RunOptions(output_partition_graphs=True)\n      sess.run(\n          r, feed_dict={x: -10.}, options=options, run_metadata=run_metadata)\n      self.assertEqual(len(run_metadata.partition_graphs), 2)\n      # Check that the Switch for `arg` gets placed on CPU.\n      self.assertEqual(\n          self._count_matching_switch_nodes_on_device(run_metadata, \"CPU\"), 1)\n      self.assertEqual(\n          self._count_matching_switch_nodes_on_device(run_metadata, \"GPU\"), 0)\n\n  @test_util.run_gpu_only\n  @test_util.run_deprecated_v1\n  def testCondSwitchColocatedWithInputWhenInputOnGPU(self):\n    x = array_ops.placeholder(dtypes.float32)\n\n    # `arg` is used in the cond then branch so a Switch node is created for it.\n    # We test that the Switch node gets placed on the same device as `arg`.\n    # Note: `arg` gets placed on GPU by default by the placer.\n    arg = x + 10.\n\n    def true_fn():\n      with ops.device(\"CPU:0\"):\n        return arg + 1\n\n    r = control_flow_ops.cond(constant_op.constant(True), true_fn, lambda: 0.)\n\n    with session.Session() as sess:\n      run_metadata = config_pb2.RunMetadata()\n      options = config_pb2.RunOptions(output_partition_graphs=True)\n      sess.run(\n          r, feed_dict={x: -10.}, options=options, run_metadata=run_metadata)\n      self.assertEqual(len(run_metadata.partition_graphs), 2)\n      # Check that the Switch for `arg` gets placed on GPU.\n      self.assertEqual(\n          self._count_matching_switch_nodes_on_device(run_metadata, \"CPU\"), 0)\n      self.assertEqual(\n          self._count_matching_switch_nodes_on_device(run_metadata, \"GPU\"), 1)\n\n  def testCondAccessTrueBranchTensorInFalseBranchRaises(self):\n\n    @def_function.function\n    def f():\n      c = constant_op.constant(1.)\n      inputs = {\"c\": c}\n\n      def true_fn(inputs):\n        inputs[\"c\"] = array_ops.identity(inputs[\"c\"], name=\"true_branch\")\n        return inputs[\"c\"]\n\n      def false_fn(inputs):\n        return array_ops.identity(inputs[\"c\"])\n\n      pred = constant_op.constant(True)\n      return control_flow_ops.cond(\n          pred, lambda: true_fn(inputs), lambda: false_fn(inputs))\n\n    # This was needed for backwards compatibility with TF2 Estimators which\n    # rely on variable names.\n    prefix = \"cond/\" if context.executing_eagerly() else \"\"\n\n    with self.assertRaisesRegex(\n        ValueError,\n        \"Tensor %strue_branch:0 in true_fn is accessed from false_fn.\" %\n        prefix):\n      f()\n\n  def testSwitchCaseAccessBranch1TensorInBranch4Raises(self):\n\n    @def_function.function\n    def f():\n      c = constant_op.constant(1.)\n      inputs = {\"c\": c}\n\n      def br1_fn(inputs):\n        inputs[\"c\"] = array_ops.identity(inputs[\"c\"], name=\"br1_identity\")\n        return inputs[\"c\"]\n\n      def br4_fn(inputs):\n        return array_ops.identity(inputs[\"c\"])\n\n      def other_fn():\n        return array_ops.identity(c)\n\n      return control_flow_ops.switch_case(\n          constant_op.constant(2),\n          [other_fn, lambda: br1_fn(inputs), other_fn, other_fn,\n           lambda: br4_fn(inputs)])\n\n    # This was needed for backwards compatibility with TF2 Estimators which\n    # rely on variable names.\n    prefix = \"switch_case/indexed_case/\" if context.executing_eagerly() else \"\"\n    with self.assertRaisesRegex(\n        ValueError, \"Tensor %sbr1_identity:0 in branch 1 is \"\n        \"accessed from branch 4.\" % prefix):\n      f()\n\n  def testCondListOutput(self):\n    with self.cached_session() as sess:\n      x = constant_op.constant(10)\n      y = constant_op.constant(200)\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: [math_ops.add(x, y), math_ops.add(x, y)]\n      fn2 = lambda: [y, y]\n      r = control_flow_ops.cond(pred, fn1, fn2)\n      test_result = self.evaluate(r)\n      self.assertListEqual([210, 210], test_result)\n\n  def testTupleOutput(self):\n    with self.cached_session() as sess:\n      x = constant_op.constant(10)\n      y = constant_op.constant(200)\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: (math_ops.add(x, y), math_ops.add(x, y))\n      fn2 = lambda: (y, y)\n      r = control_flow_ops.cond(pred, fn1, fn2)\n      test_result = self.evaluate(r)\n      self.assertTupleEqual((210, 210), test_result)\n\n  def testDictOutput(self):\n    with self.cached_session() as sess:\n      x = constant_op.constant(10)\n      y = constant_op.constant(200)\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: {\"a\": math_ops.add(x, y), \"b\": math_ops.add(x, y)}\n      fn2 = lambda: {\"a\": y, \"b\": y}\n      r = control_flow_ops.cond(pred, fn1, fn2)\n      test_result = self.evaluate(r)\n      self.assertDictEqual({\"a\": 210, \"b\": 210}, test_result)\n\n  def testEmbeddedListOutput(self):\n    x = constant_op.constant(10)\n    y = constant_op.constant(200)\n    pred = math_ops.less(1, 2)\n    fn1 = lambda: [[math_ops.add(x, y), math_ops.add(x, y)]]\n    fn2 = lambda: [[y, y]]\n    # Pass strict=True flag as cond_v2 allows for tensors to be\n    # in nested output structures as singletons\n    r = control_flow_ops.cond(pred, fn1, fn2, strict=True)\n    test_result = self.evaluate(r)\n    self.assertListEqual([[210, 210]], test_result)\n\n  def testEmbeddedTupleOutput(self):\n    with self.cached_session() as sess:\n      x = constant_op.constant(10)\n      y = constant_op.constant(200)\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: ((math_ops.add(x, y), math_ops.add(x, y)))\n      fn2 = lambda: ((y, y))\n      r = control_flow_ops.cond(pred, fn1, fn2)\n      test_result = self.evaluate(r)\n      self.assertTupleEqual(((210, 210)), test_result)\n\n  def testEmbeddedDictOutput(self):\n    with self.cached_session() as sess:\n      x = constant_op.constant(10)\n      y = constant_op.constant(200)\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: {\"a\": {\"c\": math_ops.add(x, y)},\n                     \"b\": {\"d\": math_ops.add(x, y)}}\n      fn2 = lambda: {\"a\": {\"c\": y},\n                     \"b\": {\"d\": y}}\n      r = control_flow_ops.cond(pred, fn1, fn2)\n      test_result = self.evaluate(r)\n      self.assertDictEqual({\"a\": {\"c\": 210}, \"b\": {\"d\": 210}}, test_result)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCheckNestedOutputStruct(self):\n    with self.cached_session() as sess:\n      x = constant_op.constant(10)\n      y = constant_op.constant(200)\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: {\"a\": math_ops.add(x, y), \"b\": math_ops.add(x, y)}\n      fn2 = lambda: {\"c\": y, \"d\": y}\n      v1_msg = \"The two structures don't have the same nested structure\"\n      v2_msg = (\"true_fn and false_fn arguments to tf.cond must have the same \"\n                \"number, type, and overall structure of return values.\")\n      with self.assertRaisesRegex(\n          TypeError if control_flow_util.ENABLE_CONTROL_FLOW_V2 else ValueError,\n          v2_msg if control_flow_util.ENABLE_CONTROL_FLOW_V2 else v1_msg):\n        control_flow_ops.cond(pred, fn1, fn2)\n\n  @test_util.run_deprecated_v1\n  def testCondRef(self):\n\n    with self.cached_session():\n      x = gen_state_ops.variable(\n          shape=[1],\n          dtype=dtypes.float32,\n          name=\"x\",\n          container=\"\",\n          shared_name=\"\")\n      true_fn = lambda: x\n      false_fn = lambda: constant_op.constant([2.0])\n      r = control_flow_ops.cond(constant_op.constant(False), true_fn, false_fn)\n      self.assertAllEqual([2.0], self.evaluate(r))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCondWithControl(self):\n    with self.cached_session() as sess:\n      control_holder = array_ops.placeholder(dtypes.float32, shape=())\n      a = constant_op.constant(3)\n\n      def true_branch():\n        with ops.control_dependencies([control_holder]):\n          _ = a + 1\n        return a + 2\n\n      r = control_flow_ops.cond(\n          constant_op.constant(True), true_branch,\n          lambda: constant_op.constant(1))\n      result = sess.run(r, feed_dict={control_holder: 5.})\n      self.assertEqual(5, result)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testUninitializedRefIdentity(self):\n    with self.cached_session() as sess:\n      v = gen_state_ops.variable(\n          shape=[1],\n          dtype=dtypes.float32,\n          name=\"v\",\n          container=\"\",\n          shared_name=\"\")\n      inited = state_ops.is_variable_initialized(v)\n      v_f, v_t = control_flow_ops.ref_switch(v, inited)\n      # Both v_f and v_t are uninitialized references. However, an actual use\n      # of the reference in the 'true' branch in the 'tf.identity' op will\n      # not 'fire' when v is uninitialized, so this is a valid construction.\n      # This test tests that ref_identity allows uninitialized ref as input\n      # so that this construction is allowed.\n      v_f_op = gen_array_ops.ref_identity(v_f)\n      v_t_op = gen_array_ops.ref_identity(v_t)\n      with ops.control_dependencies([v_f_op]):\n        assign_v = state_ops.assign(v, [1.0])\n      with ops.control_dependencies([v_t_op]):\n        orig_v = array_ops.identity(v)\n      merged_op = control_flow_ops.merge([assign_v, orig_v])\n      self.assertAllEqual([1.0], self.evaluate(merged_op.output))\n\n  def testCondSwitchIdentity(self):\n    # Make sure the recv identity is not removed by optimization.\n    with session.Session(config=opt_cfg()) as sess:\n      pred = constant_op.constant(True)\n\n      def fn1():\n        return control_flow_ops.no_op()\n\n      def fn2():\n        return control_flow_ops.Assert(False, [\"Wrong branch!!!\"])\n\n      r = control_flow_ops.cond(pred, fn1, fn2)\n      self.evaluate(r)\n\n  def testCondRecvIdentity(self):\n    # Make sure the switch identity is not removed by optimization.\n    with session.Session(config=opt_cfg()) as sess:\n      with ops.device(test.gpu_device_name()):\n        pred = constant_op.constant(True)\n\n      def fn1():\n        return control_flow_ops.no_op()\n\n      def fn2():\n        with ops.device(\"/cpu:0\"):\n          return control_flow_ops.Assert(False, [\"Wrong branch!!!\"])\n\n      r = control_flow_ops.cond(pred, fn1, fn2)\n      self.evaluate(r)\n\n  @test_util.run_deprecated_v1\n  @test_util.enable_control_flow_v2\n  def testDisableLoweringSwitchMerge(self):\n    if test_util.is_gpu_available():\n      self.skipTest(\n          \"Single threaded executor doesn't support partitioned graphs.  \"\n          \"Skipping GPU test.\")\n    # Make pred feedable to ensure we don't constant-fold it out.\n    run_opts = config_pb2.RunOptions(\n        trace_level=config_pb2.RunOptions.FULL_TRACE)\n    run_metadata_no_lowering = config_pb2.RunMetadata()\n    run_metadata_with_lowering = config_pb2.RunMetadata()\n\n    config = opt_cfg(do_constant_folding=False)\n\n    pred = array_ops.placeholder_with_default(\n        constant_op.constant(True), shape=())\n    r = control_flow_ops.cond(pred, lambda: True, lambda: False)\n\n    with session.Session(config=config) as sess:\n      r_value = sess.run(\n          r, options=run_opts, run_metadata=run_metadata_with_lowering)\n      self.assertEqual(r_value, True)\n\n    # Use the single threaded executor, which disables control flow lowering.\n    config.experimental.executor_type = \"SINGLE_THREADED_EXECUTOR\"\n    with session.Session(config=config) as sess:\n      r_value = sess.run(\n          r, options=run_opts, run_metadata=run_metadata_no_lowering)\n      self.assertEqual(r_value, True)\n\n    self.assertTrue(  # pylint: disable=g-complex-comprehension\n        any(\"switch\" in ns.node_name\n            for dev_stat in run_metadata_with_lowering.step_stats.dev_stats\n            for ns in dev_stat.node_stats))\n\n    self.assertTrue(  # pylint: disable=g-complex-comprehension\n        all(\"switch\" not in ns.node_name\n            for dev_stat in run_metadata_no_lowering.step_stats.dev_stats\n            for ns in dev_stat.node_stats))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCondGrad_1(self):\n    with self.cached_session():\n      x = constant_op.constant(10.0, name=\"x\")\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: array_ops.identity(x)\n      fn2 = lambda: array_ops.identity(x)\n      r = control_flow_ops.cond(pred, fn1, fn2)\n\n      grad = gradients_impl.gradients(r, [x])[0]\n      self.assertAllEqual(1.0, self.evaluate(grad))\n\n  @test_util.run_deprecated_v1\n  @test_util.enable_control_flow_v2\n  def testCondComputeGradAfterSessRunFails(self):\n    with self.cached_session():\n      x = constant_op.constant(10.0, name=\"x\")\n      pred = math_ops.less(1, 2)\n\n      def true_fn():\n        a = x * x\n        return a * a\n\n      def false_fn():\n        return x * x\n\n      r = control_flow_ops.cond(pred, true_fn, false_fn)\n\n      self.assertAllEqual(r, 10000.)\n      grad = gradients_impl.gradients(r, [x])[0]\n      with self.assertRaisesRegex(\n          errors_impl.InvalidArgumentError,\n          r\"Connecting to invalid output 1 of source node cond which has 1 \"\n          r\"outputs. Try using \"\n          \"tf.compat.v1.experimental.output_all_intermediates\\(True\\).\"):\n        self.evaluate(grad)\n\n  @test_util.run_deprecated_v1\n  @test_util.enable_output_all_intermediates\n  def testCondComputeGradAfterSessRun(self):\n    with self.cached_session():\n      x = constant_op.constant(10.0, name=\"x\")\n      pred = math_ops.less(1, 2)\n\n      def true_fn():\n        a = x * x\n        return a * a\n\n      def false_fn():\n        return x * x\n\n      r = control_flow_ops.cond(pred, true_fn, false_fn)\n\n      self.assertAllEqual(r, 10000.)\n      grad = gradients_impl.gradients(r, [x])[0]\n      self.assertAllEqual(grad, 4000.)\n\n  @test_util.run_deprecated_v1\n  @test_util.enable_output_all_intermediates\n  def testNestedCondComputeGradAfterSessRun(self):\n    with self.cached_session():\n      x = constant_op.constant(10.0, name=\"x\")\n      pred = math_ops.less(1, 2)\n\n      def true_fn():\n\n        def inner_true_fn():\n          a = x * x\n          return a * a\n\n        def inner_false_fn():\n          return x * x\n\n        return control_flow_ops.cond(\n            constant_op.constant(True), inner_true_fn, inner_false_fn)\n\n      def false_fn():\n        return x * x\n\n      r = control_flow_ops.cond(pred, true_fn, false_fn)\n\n      self.assertAllEqual(r, 10000.)\n      grad = gradients_impl.gradients(r, [x])[0]\n      self.assertAllEqual(grad, 4000.)\n\n  @test_util.run_deprecated_v1\n  def testCondGrad_2(self):\n    with self.cached_session():\n      c = array_ops.placeholder(dtypes.int32, shape=[])\n      x = constant_op.constant(10.0)\n      pred = math_ops.less(c, 2)\n      fn1 = lambda: math_ops.multiply(x, 42.0)\n      fn2 = lambda: math_ops.multiply(x, 3.0)\n      r = control_flow_ops.cond(pred, fn1, fn2)\n\n      grad = gradients_impl.gradients(r, [x])[0]\n      self.assertAllEqual(42.0, grad.eval(feed_dict={c: 1}))\n      self.assertAllEqual(3.0, grad.eval(feed_dict={c: 3}))\n\n  @test_util.disable_control_flow_v2(\n      \"b/110550782 (gradient w.r.t external variable)\")\n  @test_util.run_deprecated_v1\n  def testCondGrad_3(self):\n    with self.cached_session():\n      c = array_ops.placeholder(dtypes.int32, shape=[])\n      ox = constant_op.constant(10.0)\n      pred = math_ops.less(c, 2)\n\n      def fn1(x):\n        m = x * x\n        return gradients_impl.gradients(m, [ox])[0]\n\n      fn2 = lambda: math_ops.multiply(ox, 3.0)\n      y = math_ops.multiply(7.0, ox)\n      r = control_flow_ops.cond(pred, lambda: fn1(y), fn2)\n\n      self.assertAllEqual(980.0, r.eval(feed_dict={c: 1}))\n      self.assertAllEqual(30.0, r.eval(feed_dict={c: 3}))\n\n  @test_util.run_deprecated_v1\n  def testCondGradMultiDevice(self):\n    config = config_pb2.ConfigProto(device_count={\"CPU\": 2},\n                                    allow_soft_placement=True)\n    with self.cached_session(use_gpu=True, config=config) as sess:\n      pred = array_ops.placeholder(dtypes.bool, [])\n      x = array_ops.placeholder(dtypes.float32)\n      y = array_ops.placeholder(dtypes.float32)\n\n      with ops.device(\"/cpu:0\"):\n        z = control_flow_ops.cond(pred, lambda: x * y * 2.0, lambda: 2.0)\n\n      with ops.device(\"/cpu:1\"):\n        grad = gradients_impl.gradients(z, x)[0]\n\n      with ops.device(\"/cpu:0\"):\n        grad_grad = gradients_impl.gradients(grad, x)[0]\n\n      self.assertEqual(sess.run(grad, {pred: True, x: 1.0, y: 2.0}), 4.0)\n      self.assertEqual(sess.run(grad, {pred: False, x: 1.0, y: 2.0}), 0.0)\n\n      # v1 control flow gets None second derivative for some reason.\n      if not control_flow_util.ENABLE_CONTROL_FLOW_V2:\n        self.assertIsNone(grad_grad)\n        return\n\n      self.assertEqual(sess.run(grad_grad, {pred: True, x: 1.0, y: 2.0}), 0.0)\n      self.assertEqual(sess.run(grad_grad, {pred: False, x: 1.0, y: 2.0}), 0.0)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testNestedCond_Simple(self):\n    with self.cached_session():\n      x = constant_op.constant(0., name=\"X\")\n      y = control_flow_ops.cond(\n          constant_op.constant(True), lambda: x,\n          lambda: control_flow_ops.cond(x < 1., lambda: x, lambda: x))\n      result = gradients_impl.gradients(y, x)[0]\n      self.assertEqual(1.0, self.evaluate(result))\n\n      z = control_flow_ops.cond(\n          constant_op.constant(False), lambda: x,\n          lambda: control_flow_ops.cond(x < 1., lambda: x, lambda: x))\n      result = gradients_impl.gradients(z, x)[0]\n      self.assertEqual(1.0, self.evaluate(result))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCondGrad_Gather(self):\n    with self.cached_session() as sess:\n      v1 = variables.Variable([1.0, 42.0])\n      c = array_ops.placeholder(dtypes.int32, shape=[])\n      pred = math_ops.less(c, 2)\n      fn1 = lambda: array_ops.identity(v1)\n      fn2 = lambda: array_ops.gather(v1, [1, 1])\n      r = control_flow_ops.cond(pred, fn1, fn2)\n      # The following `grad` is a Tensor since it is the aggregation of an\n      # IndexedSlice and a Tensor. It is an `IndexedSlices` with control flow\n      # v2.\n      grad = gradients_impl.gradients(r, [v1])[0]\n      self.evaluate(variables.global_variables_initializer())\n\n      if control_flow_util.ENABLE_CONTROL_FLOW_V2:\n        self.assertIsInstance(grad, ops.IndexedSlices)\n\n      grad_value = sess.run(grad, feed_dict={c: 1})\n      self.assertAllEqual(gradient_checker_v2._to_numpy(grad_value), [1.0, 1.0])\n\n      grad_value = sess.run(grad, feed_dict={c: 3})\n      self.assertAllEqual(gradient_checker_v2._to_numpy(grad_value), [0.0, 2.0])\n\n  @test_util.run_deprecated_v1\n  def testCondGrad_ResourceVarSparseRead(self):\n    # NOTE(skyewm): this test is interesting because the\n    # ResourceVariable.sparse_read gradient function returns IndexedSlices.\n    var = resource_variable_ops.ResourceVariable(\n        np.ones((4, 2), dtype=np.float32))\n    x = constant_op.constant(1.0)\n    r = control_flow_ops.cond(\n        constant_op.constant(True),\n        lambda: x * math_ops.reduce_sum(var.sparse_read([1, 2])),\n        lambda: constant_op.constant(np.zeros((2, 3)),\n                                     dtype=dtypes.float32))\n    grad = gradients_impl.gradients(r, var)[0]\n\n    self.evaluate(variables.global_variables_initializer())\n    grad_val = self.evaluate(grad)\n    self.assertIsInstance(grad_val, ops.IndexedSlicesValue)\n    self.assertAllEqual(gradient_checker_v2._to_numpy(grad_val), [[0., 0.],\n                                                                  [1., 1.],\n                                                                  [1., 1.],\n                                                                  [0., 0.]])\n\n  def testCondGrad_MultiGather(self):\n    # NOTE(skyewm): this test is interesting because the array_ops.gather and\n    # ResourceVariable.sparse_read gradient functions returns IndexedSlices.\n    var = resource_variable_ops.ResourceVariable(\n        np.ones((4, 2), dtype=np.float32))\n    x1 = constant_op.constant(np.ones((3, 3), dtype=np.float32))\n    x2 = constant_op.constant(2.0)\n\n    def true_fn():\n      y1 = var.sparse_read([1, 2])\n      y2 = array_ops.gather(x1, [2]) * x2\n      y3 = x2 * [1., 1., 1.]\n      return y1, y2, y3\n\n    def false_fn():\n      y1 = np.zeros((2, 2), dtype=np.float32)\n      y2 = array_ops.gather(x1, [2]) * x2\n      y3 = array_ops.gather(x1, [2])\n      return y1, y2, y3\n\n    @def_function.function\n    def foo():\n      r = control_flow_ops.cond(constant_op.constant(True), true_fn, false_fn)\n      return gradients_impl.gradients(r, [var, x1, x2])\n\n    grad = foo()\n    self.evaluate(variables.global_variables_initializer())\n    var_grad, x1_grad, x2_grad = self.evaluate(grad)\n    self.assertIsInstance(var_grad, ops.IndexedSlicesValue)\n    self.assertAllEqual(gradient_checker_v2._to_numpy(var_grad), [[0., 0.],\n                                                                  [1., 1.],\n                                                                  [1., 1.],\n                                                                  [0., 0]])\n    self.assertIsInstance(x1_grad, ops.IndexedSlicesValue)\n    self.assertAllEqual(gradient_checker_v2._to_numpy(x1_grad), [[0., 0., 0.],\n                                                                 [0., 0., 0.],\n                                                                 [2., 2., 2.]])\n    self.assertIsInstance(x1_grad, ops.IndexedSlicesValue)\n    self.assertEqual(gradient_checker_v2._to_numpy(x2_grad), 6.)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCondPredicateTensor(self):\n    \"\"\"Regression test for lowering predicate from non-first output of an op.\"\"\"\n\n    @eager_function.defun\n    def foo():\n      return constant_op.constant(\"foo\"), constant_op.constant(True)\n\n    r = control_flow_ops.cond(foo()[1], lambda: 1.0, lambda: 2.0)\n    self.assertEqual(self.evaluate(r), 1.0)\n\n  @test_util.run_v1_only(\"Tests Session.run() pruning logic.\")\n  def testCondFeedConstantPredicate(self):\n    with self.cached_session() as sess:\n      value = constant_op.constant(37.0)\n      predicate = constant_op.constant(True)\n      cond_output = control_flow_ops.cond(\n          predicate, lambda: constant_op.constant(0.0), lambda: value)\n      result = array_ops.identity(cond_output)\n      self.assertEqual(37.0, sess.run(result, feed_dict={predicate: False}))\n      self.assertEqual(0.0, sess.run(result, feed_dict={predicate: True}))\n      self.assertEqual(0.0, sess.run(result))\n\n  @test_util.run_v1_only(\"Tests Session.run() pruning logic.\")\n  def testCondFeedPlaceholderWithDefaultPredicate(self):\n    with self.cached_session() as sess:\n      value = constant_op.constant(37.0)\n      predicate = array_ops.placeholder_with_default(\n          constant_op.constant(True), [])\n      cond_output = control_flow_ops.cond(\n          predicate, lambda: constant_op.constant(0.0), lambda: value)\n      result = array_ops.identity(cond_output)\n      self.assertAllEqual(37.0, sess.run(result, feed_dict={predicate: False}))\n      self.assertAllEqual(0.0, sess.run(result, feed_dict={predicate: True}))\n      self.assertAllEqual(0.0, sess.run(result))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testCondAutoControlDeps(self):\n    if test_util.is_gpu_available():\n      self.skipTest(\"b/128676188 causes OOM on opensource gpu tests\")\n\n    print_prefix = \"testCondAutoControlDeps: \"\n\n    def branch_fn():\n      enqueue_print_op(\"A\")\n      enqueue_print_op(\"B\")\n      with ops.control_dependencies([enqueue_print_op(\"C\")]):\n        return constant_op.constant(10)\n\n    def build_cond():\n      return control_flow_ops.cond(\n          constant_op.constant(True), branch_fn, lambda: 0)\n\n    def build_nested_cond():\n      return control_flow_ops.cond(\n          constant_op.constant(True), build_cond, lambda: 0)\n\n    # In v1 graph mode, pruning should make only \"C\" print.\n    if not context.executing_eagerly():\n      with self.cached_session():\n        with self.captureWritesToStream(sys.stderr) as printed:\n          self.assertEqual(self.evaluate(build_cond()), 10)\n        self.assertEqual([\"C\"], filter_test_messages(printed.contents()))\n\n        with self.captureWritesToStream(sys.stderr) as printed:\n          self.assertEqual(self.evaluate(build_nested_cond()), 10)\n        self.assertEqual([\"C\"], filter_test_messages(printed.contents()))\n\n    # In defuns, all prints should execute in program order.\n    # This doesn't work with legacy control flow.\n    if control_flow_util.ENABLE_CONTROL_FLOW_V2:\n\n      @eager_function.defun\n      def cond():\n        return build_cond()\n\n      with self.captureWritesToStream(sys.stderr) as printed:\n        self.assertEqual(self.evaluate(cond()), 10)\n      self.assertEqual([\"A\", \"B\", \"C\"],\n                       filter_test_messages(printed.contents()))\n\n      @eager_function.defun\n      def nested_cond():\n        return build_nested_cond()\n\n      with self.captureWritesToStream(sys.stderr) as printed:\n        self.assertEqual(self.evaluate(nested_cond()), 10)\n      self.assertEqual([\"A\", \"B\", \"C\"],\n                       filter_test_messages(printed.contents()))\n\n    # wrap_function should prune.\n    def pruned_cond():\n      return build_cond()\n    pruned_cond = wrap_function.wrap_function(pruned_cond, [])\n\n    with self.captureWritesToStream(sys.stderr) as printed:\n      self.assertEqual(self.evaluate(pruned_cond()), 10)\n    self.assertEqual([\"C\"], filter_test_messages(printed.contents()))\n\n    def pruned_nested_cond():\n      return build_nested_cond()\n    pruned_nested_cond = wrap_function.wrap_function(pruned_nested_cond, [])\n\n    with self.captureWritesToStream(sys.stderr) as printed:\n      self.assertEqual(self.evaluate(pruned_nested_cond()), 10)\n    self.assertEqual([\"C\"], filter_test_messages(printed.contents()))\n\n\n  @test_util.run_in_graph_and_eager_modes\n  def testWhileAutoControlDeps(self):\n    # Legacy while_loop fails this test because it produces deprecation notices\n    # in stderr.\n    if not control_flow_util.ENABLE_CONTROL_FLOW_V2: return\n\n    def cond(i, unused_x):\n      enqueue_print_op(\"A\")\n      return i < 2\n\n    def body(i, x):\n      enqueue_print_op(\"B\")\n      with ops.control_dependencies([enqueue_print_op(\"C\")]):\n        x = array_ops.identity(x)\n      with ops.control_dependencies([enqueue_print_op(\"D\")]):\n        return i + 1, x\n\n    def build_while():\n      return control_flow_ops.while_loop(\n          cond, body, [constant_op.constant(0), constant_op.constant(0)])\n\n    def build_nested_while():\n      return control_flow_ops.cond(\n          constant_op.constant(True), build_while, lambda: [0, 0])\n\n    # In v1 graph mode, pruning should make only \"D\" print.\n    if not context.executing_eagerly():\n      with self.cached_session():\n        with self.captureWritesToStream(sys.stderr) as printed:\n          self.assertEqual(self.evaluate(build_while()[0]), 2)\n        self.assertEqual([\"D\", \"D\"], filter_test_messages(printed.contents()))\n\n        with self.captureWritesToStream(sys.stderr) as printed:\n          self.assertEqual(self.evaluate(build_nested_while()[0]), 2)\n        self.assertEqual([\"D\", \"D\"], filter_test_messages(printed.contents()))\n\n    # In defuns, all prints should execute in program order.\n    @eager_function.defun\n    def while_loop():\n      return build_while()[0]\n\n    with self.captureWritesToStream(sys.stderr) as printed:\n      self.assertEqual(self.evaluate(while_loop()), 2)\n    self.assertEqual([\"A\", \"B\", \"C\", \"D\", \"A\", \"B\", \"C\", \"D\", \"A\"],\n                     filter_test_messages(printed.contents()))\n\n    @eager_function.defun\n    def nested_while_loop():\n      return build_nested_while()[0]\n\n    with self.captureWritesToStream(sys.stderr) as printed:\n      self.assertEqual(self.evaluate(nested_while_loop()), 2)\n    self.assertEqual([\"A\", \"B\", \"C\", \"D\", \"A\", \"B\", \"C\", \"D\", \"A\"],\n                     filter_test_messages(printed.contents()))\n\n    # wrap_function should prune.\n    def pruned_while():\n      return build_while()[0]\n    pruned_while = wrap_function.wrap_function(pruned_while, [])\n\n    with self.captureWritesToStream(sys.stderr) as printed:\n      self.assertEqual(self.evaluate(pruned_while()), 2)\n    self.assertEqual([\"D\", \"D\"], filter_test_messages(printed.contents()))\n\n    def pruned_nested_while():\n      return build_nested_while()[0]\n    pruned_nested_while = wrap_function.wrap_function(pruned_nested_while, [])\n\n    with self.captureWritesToStream(sys.stderr) as printed:\n      self.assertEqual(self.evaluate(pruned_nested_while()), 2)\n    self.assertEqual([\"D\", \"D\"], filter_test_messages(printed.contents()))\n\n  # Microbenchmark: 256,000 iterations/s.\n  def testWhile_1(self):\n    with self.cached_session():\n      n = constant_op.constant(0)\n      c = lambda x: math_ops.less(x, 10000)\n      b = lambda x: math_ops.add(x, 1)\n      r = control_flow_ops.while_loop(c, b, [n], parallel_iterations=20)\n      self.assertEqual(10000, self.evaluate(r))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileExternalControlDependencies(self):\n    with self.cached_session():\n      v = variables.Variable(0.0)\n      self.evaluate(v.initializer)\n      increment = v.assign_add(1.0).read_value()\n\n      def body_fn(i):\n        with ops.control_dependencies([increment]):\n          return i + 1\n\n      result = control_flow_ops.while_loop(cond=lambda i: i < 2,\n                                           body=body_fn, loop_vars=[1])\n      self.assertAllEqual(result, 2)\n      self.assertAllEqual(v.read_value(), 1.0)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileExternalControlDependenciesNoInput(self):\n    with self.cached_session():\n      v = variables.Variable(0.0)\n      self.evaluate(v.initializer)\n      # TODO(apassos): figure out why the reading is necessary here.\n      increment = v.assign_add(1.0).read_value()\n\n      def body_fn(unused_i):\n        with ops.control_dependencies([increment]):\n          return constant_op.constant(5, name=\"five\")\n\n      result = control_flow_ops.while_loop(cond=lambda i: i < 5,\n                                           body=body_fn, loop_vars=[0])\n      self.evaluate(result)\n      self.assertAllEqual(self.evaluate(v), 1.0)\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (RefVariable)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileWithRefs_1(self):\n    with self.cached_session() as sess:\n      x = variables.VariableV1(0)._ref()  # pylint: disable=protected-access\n      i = constant_op.constant(0)\n      c = lambda i, x: math_ops.less(i, 100)\n\n      self.assertEqual(x.dtype, dtypes.int32_ref)\n\n      def b(i, x):\n        self.assertEqual(x.dtype, dtypes.int32_ref)\n        return (i + 1, gen_array_ops.ref_identity(x))\n\n      r = control_flow_ops.while_loop(c, b, [i, x], parallel_iterations=5)\n\n      self.evaluate(variables.global_variables_initializer())\n\n      self.assertEqual(r[0].dtype, dtypes.int32)\n      self.assertEqual(r[1].dtype, dtypes.int32_ref)\n\n      value_i, value_x = self.evaluate(r)\n\n    self.assertEqual(100, value_i)\n    self.assertEqual(0, value_x)\n\n  def testWhile_2(self):\n    with self.cached_session():\n      s = constant_op.constant(0)\n      r = isum(s)\n      self.assertAllEqual(45, self.evaluate(r))\n\n  def testWhileWithMaximumIterations(self):\n    with self.cached_session():\n      s = constant_op.constant([1, 2, 3, 4, 5])\n      r = isum(s, maximum_iterations=3)\n      self.assertAllEqual([1 + 3, 2 + 3, 3 + 3, 4 + 3, 5 + 3], self.evaluate(r))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileWithMaximumIterationsAndSingleArgument(self):\n    with self.cached_session():\n      r = control_flow_ops.while_loop(\n          lambda i: i < 3, lambda i: i + 1, [0], maximum_iterations=1)\n      self.assertEqual(1, self.evaluate(r))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testXLAGradInLoop(self):\n    # We have an optimization that moves certain reduction ops, this test makes\n    # sure we don't do that for XLA ops.\n\n    # Use dynamic inputs, which triggers the creation of \"BroadcastGradientArgs\"\n    # and \"Shape\" op.\n    input1 = array_ops.placeholder(dtype=dtypes.float32, shape=[None, None])\n    input2 = array_ops.placeholder(dtype=dtypes.float32, shape=[None, None])\n    def cond(i1, i2):\n      return False\n\n    def body(i1, i2):\n      return math_ops.add(i1, i2), math_ops.add(i1, i2)\n\n    xla_context = control_flow_ops.XLAControlFlowContext()\n    xla_context.Enter()\n\n    out1, _ = control_flow_ops.while_loop(\n        cond, body, (input1, input2), maximum_iterations=2)\n    g = gradients_impl.gradients(out1, [input1])\n\n    for op in out1.graph.get_operations():\n      # Test that the \"Shape\" is directly passed to BroadcastGradientArgs\n      # instead of being pushed to the stack.\n      if op.type == \"BroadcastGradientArgs\":\n        self.assertEqual(op.inputs[0].op.type, \"Shape\")\n        self.assertEqual(op.inputs[1].op.type, \"Shape\")\n    xla_context.Exit()\n\n\n  @test_util.disable_control_flow_v2(\"b/115776323 (max_iters)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testSingleNestedMaximumIterationsWhileLoopGradientInXLAContext(self):\n    v = constant_op.constant(1.0)\n\n    def training_loop_with_gradient(i):\n      out = control_flow_ops.while_loop(\n          lambda i_, _: i_ < 3,\n          lambda i_, j: [i_ + 1, j * v], [0, 1.0],\n          maximum_iterations=i)\n      g = gradients_impl.gradients(out, v)\n      with ops.control_dependencies(g):\n        return i + 1\n\n    xla_context = control_flow_ops.XLAControlFlowContext()\n    xla_context.Enter()\n    # Create training loop, ensure we can call gradient() of\n    # while_loop inside the training loop.\n    loop = control_flow_ops.while_loop(lambda i: i < 3,\n                                       training_loop_with_gradient, [0])\n    xla_context.Exit()\n\n    loop_execute = array_ops.identity(loop)  # Because loop is not fetchable.\n\n    # Should execute without issue.\n    self.assertEqual(3, self.evaluate(loop_execute))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testInvalidMaximumIterationsWhileLoopGradientInXLAContext(self):\n    if control_flow_util.ENABLE_CONTROL_FLOW_V2:\n      self.skipTest(\"WhileV2 does lazy evaluation of maximum_iterations\")\n    v = constant_op.constant(1.0)\n\n    def inner_body(i, x):\n      out = control_flow_ops.while_loop(\n          lambda i, _: i < 3,\n          lambda i, j: [i + 1, j * v], [0, x],\n          maximum_iterations=i)\n      return out\n\n    def create_while_loop(maximum_iterations=None):\n      return control_flow_ops.while_loop(\n          lambda i, _: i < 3,\n          inner_body, [0, 1.0],\n          maximum_iterations=maximum_iterations)\n\n    loop_no_xla = create_while_loop(maximum_iterations=5)\n    # maximum_iterations is fine outside of an XLA scope\n    gs = gradients_impl.gradients(loop_no_xla, v)\n    self.evaluate(gs)  # This should execute without error.\n\n    xla_context = control_flow_ops.XLAControlFlowContext()\n    xla_context.Enter()\n    loop_no_maxiter = create_while_loop()\n    loop_with_maxiter = create_while_loop(maximum_iterations=2)\n    xla_context.Exit()\n\n    with self.assertRaisesRegex(\n        ValueError,\n        r\"Cannot create a gradient accumulator for tensor '.+' inside \"\n        r\"XLA while_loop because maximum_iterations was not passed to \"\n        r\"the tf.while_loop call \\('.+'\\).\"):\n      _ = gradients_impl.gradients(loop_no_maxiter, v)\n\n    with self.assertRaisesRegex(\n        ValueError,\n        r\"Cannot create a gradient accumulator for tensor '.+' inside XLA \"\n        r\"while_loop. maximum_iterations tensor '.+' for while_loop context \"\n        r\"'.+' must be statically known \\(e.g. a constant value or known \"\n        r\"shape dimension\\), or be defined at or outside the while loop \"\n        r\"context '.*' \\(currently defined in '.*'\\)\"):\n      _ = gradients_impl.gradients(loop_with_maxiter, v)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testInvalidMaximumIterationsFromSiblingContextWhileLoopInXLAContext(self):\n    v = constant_op.constant(1.0)\n\n    def create_while_loop():\n      max_iter_holder = []\n\n      def create_mi():\n        max_iter_holder.append(array_ops.placeholder(dtypes.int32, shape=()))\n        return 1.0\n\n      _ = control_flow_ops.cond(\n          constant_op.constant(True), create_mi, create_mi)\n\n      return control_flow_ops.while_loop(\n          lambda i, _: i < 3,\n          lambda i, x: (i + 1, v * x), (0, 1.0),\n          maximum_iterations=max_iter_holder[0])\n\n    if control_flow_util.ENABLE_CONTROL_FLOW_V2:\n      xla_context = control_flow_ops.XLAControlFlowContext()\n      xla_context.Enter()\n      with self.assertRaisesRegex(ValueError, r\"must be from the same graph.*\"):\n        loop = create_while_loop()\n      xla_context.Exit()\n    else:\n      xla_context = control_flow_ops.XLAControlFlowContext()\n      xla_context.Enter()\n      loop = create_while_loop()\n      xla_context.Exit()\n      with self.assertRaisesRegex(\n          ValueError,\n          r\"Cannot create a gradient accumulator for tensor '.+' inside XLA \"\n          r\"while_loop. maximum_iterations tensor '.*Placeholder:0' for \"\n          r\"while_loop context '.+' must be statically known \\(e.g. a constant \"\n          r\"value or known shape dimension\\), or be defined at or outside the \"\n          r\"while loop context '' \\(currently defined in 'cond/.+'\\)\"):\n        _ = gradients_impl.gradients(loop, v)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testNestedWhileLoopWithMaxItersFromOuterContextInXLAContext(self):\n    if test_util.is_gpu_available():\n      self.skipTest(\"b/128646372, b/128645947 fails in opensource build\")\n\n    v = constant_op.constant(1.0)\n\n    p = array_ops.placeholder(dtype=dtypes.int32)\n\n    def mid_body_builder(iterations):\n\n      def mid_body(i, x):\n        r = control_flow_ops.while_loop(\n            lambda *_: True,\n            lambda i, x: (i + 1, v * x), (0, x),\n            maximum_iterations=iterations,\n            name=\"inner\")\n        return (i + 1, gradients_impl.gradients(x + r[1], v)[0])\n\n      return mid_body\n\n    def outer_body(i, x):\n      iterations = array_ops.size(p, name=\"iterations\")\n      return (i + 1, x + control_flow_ops.while_loop(\n          lambda *_: True,\n          mid_body_builder(iterations), (0, x),\n          maximum_iterations=iterations,\n          name=\"mid\")[1])\n\n    def create_while_loop():\n      with ops.device(\"/cpu:0\"):\n        r = control_flow_ops.while_loop(\n            lambda *_: True,\n            outer_body, (0, 1.0),\n            maximum_iterations=5,\n            name=\"outer\")\n        return array_ops.identity(r[1])\n\n    xla_context = control_flow_ops.XLAControlFlowContext()\n    xla_context.Enter()\n    final_with_xla_context = create_while_loop()\n    xla_context.Exit()\n\n    final_without_xla_context = create_while_loop()\n\n    with self.session(use_gpu=False) as sess:\n      opts = config_pb2.RunOptions(trace_level=config_pb2.RunOptions.FULL_TRACE)\n      run_metadata_without_xla_context = config_pb2.RunMetadata()\n      run_metadata = config_pb2.RunMetadata()\n\n      final_value_without_xla_context = sess.run(\n          final_without_xla_context,\n          feed_dict={p: [0, 0, 0]},\n          options=opts,\n          run_metadata=run_metadata_without_xla_context)\n\n      final_value_with_xla_context = sess.run(\n          final_with_xla_context,\n          feed_dict={p: [0, 0, 0]},\n          options=opts,\n          run_metadata=run_metadata)\n\n      if control_flow_util.ENABLE_CONTROL_FLOW_V2:\n        # With while_v2 on xla, run_metadata only contains the unlowered While\n        # op so node_stats does not have statistics for the pushes. So as a\n        # loose check we check the pushes in the lowered version.\n        for dev in run_metadata_without_xla_context.step_stats.dev_stats:\n          if \"/device:CPU\" in dev.device:\n            node_stats = dev.node_stats\n        stack_push_count = len([\n            x for x in node_stats\n            if re.match(r\".*TensorListPushBack_?\\d*\", x.node_name)\n        ])\n      else:\n        for dev in run_metadata.step_stats.dev_stats:\n          if \"/device:CPU\" in dev.device:\n            node_stats = dev.node_stats\n        stack_push_op = \"StackPushV2\"\n        stack_push_count = len(\n            [x for x in node_stats if x.node_name.endswith(\"StackPushV2\")])\n      # Pushes to the stack = product of maximum_iterations values;\n      # the last two \"3\"s comes from size(p), when p == [0, 0, 0].\n      self.assertEqual(stack_push_count, 5 * 3 * 3, str(node_stats))\n\n      self.assertAllClose(final_value_with_xla_context,\n                          final_value_without_xla_context)\n\n  # Have more than 10 parallel iterations and hence exercise k-bound\n  # most of the time.\n  @test_util.run_deprecated_v1\n  def testWhile_3(self):\n    with self.cached_session():\n\n      def compute(i, m, c, o):\n        m, c = [math_ops.add(m, 1), math_ops.add(c, 1)]\n        o = math_ops.add(o, m)\n        o = math_ops.add(o, c)\n        i = math_ops.add(i, 1)\n        return [i, m, c, o]\n\n      i = ops.convert_to_tensor(0)\n      m = ops.convert_to_tensor(0)\n      c = ops.convert_to_tensor(0)\n      o = ops.convert_to_tensor(0)\n      d = ops.convert_to_tensor(100)\n      r = control_flow_ops.while_loop(lambda i, m, c, o: math_ops.less(i, d),\n                                      compute, [i, m, c, o])\n      result = r[3]\n    self.assertAllEqual(10100, result)\n\n  @test_util.run_deprecated_v1\n  def testWhile_4(self):\n    with self.cached_session():\n\n      def compute(i, m, c, o):\n        m, c = [array_ops.gather(x, i), array_ops.gather(x, i)]\n        o = math_ops.add(o, m)\n        o = math_ops.add(o, c)\n        i = math_ops.add(i, 1)\n        return [i, m, c, o]\n\n      i = ops.convert_to_tensor(0)\n      m = ops.convert_to_tensor(0)\n      c = ops.convert_to_tensor(0)\n      o = ops.convert_to_tensor(0)\n      x = ops.convert_to_tensor([1, 2, 3, 4, 5, 6])\n      s = array_ops.size(x)\n      r = control_flow_ops.while_loop(lambda i, m, c, o: math_ops.less(i, s),\n                                      compute, [i, m, c, o])\n      result = r[3]\n    self.assertAllEqual(42, result)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhile_5(self):\n    with self.cached_session():\n\n      def compute(i, c, o):\n        c = array_ops.strided_slice(x, array_ops.expand_dims(i, 0),\n                                    [1] + array_ops.expand_dims(i, 0))\n        o = array_ops.concat([o, c], 0)\n        i = math_ops.add(i, 1)\n        return [i, c, o]\n\n      i = ops.convert_to_tensor(0)\n      c = ops.convert_to_tensor([0])\n      o = ops.convert_to_tensor([0])\n      x = ops.convert_to_tensor([1, 2, 3, 4, 5, 6])\n      s = array_ops.size(x)\n      r = control_flow_ops.while_loop(lambda i, c, o: math_ops.less(i, s),\n                                      compute, [i, c, o], [\n                                          i.get_shape(),\n                                          tensor_shape.unknown_shape(),\n                                          tensor_shape.unknown_shape()\n                                      ])\n      result = r[2]\n    self.assertAllEqual(np.array([0, 1, 2, 3, 4, 5, 6]), result)\n\n  @test_util.run_gpu_only\n  @test_util.run_deprecated_v1\n  def testWhile_Device(self):\n\n    # Body function defined outside of device scope\n    def body(x):\n      return math_ops.exp(x)\n\n    with ops.device(\"CPU:0\"):\n      r = control_flow_ops.while_loop(\n          lambda x: x < 10, body, [constant_op.constant(-10.)])\n      self.assertIn(\"cpu\", r.device.lower())\n\n    with session.Session() as sess:\n      options = config_pb2.RunOptions(output_partition_graphs=True)\n      run_metadata = config_pb2.RunMetadata()\n      sess.run(r, options=options, run_metadata=run_metadata)\n      # We expect that everything runs on CPU, even if GPU is available.\n      self.assertEqual(len(run_metadata.partition_graphs), 1)\n\n  @test_util.disable_control_flow_v2(\"b/116338794 (buffer_reuse)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testBufferForwarding(self):\n    run_options = config_pb2.RunOptions(\n        trace_level=config_pb2.RunOptions.FULL_TRACE)\n    run_metadata = config_pb2.RunMetadata()\n\n    with self.cached_session() as sess:\n      with ops.device(\"/cpu:0\"):\n        c = constant_op.constant(2)\n        i0 = constant_op.constant(0)\n        r = control_flow_ops.while_loop(lambda i: i < 1000,\n                                        lambda i: math_ops.square(c) + i, [i0])\n      r_val = sess.run(r, options=run_options, run_metadata=run_metadata)\n      self.assertEqual(1000, r_val)\n      self.assertTrue(run_metadata.HasField(\"step_stats\"))\n      unique_allocs = set()\n      for node_stat in run_metadata.step_stats.dev_stats[0].node_stats:\n        for output in node_stat.output:\n          unique_allocs.add(\n              output.tensor_description.allocation_description.ptr)\n      # Prior to cl/147536680, the number of unique allocations was about 1005.\n      self.assertLess(len(unique_allocs), 756)\n\n  def _testWhile_Gpu_1(self, use_gpu):\n    with self.cached_session(use_gpu=use_gpu):\n      n = constant_op.constant(1.0)\n      c = lambda x: math_ops.less(x, 10.0)\n      b = lambda x: math_ops.add(x, 1.0)\n      r = control_flow_ops.while_loop(c, b, [n])\n      self.assertAllClose(10.0, self.evaluate(r))\n\n  def testWhile_Gpu_1(self):\n    self._testWhile_Gpu_1(use_gpu=False)\n    self._testWhile_Gpu_1(use_gpu=True)\n\n  def _testWhile_Gpu_2(self, use_gpu):\n    with self.cached_session(use_gpu=use_gpu):\n      n = constant_op.constant(1.0)\n      c = lambda x: math_ops.less(x, 10.0)\n\n      def b(x):\n        with ops.device(\"/cpu:0\"):\n          return math_ops.add(x, 1.0)\n\n      r = control_flow_ops.while_loop(c, b, [n])\n      self.assertAllClose(10.0, self.evaluate(r))\n\n  def testWhile_Gpu_2(self):\n    self._testWhile_Gpu_2(use_gpu=False)\n    self._testWhile_Gpu_2(use_gpu=True)\n\n  def testWhileShape(self):\n    with self.cached_session():\n      i = constant_op.constant(0)\n      m = array_ops.ones([2, 2])\n      c = lambda i, j: math_ops.less(i, 2)\n\n      def _b(i, j):\n        new_i = math_ops.add(i, 1)\n        new_j = array_ops.tile(j, [2, 2])\n        return [new_i, new_j]\n\n      r = control_flow_ops.while_loop(\n          c, _b, [i, m],\n          [i.get_shape(), tensor_shape.unknown_shape()])\n      r = r[1] * array_ops.ones([8, 8])\n      self.assertAllEqual(np.ones((8, 8)), self.evaluate(r))\n\n  @test_util.disable_control_flow_v2(\"b/131265085\")\n  @test_util.run_v1_only(\"b/131265085\")\n  def testWhileBadShape(self):\n    x = constant_op.constant([2.0, 4.0], name=\"values\")\n    i = constant_op.constant(0)\n    c = lambda i, _: math_ops.less(i, 10)\n    b = lambda i, x: [i + 1, x + 1]\n    with self.assertRaisesRegex(ValueError, \"is not compatible with\"):\n      # Shape of x is [2], but we specify a shape of [5].\n      control_flow_ops.while_loop(\n          c, b, [i, x], [i.shape, tensor_shape.TensorShape([5])])\n\n  @test_util.run_in_graph_and_eager_modes\n  def testWhileBadBodyReturn(self):\n    x = constant_op.constant([2.0, 4.0], name=\"values\")\n    i = constant_op.constant(0)\n    c = lambda i, *x: math_ops.less(i, 10)\n\n    # body accepts N values and returns N+1 values.\n    b = lambda i, *x: (i, i) + x\n\n    with self.assertRaisesRegex(\n        ValueError, \"The two structures don't have the same nested structure.\"):\n      control_flow_ops.while_loop(c, b, [i, x])\n\n  @test_util.run_deprecated_v1\n  def testWhileWithNonTensorInput_Scalar(self):\n    with self.cached_session():\n      n = 0\n      c = lambda x: x < 10000\n      b = lambda x: x + 1\n      r = control_flow_ops.while_loop(c, b, [n], parallel_iterations=20)\n      self.assertEqual(10000, self.evaluate(r))\n\n  def testWhileWithNonTensorInput_Vector(self):\n    with self.cached_session():\n      n = np.array([0])  # Note, [0] would not work here; that is a list\n      c = lambda x: x[0] < 10000\n      b = lambda x: array_ops.stack([x[0] + 1])\n      r = control_flow_ops.while_loop(c, b, [n], parallel_iterations=20)\n      self.assertEqual([10000], self.evaluate(r))\n\n  def testWhileShapeInference(self):\n    with self.cached_session():\n      i = constant_op.constant(0)\n      m = array_ops.ones([2, 2])\n      c = lambda i, j: math_ops.less(i, 2)\n\n      def b(i, j):\n        new_i = math_ops.add(i, 1)\n        new_j = array_ops.concat([j, j], 0)\n        return [new_i, new_j]\n\n      r = control_flow_ops.while_loop(\n          c, b, [i, m],\n          [i.get_shape(), tensor_shape.TensorShape([None, 2])])\n      self.assertTrue(r[1].shape.is_compatible_with([8, 2]))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileShapeInferenceBadShape(self):\n    with self.cached_session():\n      i = constant_op.constant(0)\n      m = array_ops.ones([2, 2])\n      c = lambda i, j: math_ops.less(i, 2)\n      b = lambda i, j: [i + 1, array_ops.concat([j, j], 0)]\n      with self.assertRaisesRegex(\n          ValueError,\n          r\"Input tensor 'ones:0' enters the loop with shape \\(2, 2\\), but has \"\n          r\"shape \\(4, 2\\) after one iteration. To allow the shape to vary \"\n          r\"across iterations, use the `shape_invariants` argument of \"\n          r\"tf.while_loop to specify a less-specific shape.\"):\n        control_flow_ops.while_loop(c, b, [i, m])\n\n  def testWhileShapeInferenceSparseTensor(self):\n    values = constant_op.constant([2.0, 4.0], name=\"values\")\n    indices = constant_op.constant([[0], [3]],\n                                   dtype=dtypes.int64,\n                                   name=\"indices\")\n    shape = constant_op.constant([10], dtype=dtypes.int64, name=\"dense_shape\")\n    i = constant_op.constant(0)\n    x = sparse_tensor.SparseTensor(indices, values, dense_shape=shape)\n\n    def c(i, _):\n      return i < 10\n\n    def b1(i, x):  # modifies values.  (shape of components is not changed.)\n      return [\n          i + 1,\n          sparse_tensor.SparseTensor(x.indices, x.values * 2.0, x.dense_shape)\n      ]\n\n    def b2(i, x):  # adds new values.  (shape of components is changed.)\n      return [\n          i + 1,\n          sparse_ops.sparse_add(\n              x,\n              sparse_tensor.SparseTensor(\n                  indices=math_ops.cast(\n                      array_ops.fill([1, 1], i), dtypes.int64),\n                  values=array_ops.fill([1], 1.0),\n                  dense_shape=x.dense_shape))\n      ]\n\n    def b3(i, x):  # modifies rank.  (shape of all components is changed.)\n      return [\n          i + 1,\n          sparse_tensor.SparseTensor(\n              array_ops.concat([x.indices, [[i], [i]]], axis=1), x.values * 2.0,\n              array_ops.concat([x.dense_shape, [10]], axis=0))\n      ]\n\n    def check_shapes(r, indices, values, dense_shape):\n      self.assertTrue(r.indices.shape.is_compatible_with(indices))\n      self.assertTrue(r.values.shape.is_compatible_with(values))\n      self.assertTrue(r.dense_shape.shape.is_compatible_with(dense_shape))\n\n    # Default shape invariant; b1 only modifies values.\n    _, r = control_flow_ops.while_loop(c, b1, [i, x])\n    check_shapes(r, indices=[None, 1], values=[None], dense_shape=[1])\n\n    # Default shape invariant; b2 adds new values\n    _, r = control_flow_ops.while_loop(c, b2, [i, x])\n    check_shapes(r, indices=[None, 1], values=[None], dense_shape=[1])\n\n    # Explicit shape invariant, allowing any rank; b1 only modifies values.\n    _, r = control_flow_ops.while_loop(\n        c, b1, [i, x],\n        [i.get_shape(), tensor_shape.TensorShape([None])])\n    check_shapes(r, indices=[None, None], values=[None], dense_shape=[None])\n\n    # Explicit shape invariant, allowing any rank; b3 modifies rank.\n    _, r = control_flow_ops.while_loop(\n        c, b3, [i, x],\n        [i.get_shape(), tensor_shape.TensorShape([None])])\n    check_shapes(r, indices=[None, None], values=[None], dense_shape=[None])\n\n    # Shape invariant with ndims=None.  Technically, this isn't supported\n    # according to the docs, but we support it for backwards compatibility.\n    _, r = control_flow_ops.while_loop(\n        c, b1, [i, x],\n        [i.get_shape(), tensor_shape.TensorShape(None)])\n    check_shapes(r, indices=[None, None], values=[None], dense_shape=[None])\n    _, r = control_flow_ops.while_loop(\n        c, b3, [i, x],\n        [i.get_shape(), tensor_shape.TensorShape(None)])\n    check_shapes(r, indices=[None, None], values=[None], dense_shape=[None])\n\n  @test_util.disable_control_flow_v2(\"b/131265085\")\n  @test_util.run_v1_only(\"b/131265085\")\n  def testWhileBadShapeSparseTensor(self):\n    values = constant_op.constant([2.0, 4.0], name=\"values\")\n    indices = constant_op.constant([[0], [3]],\n                                   dtype=dtypes.int64,\n                                   name=\"indices\")\n    shape = constant_op.constant([10], dtype=dtypes.int64, name=\"dense_shape\")\n    i = constant_op.constant(0)\n    x = sparse_tensor.SparseTensor(indices, values, dense_shape=shape)\n    c = lambda i, _: i < 10\n    b1 = lambda i, x: [i+1, x]\n    def b2(i, x):  # modifies rank.  (shape of all components is changed.)\n      return [\n          i + 1,\n          sparse_tensor.SparseTensor(\n              array_ops.concat([x.indices, [[i], [i]]], axis=1), x.values * 2.0,\n              array_ops.concat([x.dense_shape, [10]], axis=0))\n      ]\n\n    # Explicit shape invariant, with a specific (incompatible) rank.\n    with self.assertRaisesRegex(ValueError, \"is not compatible with\"):\n      control_flow_ops.while_loop(\n          c, b1, [i, x],\n          [i.get_shape(), tensor_shape.TensorShape([5])])\n\n    # Default shape invariant, but b2 modifies rank (which is not allowed).\n    with self.assertRaises(ValueError):\n      control_flow_ops.while_loop(c, b2, [i, x])\n\n  def testWhileShapeInferenceIndexedSlices(self):\n    with self.cached_session():\n      values = constant_op.constant([[2.0, 4.0], [3.0, 5.0]], name=\"values\")\n      indices = constant_op.constant([0, 3], name=\"indices\")\n      shape = constant_op.constant([10, 2], name=\"dense_shape\")\n      i = constant_op.constant(0)\n      x = ops.IndexedSlices(values, indices, dense_shape=shape)\n\n      def c(i, _):\n        return i < 10\n\n      def b(i, x):\n        return [\n            i + 1,\n            ops.IndexedSlices(x.values * 2.0, x.indices, x.dense_shape)\n        ]\n\n      _, r = control_flow_ops.while_loop(c, b, [i, x])\n      self.assertEqual(r.dense_shape.get_shape()[0], 2)\n      self.assertEqual(r.values.get_shape(), tensor_shape.TensorShape([2, 2]))\n\n      _, r = control_flow_ops.while_loop(\n          c, b, [i, x],\n          [i.get_shape(), tensor_shape.TensorShape([None, 2])])\n      self.assertEqual(r.dense_shape.get_shape()[0], 2)\n      self.assertTrue(r.values.get_shape().is_compatible_with([None, 2]))\n\n  @test_util.disable_control_flow_v2(\"b/131265085\")\n  @test_util.run_v1_only(\"b/131265085\")\n  def testWhileBadShapeIndexedSlices(self):\n    values = constant_op.constant([2.0, 4.0], name=\"values\")\n    indices = constant_op.constant([[0], [3]],\n                                   dtype=dtypes.int64,\n                                   name=\"indices\")\n    shape = constant_op.constant([10], dtype=dtypes.int64, name=\"dense_shape\")\n    i = constant_op.constant(0)\n    x = sparse_tensor.SparseTensor(indices, values, dense_shape=shape)\n    c = lambda i, _: 10\n    b = lambda i, x: [i+1, x]\n\n    # Explicit shape invariant, with a specific (incompatible) rank.\n    with self.assertRaisesRegex(ValueError, \"is not compatible with\"):\n      control_flow_ops.while_loop(\n          c, b, [i, x],\n          [i.get_shape(), tensor_shape.TensorShape([5])])\n\n  def testWhileShapeInferenceRaggedTensor(self):\n    i = constant_op.constant(0)\n    x = ragged_factory_ops.constant([[1, 2], [3], [4, 5, 6]])\n    c = lambda i, _: i < 10\n\n    def b1(i, x):  # Adds new values to rows (but doesn't create new rows)\n      return [\n          i + 1,\n          array_ops.concat([x, x], axis=1)\n      ]\n\n    def b2(i, x):  # Adds new rows.\n      return [\n          i + 1,\n          array_ops.concat([x, x], axis=0)\n      ]\n\n    def check_shapes(r, values, splits):\n      self.assertTrue(r.values.shape.is_compatible_with(values))\n      self.assertTrue(r.row_splits.shape.is_compatible_with(splits))\n\n    # Default shape invariant; b1 adds new values to rows.\n    _, r = control_flow_ops.while_loop(c, b1, [i, x])\n    check_shapes(r, values=[None], splits=[4])\n\n    # Default shape invariant; b2 adds new rows (not allowed).\n    if not context.executing_eagerly():\n      with self.assertRaises(ValueError):\n        _, r = control_flow_ops.while_loop(c, b2, [i, x])\n\n    # Explicit shape invariant; b1 adds new values to rows.\n    # (deprecated: use TensorShape instead of RaggedTensorSpec)\n    _, r = control_flow_ops.while_loop(\n        c, b1, [i, x],\n        [i.get_shape(), tensor_shape.TensorShape([None, None])])\n    check_shapes(r, values=[None], splits=[None])\n\n    # Explicit shape invariant; b1 adds new values to rows.\n    _, r = control_flow_ops.while_loop(\n        c, b1, [i, x],\n        [i.get_shape(), ragged_tensor.RaggedTensorSpec([None, None],\n                                                       dtypes.int32)])\n    check_shapes(r, values=[None], splits=[None])\n\n    # Explicit shape invariant; b2 adds new rows.\n    _, r = control_flow_ops.while_loop(\n        c, b2, [i, x],\n        [i.get_shape(), ragged_tensor.RaggedTensorSpec([None, None],\n                                                       dtypes.int32)])\n    check_shapes(r, values=[None], splits=[None])\n\n  def testWhileShapeInferenceRaggedTensorRaggedRank2(self):\n    i = constant_op.constant(0)\n    x = ragged_factory_ops.constant([[[1, 2], [3], [4, 5, 6]],\n                                     [[], [8, 9, 10]]])\n    c = lambda i, _: i < 10\n    def b(i, x):\n      return [\n          i + 1,\n          array_ops.concat([x, x[..., i:i+1]], axis=-1)\n      ]\n    _, r = control_flow_ops.while_loop(c, b, [i, x])\n    self.assertEqual(r.row_splits.shape.as_list(), [3])\n    self.assertTrue(r.values.row_splits.shape.as_list() in ([6], [None]))\n    self.assertTrue(r.values.values.shape.as_list() in ([49], [None]))\n\n  def testWhileShapeInvariantTensorSpec(self):\n    i = constant_op.constant(0)\n    x = constant_op.constant([1])\n    c = lambda i, _: i < 10\n    b = lambda i, x: (i + 1, array_ops.stack([x, x]))\n    shape_invariants = [\n        tensor_spec.TensorSpec([], dtype=dtypes.int32),\n        tensor_spec.TensorSpec(None, dtype=dtypes.int32)]\n    control_flow_ops.while_loop(c, b, [i, x], shape_invariants)\n\n  # TODO(b/131265085) Remove this decorator when bug is fixed.\n  @test_util.build_as_function_and_v1_graph\n  def testWhileShapeInvariantWrongTypeSpecType(self):\n    c = lambda i, _: i < 10\n    b = lambda i, x: (i + 1, x)\n    i = constant_op.constant(0)\n    x = sparse_tensor.SparseTensor([[0]], [1.0], [10])\n    shape_invariants = [\n        tensor_spec.TensorSpec([], dtype=dtypes.int32),\n        sparse_tensor.SparseTensorSpec([None])]\n    control_flow_ops.while_loop(c, b, [i, x], shape_invariants)\n\n    x2 = constant_op.constant([1])\n    with self.assertRaises(TypeError):\n      control_flow_ops.while_loop(c, b, [i, x2], shape_invariants)\n\n    x3 = ragged_factory_ops.constant([[1, 2], [3]])\n    with self.assertRaises(TypeError):\n      control_flow_ops.while_loop(c, b, [i, x3], shape_invariants)\n\n    i2 = constant_op.constant(0.0)\n    with self.assertRaises(TypeError):\n      control_flow_ops.while_loop(c, b, [i2, x], shape_invariants)\n\n  # TODO(b/131265085) Remove this decorator when bug is fixed.\n  @test_util.build_as_function_and_v1_graph\n  def testWhileShapeInvariantBadType(self):\n    i = constant_op.constant(0)\n    x = constant_op.constant([1])\n    c = lambda i, _: i < 10\n    b = lambda i, x: (i + 1, x)\n    with self.assertRaises((ValueError, TypeError)):\n      control_flow_ops.while_loop(c, b, [i, x], [\"foo\", \"bar\"])\n\n  def _testNestedWhile_1(self, use_gpu):\n    with self.cached_session(use_gpu=use_gpu):\n      n = constant_op.constant(0)\n\n      def cpu_sum(s):\n        c = lambda i, s: math_ops.less(i, 10)\n\n        def b(i, s):\n          i1 = math_ops.add(i, 1)\n          with ops.device(\"/cpu:0\"):\n            s1 = math_ops.add(i, s)\n          return i1, s1\n\n        _, r_s = control_flow_ops.while_loop(c, b, [n, s])\n        return r_s\n\n      c = lambda x: math_ops.less(x, 200)\n      b = lambda x: math_ops.add(x, cpu_sum(n))\n      r = control_flow_ops.while_loop(c, b, [n])\n      self.assertEqual(225, self.evaluate(r))\n\n  def testNestedWhile_1(self):\n    self._testNestedWhile_1(use_gpu=False)\n    self._testNestedWhile_1(use_gpu=True)\n\n  def _testNestedWhile_2(self, use_gpu):\n    # Test the cases that A -> Enter and Exit -> A are partitioned.\n    with self.cached_session(use_gpu=use_gpu):\n      s0 = constant_op.constant(2.0)\n\n      def inner_loop(s):\n        c = lambda s: math_ops.less(s, 20.0)\n\n        def b(s):\n          s1 = math_ops.add(s, s)\n          return s1\n\n        r_s = control_flow_ops.while_loop(c, b, [s], parallel_iterations=1)\n        return r_s\n\n      outer_c = lambda x: math_ops.less(x, 3000.0)\n\n      def outer_b(x):\n        x = logging_ops.Print(x, [x])  # Edge \"Print -> Enter\" is partitioned\n        x = inner_loop(x)\n        with ops.device(\"/cpu:0\"):\n          x = math_ops.square(x)  # Edge \"Exit -> Square\" is partitioned\n        return x\n\n      r = control_flow_ops.while_loop(\n          outer_c, outer_b, [s0], parallel_iterations=1)\n      self.assertEqual(1048576.0, self.evaluate(r))\n\n  def testNestedWhile_2(self):\n    self._testNestedWhile_2(use_gpu=False)\n    self._testNestedWhile_2(use_gpu=True)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileWithControl_1(self):\n    with self.cached_session():\n      n = constant_op.constant(0)\n      r = constant_op.constant(0)\n      condition = lambda n_, r_: math_ops.less(n_, 10)\n\n      def body(n_, r_):\n        n_ = math_ops.add(n_, 1)\n        with r_.graph.control_dependencies([r_]):\n          r_ = constant_op.constant(12)\n        return [n_, r_]\n\n      res = control_flow_ops.while_loop(\n          condition, body, [n, r], parallel_iterations=1)\n      self.assertAllEqual(12, res[1])\n\n  @test_util.run_deprecated_v1\n  def testWhileWithControl_2(self):\n    with self.cached_session():\n      r = constant_op.constant(0)\n      condition = lambda r_: math_ops.less(r_, 10)\n\n      def body(r_):\n        with r_.graph.control_dependencies([r_]):\n          r_ = constant_op.constant(12)\n        return [r_]\n\n      res = control_flow_ops.while_loop(\n          condition, body, [r], parallel_iterations=1)\n      self.assertAllEqual(12, self.evaluate(res))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileWithControl_3(self):\n    with self.cached_session() as sess:\n      b = array_ops.placeholder(dtypes.bool)\n      c = constant_op.constant(1)\n      x0 = constant_op.constant(0)\n      with ops.control_dependencies([b]):\n        r = control_flow_ops.while_loop(lambda x: x < 10, lambda x: x + c, [x0])\n      self.assertEqual(10, sess.run(r, {b: True}))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileWithControl_4(self):\n    with self.cached_session() as sess:\n      b = array_ops.placeholder(dtypes.bool)\n      c = constant_op.constant(1)\n      x0 = constant_op.constant(0)\n      with ops.control_dependencies([b]):\n        r = control_flow_ops.while_loop(\n            lambda x: x < 10, lambda x: x + array_ops.identity(c), [x0])\n      self.assertEqual(10, sess.run(r, {b: True}))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileWithControl_5(self):\n    with self.cached_session() as sess:\n      b = array_ops.placeholder(dtypes.bool)\n      c = constant_op.constant(1)\n      x0 = constant_op.constant(0)\n\n      def body(x):\n        with ops.control_dependencies([b]):\n          return x + c\n\n      r = control_flow_ops.while_loop(lambda x: x < 10, body, [x0])\n      self.assertEqual(10, sess.run(r, {b: True}))\n\n  def testWhileCondWithControl(self):\n    # Ensure that no control edges by an outer control dependency context are\n    # added to nodes inside cond/while contexts.\n    with self.cached_session() as sess:\n      const_true = lambda: constant_op.constant(True)\n      const_false = lambda: constant_op.constant(False)\n      cond = lambda i: control_flow_ops.cond(i > 0, const_true, const_false)\n      body = lambda i: control_flow_ops.cond(i > 0, lambda: i - 1, lambda: i)\n\n      with ops.control_dependencies([control_flow_ops.no_op()]):\n        loop = control_flow_ops.while_loop(cond, body,\n                                           (constant_op.constant(5),))\n      self.assertEqual(0, self.evaluate(loop))\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (ref vars)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileCondWithControl_1(self):\n    with self.cached_session():\n      v = variable_scope.get_variable(\n          \"v\", [], initializer=init_ops.constant_initializer(2))\n      i0 = constant_op.constant(0)\n      with ops.control_dependencies([i0]):\n\n        def loop_condition(i):\n          return i < 4\n\n        def loop_body(i):\n          some_cond = control_flow_ops.cond(\n              constant_op.constant(True),\n              lambda: state_ops.assign(v, math_ops.square(v)), lambda: v)\n          with ops.control_dependencies([some_cond]):\n            return i + 1\n\n      r = control_flow_ops.while_loop(loop_condition, loop_body, (i0,))\n      self.evaluate(variables.global_variables_initializer())\n      self.assertEqual(4, self.evaluate(r))\n      self.assertAllClose(65536.0, self.evaluate(v))\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (ref vars)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileCondExitControl(self):\n\n    with self.cached_session():\n      v = variables.Variable(1)\n\n      def false_branch():\n        cond = lambda i: i < 100\n\n        def body(i):\n          x = state_ops.assign(v, i)\n          return x + 1\n\n        loop = control_flow_ops.while_loop(cond, body, [0])\n        # Make sure to handle correctly control edge from Exit to a node.\n        with ops.control_dependencies([loop]):\n          return constant_op.constant(6.0)\n\n      r = control_flow_ops.cond(\n          constant_op.constant(False), lambda: constant_op.constant(1.0),\n          false_branch)\n      self.evaluate(variables.global_variables_initializer())\n      self.assertEqual(6.0, self.evaluate(r))\n      self.assertEqual(99, self.evaluate(v))\n\n  def testCondWhile_1(self):\n\n    with self.cached_session():\n      n = ops.convert_to_tensor(0, name=\"n\")\n      c = lambda x: math_ops.less(x, 10)\n      b = lambda x: math_ops.add(x, 1)\n      r = control_flow_ops.cond(\n          math_ops.less(0, 1), lambda: control_flow_ops.while_loop(c, b, [n]),\n          lambda: n)\n      self.assertAllEqual(10, self.evaluate(r))\n\n  def testCondWhile_2(self):\n\n    with self.cached_session():\n      n = ops.convert_to_tensor(0)\n      c = lambda x: math_ops.less(x, 10)\n      b = lambda x: math_ops.add(x, 1)\n      r = control_flow_ops.cond(\n          math_ops.less(1, 0), lambda: math_ops.add(n, 1),\n          lambda: control_flow_ops.while_loop(c, b, [n]))\n      self.assertAllEqual(10, self.evaluate(r))\n\n  def _testCondWhile_3(self, use_gpu):\n    with self.cached_session(use_gpu=use_gpu) as sess:\n      p = array_ops.placeholder(dtypes.bool)\n      n = constant_op.constant(0.0)\n\n      def c(x):\n        return math_ops.less(x, 10.0)\n\n      def b(x):\n        with ops.device(\"/cpu:0\"):\n          x1 = math_ops.add(x, 1.0)\n        return x1\n\n      r = control_flow_ops.cond(p,\n                                lambda: control_flow_ops.while_loop(c, b, [n]),\n                                lambda: math_ops.multiply(n, 2.0))\n      r1 = gradients_impl.gradients(r, [n])\n      self.assertEqual(10., sess.run(r, {p: True}))\n      self.assertEqual([1.0], sess.run(r1, {p: True}))\n      self.assertEqual(0.0, sess.run(r, {p: False}))\n      self.assertEqual([2.0], sess.run(r1, {p: False}))\n\n  @test_util.run_deprecated_v1\n  def testCondWhile_3(self):\n    self._testCondWhile_3(use_gpu=False)\n    self._testCondWhile_3(use_gpu=True)\n\n  def testWhileCond_1(self):\n\n    with self.cached_session():\n      i = ops.convert_to_tensor(0, name=\"i\")\n      n = ops.convert_to_tensor(10, name=\"n\")\n      one = ops.convert_to_tensor(1, name=\"one\")\n      c = lambda x: math_ops.less(x, n)\n      # pylint: disable=undefined-variable\n      # for OSS build\n      b = lambda x: control_flow_ops.cond(\n          constant_op.constant(True),\n          lambda: math_ops.add(x, one), lambda: math_ops.subtract(x, one))\n      # pylint: enable=undefined-variable\n      r = control_flow_ops.while_loop(c, b, [i])\n      self.assertAllEqual(10, self.evaluate(r))\n\n  def testWhileCond_2(self):\n\n    with self.cached_session():\n      n = ops.convert_to_tensor(0, name=\"n\")\n      c = lambda x: math_ops.less(x, 10)\n      b = lambda x: control_flow_ops.cond(constant_op.constant(True), lambda: math_ops.add(x, 1), lambda: n)\n      r = control_flow_ops.while_loop(c, b, [n])\n      self.assertAllEqual(10, self.evaluate(r))\n\n  def testWhileCond_3(self):\n\n    with self.cached_session():\n      n = ops.convert_to_tensor(0)\n      c = lambda x: math_ops.less(x, 10)\n      # pylint: disable=undefined-variable\n      # for OSS build\n      b = lambda x: control_flow_ops.cond(math_ops.less(0, 1),\n                                          lambda: math_ops.add(x, 1),\n                                          lambda: math_ops.subtract(x, 1))\n      # pylint: enable=undefined-variable\n      r = control_flow_ops.while_loop(c, b, [n])\n      self.assertAllEqual(10, self.evaluate(r))\n\n  @test_util.run_deprecated_v1\n  def testWhileCondGradMultiDevice(self):\n    config = config_pb2.ConfigProto(device_count={\"CPU\": 2},\n                                    allow_soft_placement=True)\n    with self.cached_session(use_gpu=True, config=config) as sess:\n      pred = array_ops.placeholder(dtypes.bool, [])\n      x_init = constant_op.constant(1.0)\n\n      with ops.device(\"/cpu:0\"):\n        z = control_flow_ops.while_loop(\n            lambda i, _: i < 3,\n            lambda i, x: (i + 1, control_flow_ops.cond(\n                pred, lambda: x * 2.0, lambda: 10.0)),\n            [0, x_init])\n\n      with ops.device(\"/cpu:1\"):\n        grad = gradients_impl.gradients(z, x_init)[0]\n\n      with ops.device(\"/cpu:0\"):\n        grad_grad = gradients_impl.gradients(grad, x_init)[0]\n\n      self.assertEqual(sess.run(grad, {pred: True}), 8.0)\n      self.assertEqual(sess.run(grad, {pred: False}), 0.0)\n\n      if not control_flow_util.ENABLE_CONTROL_FLOW_V2:\n        return\n\n      self.assertEqual(sess.run(grad_grad, {pred: True}), 0.0)\n      self.assertEqual(sess.run(grad_grad, {pred: False}), 0.0)\n\n  # NOTE: It is ok to have parallel_iterations > 1\n  @test_util.disable_control_flow_v2(\"b/113324949 (RefVariable)\")\n  @test_util.run_deprecated_v1\n  def testWhileUpdateVariable_1(self):\n    with self.cached_session():\n      select = variables.Variable([3.0, 4.0, 5.0])\n      n = constant_op.constant(0)\n\n      def loop_iterator(j):\n        return math_ops.less(j, 3)\n\n      def loop_body(j):\n        ns = state_ops.scatter_update(select, j, 10.0)\n        nj = math_ops.add(j, 1)\n        op = control_flow_ops.group(ns)\n        nj = control_flow_ops.with_dependencies([op], nj)\n        return [nj]\n\n      r = control_flow_ops.while_loop(\n          loop_iterator, loop_body, [n], parallel_iterations=1)\n      self.evaluate(variables.global_variables_initializer())\n      self.assertEqual(3, self.evaluate(r))\n      result = self.evaluate(select)\n      self.assertAllClose(np.array([10.0, 10.0, 10.0]), result)\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (RefVariable)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileUpdateVariable_2(self):\n    with self.cached_session():\n      select1 = variables.Variable([3.0, 4.0, 5.0])\n      select2 = variables.Variable([3.0, 4.0, 5.0])\n      n = constant_op.constant(0)\n\n      def loop_iterator(j):\n        return math_ops.less(j, 3)\n\n      def loop_body(j):\n        ns1 = state_ops.scatter_update(select1, j, 10.0)\n        ns2 = state_ops.scatter_update(select2, j, 10.0)\n        nj = math_ops.add(j, 1)\n        op = control_flow_ops.group(ns1, ns2)\n        nj = control_flow_ops.with_dependencies([op], nj)\n        return [nj]\n\n      r = control_flow_ops.while_loop(\n          loop_iterator, loop_body, [n], parallel_iterations=1)\n      self.evaluate(variables.global_variables_initializer())\n      self.assertEqual(3, self.evaluate(r))\n      result1 = self.evaluate(select1)\n      self.assertAllClose(np.array([10.0, 10.0, 10.0]), result1)\n      result2 = self.evaluate(select2)\n      self.assertAllClose(np.array([10.0, 10.0, 10.0]), result2)\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (RefVariable)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileUpdateVariable_3(self):\n    with self.cached_session():\n      select = variables.Variable([3.0, 4.0, 5.0])\n      n = constant_op.constant(0)\n\n      def loop_iterator(j, _):\n        return math_ops.less(j, 3)\n\n      def loop_body(j, _):\n        ns = state_ops.scatter_update(select, j, 10.0)\n        nj = math_ops.add(j, 1)\n        return [nj, ns]\n\n      r = control_flow_ops.while_loop(\n          loop_iterator,\n          loop_body, [n, array_ops.identity(select)],\n          parallel_iterations=1)\n      self.evaluate(variables.global_variables_initializer())\n      result = r[1]\n    self.assertAllClose(np.array([10.0, 10.0, 10.0]), result)\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (RefVariable)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileUpdateVariable_4(self):\n    with self.cached_session():\n      var_a = variables.Variable(0, name=\"a\")\n      var_b = variables.Variable(0, name=\"b\")\n      self.evaluate(variables.global_variables_initializer())\n\n      c = constant_op.constant(0, name=\"c\")\n      asn1 = state_ops.assign_add(var_a, 1, name=\"a_add\")\n\n      # Loop condition\n      def pred(i):\n        return math_ops.less(i, 10)\n\n      # Loop body\n      def loop_body(i):\n        asn2 = state_ops.assign_add(var_b, asn1, name=\"b_add\")\n        with ops.control_dependencies([asn2]):\n          ni = math_ops.add(i, 1, name=\"i_add\")\n        return ni\n\n      lpa = control_flow_ops.while_loop(\n          pred, loop_body, [c], parallel_iterations=1)\n\n      self.assertEqual(0, self.evaluate(var_b))\n      self.evaluate(lpa)  # Run the loop\n      self.assertEqual(10, self.evaluate(var_b))\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (RefVariable)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileUpdateVariable_5(self):\n    with self.cached_session():\n      # Create some variables.\n      var_a = variables.Variable(0, name=\"a\")\n      var_b = variables.Variable(0, name=\"b\")\n      self.evaluate(variables.global_variables_initializer())\n\n      # Change condition to check var_b\n      def pred(_):\n        return math_ops.less(var_b, 10)\n\n      # Change body to increment var_b\n      def loop_body(i):\n        asn1 = state_ops.assign_add(\n            var_a, constant_op.constant(1), name=\"a_add\")\n        asn2 = state_ops.assign_add(\n            var_b, constant_op.constant(1), name=\"b_add\")\n        with ops.control_dependencies([asn1, asn2]):\n          inc_b = array_ops.identity(var_b)\n        return inc_b\n\n      lpa = control_flow_ops.while_loop(\n          pred, loop_body, [var_b], parallel_iterations=1, name=\"loop\")\n\n      self.assertEqual(0, self.evaluate(var_b))\n      self.evaluate(lpa)  # Run the loop\n      self.assertEqual(10, self.evaluate(var_a))\n      self.assertEqual(10, self.evaluate(var_b))\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (RefVariable)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileUpdateVariable_6(self):\n    with self.cached_session():\n      # Create some variables.\n      var_a = variables.Variable(0, name=\"a\")\n      var_b = variables.Variable(0, name=\"b\")\n      c = constant_op.constant(0)\n      self.evaluate(variables.global_variables_initializer())\n\n      # Loop condition\n      def pred(i):\n        return math_ops.less(i, 10)\n\n      # Loop body\n      def loop_body(i):\n        asn1 = state_ops.assign_add(var_a, 1, name=\"a_add\")\n        with ops.control_dependencies([asn1]):\n          asn2 = state_ops.assign_add(var_b, var_a, name=\"b_add\")\n        with ops.control_dependencies([asn2]):\n          ni = math_ops.add(i, 1, name=\"i_add\")\n          return ni\n\n      lpa = control_flow_ops.while_loop(\n          pred, loop_body, [c], parallel_iterations=1, name=\"loop\")\n\n      self.assertEqual(0, self.evaluate(var_b))\n      self.evaluate(lpa)  # Run the loop\n      self.assertEqual(55, self.evaluate(var_b))\n      self.assertEqual(10, self.evaluate(var_a))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileQueue_1(self):\n    with self.cached_session():\n      q = data_flow_ops.FIFOQueue(-1, dtypes.int32)\n      i = constant_op.constant(0)\n\n      def c(i):\n        return math_ops.less(i, 10)\n\n      def b(i):\n        ni = math_ops.add(i, 1)\n        ni = control_flow_ops.with_dependencies([q.enqueue((i,))], ni)\n        return ni\n\n      r = control_flow_ops.while_loop(c, b, [i], parallel_iterations=1)\n      self.assertEqual([10], self.evaluate(r))\n      for i in xrange(10):\n        self.assertEqual([i], self.evaluate(q.dequeue()))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileTimeOut(self):\n    run_options = config_pb2.RunOptions(timeout_in_ms=1)\n    with self.cached_session() as sess:\n      n = constant_op.constant(0)\n      c = lambda x: True\n      b = lambda x: math_ops.add(x, 1)\n      r = control_flow_ops.while_loop(c, b, [n])\n      with self.assertRaises(errors_impl.DeadlineExceededError):\n        sess.run(r, options=run_options)\n\n  @test_util.disable_control_flow_v2(\"b/117119329 (stack)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileStack_1(self):\n    with self.cached_session():\n      s = gen_data_flow_ops.stack_v2(-1, dtypes.int32, stack_name=\"foo\")\n      i = constant_op.constant(0)\n\n      def c(i):\n        return math_ops.less(i, 10)\n\n      def b(i):\n        ni = math_ops.add(i, 1)\n        ni = control_flow_ops.with_dependencies(\n            [gen_data_flow_ops.stack_push_v2(s, i)], ni)\n        return ni\n\n      r = control_flow_ops.while_loop(c, b, [i], parallel_iterations=1)\n\n      x = constant_op.constant(0)\n\n      def c1(i, _):\n        return math_ops.greater(i, 0)\n\n      def b1(i, x):\n        ni = math_ops.subtract(i, 1)\n        nx = x + gen_data_flow_ops.stack_pop_v2(s, dtypes.int32)\n        return [ni, nx]\n\n      _, rx = control_flow_ops.while_loop(\n          c1,\n          b1, [r, x],\n          [r.get_shape(), tensor_shape.unknown_shape()],\n          parallel_iterations=1)\n      self.assertEqual(45, self.evaluate(rx))\n\n  def _testWhileGrad_ColocateGradients(self, colocate):\n    gpu_dev_name = test.gpu_device_name() if test.is_gpu_available(\n    ) else \"/device:CPU:0\"\n\n    graph = ops.Graph()\n    with graph.as_default():\n      v = constant_op.constant(2.0, name=\"v\")\n      c = lambda v: math_ops.less(v, 100.0)\n\n      def b(x):\n        with ops.device(gpu_dev_name):\n          return math_ops.square(x)\n\n      loop = control_flow_ops.while_loop(c, b, [v], parallel_iterations=1)\n      r = gradients_impl.gradients(\n          loop, v, colocate_gradients_with_ops=colocate)[0]\n\n    r_ops = graph.get_operations()\n    r_devices = [(op.name, op.device) for op in r_ops]\n\n    self.assertTrue(any(\"Square\" in op.name for op in r_ops))\n\n    for (name, dev) in r_devices:\n      if not colocate and name.endswith(\"Square\"):\n        # Only forward graph contain gpu in Square device\n        self.assertTrue(gpu_dev_name in dev)\n      elif colocate and \"Square\" in name:\n        # Forward and backward graphs contain gpu in Square/Square_grad devices\n        self.assertTrue(gpu_dev_name in dev)\n      else:\n        self.assertFalse(gpu_dev_name in dev)\n\n    with self.session(graph=graph) as sess:\n      self.assertAllClose(1024.0, self.evaluate(r))\n\n  @test_util.disable_control_flow_v2(\"b/116351701 (colocation)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_ColocateGradients(self):\n    self._testWhileGrad_ColocateGradients(colocate=False)\n    self._testWhileGrad_ColocateGradients(colocate=True)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_Square(self):\n    with self.cached_session():\n      v = constant_op.constant(2.0, name=\"v\")\n      c = lambda v: math_ops.less(v, 100.0)\n      b = math_ops.square\n      r = control_flow_ops.while_loop(c, b, [v], parallel_iterations=1)\n      r = control_flow_ops.cond(math_ops.less(1, 2), lambda: r, lambda: v)\n\n      r = gradients_impl.gradients(r, v)[0]\n      self.assertAllClose(1024.0, self.evaluate(r))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_Shape(self):\n    with self.cached_session():\n      x = array_ops.placeholder(dtypes.float32, shape=[None])\n      v = constant_op.constant([2.0], name=\"v\")\n      n = constant_op.constant(0, name=\"n\")\n      c = lambda i, v: math_ops.less(i, 5)\n      b = lambda i, v: [i + 1, math_ops.multiply(x, v)]\n      r = control_flow_ops.while_loop(\n          c,\n          b, [n, v],\n          [n.get_shape(), tensor_shape.unknown_shape()],\n          parallel_iterations=1)\n\n      r = gradients_impl.gradients(r[1], x)[0]\n      self.assertEqual([None], r.get_shape().as_list())\n      self.assertAllClose([810.0, 2560.0], r.eval(feed_dict={x: [3.0, 4.0]}))\n\n  @test_util.run_deprecated_v1\n  def testWhileGrad_BaseShape(self):\n    with self.cached_session() as sess:\n      x = array_ops.placeholder(dtypes.float32, [None])\n      v0 = constant_op.constant([2.0, 2.0], name=\"v\")\n      c = lambda v: constant_op.constant(False)\n      b = lambda v: math_ops.multiply(v, x)\n      r = control_flow_ops.while_loop(c, b, [v0])\n      y = math_ops.square(x)\n\n      r = gradients_impl.gradients([r, y], x)[0]\n      self.assertAllClose([2.0, 4.0], sess.run(r, feed_dict={x: [1.0, 2.0]}))\n\n  @test_util.run_deprecated_v1\n  @test_util.enable_output_all_intermediates\n  def testWhileGradAfterSessionRun(self):\n    v0 = constant_op.constant(2.)\n    r = control_flow_ops.while_loop(\n        lambda _: True, lambda v: v * v, [v0], maximum_iterations=3)\n\n    self.assertAllEqual(r, 256.)\n    grad = gradients_impl.gradients(r, v0)[0]\n    self.assertAllClose(grad, 1024.)\n\n  @test_util.run_deprecated_v1\n  @test_util.enable_output_all_intermediates\n  def testNestedWhileGradAfterSessionRun(self):\n    v0 = constant_op.constant(2.)\n\n    def body(v):\n      inner_v0 = constant_op.constant(1.)\n      return control_flow_ops.while_loop(\n          lambda _: True, lambda x: x * v, [inner_v0], maximum_iterations=2)\n\n    r = control_flow_ops.while_loop(\n        lambda _: True, body, [v0], maximum_iterations=3)\n\n    self.assertAllEqual(r, 256.)\n    grad = gradients_impl.gradients(r, v0)[0]\n    self.assertAllClose(grad, 1024.)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_MultipleUses(self):\n    with self.cached_session():\n      v = constant_op.constant(2.0, name=\"v\")\n      c = lambda v: math_ops.less(v, 100.0)\n      b = math_ops.square\n      r = control_flow_ops.while_loop(c, b, [v], parallel_iterations=1)\n      r = math_ops.multiply(r, r)\n\n      r = gradients_impl.gradients(r, v)[0]\n      self.assertEqual(524288.0, self.evaluate(r))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_LoopAdd(self):\n    with self.cached_session():\n      v = constant_op.constant(2.0, name=\"v\")\n      c = lambda v: math_ops.less(v, 100.0)\n      b = math_ops.square\n      r = control_flow_ops.while_loop(c, b, [v], parallel_iterations=1)\n      r = math_ops.add(r, r)\n\n      r = gradients_impl.gradients(r, v)[0]\n      self.assertAllClose(2048.0, self.evaluate(r))\n\n  def _testWhileGrad_Mul(self, use_gpu, p_iters):\n    with self.cached_session(use_gpu=use_gpu) as sess:\n      a = constant_op.constant(3.0, name=\"a\")\n      v = constant_op.constant(2.0, name=\"v\")\n      c = lambda v: math_ops.less(v, 100.0)\n      b = lambda v: math_ops.multiply(v, a)\n      r = control_flow_ops.while_loop(c, b, [v], parallel_iterations=p_iters)\n\n      grad_a, grad_v = gradients_impl.gradients(r, [a, v])\n      grad_a_val, grad_v_val = self.evaluate([grad_a, grad_v])\n      self.assertAllClose(216.0, grad_a_val)\n      self.assertAllClose(81.0, grad_v_val)\n\n  @test_util.run_deprecated_v1\n  def testWhileGrad_Mul(self):\n    self._testWhileGrad_Mul(use_gpu=False, p_iters=1)\n    self._testWhileGrad_Mul(use_gpu=False, p_iters=10)\n    self._testWhileGrad_Mul(use_gpu=True, p_iters=1)\n    self._testWhileGrad_Mul(use_gpu=True, p_iters=10)\n\n  def testWhileGradInControlDeps(self):\n\n    @def_function.function\n    def f():\n      x_init = constant_op.constant(2.)\n      loop_cond = lambda i, x: math_ops.less(i, 2)\n      loop_body = lambda i, x: [i + 1, x**2]\n      _, x = control_flow_ops.while_loop(loop_cond, loop_body, [0, x_init])\n      with ops.control_dependencies([x]):\n        (grad,) = gradients_impl.gradients(x, x_init)\n        return grad\n\n    self.assertAllEqual(f(), 4. * 2.**3)  # 4 * x_init ^ 3\n\n  @test_util.run_deprecated_v1\n  def testTfFunctionInV1WhileLoop(self):\n\n    # This test specifically tests that creating a Const node inside a\n    # tf.function inside a v1 while_loop while inlining is turned on works.\n    config = opt_cfg()\n    assert config.graph_options.optimizer_options.do_function_inlining\n    with session.Session(config=config):\n\n      @def_function.function\n      def loop_body(i):\n        # Here we create the const.\n        return i + 1.\n\n      loop_cond = lambda i: True\n      x = control_flow_ops.while_loop(\n          loop_cond, loop_body, [0.], maximum_iterations=5)\n      self.assertAllEqual(x, 5.)\n\n  def _testNestedWhileCondWhileGrad(self, use_gpu):\n\n    with self.cached_session(use_gpu=use_gpu):\n      v = constant_op.constant(1.0)\n\n      def inner_loop(s):\n        z = constant_op.constant(0)\n        c = lambda i, x: math_ops.less(i, 4)\n        b = lambda i, x: [math_ops.add(i, 1), math_ops.multiply(x, 2.0)]\n        return control_flow_ops.while_loop(c, b, [z, s])\n\n      c = lambda x: math_ops.less(x, 128.0)\n\n      def b(x):\n        return control_flow_ops.cond(\n            constant_op.constant(True),\n            lambda: math_ops.square(inner_loop(x)[1]),\n            lambda: math_ops.multiply(x, 2.0))\n\n      r = control_flow_ops.while_loop(c, b, [v])\n      r = gradients_impl.gradients(r, v)[0]\n      self.assertAllClose(512.0, self.evaluate(r))\n\n  @test_util.run_deprecated_v1\n  def testNestedWhileCondWhileGrad(self):\n    self._testNestedWhileCondWhileGrad(use_gpu=False)\n\n  @test_util.run_deprecated_v1\n  def testNestedWhileCondWhileGradGpu(self):\n    self._testNestedWhileCondWhileGrad(use_gpu=True)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_Variable(self):\n    with self.cached_session():\n      a = variables.Variable(3.0)\n      v = constant_op.constant(2.0, name=\"v\")\n      c = lambda v: math_ops.less(v, 100.0)\n      b = lambda v: math_ops.multiply(v, a)\n      r = control_flow_ops.while_loop(c, b, [v], parallel_iterations=1)\n\n      r = gradients_impl.gradients(r, a)\n      self.evaluate(variables.global_variables_initializer())\n      self.assertAllClose(216.0, r[0])\n\n  @test_util.run_deprecated_v1\n  def testWhileGrad_ResourceVariable(self):\n    with self.cached_session():\n      a = resource_variable_ops.ResourceVariable(3.0)\n      v = constant_op.constant(2.0, name=\"v\")\n      c = lambda v: math_ops.less(v, 100.0)\n      b = lambda v: math_ops.multiply(v, a)\n      r = control_flow_ops.while_loop(c, b, [v], parallel_iterations=1)\n\n      g = gradients_impl.gradients(r, a)\n      self.evaluate(variables.global_variables_initializer())\n      self.assertAllClose(216.0, g[0])\n\n  def testWhileGrad_EagerResourceVariable(self):\n    with context.eager_mode():\n      a = resource_variable_ops.ResourceVariable(\n          np.ones([2, 2], dtype=np.float32))\n      v = constant_op.constant(1.0)\n\n      @eager_function.defun\n      def fn():\n        r = control_flow_ops.while_loop(\n            lambda i, _: i < 2,\n            lambda i, x: (i + 1, x * math_ops.reduce_sum(a) * v),\n            [0, 1.0])[1]\n        return gradients_impl.gradients(r, [v])[0]\n\n      self.assertEqual(self.evaluate(fn()), 32.)\n\n  def testWhileGrad_ResourceVarInFunctionCall(self):\n\n    @def_function.function\n    def foo(x, var):\n      return x + math_ops.reduce_sum(var.sparse_read([1, 3]))\n\n    @def_function.function\n    def bar(var):\n      r = control_flow_ops.while_loop(\n          lambda i, _: i < 2,\n          lambda i, x: (i + 1, foo(x, var)),\n          [0, 0.0])[1]\n      return gradients_impl.gradients(r, var)[0]\n\n    var = resource_variable_ops.ResourceVariable([1., 2., 3., 4.])\n    self.evaluate(variables.global_variables_initializer())\n    grad = self.evaluate(bar(var))\n    self.assertAllEqual(gradient_checker_v2._to_numpy(grad), [0., 2., 0., 2.])\n\n  def testWhileGrad_ResourceVarInNestedFunctionCall(self):\n\n    @def_function.function\n    def foo(x, var):\n      return x + math_ops.reduce_sum(var.sparse_read([1, 3]))\n\n    @def_function.function\n    def foo2(x, var):\n      return foo(x, var)\n\n    @def_function.function\n    def bar(var):\n      r = control_flow_ops.while_loop(\n          lambda i, _: i < 2,\n          lambda i, x: (i + 1, foo2(x, var)),\n          [0, 0.0])[1]\n      return gradients_impl.gradients(r, var)[0]\n\n    var = resource_variable_ops.ResourceVariable([1., 1., 1., 1.])\n    self.evaluate(variables.global_variables_initializer())\n    grad = self.evaluate(bar(var))\n    self.assertAllEqual(gradient_checker_v2._to_numpy(grad), [0., 2., 0., 2.])\n\n  def testWhileGrad_ResourceVarInLoopInFunctionCall(self):\n    if test.is_gpu_available():\n      self.skipTest(\"b/128635252\")\n\n    @def_function.function\n    def foo(x, var):\n      return control_flow_ops.while_loop(\n          lambda j, _: j < 3,\n          lambda j, y: (j + 1,\n                        y + math_ops.reduce_sum(var.sparse_read([1, 2]))),\n          [0, x])[1]\n\n    @def_function.function\n    def bar(var):\n      r = control_flow_ops.while_loop(\n          lambda i, _: i < 2,\n          lambda i, x: (i + 1, foo(x, var)),\n          [0, 0.0])[1]\n      return gradients_impl.gradients(r, var)[0]\n\n    var = resource_variable_ops.ResourceVariable([1., 1., 1., 1.])\n    self.evaluate(variables.global_variables_initializer())\n    grad = self.evaluate(bar(var))\n    self.assertAllEqual(gradient_checker_v2._to_numpy(grad), [0., 6., 6., 0.])\n\n  def testWhileCondGrad_ResourceVarInFunctionCall(self):\n\n    @def_function.function\n    def foo(x, var):\n      return x + var.sparse_read([1])[0]\n\n    def body(i, x):\n      return (i + 1, control_flow_ops.cond(\n          math_ops.equal(i % 2, 0),\n          lambda: foo(x, var1),\n          lambda: foo(x, var2)))\n\n    @def_function.function\n    def bar(var1, var2):\n      r = control_flow_ops.while_loop(\n          lambda i, _: i < 4, body, [0, 0.0])\n      return gradients_impl.gradients(r, [var1, var2])\n\n    var1 = resource_variable_ops.ResourceVariable([1., 2., 3.])\n    var2 = resource_variable_ops.ResourceVariable([4., 5.])\n    self.evaluate(variables.global_variables_initializer())\n    grads = self.evaluate(bar(var1, var2))\n    self.assertAllEqual(gradient_checker_v2._to_numpy(grads[0]), [0., 2., 0.])\n    self.assertAllEqual(gradient_checker_v2._to_numpy(grads[1]), [0., 2.])\n\n  @test_util.run_deprecated_v1\n  def testWhileGrad_ResourceVarSparseRead(self):\n    # NOTE(skyewm): this test is interesting because the gradient is the\n    # aggregation result of IndexedSlices and Tensors.\n    var = resource_variable_ops.ResourceVariable(np.ones(5),\n                                                 dtype=dtypes.float32)\n    r = control_flow_ops.while_loop(\n        lambda i, _: i < 3,\n        lambda i, x: (i + 1, x * math_ops.reduce_sum(var.sparse_read([1, 3]))),\n        [0, constant_op.constant(1.0)])[1]\n    grad = gradients_impl.gradients(r, var)[0]\n\n    self.evaluate(variables.global_variables_initializer())\n    grad_val = self.evaluate(grad)\n    arr = gradient_checker_v2._to_numpy(grad_val)\n    self.assertAllEqual(arr, [0., 12., 0., 12., 0.])\n\n  @test_util.run_deprecated_v1\n  def testWhileGrad_MultiResourceVarSparseRead(self):\n    # NOTE(skyewm): this test is interesting because the gradient is the\n    # aggregation result of IndexedSlices and Tensors.\n    var1 = resource_variable_ops.ResourceVariable(np.ones(5),\n                                                  dtype=dtypes.float32)\n    var2 = resource_variable_ops.ResourceVariable(np.ones(3),\n                                                  dtype=dtypes.float32)\n    x1_init = constant_op.constant([0., 0.])\n    x2_init = constant_op.constant(1.)\n    x3_init = constant_op.constant(1.)\n\n    def body(i, unused_x1, x2, x3):\n      y1 = var1.sparse_read([1, 3])\n      y2 = x2 * 2\n      y3 = x3 * math_ops.reduce_sum(var2.sparse_read([0]))\n      return i + 1, y1, y2, y3\n\n    r = control_flow_ops.while_loop(\n        lambda i, x1, x2, x3: i < 3, body,\n        [0, x1_init, x2_init, x3_init])[1:]\n    var1_grad, var2_grad = gradients_impl.gradients(r, [var1, var2])\n\n    self.evaluate(variables.global_variables_initializer())\n    var1_grad_val = self.evaluate(var1_grad)\n    var2_grad_val = self.evaluate(var2_grad)\n    self.assertAllEqual(gradient_checker_v2._to_numpy(var1_grad_val),\n                        [0., 1., 0., 1., 0.])\n    self.assertAllEqual(gradient_checker_v2._to_numpy(var2_grad_val),\n                        [3., 0., 0.])\n\n  def testWhileGrad_Gather(self):\n    # NOTE(skyewm): this test is interesting because the gather gradient\n    # function returns an IndexedSlices.\n    @tf_function_in_tf2\n    def fn():\n      x = constant_op.constant([1., 1., 1., 1., 1.])\n      y = control_flow_ops.while_loop(\n          lambda i, _: i < 3,\n          lambda i, x: (i + 1, x + array_ops.gather(x, [0])),\n          [0, x[:1]])[1]\n      z = y * 3.0\n      grad = gradients_impl.gradients(z, x)[0]\n      return y, grad\n    y, grad = fn()\n    self.assertEqual(self.evaluate(y), 8.)\n    self.assertAllEqual(self.evaluate(grad), [24., 0., 0., 0., 0.])\n\n  def testWhileGrad_GatherNoFanOut(self):\n    # NOTE(skyewm): this test is interesting because the gather gradient\n    # function returns an IndexedSlices.\n    @tf_function_in_tf2\n    def fn():\n      x = constant_op.constant([1., 1., 1., 1., 1.])\n      y = control_flow_ops.while_loop(\n          lambda i, _: i < 3,\n          lambda i, x: (i + 1, array_ops.gather(x, [0])),\n          [0, x[:1]])[1]\n      z = y * 3.0\n      grad = gradients_impl.gradients(z, x)[0]\n      return y, grad\n    y, grad = fn()\n    self.assertEqual(self.evaluate(y), 1.)\n    self.assertAllEqual(self.evaluate(grad), [3., 0., 0., 0., 0.])\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGradInCond(self):\n\n    with self.cached_session():\n      n = ops.convert_to_tensor(1.0, name=\"n\")\n      x = array_ops.placeholder(dtypes.float32, shape=None)\n      c = lambda n: math_ops.less(n, 10.0)\n      b = lambda n: math_ops.add(n, x)\n\n      def fn1():\n        r = control_flow_ops.while_loop(c, b, [n],\n                                        [tensor_shape.unknown_shape()])\n        return gradients_impl.gradients(r, x)[0]\n\n      r = control_flow_ops.cond(math_ops.less(1, 2), fn1, lambda: x)\n      self.assertAllClose(9.0, r.eval(feed_dict={x: 1.0}))\n\n  @test_util.disable_control_flow_v2(\"b/116340060\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testGradInWhileWrtInitialLoopVal(self):\n    with self.cached_session():\n      x = array_ops.placeholder(dtypes.float32, shape=(), name=\"x\")\n      y = x + 1\n\n      def body(i, v):\n        z = v * 2\n        return i + 1, gradients_impl.gradients(z, x)[0]\n\n      with self.assertRaisesRegex(\n          ValueError,\n          \"Cannot compute gradient inside while loop with respect to op 'x'. \"\n          \"We do not support taking the gradient wrt or through the initial \"\n          \"value of a loop variable. Gradients can be computed through \"\n          \"loop invariants or wrt the input parameters to the loop body.\"):\n        control_flow_ops.while_loop(lambda i, x: i < 3, body, [0, y])\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGradInWhile(self):\n    with self.cached_session():\n      n = ops.convert_to_tensor(1.0, name=\"n\")\n      x = array_ops.placeholder(dtypes.float32, shape=None)\n      c = lambda n: math_ops.less(n, 10.0)\n      b = lambda n: math_ops.add(n, x)\n\n      def b1(n):\n        r = control_flow_ops.while_loop(c, b, [n],\n                                        [tensor_shape.unknown_shape()])\n        return gradients_impl.gradients(r, x)\n\n      r = control_flow_ops.while_loop(lambda n: n < 6.0, b1, [n],\n                                      [tensor_shape.unknown_shape()])\n      self.assertAllClose(9.0, r.eval(feed_dict={x: 1.0}))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCondGradInNestedWhiles(self):\n\n    def outer_body(i, x):\n      _, x = control_flow_ops.while_loop(\n          lambda j, x: j < 3, inner_body, [0, 0.0])\n      return i + 1, x\n\n    def inner_body(j, x):\n      y = control_flow_ops.cond(math_ops.less(x, 1), lambda: 2 * x, lambda: x)\n      return j + 1, gradients_impl.gradients(y, x)[0]\n\n    i, x = control_flow_ops.while_loop(lambda i, x: i < 3, outer_body, [0, 0.0])\n\n    with self.cached_session() as sess:\n      i_val, x_val = self.evaluate([i, x])\n      self.assertEqual(i_val, 3)\n      self.assertAllClose(x_val, 1.0)\n\n  @test_util.run_gpu_only\n  def testGpuResourceAccess(self):\n    with ops.device(test.gpu_device_name()):\n      var = resource_variable_ops.ResourceVariable(constant_op.constant(3.0))\n\n    @def_function.function\n    def foo():\n      return control_flow_ops.while_loop(\n          lambda i, _: i < 3,\n          lambda i, x: (i + 1, control_flow_ops.cond(\n              constant_op.constant(True),\n              lambda: x + var,\n              lambda: x)),\n          [0, 0.0])[1]\n\n    self.evaluate(variables.global_variables_initializer())\n    self.assertEqual(self.evaluate(foo()), 9.0)\n\n  def testNestedResourceAccess(self):\n    var = resource_variable_ops.ResourceVariable(constant_op.constant(3.0))\n\n    @eager_function.defun\n    def test_fn():\n      x = constant_op.constant(0.0)\n      r = control_flow_ops.while_loop(\n          # Outer loop condition\n          lambda i, y: i < 2,\n          # Outer loop body\n          lambda i, y: (i + 1, y + control_flow_ops.cond(\n              constant_op.constant(True),\n              # True branch\n              lambda: control_flow_ops.while_loop(\n                  # Inner loop condition\n                  lambda j, z: j < 3,\n                  # Inner loop body\n                  lambda j, z: (j + 1, z + math_ops.square(var)),\n                  # Inner initial loop value\n                  [0, y])[1],\n              # False branch\n              lambda: (0.0))),\n          # Outer initial loop value\n          [0, x])[1]\n\n      grad = gradients_impl.gradients(r, x)[0]\n      return r, grad\n\n    self.evaluate(variables.global_variables_initializer())\n    r, grad = self.evaluate(test_fn())\n    # 2 * 3 * 3^2\n    self.assertEqual(r, 81.0)\n    # v1 control flow gets the wrong answer!!!\n    # Gradient computation:\n    #   f(x) = x + 3^2\n    #   inner_loop(x) = f(f(f(x))) = x + 3*3^2 = x + 27\n    #   g(x) = x + inner_loop(x) = 2x + 27\n    #   outer_loop(x) = g(g(x)) = 4x + 81\n    #   outer_loop'(x) = 4\n    # Note that v1 control flow gets 4.0 as well if the cond is removed.\n    if control_flow_util.ENABLE_CONTROL_FLOW_V2:\n      self.assertEqual(grad, 4.0)\n\n  def testWhile_NestedInput(self):\n    with self.cached_session() as sess:\n      named = collections.namedtuple(\"named\", (\"a\", \"b\"))\n      loop_vars = [\n          named(a=constant_op.constant(0.0), b=constant_op.constant(1.0)),\n          (constant_op.constant(2.0), constant_op.constant(3.0)),\n          constant_op.constant(4.0)\n      ]\n      c = lambda lv0, _1, _2: lv0.a < 100.0\n\n      def b(lv0, lv1, lv2):\n        lv0 = named(a=lv0.a + 1, b=lv0.b)\n        lv1 = (lv1[0] + 1, lv1[1])\n        lv2 += 2\n        return [lv0, lv1, lv2]\n\n      r = control_flow_ops.while_loop(c, b, loop_vars)\n\n      self.assertTrue(isinstance(r, list))\n      self.assertTrue(isinstance(r[0], named))\n      self.assertTrue(isinstance(r[1], tuple))\n      self.assertTrue(isinstance(r[2], ops.Tensor))\n\n      r_flattened = nest.flatten(r)\n      self.assertEqual([100.0, 1.0, 102.0, 3.0, 4.0 + 100 * 2.0],\n                       self.evaluate(r_flattened))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhile_NestedBadArityFails(self):\n    with self.cached_session():\n      named = collections.namedtuple(\"named\", (\"a\", \"b\"))\n      loop_vars = [\n          named(a=constant_op.constant(0.0), b=constant_op.constant(1.0)),\n          (constant_op.constant(2.0), constant_op.constant(3.0)),\n          constant_op.constant(4.0)\n      ]\n      c = lambda lv0, _1, _2: lv0.a < 100.0\n\n      def b(lv0, lv1, _):\n        return [lv0, lv1]\n\n      with self.assertRaisesRegex(ValueError, \"the same number of elements\"):\n        control_flow_ops.while_loop(c, b, loop_vars)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_ys_xs(self):\n    with self.cached_session():\n      x = constant_op.constant(3.0, name=\"x\")\n      y = constant_op.constant(2.0, name=\"y\")\n\n      c = lambda x, y: math_ops.less(x, 100.0)\n\n      def b(x, y):\n        y1 = math_ops.add(x, y)\n        x1 = math_ops.multiply(x, y1)\n        return x1, y1\n\n      rx, ry = control_flow_ops.while_loop(c, b, [x, y], parallel_iterations=1)\n\n      r = gradients_impl.gradients([rx, ry], x)\n      self.assertAllClose(304.0, r[0])\n      r = gradients_impl.gradients([rx, ry], y)\n      self.assertAllClose(124.0, r[0])\n      r = gradients_impl.gradients([rx], x)\n      self.assertAllClose(295.0, r[0])\n      r = gradients_impl.gradients([rx], y)\n      self.assertAllClose(120.0, r[0])\n\n  @test_util.run_deprecated_v1\n  def testWhileGrad_Dependency(self):\n    with self.cached_session():\n      i = constant_op.constant(0, name=\"i\")\n      x = constant_op.constant(2.0, name=\"x\")\n\n      c = lambda i, x: math_ops.less(i, 10)\n\n      def b(i, x):\n        x = math_ops.multiply(x, 2.0)\n        i = math_ops.add(i, 1)\n        return i, x\n\n      ri, rx = control_flow_ops.while_loop(c, b, [i, x], parallel_iterations=1)\n\n      r = gradients_impl.gradients([ri, rx], x)\n      self.assertAllClose(1024.0, r[0])\n      r = gradients_impl.gradients([rx], x)\n      self.assertAllClose(1024.0, r[0])\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_NoGradient(self):\n    with self.cached_session():\n      v = constant_op.constant(2.0, name=\"v\")\n      c = lambda v: math_ops.less(v, 100.0)\n      b = math_ops.square\n      r = control_flow_ops.while_loop(c, b, [v], back_prop=False)\n      r = math_ops.add(r, v)\n      r = gradients_impl.gradients(r, v)\n      self.assertAllClose(1.0, r[0])\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (RefVariable)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_NoDependency(self):\n    with self.cached_session() as sess:\n      variable = variables.Variable(array_ops.ones([2, 3]))\n      duration = array_ops.zeros([], dtype=dtypes.int32)\n\n      def cond(duration, tensor, _):\n        del tensor\n        return duration < 10\n\n      def body(duration, tensor, _):\n        return (duration + 1, tensor, tensor)\n\n      loop_vars = [duration, variable, variable]\n      tensors = control_flow_ops.while_loop(\n          cond=cond, body=body, loop_vars=loop_vars)\n      cost = math_ops.reduce_sum(tensors[2])\n      grad = gradients_impl.gradients(cost, [variable])\n      self.evaluate(variables.global_variables_initializer())\n      self.assertAllClose(np.ones([2, 3]), sess.run(grad[0]))\n\n  @test_util.run_deprecated_v1\n  def testWhileGrad_Const(self):\n    with self.cached_session() as sess:\n      c0 = constant_op.constant(0.0, name=\"c0\")\n      c1 = constant_op.constant(1.0, name=\"c1\")\n      duration = constant_op.constant(0, name=\"t\")\n\n      def cond(duration, _):\n        return duration < 1\n\n      def body(duration, _):\n        return duration + 1, c1\n\n      loop_vars = [duration, c0]\n      tensors = control_flow_ops.while_loop(\n          cond=cond, body=body, loop_vars=loop_vars)\n      cost = math_ops.reduce_sum(tensors[1])\n      grad = gradients_impl.gradients(cost, [c0])\n      self.assertAllClose(0.0, sess.run(grad[0]))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_SerialTwoLoops(self):\n    with self.cached_session():\n      i = constant_op.constant(0, name=\"i\")\n      x = constant_op.constant(2.0, name=\"x\")\n\n      c = lambda i, x: math_ops.less(i, 5)\n\n      def b(i, x):\n        x = math_ops.multiply(x, 2.0)\n        i = math_ops.add(i, 1)\n        return i, x\n\n      _, rx = control_flow_ops.while_loop(c, b, [i, x], parallel_iterations=1)\n      _, rx = control_flow_ops.while_loop(c, b, [i, rx], parallel_iterations=1)\n\n      r = gradients_impl.gradients([rx], x)\n      self.assertAllClose(1024.0, r[0])\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_ParallelTwoLoops(self):\n    with self.cached_session():\n      i = constant_op.constant(0, name=\"i\")\n      x = constant_op.constant(2.0, name=\"x\")\n\n      c = lambda i, x: math_ops.less(i, 5)\n\n      def b(i, x):\n        x = math_ops.multiply(x, 2.0)\n        i = math_ops.add(i, 1)\n        return i, x\n\n      _, r1 = control_flow_ops.while_loop(c, b, [i, x], parallel_iterations=1)\n      _, r2 = control_flow_ops.while_loop(c, b, [i, x], parallel_iterations=1)\n      rx = math_ops.add(r1, r2)\n\n      r = gradients_impl.gradients([rx], x)\n      self.assertAllClose(64.0, r[0])\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_OneOutputWithControlDependencyOnSecond(self):\n    with self.cached_session():\n      i = constant_op.constant(0, name=\"i\")\n      x = constant_op.constant(1.0, name=\"x\")\n      y = constant_op.constant(1.0, name=\"y\")\n      c = lambda i, *_: math_ops.less(i, 1, name=\"cond_less\")\n\n      def b(i, xi, yi):\n        # return (i + 1, xi, xi + yi)\n        return (math_ops.add(i, 1, name=\"inc\"), array_ops.identity(\n            xi, name=\"xi\"), math_ops.add(xi, yi, name=\"xi_plus_yi\"))\n\n      _, x_f, y_f = control_flow_ops.while_loop(c, b, [i, x, y])\n      with ops.control_dependencies([x_f]):\n        y_f_d = array_ops.identity(y_f, name=\"y_f_d\")\n\n      self.assertAllClose(2.0, self.evaluate(y_f_d))  # y_f_d = 1.0 + 1.0\n      g = gradients_impl.gradients([y_f_d], [x])[0]\n      self.assertTrue(g is not None)\n      self.assertAllClose(1.0,\n                          self.evaluate(g))  # y_f_d = x + 1.0, dy_f_d/dx = 1.0\n\n  def _testNestedWhileGrad_Simple(self, use_gpu):\n    with self.cached_session(use_gpu=use_gpu):\n      v = constant_op.constant(1.0)\n\n      def inner_loop(s):\n        c = lambda x: math_ops.less(x, 4.0)\n        b = lambda x: math_ops.multiply(x, 2.0)\n        return control_flow_ops.while_loop(c, b, [s])\n\n      c = lambda x: math_ops.less(x, 2.0)\n      b = lambda x: math_ops.multiply(inner_loop(x), 2.0)\n      r = control_flow_ops.while_loop(c, b, [v])\n\n      r = gradients_impl.gradients(r, v)[0]\n      self.assertAllClose(8.0, self.evaluate(r))\n\n  @test_util.run_deprecated_v1\n  def testNestedWhileGrad_Simple(self):\n    self._testNestedWhileGrad_Simple(use_gpu=False)\n    self._testNestedWhileGrad_Simple(use_gpu=True)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testNestedWhileGrad_SerialInner(self):\n    with self.cached_session():\n      v = constant_op.constant(1.0)\n\n      def inner_loop1(s):\n        z = constant_op.constant(0)\n        c = lambda i, x: math_ops.less(i, 4)\n        b = lambda i, x: [math_ops.add(i, 1), math_ops.multiply(x, 2.0)]\n        return control_flow_ops.while_loop(c, b, [z, s])\n\n      def inner_loop2(s):\n        z = constant_op.constant(0)\n        c = lambda i, x: math_ops.less(i, 4)\n        b = lambda i, x: [math_ops.add(i, 1), math_ops.multiply(x, 2.0)]\n        return control_flow_ops.while_loop(c, b, [z, s])\n\n      c = lambda x: math_ops.less(x, 128.0)\n      b = lambda x: inner_loop2(inner_loop1(x)[1])[1]\n      r = control_flow_ops.while_loop(c, b, [v])\n\n      r = gradients_impl.gradients(r, v)[0]\n      self.assertAllClose(256.0, self.evaluate(r))\n\n  @test_util.run_deprecated_v1\n  def testNestedWhileGrad_ParallelInner(self):\n    with self.cached_session():\n      v = constant_op.constant(1.0)\n\n      def inner_loop1(s):\n        z = constant_op.constant(0)\n        c = lambda i, x: math_ops.less(i, 4)\n        b = lambda i, x: [math_ops.add(i, 1), math_ops.multiply(x, 2.0)]\n        return control_flow_ops.while_loop(c, b, [z, s])\n\n      def inner_loop2(s):\n        z = constant_op.constant(0)\n        c = lambda i, x: math_ops.less(i, 4)\n        b = lambda i, x: [math_ops.add(i, 1), math_ops.multiply(x, 2.0)]\n        return control_flow_ops.while_loop(c, b, [z, s])\n\n      c = lambda x: math_ops.less(x, 128.0)\n      b = lambda x: math_ops.multiply(inner_loop1(x)[1], inner_loop2(x)[1])\n      r = control_flow_ops.while_loop(c, b, [v])\n\n      r = gradients_impl.gradients(r, v)[0]\n      self.assertAllClose(512.0, self.evaluate(r))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testNestedWhileGrad_ParallelIterations(self):\n    # Make sure the stack pushes and pops of an inner loop are executed in\n    # the sequential order of the iterations of its outer loop.\n    with self.cached_session() as sess:\n\n      def inner_loop(t):\n        fn = lambda n: n + math_ops.square(var)\n        return map_fn.map_fn(fn=fn, elems=t, parallel_iterations=10)\n\n      def outer_loop(inp):\n        return map_fn.map_fn(\n            fn=inner_loop, elems=inp, parallel_iterations=10)\n\n      var = variables.Variable(constant_op.constant(3.0))\n      inp = constant_op.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n      res = outer_loop(inp)\n      optimizer = adam.AdamOptimizer(learning_rate=0.001)\n      train_op = optimizer.minimize(math_ops.reduce_mean(math_ops.square(res)))\n      self.evaluate(variables.global_variables_initializer())\n      self.evaluate(train_op)\n      self.assertAllClose(2.999, var.read_value())\n\n  def _testWhileCondGrad_Simple(self, use_gpu):\n    with self.cached_session(use_gpu=use_gpu):\n      v = ops.convert_to_tensor(2.0, name=\"v\")\n      n = ops.convert_to_tensor(100.0, name=\"n\")\n      one = ops.convert_to_tensor(1.0, name=\"one\")\n      c = lambda x: math_ops.less(x, n)\n      # pylint: disable=undefined-variable\n      # for OSS build\n      b = lambda x: control_flow_ops.cond(constant_op.constant(True),\n                                          lambda: math_ops.square(x),\n                                          lambda: math_ops.subtract(x, one))\n      # pylint: enable=undefined-variable\n      r = control_flow_ops.while_loop(c, b, [v])\n      r = gradients_impl.gradients(r, v)[0]\n      self.assertAllClose(1024.0, self.evaluate(r))\n\n  @test_util.run_deprecated_v1\n  def testWhileCondGrad_Simple(self):\n    self._testWhileCondGrad_Simple(use_gpu=False)\n    self._testWhileCondGrad_Simple(use_gpu=True)\n\n  @test_util.run_deprecated_v1\n  def testWhileCondGrad_UnknownShape(self):\n    with self.cached_session() as sess:\n      v = array_ops.placeholder(dtypes.float32)\n      n = ops.convert_to_tensor(100.0, name=\"n\")\n      one = ops.convert_to_tensor(1.0, name=\"one\")\n      c = lambda x: math_ops.less(x, n)\n      # pylint: disable=undefined-variable\n      # for OSS build\n      b = lambda x: control_flow_ops.cond(constant_op.constant(True),\n                                          lambda: math_ops.square(x),\n                                          lambda: math_ops.subtract(x, one))\n      # pylint: enable=undefined-variable\n      r = control_flow_ops.while_loop(c, b, [v])\n      r = gradients_impl.gradients(r, v)[0]\n      r = sess.run(r, feed_dict={v: 2.0})\n      self.assertAllClose(1024.0, r)\n\n  @test_util.run_deprecated_v1\n  def testWhileGrad_Concat(self):\n    with self.cached_session() as sess:\n      x = variable_scope.get_variable(\"x\", initializer=[[1., 2.]])\n      i0 = constant_op.constant(0)\n      h0 = array_ops.zeros([0, 2])\n\n      def condition(i, _):\n        return i < 2\n\n      def body(i, h):\n        return i + 1, array_ops.concat([h, x], 0)\n\n      _, h = control_flow_ops.while_loop(\n          condition, body, [i0, h0],\n          [i0.get_shape(), tensor_shape.TensorShape([None, 2])])\n      s = math_ops.reduce_sum(h)\n\n      optimizer = gradient_descent.GradientDescentOptimizer(0.01)\n      op = optimizer.minimize(s)\n\n      self.evaluate(variables.global_variables_initializer())\n      self.evaluate(op)\n      self.assertAllClose([[0.98000002, 1.98000002]], self.evaluate(x))\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (RefVariable)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileWithRefsWithGradients_1(self):\n    with self.cached_session() as sess:\n      x = variables.VariableV1(0.)._ref()  # pylint: disable=protected-access\n      i = constant_op.constant(0)\n      c = lambda i, x: math_ops.less(i, 10)\n\n      self.assertEqual(x.dtype, dtypes.float32_ref)\n\n      def body(i, x):\n        self.assertEqual(x.dtype, dtypes.float32_ref)\n        return [i + 1, gen_array_ops.ref_identity(x)]\n\n      r = control_flow_ops.while_loop(c, body, [i, x], parallel_iterations=5)\n\n      grad_ys = [variables.VariableV1(73)._ref()]  # pylint: disable=protected-access\n      grad = gradients_impl.gradients([r[1]], [x], grad_ys=grad_ys)\n\n      self.evaluate(variables.global_variables_initializer())\n\n      self.assertEqual(r[0].dtype, dtypes.int32)\n      self.assertEqual(r[1].dtype, dtypes.float32_ref)\n\n      value_i, value_x, value_x_grad = sess.run(r + grad)\n\n    self.assertEqual(10, value_i)\n    self.assertEqual(0, value_x)\n    self.assertEqual(73, value_x_grad)\n\n  @test_util.deprecated_graph_mode_only\n  def testWhileGrad_IndexedSlices(self):\n    with self.cached_session():\n      values = constant_op.constant([2.0, 4.0], name=\"values\")\n      indices = constant_op.constant([0, 3], name=\"indices\")\n      shape = constant_op.constant([10], name=\"dense_shape\")\n      i = constant_op.constant(0)\n      x = ops.IndexedSlices(values, indices, dense_shape=shape)\n\n      def c(i, _):\n        return i < 10\n\n      def b(i, x):\n        return [\n            i + 1,\n            ops.IndexedSlices(x.values * 2.0, x.indices, x.dense_shape)\n        ]\n\n      _, r = control_flow_ops.while_loop(c, b, [i, x])\n      r = gradients_impl.gradients(r.values, values)[0]\n      self.assertAllClose(np.array([1024.0, 1024.0]), self.evaluate(r))\n\n  @test_util.deprecated_graph_mode_only\n  def testWhileGrad_SparseTensor(self):\n    with self.cached_session():\n      values = constant_op.constant([2.0, 4.0], name=\"values\")\n      indices = constant_op.constant(\n          [[0], [3]], dtype=dtypes.int64, name=\"indices\")\n      shape = constant_op.constant([10], dtype=dtypes.int64, name=\"dense_shape\")\n      i = constant_op.constant(0)\n      x = sparse_tensor.SparseTensor(indices, values, dense_shape=shape)\n\n      def c(i, _):\n        return i < 10\n\n      def b(i, x):\n        return [\n            i + 1,\n            sparse_tensor.SparseTensor(x.indices, x.values * 2.0, x.dense_shape)\n        ]\n\n      _, r = control_flow_ops.while_loop(c, b, [i, x])\n      r = gradients_impl.gradients(r.values, values)[0]\n      self.assertAllClose(np.array([1024.0, 1024.0]), self.evaluate(r))\n\n  @test_util.deprecated_graph_mode_only\n  def testCallGradInLoop(self):\n    with self.cached_session() as sess:\n      i0 = constant_op.constant(0)\n      params = constant_op.constant(5.0)\n      params_1 = math_ops.square(params)\n\n      def c(i, _):\n        return i < 10\n\n      def b(i, x):\n        data = constant_op.constant([1.0, 2.0, 3.0])\n        data = math_ops.multiply(data, params_1)\n        x1 = x + gradients_impl.gradients(data, params)[0]\n        return i + 1, x1\n\n      output_grad = control_flow_ops.while_loop(\n          c, b, [i0, constant_op.constant(0.0)])\n      self.assertAllClose(600.0, self.evaluate(output_grad)[1])\n\n  @test_util.run_deprecated_v1\n  def testWhileAndTensorArray(self):\n    with self.cached_session() as sess:\n      param = constant_op.constant(2.0)\n      n0 = constant_op.constant(0)\n      y0 = constant_op.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], name=\"elems\")\n\n      def c(i, _):\n        return i < 10\n\n      def b(i, y):\n        return [\n            i + 1,\n            map_fn.map_fn(lambda x: math_ops.multiply(x, param), y)\n        ]\n\n      r = control_flow_ops.while_loop(c, b, [n0, y0], parallel_iterations=1)\n      r = gradients_impl.gradients(r, param)[0]\n      self.assertAllClose(107520.0, self.evaluate(r))\n\n  @test_util.run_deprecated_v1\n  def testNestedWhileAndTensorArray(self):\n    n = constant_op.constant(3.0)\n\n    def Body(row, ta):\n\n      def InnerBody(row, col, ta):\n        # Note: row and col are 1-based.\n        ta = ta.write(\n            math_ops.cast(n * (row - 1.) + col - 1., dtypes.int32), row * col)\n        return row, col + 1., ta\n\n      ta = control_flow_ops.while_loop(\n          lambda _, col, _1: col <= n,\n          InnerBody, [row, constant_op.constant(1.), ta],\n          return_same_structure=False)[2]\n      return row + 1., ta\n\n    ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=9)\n    ta = control_flow_ops.while_loop(\n        lambda row, _: row <= n,\n        Body, [constant_op.constant(1.), ta],\n        return_same_structure=False)[1]\n\n    output = array_ops.reshape(ta.stack(), [3, 3])\n    self.assertAllEqual(\n        self.evaluate(output), [[1., 2., 3.], [2., 4., 6.], [3., 6., 9.]])\n    # TODO(b/117675481): This does not work with current TA. Enable with new TA.\n    # grad = gradients_impl.gradients(output, [n])\n    # self.assertEqual(self.evaluate(grad), 3.5)\n\n  @test_util.run_deprecated_v1\n  def testWhileGrad_StopGrad(self):\n    with self.cached_session():\n      x = constant_op.constant(3.0, name=\"x\")\n      y = constant_op.constant(2.0, name=\"y\")\n\n      c = lambda x, y: math_ops.less(x, 100.0)\n\n      def b(x, y):\n        y1 = math_ops.square(y)\n        x1 = math_ops.add(math_ops.square(x), y1)\n        return x1, y1\n\n      rx, ry = control_flow_ops.while_loop(c, b, [x, y])\n\n      r = gradients_impl.gradients(rx, y)[0]\n      self.assertEqual(136.0, self.evaluate(r))\n      r = gradients_impl.gradients(ry, y)[0]\n      self.assertEqual(32.0, self.evaluate(r))\n\n      r = gradients_impl.gradients(array_ops.stop_gradient(rx), y)[0]\n      self.assertEqual(r, None)\n      r = gradients_impl.gradients(array_ops.stop_gradient(ry), y)[0]\n      self.assertEqual(r, None)\n\n      r = gradients_impl.gradients(\n          array_ops.stop_gradient(math_ops.square(rx)), y)[0]\n      self.assertEqual(r, None)\n      r = gradients_impl.gradients(\n          array_ops.stop_gradient(math_ops.add(rx, ry)), x)[0]\n      self.assertEqual(r, None)\n      r = gradients_impl.gradients(\n          array_ops.stop_gradient(math_ops.add(rx, ry)), y)[0]\n      self.assertEqual(r, None)\n\n      r = gradients_impl.gradients(math_ops.add(rx, ry), y)[0]\n      self.assertEqual(168.0, self.evaluate(r))\n      r = gradients_impl.gradients(\n          math_ops.add(rx, array_ops.stop_gradient(ry)), y)[0]\n      self.assertEqual(136.0, self.evaluate(r))\n      r = gradients_impl.gradients(\n          math_ops.add(array_ops.stop_gradient(rx), ry), y)[0]\n      self.assertEqual(32.0, self.evaluate(r))\n\n  @test_util.run_deprecated_v1\n  def testWhileGrad_StopGradInside(self):\n    with self.cached_session():\n      x = constant_op.constant(3.0, name=\"x\")\n      y = constant_op.constant(2.0, name=\"y\")\n\n      c = lambda x, y: math_ops.less(x, 100.0)\n\n      def b(x, y):\n        y1 = array_ops.stop_gradient(math_ops.square(y))\n        x1 = math_ops.add(math_ops.square(x), y1)\n        return x1, y1\n\n      rx, _ = control_flow_ops.while_loop(c, b, [x, y])\n\n      r = gradients_impl.gradients(rx, y)[0]\n      self.assertAllClose(0.0, self.evaluate(r))\n      r = gradients_impl.gradients(rx, x)[0]\n      self.assertAllClose(156.0, self.evaluate(r))\n\n  @test_util.run_deprecated_v1\n  def testWhileGrad_StopGradInsideNoShape(self):\n    with self.cached_session() as sess:\n      x = array_ops.placeholder(dtypes.float32)\n      y = array_ops.placeholder(dtypes.float32)\n\n      c = lambda x, y: math_ops.less(math_ops.reduce_sum(x), 100.0)\n\n      def b(x, y):\n        y1 = array_ops.stop_gradient(math_ops.square(y, name=\"stopped\"))\n        x1 = math_ops.add(math_ops.square(x), y1)\n        return x1, y1\n\n      rx, _ = control_flow_ops.while_loop(c, b, [x, y])\n\n      grad_y = gradients_impl.gradients(rx, y)[0]\n      grad_x = gradients_impl.gradients(rx, x)[0]\n      feed_dict = {x: [3.0, 4.0], y: [2.0, 3.0]}\n      self.assertAllClose([0.0, 0.0], sess.run(grad_y, feed_dict=feed_dict))\n      self.assertAllClose([156.0, 400.0], sess.run(grad_x, feed_dict=feed_dict))\n      name = \"gradients/while/stopped_grad\"\n      all_ops = x.graph.get_operations()\n      self.assertFalse(any(name in op.name for op in all_ops))\n\n  @test_util.run_deprecated_v1\n  def testWhileGradGradFail(self):\n    theta = variables.Variable(initial_value=1.)\n\n    def fn(prev, x):\n      return prev + x * theta\n\n    result = functional_ops.scan(fn, np.array([1., 2., 3.], dtype=np.float32))\n    grad_theta = gradients_impl.gradients(result, theta)\n    if not control_flow_util.ENABLE_CONTROL_FLOW_V2:\n      with self.assertRaisesRegex(TypeError, \"Second-order gradient\"):\n        gradients_impl.gradients(grad_theta, theta)\n    grad_theta_stopped = array_ops.stop_gradient(grad_theta)\n    gradients_impl.gradients(grad_theta_stopped, theta)\n\n  @test_util.run_deprecated_v1\n  def testStopGradOnWhileGrad(self):\n    with self.cached_session():\n      x = constant_op.constant(2.0, name=\"x\")\n      y = constant_op.constant(2.0, name=\"y\")\n\n      c = lambda x: math_ops.less(x, 100.0)\n      b = lambda x: math_ops.multiply(x, y)\n      rx = control_flow_ops.while_loop(c, b, [x])\n\n      rg = gradients_impl.gradients(rx, y)[0]\n      rg = array_ops.stop_gradient(rg)\n      r = math_ops.add(math_ops.square(y), rx)\n      r = math_ops.add(r, rg)\n      r = gradients_impl.gradients(r, y)[0]\n      self.assertEqual(388.0, self.evaluate(r))\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (RefVariable)\")\n  @test_util.run_deprecated_v1\n  def testWhileGradientWithNontrainablePath1(self):\n    q = variables.Variable([7., 8.])\n\n    def cond(_, y):\n      del y\n      return False\n\n    def body(x, _):\n      return x, math_ops.cast(x, dtypes.float32) + math_ops.reduce_sum(q)\n\n    _, y = control_flow_ops.while_loop(cond, body, (math_ops.argmin(q), 0.))\n    dy_dq, = gradients_impl.gradients(y, q)\n    self.assertIsNotNone(dy_dq)\n    with self.cached_session() as sess:\n      self.evaluate(q.initializer)\n      self.assertAllClose([0., 0.], self.evaluate(dy_dq))\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (RefVariable)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGradientWithNontrainablePath2(self):\n    q = variables.Variable([7., 8.])\n\n    def cond(_, y):\n      return math_ops.equal(y, 0.)\n\n    def body(x, _):\n      zero = constant_op.constant(0, dtype=dtypes.int64)\n      return zero, math_ops.cast(x, dtypes.float32) + math_ops.reduce_sum(q)\n\n    _, y = control_flow_ops.while_loop(cond, body, (math_ops.argmin(q), 0.))\n    dy_dq, = gradients_impl.gradients(y, q)\n    self.assertIsNotNone(dy_dq)\n    with self.cached_session() as sess:\n      self.evaluate(q.initializer)\n      self.assertAllClose([1., 1.], self.evaluate(dy_dq))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testIssue16504(self):\n    c = constant_op.constant(np.arange(100), dtype=dtypes.float32)\n    w = variables.Variable(\n        initial_value=np.ones(100), dtype=dtypes.float32) / 100\n    k = variables.Variable(0, dtype=dtypes.int32)\n    chg_w = constant_op.constant(np.inf, dtype=dtypes.float32)\n\n    def cond(k, _, chg_w):\n      return math_ops.logical_and(k < 10, chg_w > 1e-3)\n\n    def body(k, w, chg_w):\n      grad, = gradients_impl.gradients(-math_ops.reduce_sum(w * c), w)\n      w_n = w * math_ops.exp(-0.1 * grad)\n      w_n /= math_ops.reduce_sum(w_n)\n      chg_w = (\n          math_ops.reduce_sum(math_ops.abs(w_n - w)) / math_ops.reduce_sum(\n              math_ops.abs(w)))\n      return k + 1, w_n, chg_w\n\n    _, w, _ = control_flow_ops.while_loop(cond, body, [k, w, chg_w])\n    grad, = gradients_impl.gradients(w, c)\n    self.assertIsNotNone(grad)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testStopGradMultiFlows(self):\n    with self.cached_session():\n\n      def body(i, y, r):\n        x = variable_scope.get_variable(\n            \"x\",\n            shape=(),\n            dtype=dtypes.float32,\n            initializer=init_ops.ones_initializer())\n        y *= x\n        return [i + 1, y, r + math_ops.reduce_sum(y)]\n\n      i0 = constant_op.constant(0)\n      y0 = array_ops.ones(5)\n      r0 = constant_op.constant(0.0)\n      cond = lambda i, y, r: i < 1\n      _, _, r = control_flow_ops.while_loop(\n          cond, body, [i0, y0, r0], back_prop=True)\n\n      vars_ = variables.global_variables()\n      grads = linalg_ops.norm(gradients_impl.gradients(r, vars_)[0])\n      z = math_ops.add(r, array_ops.stop_gradient(math_ops.reduce_sum(grads)))\n      result = gradients_impl.gradients(z, vars_)[0]\n      self.evaluate(variables.global_variables_initializer())\n      self.assertEqual(5.0, self.evaluate(result))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testOneValueCond(self):\n\n    with self.cached_session():\n      c = array_ops.placeholder(dtypes.int32, shape=[])\n      one = ops.convert_to_tensor(1, name=\"one\")\n      two = ops.convert_to_tensor(2, name=\"two\")\n      p = math_ops.greater_equal(c, 1)\n      i = control_flow_ops.cond(p, lambda: one, lambda: two)\n      self.assertTrue(isinstance(i, ops.Tensor))\n\n      # True case: c = 2 is >= 1\n      self.assertEqual([1], i.eval(feed_dict={c: 2}))\n\n      # False case: c = 0 is not >= 1\n      self.assertEqual([2], i.eval(feed_dict={c: 0}))\n\n  @test_util.run_deprecated_v1\n  def testExampleCond(self):\n\n    with self.cached_session():\n      x = ops.convert_to_tensor([-2.0, 2.0], name=\"x\")\n      d = array_ops.placeholder(dtypes.int32, shape=[])\n\n      def l2():\n        return math_ops.sqrt(math_ops.reduce_sum(math_ops.square(x)))\n\n      def l1():\n        return math_ops.reduce_sum(math_ops.abs(x))\n\n      i = control_flow_ops.cond(math_ops.equal(d, 2), l2, l1)\n      self.assertAllClose(4.0, i.eval(feed_dict={d: 1}))\n      self.assertAllClose(2.0 * math.sqrt(2), i.eval(feed_dict={d: 2}))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCase(self):\n    with self.cached_session():\n      x = constant_op.constant(1)\n      y = constant_op.constant(2)\n      z = constant_op.constant(3)\n      f1 = lambda: constant_op.constant(17)\n      f2 = lambda: constant_op.constant(23)\n      f3 = lambda: constant_op.constant(-1)\n\n      r1 = control_flow_ops.case(\n          {\n              x < y: f1,\n              x > z: f2\n          }, default=f3, exclusive=True)\n      self.assertAllEqual(r1, 17)\n\n      r2 = control_flow_ops.case([(y > z, f1), (y > x, f2)], default=f3)\n      self.assertAllEqual(r2, 23)\n\n      # Duplicate events can happen, first one is selected\n      r3 = control_flow_ops.case([(x < y, f1), (x < y, f2)], default=f3)\n      self.assertAllEqual(r3, 17)\n\n      # Duplicate events cause an error if exclusive = True\n      r4 = control_flow_ops.case(\n          [(x < y, f1), (x < y, f2)], default=f3, exclusive=True)\n      with self.assertRaisesOpError(\"Input error:\"):\n        self.evaluate(r4)\n\n      # Check that the default is called if none of the others are\n      r5 = control_flow_ops.case({x > y: f1}, default=f3)\n      self.assertAllEqual(r5, -1)\n\n      ran_once = [False, False, False]\n\n      def break_run_twice(ix):\n\n        def _break():\n          ran_once[ix] = True\n          return constant_op.constant(ix)\n\n        return _break\n\n      # Should not fail - each conditional gets called exactly once\n      # except default.  Default gets called twice: once to create an\n      # empty output and once for the actual cond switch.\n      r6 = control_flow_ops.case(\n          [(x < y, break_run_twice(0)), (x > y, break_run_twice(1))],\n          default=lambda: constant_op.constant(2))\n\n      self.assertAllEqual(r6, 0)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCaseSideEffects(self):\n    with self.cached_session() as sess:\n      v0 = variables.Variable(-1)\n      v1 = variables.Variable(-1)\n      v2 = variables.Variable(-1)\n\n      a = lambda: control_flow_ops.with_dependencies([state_ops.assign(v0, 0)], 0)\n      b = lambda: control_flow_ops.with_dependencies([state_ops.assign(v1, 1)], 1)\n      c = lambda: control_flow_ops.with_dependencies([state_ops.assign(v2, 2)], 2)\n\n      x = constant_op.constant(1)\n      y = constant_op.constant(2)\n\n      r0 = control_flow_ops.case(\n          ((x < y, a), (x > y, b)), default=c, exclusive=True)\n      r1 = control_flow_ops.case(\n          ((x > y, a), (x < y, b)), default=c, exclusive=True)\n      r2 = control_flow_ops.case(\n          ((x > y, a), (x > y, b)), default=c, exclusive=True)\n\n      self.evaluate(variables.global_variables_initializer())\n      self.assertAllEqual(self.evaluate([v0, v1, v2]), [-1] * 3)\n      self.assertEqual(2, self.evaluate(r2))\n      self.assertAllEqual(self.evaluate([v0, v1, v2]), [-1, -1, 2])\n\n      self.evaluate(variables.global_variables_initializer())\n      self.assertAllEqual(self.evaluate([v0, v1, v2]), [-1] * 3)\n      self.assertEqual(1, self.evaluate(r1))\n      self.assertAllEqual(self.evaluate([v0, v1, v2]), [-1, 1, -1])\n\n      self.evaluate(variables.global_variables_initializer())\n      self.assertAllEqual(self.evaluate([v0, v1, v2]), [-1] * 3)\n      self.assertEqual(0, self.evaluate(r0))\n      self.assertAllEqual(self.evaluate([v0, v1, v2]), [0, -1, -1])\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (ref vars)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testOneOpCond(self):\n    with self.cached_session():\n      v = variables.Variable(0)\n      c = ops.convert_to_tensor(0)\n      one = ops.convert_to_tensor(1)\n      two = ops.convert_to_tensor(2)\n      p = math_ops.greater_equal(c, 1)\n\n      def a():\n        return state_ops.assign(v, one)\n\n      def b():\n        return state_ops.assign(v, two)\n\n      i = control_flow_ops.cond(p, a, b)\n      self.assertTrue(isinstance(i, ops.Tensor))\n      self.evaluate(variables.global_variables_initializer())\n\n      self.assertEqual(0, self.evaluate(v))\n\n      # True case: c = 2 is >= 1, v is set to 1.\n      self.assertEqual(1, i.eval(feed_dict={c.name: 2}))\n      self.assertEqual(1, self.evaluate(v))\n\n      # False case: c = 0 is not >= 1, v is set to 2.\n      self.assertEqual(2, i.eval(feed_dict={c.name: 0}))\n      self.assertEqual(2, self.evaluate(v))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWithOpsDependencies(self):\n    with self.cached_session() as sess:\n      v = variables.VariableV1(0.0)\n      c = constant_op.constant(10)\n\n      # Fetching v directly will result in an uninitialized error\n      with self.assertRaisesOpError(\"Attempting to use uninitialized value\"):\n        self.evaluate([c, v])\n\n      # Use a control dependency to ensure init_variable is run\n      # while asking for c\n      real_v = control_flow_ops.with_dependencies(\n          name=\"real_tensor\",\n          output_tensor=v._ref(),  # pylint: disable=protected-access\n          dependencies=[v.initializer])\n      c_val, real_v_val = self.evaluate([c, real_v])\n\n    # Ensure the result of 'real_c' is the same as 'c'\n    self.assertAllEqual(10, c_val)\n\n    # Ensure that 'v' is initialized\n    self.assertAllClose(0.0, real_v_val)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWithTensorDependencies(self):\n    with self.cached_session():\n      v = variables.VariableV1(0.0)\n      c1 = constant_op.constant(10)\n      c2 = constant_op.constant(20)\n\n      # c1_with_init_v depends on the init op for v\n      c1_with_init_v = control_flow_ops.with_dependencies(\n          name=\"c1_with_init_v\", output_tensor=c1, dependencies=[v.initializer])\n      # c2_with_c1 depends on the value of c1_with_init_v\n      c2_with_c1_dep = control_flow_ops.with_dependencies(\n          name=\"c2_with_c1_dep\",\n          output_tensor=c2,\n          dependencies=[c1_with_init_v])\n\n      # Fetching v directly will result in an uninitialized error\n      with self.assertRaisesOpError(\"Attempting to use uninitialized value\"):\n        self.evaluate(v)\n\n      # Get the value of 'c2_with_c1_dep', which should cause 'v'\n      # to be initialized.\n      self.assertAllEqual(20, self.evaluate(c2_with_c1_dep))\n\n      # Ensure that 'v' is initialized\n      self.assertAllClose(0.0, self.evaluate(v))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWithIndexedSlicesDependencies(self):\n    with self.cached_session():\n      v = variables.VariableV1(\n          np.array([[0.0, 1.0], [10.0, 11.0], [20.0, 21.0]]).astype(np.float32))\n      v_at_1 = ops.IndexedSlices(v, constant_op.constant([1]))\n      gather_v_at_1 = array_ops.gather(v_at_1.values, v_at_1.indices)\n      v_at_1_after_init = control_flow_ops.with_dependencies([v.initializer],\n                                                             v_at_1)\n      gather_v_at_1_after_init = array_ops.gather(v_at_1_after_init.values,\n                                                  v_at_1_after_init.indices)\n\n      # Fetching gather_v_at_1 will result in an uninitialized error\n      with self.assertRaisesOpError(\"Attempting to use uninitialized value\"):\n        self.evaluate(gather_v_at_1)\n\n      # Getting gather_v_at_1_after_init will work, and initialize v.\n      self.assertAllEqual([[10.0, 11.0]],\n                          self.evaluate(gather_v_at_1_after_init))\n\n      # Double check that 'v' is initialized\n      self.assertAllClose([[0.0, 1.0], [10.0, 11.0], [20.0, 21.0]],\n                          self.evaluate(v))\n\n  def testDependenciesDevice(self):\n    with ops.Graph().as_default():\n      # device set on tensor => same device on dep.\n      with ops.device(\"/job:ps\"):\n        vd = variables.VariableV1([0.0])\n      with_vd_dep = control_flow_ops.with_dependencies([vd.initializer], vd)\n      self.assertTrue(\"/job:ps\" in with_vd_dep.device)\n\n      # No device set on tensor => no device on dep.\n      vnod = variables.VariableV1([0.0])\n      with_vnod_dep = control_flow_ops.with_dependencies([vnod.initializer],\n                                                         vnod)\n      self.assertDeviceEqual(None, with_vnod_dep.device)\n\n      # device set on tensor, default device on graph => default device on dep.\n      vdef = variables.VariableV1([0.0], name=\"vdef\")\n      with ops.device(\"/job:worker/device:GPU:1\"):\n        with_vdef_dep = control_flow_ops.with_dependencies([vdef.initializer],\n                                                           vdef)\n        # The device is empty, but the colocation constraint is set.\n        self.assertDeviceEqual(\"\", with_vdef_dep.device)\n        self.assertEqual([b\"loc:@vdef\"], with_vdef_dep.op.colocation_groups())\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testGroup(self):\n    with self.cached_session() as sess:\n      v1 = variables.VariableV1([0.0])\n      v2 = variables.VariableV1([1.0])\n\n      # Group init1 and init2 and run.\n      init = control_flow_ops.group(v1.initializer, v2.initializer)\n      # Fetching v1 directly will result in an uninitialized error\n      with self.assertRaisesOpError(\"Attempting to use uninitialized value\"):\n        self.evaluate(v1)\n\n      # Runs \"init\" before fetching v1 and v2.\n      init.run()\n      v1_val, v2_val = self.evaluate([v1, v2])\n\n    # Ensure that v1 and v2 are initialized\n    self.assertAllClose([0.0], v1_val)\n    self.assertAllClose([1.0], v2_val)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testGroupEmpty(self):\n    op = control_flow_ops.group()\n    self.assertEqual(op.type, \"NoOp\")\n    self.assertEqual(op.control_inputs, [])\n\n  @test_util.run_deprecated_v1\n  def testMergeShapes(self):\n    # All inputs unknown.\n    p1 = array_ops.placeholder(dtypes.float32)\n    p2 = array_ops.placeholder(dtypes.float32)\n    p3 = array_ops.placeholder(dtypes.float32)\n    m, index = control_flow_ops.merge([p1, p2, p3])\n    self.assertIs(None, m.get_shape().ndims)\n    self.assertEqual([], index.get_shape())\n\n    # All inputs known with different ranks.\n    p1 = array_ops.placeholder(dtypes.float32, shape=[1, 2])\n    p2 = array_ops.placeholder(dtypes.float32, shape=[1, 2, 3])\n    m, index = control_flow_ops.merge([p1, p2])\n    self.assertIs(None, m.get_shape().ndims)\n    self.assertEqual([], index.get_shape())\n\n    # All inputs known with some dimensions different.\n    p1 = array_ops.placeholder(dtypes.float32, shape=[1, 2])\n    p2 = array_ops.placeholder(dtypes.float32, shape=[2, 1])\n    m, index = control_flow_ops.merge([p1, p2])\n    self.assertEqual([None, None], m.get_shape().as_list())\n    self.assertEqual([], index.get_shape())\n\n    p1 = array_ops.placeholder(dtypes.float32, shape=[1, 2])\n    p2 = array_ops.placeholder(dtypes.float32, shape=[None, 2])\n    m, index = control_flow_ops.merge([p1, p2])\n    self.assertEqual([None, 2], m.get_shape().as_list())\n    self.assertEqual([], index.get_shape())\n\n    p1 = array_ops.placeholder(dtypes.float32, shape=[1, 2])\n    p2 = array_ops.placeholder(dtypes.float32, shape=[2, 2])\n    m, index = control_flow_ops.merge([p1, p2])\n    self.assertEqual([None, 2], m.get_shape().as_list())\n    self.assertEqual([], index.get_shape())\n\n    # All inputs known with same dimensions.\n    p1 = array_ops.placeholder(dtypes.float32, shape=[1, 2])\n    p2 = array_ops.placeholder(dtypes.float32, shape=[1, 2])\n    m, index = control_flow_ops.merge([p1, p2])\n    self.assertEqual([1, 2], m.get_shape().as_list())\n    self.assertEqual([], index.get_shape())\n\n    p1 = array_ops.placeholder(dtypes.float32, shape=[None, 2])\n    p2 = array_ops.placeholder(dtypes.float32, shape=[None, 2])\n    m, index = control_flow_ops.merge([p1, p2])\n    self.assertEqual([None, 2], m.get_shape().as_list())\n    self.assertEqual([], index.get_shape())\n\n    p1 = array_ops.placeholder(dtypes.float32, shape=[None, None])\n    p2 = array_ops.placeholder(dtypes.float32, shape=[None, None])\n    m, index = control_flow_ops.merge([p1, p2])\n    self.assertEqual([None, None], m.get_shape().as_list())\n    self.assertEqual([], index.get_shape())\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testRefSelect(self):\n    index = array_ops.placeholder(dtypes.int32)\n\n    # All inputs unknown.\n    p1 = array_ops.placeholder(dtypes.float32)\n    p2 = array_ops.placeholder(dtypes.float32)\n    p3 = array_ops.placeholder(dtypes.float32)\n    v1 = variables.VariableV1(p1, validate_shape=False)\n    v2 = variables.VariableV1(p2, validate_shape=False)\n    v3 = variables.VariableV1(p3, validate_shape=False)\n    self.assertIs(None, v1.get_shape().ndims)\n    s = control_flow_ops.ref_select(index, [v1, v2, v3])\n    self.assertIs(None, s.get_shape().ndims)\n\n    # All inputs known but different.\n    v1 = variables.VariableV1([[1, 2]])\n    v2 = variables.VariableV1([[2], [1]])\n    s = control_flow_ops.ref_select(index, [v1, v2])\n    self.assertIs(None, s.get_shape().ndims)\n\n    # All inputs known and same.\n    v1 = variables.VariableV1([[1, 2]])\n    v2 = variables.VariableV1([[1, 2]])\n    s = control_flow_ops.ref_select(index, [v1, v2])\n    self.assertEqual([1, 2], s.get_shape())\n\n    # Possibly the same but not guaranteed.\n    v1 = variables.VariableV1([[1., 2.]])\n    p2 = array_ops.placeholder(dtypes.float32, shape=[None, 2])\n    v2 = variables.VariableV1(p2, validate_shape=False)\n    s = control_flow_ops.ref_select(index, [v1, v2])\n    self.assertEqual(None, s.get_shape())\n\n  @test_util.run_deprecated_v1\n  def testRunLoopTensor(self):\n    with self.cached_session() as sess:\n      tensor_list = []\n\n      def condition(t):\n        return t < constant_op.constant(5)\n\n      def body(_):\n        tensor_list.append(constant_op.constant(5))\n        return constant_op.constant(10)\n\n      result = control_flow_ops.while_loop(condition, body,\n                                           [constant_op.constant(4)])\n      self.assertEqual(10, self.evaluate(result))\n\n      # Ensure that we cannot run a tensor that escapes the loop body\n      # accidentally.\n      with self.assertRaises(ValueError):\n        sess.run(tensor_list[0])\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhilePyFuncBasic(self):\n\n    def func(x):\n      return np.square(x)\n\n    with self.cached_session():\n      r = control_flow_ops.while_loop(\n          lambda i, v: i < 4,\n          lambda i, v: [i + 1, script_ops.py_func(func, [v], [dtypes.float32])[0]],\n          [constant_op.constant(0), constant_op.constant(2.0, dtypes.float32)],\n          [tensor_shape.unknown_shape(), tensor_shape.unknown_shape()])\n      self.assertEqual(self.evaluate(r[1]), 65536.0)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileFuncBasic(self):\n\n    @function.Defun(dtypes.float32)\n    def func(x):\n      return math_ops.square(math_ops.square(x))\n\n    with self.cached_session():\n      x = constant_op.constant(2.0, dtypes.float32)\n      r = control_flow_ops.while_loop(\n          lambda i, v: i < 2, lambda i, v: [i + 1, func(v)],\n          [constant_op.constant(0), x],\n          [tensor_shape.unknown_shape(),\n           tensor_shape.unknown_shape()])\n      grad = gradients_impl.gradients(r, x)[0]\n      self.assertEqual(self.evaluate(r[1]), 65536.0)\n      self.assertEqual(self.evaluate(grad), 524288.0)\n      # while_v2 does not have stacks.\n      if not control_flow_util.ENABLE_CONTROL_FLOW_V2:\n        self.assertEqual(\n            len([op for op in x.graph.get_operations() if op.type == \"StackV2\"\n                ]), 1)\n\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testQIntSwitchMerge(self):\n    with self.cached_session(force_gpu=test.is_gpu_available()) as sess:\n      constant_qint = constant_op.constant(np.array([42]), dtypes.qint8)\n      cond = constant_op.constant(True, dtypes.bool)\n      v_f, v_t = control_flow_ops.switch(constant_qint, cond)\n      result = control_flow_ops.merge([v_f, v_t])\n      self.evaluate(result)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testQIntRefSwitchMerge(self):\n    with self.cached_session(use_gpu=test.is_gpu_available()) as sess:\n      var_qint = gen_state_ops.variable(\n          shape=[1], dtype=dtypes.qint8, name=\"v\", container=\"\", shared_name=\"\")\n      assign_op = state_ops.assign(\n          var_qint, constant_op.constant(np.array([42]), dtypes.qint8))\n      self.evaluate(assign_op)\n\n      cond = constant_op.constant(True, dtypes.bool)\n      v_f, v_t = control_flow_ops.ref_switch(var_qint, cond)\n      result = control_flow_ops.ref_merge([v_f, v_t])\n      self.evaluate(result)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testUInt64SwitchMerge(self):\n    with self.cached_session(force_gpu=test.is_gpu_available()) as sess:\n      constant_uint64 = constant_op.constant(np.array([42]), dtypes.uint64)\n      cond = constant_op.constant(True, dtypes.bool)\n      v_f, v_t = control_flow_ops.switch(constant_uint64, cond)\n      result = control_flow_ops.merge([v_f, v_t])\n      self.evaluate(result)\n\n  def testSwitchEagerMode(self):\n    if not context.executing_eagerly():\n      return\n    input_data = [1, 2, 3, 4]\n    vf, vt = control_flow_ops.switch(input_data, False)\n    self.assertAllEqual(vf, input_data)\n    self.assertAllEqual(vt, [])\n\n  @test_util.run_deprecated_v1\n  def testQIntArgAndRet(self):\n\n    @function.Defun(dtypes.qint8)\n    def func(x):\n      return x\n\n    with self.cached_session(force_gpu=test.is_gpu_available()) as sess:\n      qint = constant_op.constant(np.array([42]), dtypes.qint8)\n      result = func(qint)\n      self.evaluate(result)\n\n  def testSparseIdentity(self):\n    st1 = sparse_tensor.SparseTensor([[0, 5]], ['x'], [10, 10])\n    st2 = control_flow_ops._Identity(st1)\n    self.assertAllEqual(st1.indices, st2.indices)\n    self.assertAllEqual(st1.values, st2.values)\n    self.assertAllEqual(st1.dense_shape, st2.dense_shape)\n\n  def testSparseEnterExit(self):\n    st1 = sparse_tensor.SparseTensor([[0, 5]], ['x'], [10, 10])\n    st2 = control_flow_ops._Enter(st1, \"foo_1\")\n    st3 = control_flow_ops.exit(st2)\n    self.assertAllEqual(st1.indices, st3.indices)\n    self.assertAllEqual(st1.values, st3.values)\n    self.assertAllEqual(st1.dense_shape, st3.dense_shape)\n\n  def _buildWhileWithShapeInvariants(self, shape_invariants):\n    r = constant_op.constant([1, 2])\n\n    def cond(_):\n      return False\n\n    def body(_):\n      return constant_op.constant([1])\n\n    return control_flow_ops.while_loop(\n        cond, body, [r], shape_invariants=shape_invariants)\n\n  def testWhileOutputShapeWithShapeInvariantsUnknownRank(self):\n    @def_function.function\n    def runTest():\n      while_output = self._buildWhileWithShapeInvariants(\n          [tensor_shape.TensorShape(None)])\n      self.assertIsNone(while_output.shape.rank)\n    runTest()\n\n  def testWhileOutputShapeWithShapeInvariantsPartialShape(self):\n    @def_function.function\n    def runTest():\n      while_output = self._buildWhileWithShapeInvariants(\n          [tensor_shape.TensorShape([None])])\n      self.assertAllEqual(while_output.shape.as_list(), [None])\n    runTest()\n\n  def testFunctionInWhile(self):\n\n    @def_function.function\n    def body(x):\n      return x + 1\n\n    r = control_flow_ops.while_loop(lambda x: x < 5, body, [0])\n    self.assertAllEqual(r, 5.)\n\n\nclass ControlFlowContextCheckTest(test.TestCase):\n\n  def _getWhileTensor(self):\n    \"\"\"Creates and returns a tensor from a while context.\"\"\"\n    tensor = []\n\n    def body(i):\n      if not tensor:\n        tensor.append(constant_op.constant(1))\n      return i + tensor[0]\n\n    control_flow_ops.while_loop(lambda i: i < 10, body, [0])\n    return tensor[0]\n\n  def _getCondTensor(self):\n    cond_tensor = []\n\n    def true_fn():\n      if not cond_tensor:\n        cond_tensor.append(constant_op.constant(1))\n      return cond_tensor[0]\n\n    control_flow_ops.cond(\n        math_ops.less(1, 2), true_fn, lambda: constant_op.constant(0))\n    return cond_tensor[0]\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testInvalidContext(self):\n    # Accessing a while loop tensor outside of control flow is illegal.\n    while_tensor = self._getWhileTensor()\n    with self.assertRaisesRegex(\n        ValueError,\n        \"Cannot use 'while/Const_1' as input to 'Add' because 'while/Const_1' \"\n        \"is in a while loop. See info log for more details.\"):\n      math_ops.add(1, while_tensor)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testInvalidContextInCond(self):\n    # Accessing a while loop tensor in cond is illegal.\n    while_tensor = self._getWhileTensor()\n    with self.assertRaisesRegex(\n        ValueError, \"Cannot use 'while/Const_1' as input to 'cond/Add' because \"\n        \"'while/Const_1' is in a while loop. See info log for more details.\"):\n      # TODO(skyewm): this passes if we return while_tensor directly instead\n      # of using it as input to another op.\n      control_flow_ops.cond(\n          math_ops.less(1, 2), lambda: math_ops.add(1, while_tensor),\n          lambda: constant_op.constant(0))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testInvalidContextInWhile(self):\n    # Accessing a while loop tensor in a different while loop is illegal.\n    while_tensor = self._getWhileTensor()\n    with self.assertRaisesRegex(\n        ValueError,\n        \"Cannot use 'while/Const_1' as input to 'while_1/Add' because they are \"\n        \"in different while loops. See info log for more details.\"):\n      control_flow_ops.while_loop(lambda i: i < 10,\n                                  lambda x: math_ops.add(1, while_tensor), [0])\n\n    with self.assertRaisesRegex(\n        ValueError,\n        \"Cannot use 'while/Const_1' as input to 'while_2/NextIteration' \"\n        \"because they are in different while loops. See info log for more \"\n        \"details.\"):\n      control_flow_ops.while_loop(lambda i: i < 10, lambda i: while_tensor, [0])\n\n  def testValidCondContext(self):\n    # Accessing a tensor from a cond context is OK (although dangerous).\n    cond_tensor = self._getCondTensor()\n    math_ops.add(1, cond_tensor)\n\n  def testValidCondContextBranches(self):\n    # Accessing a tensor from a cond context from the other branch's cond\n    # context is OK (although dangerous).\n    cond_tensor = []\n\n    def branch_fn():\n      if not cond_tensor:\n        cond_tensor.append(constant_op.constant(1))\n      return cond_tensor[0]\n\n    control_flow_ops.cond(math_ops.less(1, 2), branch_fn, branch_fn)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testValidWhileContext(self):\n    # Accessing a tensor in a nested while is OK.\n    def body(_):\n      c = constant_op.constant(1)\n      return control_flow_ops.while_loop(lambda i: i < 3, lambda i: i + c, [0])\n\n    control_flow_ops.while_loop(lambda i: i < 5, body, [0])\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testValidNestedContexts(self):\n    # Accessing a tensor from a cond context in a while context, all inside an\n    # outer while context, is OK.\n    def body(_):\n      cond_tensor = self._getCondTensor()\n      # Create another cond containing the while loop for good measure\n      return control_flow_ops.cond(\n          math_ops.less(1, 2),\n          lambda: control_flow_ops.while_loop(lambda i: i < 3,\n                                              lambda i: i + cond_tensor, [0]),\n          lambda: constant_op.constant(0))\n\n    control_flow_ops.while_loop(lambda i: i < 5, body, [0])\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testInvalidNestedContexts(self):\n    # Accessing a tensor from a while context in a different while context, all\n    # inside a cond context, is illegal.\n    def true_fn():\n      while_tensor = self._getWhileTensor()\n      return control_flow_ops.while_loop(lambda i: i < 3,\n                                         lambda i: i + while_tensor, [0])\n\n    with self.assertRaisesRegex(\n        ValueError,\n        \"Cannot use 'cond/while/Const_1' as input to 'cond/while_1/add' because\"\n        \" they are in different while loops. See info log for more details.\"):\n      control_flow_ops.cond(\n          math_ops.less(1, 2), true_fn, lambda: constant_op.constant(0))\n\n\nclass TupleTest(test.TestCase):\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testTensors(self):\n    for v1_first in [True, False]:\n      with self.cached_session():\n        v1 = variables.VariableV1([1.0])\n        add1 = math_ops.add(\n            control_flow_ops.with_dependencies([v1.initializer], v1._ref()),  # pylint: disable=protected-access\n            2.0)\n        v2 = variables.VariableV1([10.0])\n        add2 = math_ops.add(\n            control_flow_ops.with_dependencies([v2.initializer], v2._ref()),  # pylint: disable=protected-access\n            20.0)\n        t1, _, t2 = control_flow_ops.tuple([add1, None, add2])\n\n        # v1 is not initialized.\n        with self.assertRaisesOpError(\"Attempting to use uninitialized value\"):\n          self.evaluate(v1)\n\n        # v2 is not initialized.\n        with self.assertRaisesOpError(\"Attempting to use uninitialized value\"):\n          self.evaluate(v2)\n\n        if v1_first:\n          # Getting t1 initializes v2.\n          self.assertAllClose([3.0], self.evaluate(t1))\n          self.assertAllClose([10.0], self.evaluate(v2))\n        else:\n          # Getting t2 initializes v1.\n          self.assertAllClose([30.0], self.evaluate(t2))\n          self.assertAllClose([1.0], self.evaluate(v1))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testIndexedSlices(self):\n    for v1_first in [True, False]:\n      with self.cached_session():\n        v1 = variables.VariableV1(\n            np.array([[0.0, 1.0], [10.0, 11.0], [20.0, 21.0]]).astype(\n                np.float32))\n        v1_at_1 = ops.IndexedSlices(\n            control_flow_ops.with_dependencies([v1.initializer], v1._ref()),  # pylint: disable=protected-access\n            constant_op.constant([1]))\n\n        v2 = variables.VariableV1(\n            np.array([[0.1, 1.1], [10.1, 11.1], [20.1, 21.1]]).astype(\n                np.float32))\n        v2_at_1 = ops.IndexedSlices(\n            control_flow_ops.with_dependencies([v2.initializer], v2._ref()),  # pylint: disable=protected-access\n            constant_op.constant([1]))\n\n        st1, st2 = control_flow_ops.tuple([v1_at_1, v2_at_1])\n        g1 = array_ops.gather(st1.values, st1.indices)\n        g2 = array_ops.gather(st2.values, st2.indices)\n\n        # v1 is not initialized.\n        with self.assertRaisesOpError(\"Attempting to use uninitialized value\"):\n          self.evaluate(v1)\n\n        # v2 is not initialized.\n        with self.assertRaisesOpError(\"Attempting to use uninitialized value\"):\n          self.evaluate(v2)\n\n        if v1_first:\n          # Getting g1 initializes v2.\n          self.assertAllClose([[10.0, 11.0]], self.evaluate(g1))\n          self.assertAllClose([[0.1, 1.1], [10.1, 11.1], [20.1, 21.1]],\n                              self.evaluate(v2))\n        else:\n          # Getting g2 initializes v1.\n          self.assertAllClose([[10.1, 11.1]], self.evaluate(g2))\n          self.assertAllClose([[0.0, 1.0], [10.0, 11.0], [20.0, 21.0]],\n                              self.evaluate(v1))\n\n  def testAcceptTensorsAsControlInputs(self):\n    with self.cached_session():\n      var = variables.VariableV1(0)\n      assign = state_ops.assign(var, 1)\n      t, = control_flow_ops.tuple(\n          [constant_op.constant(0)], control_inputs=[assign])\n\n      # Should trigger the assign.\n      self.evaluate(t)\n\n      self.assertEqual(1, self.evaluate(var))\n\n\nclass AssertTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testGuardedAssertDoesNotCopyWhenTrue(self):\n    if test_util.is_gpu_available():\n      self.skipTest(\"b/128646478 fails in opensource\")\n\n    with self.session(use_gpu=True) as sess:\n      with ops.device(test.gpu_device_name()):\n        value = constant_op.constant(1.0)\n      with ops.device(\"/cpu:0\"):\n        true = constant_op.constant(True)\n        guarded_assert = control_flow_ops.Assert(true, [value], name=\"guarded\")\n        unguarded_assert = gen_logging_ops._assert(\n            true, [value], name=\"unguarded\")\n      opts = config_pb2.RunOptions(trace_level=config_pb2.RunOptions.FULL_TRACE)\n      guarded_metadata = config_pb2.RunMetadata()\n      sess.run(guarded_assert, options=opts, run_metadata=guarded_metadata)\n      unguarded_metadata = config_pb2.RunMetadata()\n      sess.run(unguarded_assert, options=opts, run_metadata=unguarded_metadata)\n      guarded_nodestat_names = [\n          n.node_name\n          for d in guarded_metadata.step_stats.dev_stats\n          for n in d.node_stats\n      ]\n      unguarded_nodestat_names = [\n          n.node_name\n          for d in unguarded_metadata.step_stats.dev_stats\n          for n in d.node_stats\n      ]\n      guarded_memcpy_nodestat_names = [\n          n for n in guarded_nodestat_names if \"MEMCPYDtoH\" in n\n      ]\n      unguarded_memcpy_nodestat_names = [\n          n for n in unguarded_nodestat_names if \"MEMCPYDtoH\" in n\n      ]\n      if \"GPU\" in [d.device_type for d in device_lib.list_local_devices()]:\n        # A copy was performed for the unguarded assert\n        self.assertLess(0, len(unguarded_memcpy_nodestat_names),\n                        str(unguarded_nodestat_names))\n      # No copy was performed for the guarded assert\n      self.assertEqual([], guarded_memcpy_nodestat_names)\n\n\nclass WhileOpBenchmark(test.Benchmark):\n  \"\"\"Evaluate the performance of while_loop op.\"\"\"\n\n  def _getInitVariables(self):\n    batch_size = 10\n    image_size = 256\n    kernel_size = 3\n    depth = 16\n\n    init_step = constant_op.constant(-1)\n    image = variable_scope.get_variable(\n        \"image\",\n        initializer=random_ops.random_normal(\n            [batch_size, image_size, image_size, depth],\n            dtype=dtypes.float32,\n            stddev=1e-1))\n    kernel = variable_scope.get_variable(\n        \"weights\",\n        initializer=random_ops.truncated_normal(\n            [kernel_size, kernel_size, depth, depth],\n            dtype=dtypes.float32,\n            stddev=1e-1))\n    return init_step, image, kernel\n\n  def _runOneBenchmark(self,\n                       default_device,\n                       num_iters=10,\n                       static_unroll=False,\n                       steps=10):\n    \"\"\"Evaluate the while loop performance.\n\n    Args:\n      default_device: The default device to run all ops except the loop_body.\n        loop_body is always run on GPU.\n      num_iters: Number of iterations to run.\n      static_unroll: If true, run unrolled version; otherwise, run while_loop.\n      steps: Total number of repeated steps to run the loop.\n\n    Returns:\n      The duration of the run in seconds.\n    \"\"\"\n\n    def loop_body(i, x):\n      with ops.device(\"/gpu:0\"):\n        # Always put loop body on GPU.\n        nx = nn_ops.conv2d(\n            input=x,\n            filter=kernel,\n            strides=[1, 1, 1, 1],\n            padding=\"SAME\",\n            data_format=\"NHWC\",\n            name=\"conv2d\")\n        ni = math_ops.add(i, 1)\n        return ni, nx\n\n    ops.reset_default_graph()\n    with session.Session() as sess, ops.device(default_device):\n      # Get the initial id i, input x, and kernel.\n      i, x, kernel = self._getInitVariables()\n      self.evaluate(variables.global_variables_initializer())\n\n      if static_unroll:\n        for _ in xrange(steps):\n          i, x = loop_body(i, x)\n      else:\n        i, x = control_flow_ops.while_loop(\n            lambda i, _: i < steps,\n            loop_body, [i, x],\n            parallel_iterations=steps,\n            swap_memory=True)\n\n      r = math_ops.reduce_sum(x)\n      dx, dk = gradients_impl.gradients(r, [x, kernel])\n      # Use group to avoid fetching back results.\n      r = control_flow_ops.group(dx, dk)\n\n      for _ in xrange(3):\n        # exclude warm up time\n        self.evaluate(r)\n\n      start_time = time.time()\n      for _ in xrange(num_iters):\n        self.evaluate(r)\n      return (time.time() - start_time) / num_iters\n\n  def benchmarkWhileOpCrossDevicePlacement(self):\n    iters = 10\n    # Run loop body on GPU, but other ops on CPU.\n    duration = self._runOneBenchmark(\"cpu\", iters, static_unroll=False)\n    self.report_benchmark(\n        name=\"while_op_cross_device\", iters=iters, wall_time=duration)\n\n  def benchmarkWhileOpSameDevicePlacement(self):\n    iters = 10\n    # Run all ops on the same GPU device.\n    duration = self._runOneBenchmark(\"gpu\", iters, static_unroll=False)\n    self.report_benchmark(\n        name=\"while_op_same_device\", iters=iters, wall_time=duration)\n\n  def benchmarkWhileOpUnrollCrossDevicePlacement(self):\n    iters = 10\n    # Run loop body on GPU, but other ops on CPU.\n    duration = self._runOneBenchmark(\"cpu\", iters, static_unroll=True)\n    self.report_benchmark(\n        name=\"unroll_cross_device_cpu\", iters=iters, wall_time=duration)\n\n  def benchmarkWhileOpUnrollSameDevicePlacement(self):\n    iters = 10\n    # Run all ops on GPU.\n    duration = self._runOneBenchmark(\"gpu\", iters, static_unroll=True)\n    self.report_benchmark(\n        name=\"unroll_same_device\", iters=iters, wall_time=duration)\n\n\n@test_util.with_control_flow_v2\nclass EagerTest(test.TestCase):\n\n  def testCond(self):\n    with context.eager_mode():\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: [constant_op.constant(10)]\n      fn2 = lambda: [constant_op.constant(20)]\n      r = control_flow_ops.cond(pred, fn1, fn2)\n\n      self.assertAllEqual(r.numpy(), 10)\n      self.assertFalse(isinstance(r, list))\n\n  # TODO(b/117279927): Re-enable once msan failure is fixed.\n  def DISABLED_testCondInDefun(self):\n    with context.eager_mode():\n\n      @eager_function.defun\n      def foo(pred):\n        # TODO(b/111124878): this only needs to output one element.\n        fn1 = lambda: (constant_op.constant(10), constant_op.constant(100))\n        fn2 = lambda: (constant_op.constant(20), constant_op.constant(200))\n        return control_flow_ops.cond(constant_op.constant(pred), fn1, fn2)\n\n      r = foo(True)\n      self.assertAllEqual(r[0].numpy(), 10)\n      self.assertNotIsInstance(r, list)\n\n      r = foo(False)\n      self.assertAllEqual(r[0].numpy(), 20)\n      self.assertFalse(isinstance(r, list))\n\n  def testWhileLoop(self):\n    with context.eager_mode():\n      tensor = constant_op.constant([1, 2, 3, 4, 5])\n      self.assertAllEqual(isum(tensor).numpy(), [46, 47, 48, 49, 50])\n\n  def testWhileLoopWithMaxIterations(self):\n    with context.eager_mode():\n      tensor = constant_op.constant([1, 2, 3, 4, 5])\n      self.assertAllEqual(\n          isum(tensor, maximum_iterations=3).numpy(),\n          [1 + 3, 2 + 3, 3 + 3, 4 + 3, 5 + 3])\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileWithMaximumIterationsAndSingleArgument(self):\n    with context.eager_mode():\n      tensor = constant_op.constant(0)\n      r = control_flow_ops.while_loop(\n          lambda i: i < 3, lambda i: i + 1, [tensor], maximum_iterations=1)\n      self.assertEqual(1, r.numpy())\n\n  def testWithDependencies(self):\n    with context.eager_mode():\n      t1 = constant_op.constant(1)\n      t2 = constant_op.constant(2)\n      t3 = control_flow_ops.with_dependencies(t1, t2)\n      self.assertAllEqual(t2.numpy(), t3.numpy())\n\n  def testTuple(self):\n    with context.eager_mode():\n      t1 = constant_op.constant(1)\n      t2 = constant_op.constant(2)\n      tup1, tup2 = control_flow_ops.tuple([t1, t2])\n      self.assertAllEqual(t1.numpy(), tup1.numpy())\n      self.assertAllEqual(t2.numpy(), tup2.numpy())\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCase(self):\n    with context.eager_mode():\n      x = constant_op.constant(1)\n      y = constant_op.constant(2)\n      z = constant_op.constant(3)\n      f1 = lambda: constant_op.constant(17)\n      f2 = lambda: constant_op.constant(23)\n      f3 = lambda: constant_op.constant(-1)\n\n      r1 = control_flow_ops.case(\n          [(x < y, f1), (x > z, f2)], default=f3, exclusive=True)\n      self.assertAllEqual(r1.numpy(), 17)\n\n\nif __name__ == \"__main__\":\n  test.main()"