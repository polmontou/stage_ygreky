"/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/c/eager/dlpack.h\"\n\n#include \"include/dlpack/dlpack.h\"  // from @dlpack\n#include \"tensorflow/c/eager/c_api.h\"\n#include \"tensorflow/c/eager/c_api_experimental.h\"\n#include \"tensorflow/c/eager/tfe_tensorhandle_internal.h\"\n#include \"tensorflow/c/tf_status_internal.h\"\n#include \"tensorflow/core/common_runtime/eager/tensor_handle.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_reference.h\"\n#include \"tensorflow/core/platform/logging.h\"\n\nnamespace tensorflow {\n\nnamespace {\n\n// Managing context for the DLManagedTensor, will manage the lifetime of\n// DLManagedTensor. When calling DLManagedTensor::deleter, it will notify the\n// original framework of destruction, and this context will be deleted also.\nstruct TfDlManagedTensorCtx {\n  TensorReference reference;\n  std::vector<int64_t> shape;\n  std::vector<int64_t> strides;\n  DLManagedTensor tensor;\n\n  explicit TfDlManagedTensorCtx(const TensorReference& ref) : reference(ref) {}\n};\n\n// Gets tensor from eager tensor handle.\nconst Tensor* GetTensorFromHandle(TFE_TensorHandle* h, TF_Status* status) {\n  if (h == nullptr) {\n    status->status = tensorflow::errors::InvalidArgument(\"Invalid handle\");\n    return nullptr;\n  }\n  tensorflow::TensorHandle* handle =\n      tensorflow::TensorHandleFromInterface(tensorflow::unwrap(h));\n  if (handle->Type() != TensorHandle::LOCAL) {\n    status->status = tensorflow::errors::InvalidArgument(\n        \"DLPack doesn't support \", handle->TypeString(), \" tensor\");\n    return nullptr;\n  }\n  const tensorflow::Tensor* tensor;\n  status->status = handle->Tensor(&tensor);\n  if (!status->status.ok()) {\n    return nullptr;\n  }\n  return tensor;\n}\n\n// Deleter for DLManagedTensor\nvoid DLManagedTensorDeleter(DLManagedTensor* arg) {\n  TfDlManagedTensorCtx* owner =\n      static_cast<TfDlManagedTensorCtx*>(arg->manager_ctx);\n  owner->reference.Unref();\n  delete owner;\n}\n\n// Converts TF_DATAType to DLPack data type.\nDLDataType GetDlDataType(TF_DataType data_type, TF_Status* status) {\n  DLDataType dtype;\n  dtype.lanes = 1;\n  dtype.bits = TF_DataTypeSize(data_type) * 8;\n  switch (data_type) {\n    case TF_DataType::TF_HALF:\n    case TF_DataType::TF_FLOAT:\n    case TF_DataType::TF_DOUBLE:\n      dtype.code = DLDataTypeCode::kDLFloat;\n      break;\n    case TF_DataType::TF_INT8:\n    case TF_DataType::TF_INT16:\n    case TF_DataType::TF_INT32:\n    case TF_DataType::TF_INT64:\n      dtype.code = DLDataTypeCode::kDLInt;\n      break;\n    case TF_DataType::TF_BOOL:\n    case TF_DataType::TF_UINT8:\n    case TF_DataType::TF_UINT16:\n    case TF_DataType::TF_UINT32:\n    case TF_DataType::TF_UINT64:\n      dtype.code = DLDataTypeCode::kDLUInt;\n      break;\n    case TF_DataType::TF_BFLOAT16:\n      dtype.code = DLDataTypeCode::kDLBfloat;\n      break;\n    default:\n      status->status = tensorflow::errors::InvalidArgument(\n          DataType_Name(static_cast<DataType>(data_type)),\n          \" is not supported by dlpack\");\n      break;\n  }\n  return dtype;\n}\n\n// Gets DLPack's DLContext from eager tensor handle.\nDLContext GetDlContext(TFE_TensorHandle* h, TF_Status* status) {\n  DLContext ctx;\n  const char* device_name =\n      tensorflow::unwrap(h)->BackingDeviceName(&status->status);\n  DeviceNameUtils::ParsedName parsed_name;\n  tensorflow::DeviceNameUtils::ParseFullName(device_name, &parsed_name);\n  std::string device_type = parsed_name.type;\n  int device_id = 0;\n  if (parsed_name.has_id) {\n    device_id = parsed_name.id;\n  }\n\n  ctx.device_id = device_id;\n  if (device_type == \"CPU\") {\n    ctx.device_type = DLDeviceType::kDLCPU;\n  } else if (device_type == \"GPU\") {\n    ctx.device_type = DLDeviceType::kDLGPU;\n  } else {\n    status->status = tensorflow::errors::InvalidArgument(\n        \"Unsupported Device Type for dlpack\");\n  }\n\n  return ctx;\n}\n\n// Converts DLContext to TF device name.\nabsl::optional<std::string> DeviceNameFromDlContext(const DLContext& ctx,\n                                                    TF_Status* status) {\n  switch (ctx.device_type) {\n    case DLDeviceType::kDLCPU:\n      return \"CPU:0\";\n    case DLDeviceType::kDLGPU:\n      return absl::StrCat(\"GPU:\", ctx.device_id);\n    default:\n      return absl::nullopt;\n  }\n}\n\n// Converts DLPack data type to TF_DATATYPE.\nStatus TfDataTypeFormDlDataType(const DLDataType& dtype,\n                                TF_DataType* tf_dtype) {\n  switch (dtype.code) {\n    case DLDataTypeCode::kDLUInt:\n      switch (dtype.bits) {\n        case 8:\n          *tf_dtype = TF_DataType::TF_UINT8;\n          return Status::OK();\n        case 16:\n          *tf_dtype = TF_DataType::TF_UINT16;\n          return Status::OK();\n        case 32:\n          *tf_dtype = TF_DataType::TF_UINT32;\n          return Status::OK();\n        case 64:\n          *tf_dtype = TF_DataType::TF_UINT64;\n          return Status::OK();\n        default:\n          return tensorflow::errors::InvalidArgument(\"Unsupported UInt bits: \",\n                                                     dtype.bits);\n      }\n      return Status::OK();\n    case DLDataTypeCode::kDLInt:\n      switch (dtype.bits) {\n        case 8:\n          *tf_dtype = TF_DataType::TF_INT8;\n          return Status::OK();\n        case 16:\n          *tf_dtype = TF_DataType::TF_INT16;\n          return Status::OK();\n        case 32:\n          *tf_dtype = TF_DataType::TF_INT32;\n          return Status::OK();\n        case 64:\n          *tf_dtype = TF_DataType::TF_INT64;\n          return Status::OK();\n        default:\n          return tensorflow::errors::InvalidArgument(\"Unsupported Int bits: \",\n                                                     dtype.bits);\n      }\n      return Status::OK();\n    case DLDataTypeCode::kDLFloat:\n      switch (dtype.bits) {\n        case 16:\n          *tf_dtype = TF_DataType::TF_HALF;\n          return Status::OK();\n        case 32:\n          *tf_dtype = TF_DataType::TF_FLOAT;\n          return Status::OK();\n        case 64:\n          *tf_dtype = TF_DataType::TF_DOUBLE;\n          return Status::OK();\n        default:\n          return tensorflow::errors::InvalidArgument(\"Unsupported Float bits: \",\n                                                     dtype.bits);\n      }\n      break;\n    case DLDataTypeCode::kDLBfloat:\n      switch (dtype.bits) {\n        case 16:\n          *tf_dtype = TF_DataType::TF_BFLOAT16;\n          return Status::OK();\n        default:\n          return tensorflow::errors::InvalidArgument(\n              \"Unsupported BFloat bits: \", dtype.bits);\n      }\n      break;\n    default:\n      return tensorflow::errors::InvalidArgument(\"Unsupported Type Codes: \",\n                                                 dtype.code);\n  }\n}\n\n// Wraps the deleter function of DLManagedTensor to match the function signature\n// TFE_NewTensorHandleFromDeviceMemory.\nvoid DeallocatorWrapperFunc(void* data, size_t len, void* dlmt_vptr) {\n  TFE_CallDLManagedTensorDeleter(dlmt_vptr);\n}\n\n// Checks whether the stride array matches the layout of compact, row-majored\n// data.\nbool IsValidStrideCompactRowMajorData(int64_t* shape_arr, int64_t* stride_arr,\n                                      int ndim) {\n  if (ndim >= 1 && stride_arr[ndim - 1] != 1) {\n    return false;\n  }\n  for (int i = ndim - 2; i >= 0; --i) {\n    if (stride_arr[i] != shape_arr[i + 1] * stride_arr[i + 1]) {\n      return false;\n    }\n  }\n  return true;\n}\n}  // namespace\n\nvoid TFE_CallDLManagedTensorDeleter(void* dlm_ptr) {\n  DLManagedTensor* dlMTensor = static_cast<DLManagedTensor*>(dlm_ptr);\n  if (dlMTensor->deleter != nullptr) {\n    dlMTensor->deleter(dlMTensor);\n  }\n}\n\nvoid* TFE_HandleToDLPack(TFE_TensorHandle* h, TF_Status* status) {\n  auto tf_dlm_context = GetDlContext(h, status);\n  if (!status->status.ok()) {\n    return nullptr;\n  }\n\n  auto* tf_dlm_data = TFE_TensorHandleDevicePointer(h, status);\n  if (!status->status.ok()) {\n    return nullptr;\n  }\n\n  const Tensor* tensor = GetTensorFromHandle(h, status);\n  TF_DataType data_type = static_cast<TF_DataType>(tensor->dtype());\n\n  auto tf_dlm_type = GetDlDataType(data_type, status);\n  if (!status->status.ok()) {\n    return nullptr;\n  }\n\n  TensorReference tensor_ref(*tensor);  // This will call buf_->Ref()\n  auto* tf_dlm_tensor_ctx = new TfDlManagedTensorCtx(tensor_ref);\n  tf_dlm_tensor_ctx->reference = tensor_ref;\n\n  DLManagedTensor* dlm_tensor = &tf_dlm_tensor_ctx->tensor;\n  dlm_tensor->manager_ctx = tf_dlm_tensor_ctx;\n  dlm_tensor->deleter = &DLManagedTensorDeleter;\n  dlm_tensor->dl_tensor.ctx = tf_dlm_context;\n  int ndim = tensor->dims();\n  dlm_tensor->dl_tensor.ndim = ndim;\n  dlm_tensor->dl_tensor.data = tf_dlm_data;\n  dlm_tensor->dl_tensor.dtype = tf_dlm_type;\n\n  std::vector<int64_t>* shape_arr = &tf_dlm_tensor_ctx->shape;\n  std::vector<int64_t>* stride_arr = &tf_dlm_tensor_ctx->strides;\n  shape_arr->resize(ndim);\n  stride_arr->resize(ndim, 1);\n  for (int i = 0; i < ndim; i++) {\n    (*shape_arr)[i] = tensor->dim_size(i);\n  }\n  for (int i = ndim - 2; i >= 0; --i) {\n    (*stride_arr)[i] = (*shape_arr)[i + 1] * (*stride_arr)[i + 1];\n  }\n\n  dlm_tensor->dl_tensor.shape = shape_arr->data();\n  // There are two ways to represent compact row-major data\n  // 1) nullptr indicates tensor is compact and row-majored.\n  // 2) fill in the strides array as the real case for compact row-major data.\n  // Here we choose option 2, since some frameworks didn't handle the strides\n  // argument properly.\n  dlm_tensor->dl_tensor.strides = stride_arr->data();\n\n  dlm_tensor->dl_tensor.byte_offset =\n      0;  // TF doesn't handle the strides and byte_offsets here\n  return static_cast<void*>(dlm_tensor);\n}\n\nTFE_TensorHandle* TFE_HandleFromDLPack(void* dlm, TF_Status* status,\n                                       TFE_Context* ctx) {\n  DLManagedTensor* dlmt = static_cast<DLManagedTensor*>(dlm);\n  DLTensor* dl_tensor = &dlmt->dl_tensor;\n  absl::optional<std::string> device_name =\n      DeviceNameFromDlContext(dl_tensor->ctx, status);\n  if (!device_name.has_value()) {\n    status->status =\n        tensorflow::errors::InvalidArgument(\"Unsupported Device Type\");\n    return nullptr;\n  }\n  TF_DataType dtype;\n  Status s = TfDataTypeFormDlDataType(dl_tensor->dtype, &dtype);\n  if (!s.ok()) {\n    status->status = std::move(s);\n    return nullptr;\n  }\n  int num_dims = dl_tensor->ndim;\n  const int64_t* dims = dl_tensor->shape;\n  void* data = dl_tensor->data;\n\n  size_t total_bytes = dl_tensor->dtype.bits / 8;\n  for (int i = 0; i < num_dims; i++) {\n    total_bytes *= dims[i];\n  }\n\n  if (dl_tensor->strides != nullptr &&\n      !IsValidStrideCompactRowMajorData(dl_tensor->shape, dl_tensor->strides,\n                                        num_dims)) {\n    status->status = tensorflow::errors::InvalidArgument(\n        \"Invalid strides array from DLPack\");\n    return nullptr;\n  }\n\n  TFE_TensorHandle* handle = TFE_NewTensorHandleFromDeviceMemory(\n      ctx, device_name.value().c_str(), dtype, dims, num_dims, data,\n      total_bytes, &DeallocatorWrapperFunc, dlmt, status);\n\n  return handle;\n}\n\n}  // namespace tensorflow"