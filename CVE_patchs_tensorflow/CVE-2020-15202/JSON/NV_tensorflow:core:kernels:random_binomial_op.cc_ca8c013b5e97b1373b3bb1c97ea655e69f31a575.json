"/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/random_ops.cc.\n// NOTE: If the algorithm is changed, please run the test\n// .../python/kernel_tests/random:random_binomial_test\n// commenting out the \"tf.set_random_seed(seed)\" lines, and using the\n// \"--runs-per-test=1000\" flag. This tests the statistical correctness of the\n// op results.\n\n#define EIGEN_USE_THREADS\n\n#include \"tensorflow/core/kernels/random_binomial_op.h\"\n\n#include <algorithm>\n#include <cmath>\n#include <memory>\n\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/rng_alg.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/kernels/random_ops_util.h\"\n#include \"tensorflow/core/kernels/stateful_random_ops_cpu_gpu.h\"\n#include \"tensorflow/core/kernels/stateless_random_ops.h\"\n#include \"tensorflow/core/kernels/training_op_helpers.h\"\n#include \"tensorflow/core/lib/core/refcount.h\"\n#include \"tensorflow/core/lib/random/random_distributions.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/util/bcast.h\"\n#include \"tensorflow/core/util/guarded_philox_random.h\"\n#include \"tensorflow/core/util/work_sharder.h\"\n\n#define UNIFORM(X)                                    \\\n  if (uniform_remaining == 0) {                       \\\n    uniform_remaining = Uniform::kResultElementCount; \\\n    uniform_result = uniform(gen);                    \\\n  }                                                   \\\n  uniform_remaining--;                                \\\n  double X = uniform_result[uniform_remaining]\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\ntypedef Eigen::GpuDevice GPUDevice;\n\nnamespace {\n\ntypedef random::UniformDistribution<random::PhiloxRandom, double> Uniform;\n\n// Binomial inversion. Given prob, sum geometric random variables until they\n// exceed count. The number of random variables used is binomially distributed.\n// This is also known as binomial inversion, as this is equivalent to inverting\n// the Binomial CDF.\ndouble binomial_inversion(double count, double prob,\n                          random::PhiloxRandom* gen) {\n  using Eigen::numext::ceil;\n  using Eigen::numext::log;\n  using Eigen::numext::log1p;\n\n  double geom_sum = 0;\n  int num_geom = 0;\n\n  Uniform uniform;\n  typename Uniform::ResultType uniform_result;\n  int16 uniform_remaining = 0;\n\n  while (true) {\n    UNIFORM(u);\n    double geom = ceil(log(u) / log1p(-prob));\n    geom_sum += geom;\n    if (geom_sum > count) {\n      break;\n    }\n    ++num_geom;\n  }\n  return num_geom;\n}\n\ninline double stirling_approx_tail(double k) {\n  static double kTailValues[] = {0.0810614667953272,  0.0413406959554092,\n                                 0.0276779256849983,  0.02079067210376509,\n                                 0.0166446911898211,  0.0138761288230707,\n                                 0.0118967099458917,  0.0104112652619720,\n                                 0.00925546218271273, 0.00833056343336287};\n  if (k <= 9) {\n    return kTailValues[static_cast<int>(k)];\n  }\n  double kp1sq = (k + 1) * (k + 1);\n  return (1.0 / 12 - (1.0 / 360 - 1.0 / 1260 / kp1sq) / kp1sq) / (k + 1);\n}\n\n// We use a transformation-rejection algorithm from\n// pairs of uniform random variables due to Hormann.\n// https://www.tandfonline.com/doi/abs/10.1080/00949659308811496\ninline double btrs(double count, double prob, random::PhiloxRandom* gen) {\n  using Eigen::numext::abs;\n  using Eigen::numext::floor;\n  using Eigen::numext::log;\n  using Eigen::numext::log1p;\n  using Eigen::numext::sqrt;\n\n  // This is spq in the paper.\n  const double stddev = sqrt(count * prob * (1 - prob));\n\n  // Other coefficients for Transformed Rejection sampling.\n  const double b = 1.15 + 2.53 * stddev;\n  const double a = -0.0873 + 0.0248 * b + 0.01 * prob;\n  const double c = count * prob + 0.5;\n  const double v_r = 0.92 - 4.2 / b;\n  const double r = prob / (1 - prob);\n\n  const double alpha = (2.83 + 5.1 / b) * stddev;\n  const double m = floor((count + 1) * prob);\n\n  Uniform uniform;\n  typename Uniform::ResultType uniform_result;\n  int16 uniform_remaining = 0;\n\n  while (true) {\n    UNIFORM(u);\n    UNIFORM(v);\n    u = u - 0.5;\n    double us = 0.5 - abs(u);\n    double k = floor((2 * a / us + b) * u + c);\n\n    // Region for which the box is tight, and we\n    // can return our calculated value This should happen\n    // 0.86 * v_r times. In the limit as n * p is large,\n    // the acceptance rate converges to ~79% (and in the lower\n    // regime it is ~24%).\n    if (us >= 0.07 && v <= v_r) {\n      return k;\n    }\n    // Reject non-sensical answers.\n    if (k < 0 || k > count) {\n      continue;\n    }\n\n    // This deviates from Hormann's BRTS algorithm, as there is a log missing.\n    // For all (u, v) pairs outside of the bounding box, this calculates the\n    // transformed-reject ratio.\n    v = log(v * alpha / (a / (us * us) + b));\n    double upperbound =\n        ((m + 0.5) * log((m + 1) / (r * (count - m + 1))) +\n         (count + 1) * log((count - m + 1) / (count - k + 1)) +\n         (k + 0.5) * log(r * (count - k + 1) / (k + 1)) +\n         stirling_approx_tail(m) + stirling_approx_tail(count - m) -\n         stirling_approx_tail(k) - stirling_approx_tail(count - k));\n    if (v <= upperbound) {\n      return k;\n    }\n  }\n}\n\n}  // namespace\n\nnamespace functor {\n\ntemplate <typename T, typename U>\nstruct RandomBinomialFunctor<CPUDevice, T, U> {\n  void operator()(OpKernelContext* ctx, const CPUDevice& d, int64 num_batches,\n                  int64 samples_per_batch, int64 num_elements,\n                  const BCast& bcast, typename TTypes<T>::ConstFlat counts,\n                  typename TTypes<T>::ConstFlat probs,\n                  const random::PhiloxRandom& gen,\n                  typename TTypes<U>::Flat output) {\n    auto worker_threads = *(ctx->device()->tensorflow_cpu_worker_threads());\n\n    // The output layout is [B1, ... Bk, H1, ... Hm]. We have [B1, ... Bk] for\n    // the sample shape and [H1, ... Hm] for the batch shape of the samples.\n    // We have B1 * ... * Bk samples per batch member we need.\n    auto DoWork = [num_batches, samples_per_batch, &bcast, &counts, &probs,\n                   &gen, &output](int64 start_output, int64 limit_output) {\n      // Vectorized intermediate calculations for uniform rejection sampling.\n      // We always generate at most 4 samples.\n      Eigen::array<T, 4> z;\n      Eigen::array<T, 4> g;\n      const bool should_bcast = bcast.IsBroadcastingRequired();\n      const auto& counts_batch_indices = bcast.x_batch_indices();\n      const auto& probs_batch_indices = bcast.y_batch_indices();\n      auto output_flat = output.data();\n\n      // We partition work across batches (count, prob) and then across samples\n      // per batch member, to avoid extra work.\n      for (int64 output_idx = start_output; output_idx < limit_output;\n           // output_idx is incremented with the inner loops below.\n      ) {\n        int64 batch_idx = output_idx / samples_per_batch;\n        U* const output_batch_offset = output_flat + batch_idx;\n        // Generate batch counts from BCast, as it has the right indices to loop\n        // over.\n        T count, prob;\n        if (should_bcast) {\n          count = counts(counts_batch_indices[batch_idx]);\n          prob = probs(probs_batch_indices[batch_idx]);\n        } else {\n          count = counts(batch_idx);\n          prob = probs(batch_idx);\n        }\n\n        // Calculate normalized samples, then convert them.\n        // Determine the method to use.\n        double dcount = static_cast<double>(count);\n        if (dcount <= 0.0 || prob <= T(0.0)) {\n          for (int64 sample_idx = output_idx % samples_per_batch;\n               sample_idx < samples_per_batch && output_idx < limit_output;\n               ++sample_idx, ++output_idx) {\n            output_batch_offset[sample_idx * num_batches] = static_cast<U>(0.0);\n          }\n        } else if (prob >= T(1.0)) {\n          for (int64 sample_idx = output_idx % samples_per_batch;\n               sample_idx < samples_per_batch && output_idx < limit_output;\n               ++sample_idx, ++output_idx) {\n            output_batch_offset[sample_idx * num_batches] =\n                static_cast<U>(dcount);\n          }\n        } else if (prob <= T(0.5)) {\n          double dp = static_cast<double>(prob);\n          if (count * prob >= T(10)) {\n            for (int64 sample_idx = output_idx % samples_per_batch;\n                 sample_idx < samples_per_batch && output_idx < limit_output;\n                 ++sample_idx, ++output_idx) {\n              random::PhiloxRandom gen_copy = gen;\n              gen_copy.Skip(256 * output_idx);\n              output_batch_offset[sample_idx * num_batches] =\n                  static_cast<U>(btrs(dcount, dp, &gen_copy));\n            }\n          } else {\n            for (int64 sample_idx = output_idx % samples_per_batch;\n                 sample_idx < samples_per_batch && output_idx < limit_output;\n                 ++sample_idx, ++output_idx) {\n              random::PhiloxRandom gen_copy = gen;\n              // For binomial inversion, we have mean <= 10, variance <= 10.\n              // This means on average we need at most 10 number of samples,\n              // and for 10 standard deviations, we need 42 samples. We reserve\n              // that much.\n              gen_copy.Skip(42 * output_idx);\n              output_batch_offset[sample_idx * num_batches] =\n                  static_cast<U>(binomial_inversion(dcount, dp, &gen_copy));\n            }\n          }\n        } else if (prob > T(0.5)) {\n          T q = T(1) - prob;\n          double dcount = static_cast<double>(count);\n          double dq = static_cast<double>(q);\n          if (count * q >= T(10)) {\n            for (int64 sample_idx = output_idx % samples_per_batch;\n                 sample_idx < samples_per_batch && output_idx < limit_output;\n                 ++sample_idx, ++output_idx) {\n              random::PhiloxRandom gen_copy = gen;\n              gen_copy.Skip(256 * output_idx);\n              output_batch_offset[sample_idx * num_batches] =\n                  static_cast<U>(dcount - btrs(dcount, dq, &gen_copy));\n            }\n          } else {\n            for (int64 sample_idx = output_idx % samples_per_batch;\n                 sample_idx < samples_per_batch && output_idx < limit_output;\n                 ++sample_idx, ++output_idx) {\n              random::PhiloxRandom gen_copy = gen;\n              // For binomial inversion, we have mean <= 10, variance <= 10.\n              // This means on average we need at most 10 number of samples,\n              // and for 10 standard deviations, we need 42 samples. We reserve\n              // that much.\n              gen_copy.Skip(42 * output_idx);\n              output_batch_offset[sample_idx * num_batches] = static_cast<U>(\n                  dcount - binomial_inversion(dcount, dq, &gen_copy));\n            }\n          }\n        } else {  // prob is NaN\n          // TODO(srvasude): What should happen if prob is NaN but the output\n          // type is an integer (which doesn't have a sentinel for NaN)?  Fail\n          // the whole batch sample?  Return a specialized sentinel like -1?\n          for (int64 sample_idx = output_idx % samples_per_batch;\n               sample_idx < samples_per_batch && output_idx < limit_output;\n               ++sample_idx, ++output_idx) {\n            output_batch_offset[sample_idx * num_batches] = static_cast<U>(NAN);\n          }\n        }\n      }\n    };\n\n    // This will depend on count * p (or count * q).\n    // For n * p < 10, on average, O(n * p) calls to uniform are\n    // needed, with that\n    // many multiplies. ~10 uniform calls on average with ~200 cost op calls.\n    //\n    // Very roughly, for rate >= 10, the four calls to log\n    // occur for ~72 percent of samples.\n    // 4 x 100 (64-bit cycles per log) * 0.72 = ~288\n    // Additionally, there are ~10 other ops (+, *, /, ...) at 3-6 cycles each:\n    // 40 * .72  = ~25.\n    //\n    // Finally, there are several other ops that are done every loop along with\n    // 2 uniform generations along with 5 other ops at 3-6 cycles each.\n    // ~15 / .89 = ~16\n    //\n    // In total this (rate >= 10) should be ~329 + 2 * Uniform::kElementCost.\n    // We assume that half the tensor has rate < 10, so on average 6\n    // uniform's\n    // will be needed. We will upper bound the other op cost by the one for\n    // rate > 10.\n    static const int kElementCost = 329 + 6 * Uniform::kElementCost +\n                                    6 * random::PhiloxRandom::kElementCost;\n    Shard(worker_threads.num_threads, worker_threads.workers, num_elements,\n          kElementCost, DoWork);\n  }\n};\n\n}  // namespace functor\n\nnamespace {\n\n// Samples from a binomial distribution, using the given parameters.\ntemplate <typename Device, typename T, typename U>\nclass RandomBinomialOp : public OpKernel {\n  // Reshape batches so each batch is this size if possible.\n  static constexpr int32 kDesiredBatchSize = 100;\n\n public:\n  explicit RandomBinomialOp(OpKernelConstruction* context)\n      : OpKernel(context) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& alg_tensor = ctx->input(1);\n    const Tensor& shape_tensor = ctx->input(2);\n    const Tensor& counts_tensor = ctx->input(3);\n    const Tensor& probs_tensor = ctx->input(4);\n\n    tensorflow::BCast bcast(counts_tensor.shape().dim_sizes(),\n                            probs_tensor.shape().dim_sizes(),\n                            /*fewer_dims_optimization=*/false,\n                            /*return_flattened_batch_indices=*/true);\n    OP_REQUIRES(ctx, bcast.IsValid(),\n                errors::InvalidArgument(\n                    \"counts and probs must have compatible batch dimensions: \",\n                    counts_tensor.shape().DebugString(), \" vs. \",\n                    probs_tensor.shape().DebugString()));\n    OP_REQUIRES(\n        ctx, TensorShapeUtils::IsVector(shape_tensor.shape()),\n        errors::InvalidArgument(\"Input shape should be a vector, got shape: \",\n                                shape_tensor.shape().DebugString()));\n    OP_REQUIRES(ctx,\n                (shape_tensor.dtype() == DataType::DT_INT32 ||\n                 shape_tensor.dtype() == DataType::DT_INT64),\n                errors::InvalidArgument(\n                    \"Input shape should have dtype {int32, int64}.\"));\n\n    // Let's check that the shape tensor dominates the broadcasted tensor.\n    TensorShape bcast_shape = BCast::ToShape(bcast.output_shape());\n    TensorShape output_shape;\n    if (shape_tensor.dtype() == DataType::DT_INT32) {\n      OP_REQUIRES_OK(ctx, TensorShapeUtils::MakeShape(shape_tensor.vec<int32>(),\n                                                      &output_shape));\n    } else {\n      OP_REQUIRES_OK(ctx, TensorShapeUtils::MakeShape(shape_tensor.vec<int64>(),\n                                                      &output_shape));\n    }\n    OP_REQUIRES(ctx, TensorShapeUtils::EndsWith(output_shape, bcast_shape),\n                errors::InvalidArgument(\n                    \"Shape passed in must end with broadcasted shape.\"));\n    // Now that we have a guarantee, we can get the additional dimensions added\n    // by sampling.\n    OP_REQUIRES(ctx, alg_tensor.dims() == 0,\n                errors::InvalidArgument(\"algorithm must be of shape [], not \",\n                                        alg_tensor.shape().DebugString()));\n    Algorithm alg = Algorithm(alg_tensor.flat<int64>()(0));\n\n    int64 samples_per_batch = 1;\n    const int64 num_sample_dims =\n        (shape_tensor.dim_size(0) - bcast.output_shape().size());\n    for (int64 i = 0; i < num_sample_dims; ++i) {\n      samples_per_batch *= shape_tensor.flat<int32>()(i);\n    }\n    int64 num_batches = 1;\n    for (int64 i = num_sample_dims; i < shape_tensor.dim_size(0); ++i) {\n      num_batches *= shape_tensor.flat<int32>()(i);\n    }\n    const int64 num_elements = num_batches * samples_per_batch;\n\n    Tensor* samples_tensor;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, output_shape, &samples_tensor));\n\n    core::RefCountPtr<Var> var;\n    OP_REQUIRES_OK(ctx, LookupResource(ctx, HandleFromInput(ctx, 0), &var));\n\n    Tensor* var_tensor = var->tensor();\n    OP_REQUIRES(\n        ctx, var_tensor->dtype() == STATE_ELEMENT_DTYPE,\n        errors::InvalidArgument(\"dtype of RNG state variable must be \",\n                                DataTypeString(STATE_ELEMENT_DTYPE), \", not \",\n                                DataTypeString(var_tensor->dtype())));\n    OP_REQUIRES(ctx, var_tensor->dims() == 1,\n                errors::InvalidArgument(\n                    \"RNG state must have one and only one dimension, not \",\n                    var_tensor->dims()));\n    auto var_tensor_flat = var_tensor->flat<StateElementType>();\n    OP_REQUIRES(ctx, alg == RNG_ALG_PHILOX,\n                errors::InvalidArgument(\"Unsupported algorithm id: \", alg));\n    static_assert(std::is_same<StateElementType, int64>::value,\n                  \"StateElementType must be int64\");\n    static_assert(std::is_same<PhiloxRandom::ResultElementType, uint32>::value,\n                  \"PhiloxRandom::ResultElementType must be uint32\");\n    OP_REQUIRES(ctx, var_tensor_flat.size() >= PHILOX_MIN_STATE_SIZE,\n                errors::InvalidArgument(\n                    \"For Philox algorithm, the size of state must be at least \",\n                    PHILOX_MIN_STATE_SIZE, \"; got \", var_tensor_flat.size()));\n\n    OP_REQUIRES_OK(ctx, PrepareToUpdateVariable<Device, StateElementType>(\n                            ctx, var_tensor, var->copy_on_read_mode.load()));\n    auto var_data = var_tensor_flat.data();\n    auto philox = GetPhiloxRandomFromMem(var_data);\n    UpdateMemWithPhiloxRandom(\n        philox, num_batches * 2 * 100 * (samples_per_batch + 3) / 4, var_data);\n\n    auto binomial_functor = functor::RandomBinomialFunctor<Device, T, U>();\n    binomial_functor(ctx, ctx->eigen_device<Device>(), num_batches,\n                     samples_per_batch, num_elements, bcast,\n                     counts_tensor.flat<T>(), probs_tensor.flat<T>(), philox,\n                     samples_tensor->flat<U>());\n  }\n\n private:\n  TF_DISALLOW_COPY_AND_ASSIGN(RandomBinomialOp);\n};\n\n// Samples from a binomial distribution, using the given parameters.\ntemplate <typename Device, typename T, typename U>\nclass StatelessRandomBinomialOp : public OpKernel {\n  // Reshape batches so each batch is this size if possible.\n  static constexpr int32 kDesiredBatchSize = 100;\n\n public:\n  explicit StatelessRandomBinomialOp(OpKernelConstruction* context)\n      : OpKernel(context) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& shape_tensor = ctx->input(0);\n    const Tensor& seed_tensor = ctx->input(1);\n    const Tensor& counts_tensor = ctx->input(2);\n    const Tensor& probs_tensor = ctx->input(3);\n\n    OP_REQUIRES(ctx, seed_tensor.dims() == 1 && seed_tensor.dim_size(0) == 2,\n                errors::InvalidArgument(\"seed must have shape [2], not \",\n                                        seed_tensor.shape().DebugString()));\n\n    tensorflow::BCast bcast(counts_tensor.shape().dim_sizes(),\n                            probs_tensor.shape().dim_sizes(),\n                            /*fewer_dims_optimization=*/false,\n                            /*return_flattened_batch_indices=*/true);\n    OP_REQUIRES(ctx, bcast.IsValid(),\n                errors::InvalidArgument(\n                    \"counts and probs must have compatible batch dimensions: \",\n                    counts_tensor.shape().DebugString(), \" vs. \",\n                    probs_tensor.shape().DebugString()));\n    OP_REQUIRES(\n        ctx, TensorShapeUtils::IsVector(shape_tensor.shape()),\n        errors::InvalidArgument(\"Input shape should be a vector, got shape: \",\n                                shape_tensor.shape().DebugString()));\n    OP_REQUIRES(ctx,\n                (shape_tensor.dtype() == DataType::DT_INT32 ||\n                 shape_tensor.dtype() == DataType::DT_INT64),\n                errors::InvalidArgument(\n                    \"Input shape should have dtype {int32, int64}.\"));\n\n    // Let's check that the shape tensor dominates the broadcasted tensor.\n    TensorShape bcast_shape = BCast::ToShape(bcast.output_shape());\n    TensorShape output_shape;\n    if (shape_tensor.dtype() == DataType::DT_INT32) {\n      OP_REQUIRES_OK(ctx, TensorShapeUtils::MakeShape(shape_tensor.vec<int32>(),\n                                                      &output_shape));\n    } else {\n      OP_REQUIRES_OK(ctx, TensorShapeUtils::MakeShape(shape_tensor.vec<int64>(),\n                                                      &output_shape));\n    }\n    OP_REQUIRES(ctx, TensorShapeUtils::EndsWith(output_shape, bcast_shape),\n                errors::InvalidArgument(\n                    \"Shape passed in must end with broadcasted shape.\"));\n    // Now that we have a guarantee, we can get the additional dimensions added\n    // by sampling.\n    int64 samples_per_batch = 1;\n    const int64 num_sample_dims =\n        (shape_tensor.dim_size(0) - bcast.output_shape().size());\n    for (int64 i = 0; i < num_sample_dims; ++i) {\n      samples_per_batch *= shape_tensor.flat<int32>()(i);\n    }\n    int64 num_batches = 1;\n    for (int64 i = num_sample_dims; i < shape_tensor.dim_size(0); ++i) {\n      num_batches *= shape_tensor.flat<int32>()(i);\n    }\n    const int64 num_elements = num_batches * samples_per_batch;\n\n    Tensor* samples_tensor;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, output_shape, &samples_tensor));\n    if (output_shape.num_elements() == 0) return;\n\n    random::PhiloxRandom::Key key;\n    random::PhiloxRandom::ResultType counter;\n    OP_REQUIRES_OK(ctx, GenerateKey(seed_tensor, &key, &counter));\n\n    auto philox = random::PhiloxRandom(counter, key);\n    auto binomial_functor = functor::RandomBinomialFunctor<Device, T, U>();\n    binomial_functor(ctx, ctx->eigen_device<Device>(), num_batches,\n                     samples_per_batch, num_elements, bcast,\n                     counts_tensor.flat<T>(), probs_tensor.flat<T>(), philox,\n                     samples_tensor->flat<U>());\n  }\n\n private:\n  TF_DISALLOW_COPY_AND_ASSIGN(StatelessRandomBinomialOp);\n};\n\n}  // namespace\n\n#define REGISTER(RTYPE, TYPE)                                        \\\n  REGISTER_KERNEL_BUILDER(Name(\"StatefulRandomBinomial\")             \\\n                              .Device(DEVICE_CPU)                    \\\n                              .HostMemory(\"resource\")                \\\n                              .HostMemory(\"algorithm\")               \\\n                              .HostMemory(\"shape\")                   \\\n                              .HostMemory(\"counts\")                  \\\n                              .HostMemory(\"probs\")                   \\\n                              .TypeConstraint<RTYPE>(\"dtype\")        \\\n                              .TypeConstraint<TYPE>(\"T\"),            \\\n                          RandomBinomialOp<CPUDevice, TYPE, RTYPE>); \\\n  REGISTER_KERNEL_BUILDER(Name(\"StatelessRandomBinomial\")            \\\n                              .Device(DEVICE_CPU)                    \\\n                              .HostMemory(\"shape\")                   \\\n                              .HostMemory(\"seed\")                    \\\n                              .HostMemory(\"counts\")                  \\\n                              .HostMemory(\"probs\")                   \\\n                              .TypeConstraint<RTYPE>(\"dtype\")        \\\n                              .TypeConstraint<TYPE>(\"T\"),            \\\n                          StatelessRandomBinomialOp<CPUDevice, TYPE, RTYPE>)\n\n#define REGISTER_ALL(RTYPE)     \\\n  REGISTER(RTYPE, Eigen::half); \\\n  REGISTER(RTYPE, float);       \\\n  REGISTER(RTYPE, double);\n\nREGISTER_ALL(Eigen::half);\nREGISTER_ALL(float);\nREGISTER_ALL(double);\nREGISTER_ALL(int32);\nREGISTER_ALL(int64);\n\n#undef REGISTER\n#undef REGISTER_ALL\n\n}  // end namespace tensorflow"