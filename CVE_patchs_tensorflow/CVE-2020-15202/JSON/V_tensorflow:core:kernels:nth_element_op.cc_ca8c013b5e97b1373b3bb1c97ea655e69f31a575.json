"/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/nn_ops.cc.\n#include \"tensorflow/core/kernels/nth_element_op.h\"\n\n#include <algorithm>\n#include <iostream>\n#include <vector>\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/util/work_sharder.h\"\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n\ntemplate <typename Device, typename T>\nclass NthElementOp : public OpKernel {\n public:\n  explicit NthElementOp(OpKernelConstruction* context) : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"reverse\", &reverse_));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    // The second args is N, which must be a positive scalar.\n    const auto& n_in = context->input(1);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsScalar(n_in.shape()),\n        errors::InvalidArgument(\"N must be scalar but has rank \", n_in.dims()));\n    int n = n_in.scalar<int32>()();\n    OP_REQUIRES(context, n >= 0,\n                errors::InvalidArgument(\"n must be non-negative but is \", n));\n\n    // The first args is input tensor, which must have 1 dimension at least.\n    const Tensor& input_in = context->input(0);\n    const int num_dims = input_in.dims();\n    OP_REQUIRES(context, num_dims >= 1,\n                errors::InvalidArgument(\n                    \"Input must be at least rank 1 but is rank \", num_dims));\n    // The last dimension of input tensor must be greater than N.\n    OP_REQUIRES(\n        context, input_in.dim_size(num_dims - 1) > n,\n        errors::InvalidArgument(\"Input must have last dimension > n = \", n));\n\n    // std::nth_element only support the nth-smallest selection.\n    if (reverse_) {\n      n = input_in.dim_size(num_dims - 1) - n - 1;\n    }\n\n    // Assume input_shape is [d1,d2,...dk], and output_shape is [d1,d2...dk-1].\n    TensorShape out_shape;\n    for (int i = 0; i < num_dims - 1; ++i) {\n      out_shape.AddDim(input_in.dim_size(i));\n    }\n    Tensor* output_tensor = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, out_shape, &output_tensor));\n\n    functor::NthElementFunctor<Device, T> nthElementFunc;\n    nthElementFunc(context, input_in, *output_tensor, n, reverse_);\n  }\n\n private:\n  bool reverse_;\n};\n\nnamespace functor {\n\ntemplate <typename T>\nstruct NthElementFunctor<CPUDevice, T> {\n  void operator()(OpKernelContext* context, const Tensor& input_tensor,\n                  Tensor& output_tensor, int n, bool reverse) {\n    const T* input = input_tensor.flat<T>().data();\n    T* output = output_tensor.flat<T>().data();\n\n    // Assume input_shape is [d1,d2,...dk], and output_shape is [d1,d2...dk-1],\n    // then num_rows = d1*d2...dk-1, last_dim = dk.\n    const int num_rows = output_tensor.NumElements();\n    const int last_dim = input_tensor.dim_size(input_tensor.dims() - 1);\n\n    // Allocate each row to different shard.\n    auto SubNthElement = [&, input, output, last_dim, n](int start, int limit) {\n      // std::nth_element would rearrange the array, so we need a new buffer.\n      std::vector<T> buf(last_dim);\n\n      for (int b = start; b < limit; ++b) {\n        // Copy from one row of elements to buffer\n        const T* input_start = input + b * last_dim;\n        const T* input_end = input + (b + 1) * last_dim;\n        std::copy(input_start, input_end, buf.begin());\n\n        std::nth_element(buf.begin(), buf.begin() + n, buf.end());\n        // The element placed in the nth position is exactly the element that\n        // would occur in this position if the range was fully sorted.\n        output[b] = buf[n];\n      }\n    };\n\n    auto worker_threads = *(context->device()->tensorflow_cpu_worker_threads());\n    // The average time complexity of partition-based nth_element (BFPRT) is\n    // O(n), although the worst time complexity could be O(n^2). Here, 20 is a\n    // empirical factor of cost_per_unit.\n    Shard(worker_threads.num_threads, worker_threads.workers, num_rows,\n          20 * last_dim, SubNthElement);\n  }\n};\n\n}  // namespace functor\n\n#define REGISTER_NTHOP(T)                                           \\\n  REGISTER_KERNEL_BUILDER(                                          \\\n      Name(\"NthElement\").Device(DEVICE_CPU).TypeConstraint<T>(\"T\"), \\\n      NthElementOp<CPUDevice, T>)\n\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_NTHOP);\n#undef REGISTER_NTHOP\n\n}  // end namespace tensorflow"