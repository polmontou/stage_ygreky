"diff --git a/tensorflow/lite/experimental/kernels/ctc_beam_search_decoder.cc b/tensorflow/lite/experimental/kernels/ctc_beam_search_decoder.cc\nindex c5e019fc2ee..9b7d731c5f8 100644\n--- a/tensorflow/lite/experimental/kernels/ctc_beam_search_decoder.cc\n+++ b/tensorflow/lite/experimental/kernels/ctc_beam_search_decoder.cc\n@@ -62,14 +62,17 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   // The outputs should be top_paths * 3 + 1.\n   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 3 * top_paths + 1);\n \n-  const TfLiteTensor* inputs = GetInput(context, node, kInputsTensor);\n+  const TfLiteTensor* inputs;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kInputsTensor, &inputs));\n   TF_LITE_ENSURE_EQ(context, NumDimensions(inputs), 3);\n   // TensorFlow only supports float.\n   TF_LITE_ENSURE_EQ(context, inputs->type, kTfLiteFloat32);\n   const int batch_size = SizeOfDimension(inputs, 1);\n \n-  const TfLiteTensor* sequence_length =\n-      GetInput(context, node, kSequenceLengthTensor);\n+  const TfLiteTensor* sequence_length;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kSequenceLengthTensor,\n+                                          &sequence_length));\n   TF_LITE_ENSURE_EQ(context, NumDimensions(sequence_length), 1);\n   TF_LITE_ENSURE_EQ(context, NumElements(sequence_length), batch_size);\n   // TensorFlow only supports int32.\n@@ -78,17 +81,23 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   // Resize decoded outputs.\n   // Do not resize indices & values cause we don't know the values yet.\n   for (int i = 0; i < top_paths; ++i) {\n-    TfLiteTensor* indices = GetOutput(context, node, i);\n+    TfLiteTensor* indices;\n+    TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, i, &indices));\n     SetTensorToDynamic(indices);\n-    TfLiteTensor* values = GetOutput(context, node, i + top_paths);\n+    TfLiteTensor* values;\n+    TF_LITE_ENSURE_OK(context,\n+                      GetOutputSafe(context, node, i + top_paths, &values));\n     SetTensorToDynamic(values);\n-    TfLiteTensor* output_shape = GetOutput(context, node, i + 2 * top_paths);\n+    TfLiteTensor* output_shape;\n+    TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, i + 2 * top_paths,\n+                                             &output_shape));\n     SetTensorToDynamic(output_shape);\n   }\n \n   // Resize log probability outputs.\n-  TfLiteTensor* log_probability_output =\n-      GetOutput(context, node, top_paths * 3);\n+  TfLiteTensor* log_probability_output;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, top_paths * 3,\n+                                           &log_probability_output));\n   TfLiteIntArray* log_probability_output_shape_array = TfLiteIntArrayCreate(2);\n   log_probability_output_shape_array->data[0] = batch_size;\n   log_probability_output_shape_array->data[1] = top_paths;\n@@ -127,13 +136,18 @@ TfLiteStatus StoreAllDecodedSequences(\n     const int32_t p_num = num_entries[p];\n \n     // Resize the decoded outputs.\n-    TfLiteTensor* indices = GetOutput(context, node, p);\n+    TfLiteTensor* indices;\n+    TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, p, &indices));\n     TF_LITE_ENSURE_OK(context, Resize(context, {p_num, 2}, indices));\n \n-    TfLiteTensor* values = GetOutput(context, node, p + top_paths);\n+    TfLiteTensor* values;\n+    TF_LITE_ENSURE_OK(context,\n+                      GetOutputSafe(context, node, p + top_paths, &values));\n     TF_LITE_ENSURE_OK(context, Resize(context, {p_num}, values));\n \n-    TfLiteTensor* decoded_shape = GetOutput(context, node, p + 2 * top_paths);\n+    TfLiteTensor* decoded_shape;\n+    TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, p + 2 * top_paths,\n+                                             &decoded_shape));\n     TF_LITE_ENSURE_OK(context, Resize(context, {2}, decoded_shape));\n \n     int32_t max_decoded = 0;\n@@ -161,9 +175,12 @@ TfLiteStatus StoreAllDecodedSequences(\n }\n \n TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n-  const TfLiteTensor* inputs = GetInput(context, node, kInputsTensor);\n-  const TfLiteTensor* sequence_length =\n-      GetInput(context, node, kSequenceLengthTensor);\n+  const TfLiteTensor* inputs;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kInputsTensor, &inputs));\n+  const TfLiteTensor* sequence_length;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kSequenceLengthTensor,\n+                                          &sequence_length));\n   const CTCBeamSearchDecoderParams* option =\n       reinterpret_cast<CTCBeamSearchDecoderParams*>(node->user_data);\n \n@@ -207,7 +224,9 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n   std::vector<std::vector<std::vector<int>>> best_paths(batch_size);\n   std::vector<float> log_probs;\n \n-  TfLiteTensor* log_probabilities = GetOutput(context, node, 3 * top_paths);\n+  TfLiteTensor* log_probabilities;\n+  TF_LITE_ENSURE_OK(\n+      context, GetOutputSafe(context, node, 3 * top_paths, &log_probabilities));\n   float* log_probabilities_output = GetTensorData<float>(log_probabilities);\n \n   // Assumption: the blank index is num_classes - 1"