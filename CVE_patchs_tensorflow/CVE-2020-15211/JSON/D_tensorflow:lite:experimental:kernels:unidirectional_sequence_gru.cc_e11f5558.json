"diff --git a/tensorflow/lite/experimental/kernels/unidirectional_sequence_gru.cc b/tensorflow/lite/experimental/kernels/unidirectional_sequence_gru.cc\nindex 84f54b8d5aa..1fd6dabb52f 100644\n--- a/tensorflow/lite/experimental/kernels/unidirectional_sequence_gru.cc\n+++ b/tensorflow/lite/experimental/kernels/unidirectional_sequence_gru.cc\n@@ -127,44 +127,55 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   TF_LITE_ENSURE_EQ(context, node->outputs->size, kOutputNum);\n \n   // input's dim = [n_time, n_batch, n_input]\n-  const TfLiteTensor* input = GetInput(context, node, kInput);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInput, &input));\n   TF_LITE_ENSURE_EQ(context, input->dims->size, 3);\n   const int n_time = input->dims->data[0];\n   const int n_batch = input->dims->data[1];\n   const int n_input = input->dims->data[2];\n \n   // input_state's dim = [n_batch, n_output]\n-  const TfLiteTensor* input_state = GetInput(context, node, kInputState);\n+  const TfLiteTensor* input_state;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kInputState, &input_state));\n   TF_LITE_ENSURE_EQ(context, input_state->dims->size, 2);\n   TF_LITE_ENSURE_EQ(context, input_state->dims->data[0], n_batch);\n   const int n_output = input_state->dims->data[1];\n \n   // gate_weight' dim = [2 * n_output, n_input + n_output]\n-  const TfLiteTensor* gate_weight = GetInput(context, node, kGateWeight);\n+  const TfLiteTensor* gate_weight;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kGateWeight, &gate_weight));\n   TF_LITE_ENSURE_EQ(context, gate_weight->dims->size, 2);\n   TF_LITE_ENSURE_EQ(context, gate_weight->dims->data[0], 2 * n_output);\n   TF_LITE_ENSURE_EQ(context, gate_weight->dims->data[1], n_input + n_output);\n \n   // gate_bias' dim = [2 * n_output]\n-  const TfLiteTensor* gate_bias = GetInput(context, node, kGateBias);\n+  const TfLiteTensor* gate_bias;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kGateBias, &gate_bias));\n   TF_LITE_ENSURE_EQ(context, gate_bias->dims->size, 1);\n   TF_LITE_ENSURE_EQ(context, gate_bias->dims->data[0], 2 * n_output);\n \n   // candidate_weight' dim = [n_output, n_input + n_output]\n-  const TfLiteTensor* candidate_weight =\n-      GetInput(context, node, kCandidateWeight);\n+  const TfLiteTensor* candidate_weight;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kCandidateWeight,\n+                                          &candidate_weight));\n   TF_LITE_ENSURE_EQ(context, candidate_weight->dims->size, 2);\n   TF_LITE_ENSURE_EQ(context, candidate_weight->dims->data[0], n_output);\n   TF_LITE_ENSURE_EQ(context, candidate_weight->dims->data[1],\n                     n_input + n_output);\n \n   // candidate_bias' dim = [n_output]\n-  const TfLiteTensor* candidate_bias = GetInput(context, node, kCandidateBias);\n+  const TfLiteTensor* candidate_bias;\n+  TF_LITE_ENSURE_OK(\n+      context, GetInputSafe(context, node, kCandidateBias, &candidate_bias));\n   TF_LITE_ENSURE_EQ(context, candidate_bias->dims->size, 1);\n   TF_LITE_ENSURE_EQ(context, candidate_bias->dims->data[0], n_output);\n \n   // output's dim = [n_time, n_batch, n_output]\n-  TfLiteTensor* output = GetOutput(context, node, kOutput);\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, kOutput, &output));\n   TfLiteIntArray* output_size = TfLiteIntArrayCreate(3);\n   output_size->data[0] = n_time;\n   output_size->data[1] = n_batch;\n@@ -173,7 +184,9 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n                     context->ResizeTensor(context, output, output_size));\n \n   // output_state's dim = [n_batch, n_output]\n-  TfLiteTensor* output_state = GetOutput(context, node, kOutputState);\n+  TfLiteTensor* output_state;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetOutputSafe(context, node, kOutputState, &output_state));\n   TF_LITE_ENSURE_OK(\n       context, context->ResizeTensor(context, output_state,\n                                      TfLiteIntArrayCopy(input_state->dims)));\n@@ -183,7 +196,9 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n \n   // activation's dim = [n_batch, 2 * n_output]\n   node->temporaries->data[kActivation] = *scratch_tensor_index;\n-  TfLiteTensor* activation = GetTemporary(context, node, kActivation);\n+  TfLiteTensor* activation;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetTemporarySafe(context, node, kActivation, &activation));\n   activation->type = input->type;\n   activation->allocation_type = kTfLiteArenaRw;\n   TfLiteIntArray* activation_size = TfLiteIntArrayCreate(2);\n@@ -194,7 +209,8 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n \n   // concat's dim  = [n_batch, n_input + n_output]\n   node->temporaries->data[kConcat] = (*scratch_tensor_index) + kConcat;\n-  TfLiteTensor* concat = GetTemporary(context, node, kConcat);\n+  TfLiteTensor* concat;\n+  TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, kConcat, &concat));\n   concat->type = input->type;\n   concat->allocation_type = kTfLiteArenaRw;\n   TfLiteIntArray* concat_size = TfLiteIntArrayCreate(2);\n@@ -207,17 +223,33 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n }\n \n TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n-  const TfLiteTensor* input = GetInput(context, node, kInput);\n-  const TfLiteTensor* input_state = GetInput(context, node, kInputState);\n-  const TfLiteTensor* gate_weight = GetInput(context, node, kGateWeight);\n-  const TfLiteTensor* gate_bias = GetInput(context, node, kGateBias);\n-  const TfLiteTensor* candidate_weight =\n-      GetInput(context, node, kCandidateWeight);\n-  const TfLiteTensor* candidate_bias = GetInput(context, node, kCandidateBias);\n-  TfLiteTensor* output = GetOutput(context, node, kOutput);\n-  TfLiteTensor* output_state = GetOutput(context, node, kOutputState);\n-  TfLiteTensor* activation = GetTemporary(context, node, kActivation);\n-  TfLiteTensor* concat = GetTemporary(context, node, kConcat);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInput, &input));\n+  const TfLiteTensor* input_state;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kInputState, &input_state));\n+  const TfLiteTensor* gate_weight;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kGateWeight, &gate_weight));\n+  const TfLiteTensor* gate_bias;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kGateBias, &gate_bias));\n+  const TfLiteTensor* candidate_weight;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kCandidateWeight,\n+                                          &candidate_weight));\n+  const TfLiteTensor* candidate_bias;\n+  TF_LITE_ENSURE_OK(\n+      context, GetInputSafe(context, node, kCandidateBias, &candidate_bias));\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, kOutput, &output));\n+  TfLiteTensor* output_state;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetOutputSafe(context, node, kOutputState, &output_state));\n+  TfLiteTensor* activation;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetTemporarySafe(context, node, kActivation, &activation));\n+  TfLiteTensor* concat;\n+  TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, kConcat, &concat));\n   auto cpu_backend_context = CpuBackendContext::GetFromContext(context);\n \n   if (gate_weight->type == kTfLiteFloat32) {"