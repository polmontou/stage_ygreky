"diff --git a/tensorflow/lite/kernels/activations.cc b/tensorflow/lite/kernels/activations.cc\nindex 00cd035470f..ee9fc2c3418 100644\n--- a/tensorflow/lite/kernels/activations.cc\n+++ b/tensorflow/lite/kernels/activations.cc\n@@ -252,8 +252,10 @@ void* HardSwishInit(TfLiteContext* context, const char* buffer, size_t length) {\n TfLiteStatus GenericPrepare(TfLiteContext* context, TfLiteNode* node) {\n   TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);\n   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n-  const TfLiteTensor* input = GetInput(context, node, 0);\n-  TfLiteTensor* output = GetOutput(context, node, 0);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n   TF_LITE_ENSURE_TYPES_EQ(context, input->type, output->type);\n \n   return context->ResizeTensor(context, output,\n@@ -272,8 +274,10 @@ TfLiteStatus ReluPrepare(TfLiteContext* context, TfLiteNode* node) {\n   ReluOpData* data = reinterpret_cast<ReluOpData*>(node->user_data);\n   TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);\n   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n-  const TfLiteTensor* input = GetInput(context, node, 0);\n-  TfLiteTensor* output = GetOutput(context, node, 0);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n   TF_LITE_ENSURE_TYPES_EQ(context, input->type, output->type);\n \n   if (input->type == kTfLiteInt8 || input->type == kTfLiteUInt8) {\n@@ -300,12 +304,14 @@ void HardSwishFree(TfLiteContext* context, void* buffer) {\n \n TfLiteStatus HardSwishPrepare(TfLiteContext* context, TfLiteNode* node) {\n   TF_LITE_ENSURE_STATUS(GenericPrepare(context, node));\n-  TfLiteTensor* output = GetOutput(context, node, 0);\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n \n   if (output->type == kTfLiteUInt8 || output->type == kTfLiteInt8) {\n     HardSwishData* data = static_cast<HardSwishData*>(node->user_data);\n     HardSwishParams* params = &data->params;\n-    const TfLiteTensor* input = GetInput(context, node, 0);\n+    const TfLiteTensor* input;\n+    TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n     params->input_zero_point = input->params.zero_point;\n     params->output_zero_point = output->params.zero_point;\n     const float input_scale = input->params.scale;\n@@ -337,8 +343,10 @@ TfLiteStatus HardSwishPrepare(TfLiteContext* context, TfLiteNode* node) {\n TfLiteStatus LeakyReluPrepare(TfLiteContext* context, TfLiteNode* node) {\n   TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);\n   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n-  const TfLiteTensor* input = GetInput(context, node, 0);\n-  TfLiteTensor* output = GetOutput(context, node, 0);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n   TF_LITE_ENSURE_TYPES_EQ(context, input->type, output->type);\n \n   LeakyReluOpData* data = reinterpret_cast<LeakyReluOpData*>(node->user_data);\n@@ -366,8 +374,10 @@ TfLiteStatus TanhPrepare(TfLiteContext* context, TfLiteNode* node) {\n \n   TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);\n   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n-  const TfLiteTensor* input = GetInput(context, node, 0);\n-  TfLiteTensor* output = GetOutput(context, node, 0);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n   TF_LITE_ENSURE_TYPES_EQ(context, input->type, output->type);\n \n   if (kernel_type == kFixedPointOptimized) {\n@@ -451,8 +461,10 @@ TfLiteStatus SigmoidPrepare(TfLiteContext* context, TfLiteNode* node) {\n \n   TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);\n   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n-  const TfLiteTensor* input = GetInput(context, node, 0);\n-  TfLiteTensor* output = GetOutput(context, node, 0);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n   TF_LITE_ENSURE_TYPES_EQ(context, input->type, output->type);\n \n   if (kernel_type == kFixedPointOptimized) {\n@@ -546,8 +558,10 @@ TfLiteStatus SoftmaxPrepare(TfLiteContext* context, TfLiteNode* node) {\n \n   TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);\n   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n-  const TfLiteTensor* input = GetInput(context, node, 0);\n-  TfLiteTensor* output = GetOutput(context, node, 0);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n   if (output->type == kTfLiteInt16) {\n     TF_LITE_ENSURE(context, input->type == kTfLiteInt8 ||\n                                 input->type == kTfLiteUInt8 ||\n@@ -614,8 +628,10 @@ TfLiteStatus LogSoftmaxPrepare(TfLiteContext* context, TfLiteNode* node) {\n \n   TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);\n   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n-  const TfLiteTensor* input = GetInput(context, node, 0);\n-  TfLiteTensor* output = GetOutput(context, node, 0);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n   TF_LITE_ENSURE_TYPES_EQ(context, input->type, output->type);\n \n   if (input->type == kTfLiteUInt8 || input->type == kTfLiteInt8) {\n@@ -650,9 +666,12 @@ TfLiteStatus LogSoftmaxPrepare(TfLiteContext* context, TfLiteNode* node) {\n TfLiteStatus PreluPrepare(TfLiteContext* context, TfLiteNode* node) {\n   TF_LITE_ENSURE_EQ(context, NumInputs(node), 2);\n   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n-  const TfLiteTensor* input = GetInput(context, node, 0);\n-  TfLiteTensor* output = GetOutput(context, node, 0);\n-  const TfLiteTensor* alpha = GetInput(context, node, 1);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n+  const TfLiteTensor* alpha;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 1, &alpha));\n   PreluOpData* data = reinterpret_cast<PreluOpData*>(node->user_data);\n \n   TF_LITE_ENSURE_TYPES_EQ(context, input->type, alpha->type);\n@@ -704,8 +723,10 @@ TfLiteStatus PreluPrepare(TfLiteContext* context, TfLiteNode* node) {\n }\n \n TfLiteStatus ReluEval(TfLiteContext* context, TfLiteNode* node) {\n-  const TfLiteTensor* input = GetInput(context, node, 0);\n-  TfLiteTensor* output = GetOutput(context, node, 0);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n   const ReluOpData* data = reinterpret_cast<ReluOpData*>(node->user_data);\n   switch (input->type) {\n     case kTfLiteFloat32: {\n@@ -732,8 +753,10 @@ TfLiteStatus ReluEval(TfLiteContext* context, TfLiteNode* node) {\n }\n \n TfLiteStatus Relu1Eval(TfLiteContext* context, TfLiteNode* node) {\n-  const TfLiteTensor* input = GetInput(context, node, 0);\n-  TfLiteTensor* output = GetOutput(context, node, 0);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n   const ReluOpData* data = reinterpret_cast<ReluOpData*>(node->user_data);\n   switch (input->type) {\n     case kTfLiteFloat32: {\n@@ -763,8 +786,10 @@ template <KernelType kernel_type>\n TfLiteStatus HardSwishEval(TfLiteContext* context, TfLiteNode* node) {\n   HardSwishData* data = static_cast<HardSwishData*>(node->user_data);\n \n-  const TfLiteTensor* input = GetInput(context, node, 0);\n-  TfLiteTensor* output = GetOutput(context, node, 0);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n   switch (input->type) {\n     case kTfLiteFloat32: {\n       if (kernel_type == kReference) {\n@@ -814,8 +839,10 @@ TfLiteStatus HardSwishEval(TfLiteContext* context, TfLiteNode* node) {\n }\n \n TfLiteStatus Relu6Eval(TfLiteContext* context, TfLiteNode* node) {\n-  const TfLiteTensor* input = GetInput(context, node, 0);\n-  TfLiteTensor* output = GetOutput(context, node, 0);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n   ReluOpData* data = reinterpret_cast<ReluOpData*>(node->user_data);\n   switch (input->type) {\n     case kTfLiteFloat32: {\n@@ -845,8 +872,10 @@ TfLiteStatus Relu6Eval(TfLiteContext* context, TfLiteNode* node) {\n template <KernelType kernel_type>\n TfLiteStatus TanhEval(TfLiteContext* context, TfLiteNode* node) {\n   OpData* data = reinterpret_cast<OpData*>(node->user_data);\n-  const TfLiteTensor* input = GetInput(context, node, 0);\n-  TfLiteTensor* output = GetOutput(context, node, 0);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n   switch (input->type) {\n     case kTfLiteFloat32: {\n       if (kernel_type == kReference) {\n@@ -919,8 +948,10 @@ template <KernelType kernel_type>\n TfLiteStatus SigmoidEval(TfLiteContext* context, TfLiteNode* node) {\n   OpData* data = reinterpret_cast<OpData*>(node->user_data);\n \n-  const TfLiteTensor* input = GetInput(context, node, 0);\n-  TfLiteTensor* output = GetOutput(context, node, 0);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n   switch (input->type) {\n     case kTfLiteFloat32: {\n       if (kernel_type == kReference) {\n@@ -1067,8 +1098,10 @@ TfLiteStatus SoftmaxEval(TfLiteContext* context, TfLiteNode* node) {\n   auto* params = reinterpret_cast<TfLiteSoftmaxParams*>(node->builtin_data);\n   SoftmaxOpData* data = reinterpret_cast<SoftmaxOpData*>(node->user_data);\n \n-  const TfLiteTensor* input = GetInput(context, node, 0);\n-  TfLiteTensor* output = GetOutput(context, node, 0);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n \n   switch (input->type) {\n     case kTfLiteFloat32: {\n@@ -1122,8 +1155,10 @@ template <KernelType kernel_type>\n TfLiteStatus LogSoftmaxEval(TfLiteContext* context, TfLiteNode* node) {\n   const LogSoftmaxOpData* data =\n       reinterpret_cast<LogSoftmaxOpData*>(node->user_data);\n-  const TfLiteTensor* input = GetInput(context, node, 0);\n-  TfLiteTensor* output = GetOutput(context, node, 0);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n   switch (input->type) {\n     case kTfLiteFloat32: {\n       SoftmaxParams op_params;\n@@ -1183,9 +1218,12 @@ T ApplyPrelu(T input, T alpha) {\n \n template <KernelType kernel_type>\n TfLiteStatus PreluEval(TfLiteContext* context, TfLiteNode* node) {\n-  const TfLiteTensor* input = GetInput(context, node, 0);\n-  const TfLiteTensor* alpha = GetInput(context, node, 1);\n-  TfLiteTensor* output = GetOutput(context, node, 0);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n+  const TfLiteTensor* alpha;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 1, &alpha));\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n   const PreluOpData* data = reinterpret_cast<PreluOpData*>(node->user_data);\n   switch (input->type) {\n     case kTfLiteFloat32: {\n@@ -1294,8 +1332,10 @@ void QuantizeLeakyRelu(const TfLiteTensor* input, TfLiteTensor* output,\n }\n \n TfLiteStatus LeakyReluEval(TfLiteContext* context, TfLiteNode* node) {\n-  const TfLiteTensor* input = GetInput(context, node, 0);\n-  TfLiteTensor* output = GetOutput(context, node, 0);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n   const auto* params =\n       reinterpret_cast<TfLiteLeakyReluParams*>(node->builtin_data);\n   const LeakyReluOpData* data =\n@@ -1332,8 +1372,10 @@ TfLiteStatus LeakyReluEval(TfLiteContext* context, TfLiteNode* node) {\n }\n \n TfLiteStatus EluPrepare(TfLiteContext* context, TfLiteNode* node) {\n-  const TfLiteTensor* input = GetInput(context, node, 0);\n-  TfLiteTensor* output = GetOutput(context, node, 0);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n   OpData* data = reinterpret_cast<OpData*>(node->user_data);\n \n   // Use LUT to handle quantized elu path.\n@@ -1346,8 +1388,10 @@ TfLiteStatus EluPrepare(TfLiteContext* context, TfLiteNode* node) {\n }\n \n TfLiteStatus EluEval(TfLiteContext* context, TfLiteNode* node) {\n-  const TfLiteTensor* input = GetInput(context, node, 0);\n-  TfLiteTensor* output = GetOutput(context, node, 0);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n   switch (input->type) {\n     case kTfLiteFloat32: {\n       optimized_ops::Elu(GetTensorShape(input), GetTensorData<float>(input),"