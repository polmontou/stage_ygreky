"diff --git a/tensorflow/lite/kernels/batch_matmul.cc b/tensorflow/lite/kernels/batch_matmul.cc\nindex a414a226504..35cf57128e7 100644\n--- a/tensorflow/lite/kernels/batch_matmul.cc\n+++ b/tensorflow/lite/kernels/batch_matmul.cc\n@@ -154,7 +154,9 @@ TfLiteStatus InitializeTemporaries(TfLiteContext* context, TfLiteNode* node,\n   // Temp tensor for Transposed LHS;\n   {\n     node->temporaries->data[0] = op_data->scratch_tensor_index;\n-    TfLiteTensor* scratch_buffer = GetTemporary(context, node, /*index=*/0);\n+    TfLiteTensor* scratch_buffer;\n+    TF_LITE_ENSURE_OK(\n+        context, GetTemporarySafe(context, node, /*index=*/0, &scratch_buffer));\n     TfLiteIntArray* scratch_buffer_size = TfLiteIntArrayCreate(lhs_rank);\n     for (int i = 0; i < lhs_rank - 2; ++i) {\n       scratch_buffer_size->data[i] = lhs->dims->data[i];\n@@ -175,7 +177,9 @@ TfLiteStatus InitializeTemporaries(TfLiteContext* context, TfLiteNode* node,\n   // is set by the caller, the data is already in the desired layout.\n   {\n     node->temporaries->data[1] = op_data->scratch_tensor_index + 1;\n-    TfLiteTensor* scratch_buffer = GetTemporary(context, node, /*index=*/1);\n+    TfLiteTensor* scratch_buffer;\n+    TF_LITE_ENSURE_OK(\n+        context, GetTemporarySafe(context, node, /*index=*/1, &scratch_buffer));\n     const TfLiteTensor* rhs = op_context->rhs;\n     int rhs_rank = NumDimensions(rhs);\n     TfLiteIntArray* scratch_buffer_size = TfLiteIntArrayCreate(rhs_rank);\n@@ -215,7 +219,9 @@ TfLiteStatus InitializeTemporaries(TfLiteContext* context, TfLiteNode* node,\n     }\n     op_data->compute_row_sums = true;\n     node->temporaries->data[2] = op_data->scratch_tensor_index + 2;\n-    TfLiteTensor* input_quantized = GetTemporary(context, node, /*index=*/2);\n+    TfLiteTensor* input_quantized;\n+    TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, /*index=*/2,\n+                                                &input_quantized));\n     input_quantized->type = op_context->rhs->type;\n     input_quantized->allocation_type = kTfLiteArenaRw;\n \n@@ -225,7 +231,9 @@ TfLiteStatus InitializeTemporaries(TfLiteContext* context, TfLiteNode* node,\n                                                      input_quantized_size));\n \n     node->temporaries->data[3] = op_data->scratch_tensor_index + 3;\n-    TfLiteTensor* scaling_factors = GetTemporary(context, node, /*index=*/3);\n+    TfLiteTensor* scaling_factors;\n+    TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, /*index=*/3,\n+                                                &scaling_factors));\n     scaling_factors->type = kTfLiteFloat32;\n     scaling_factors->allocation_type = kTfLiteArenaRw;\n     // Total size of scaling factors is batch size * number of total batches\n@@ -238,7 +246,9 @@ TfLiteStatus InitializeTemporaries(TfLiteContext* context, TfLiteNode* node,\n     }\n \n     node->temporaries->data[4] = op_data->scratch_tensor_index + 4;\n-    TfLiteTensor* accum_scratch = GetTemporary(context, node, /*index=*/4);\n+    TfLiteTensor* accum_scratch;\n+    TF_LITE_ENSURE_OK(\n+        context, GetTemporarySafe(context, node, /*index=*/4, &accum_scratch));\n     accum_scratch->type = kTfLiteInt32;\n     accum_scratch->allocation_type = kTfLiteArenaRw;\n     int accum_scratch_dims[2] = {num_units, batch_size};\n@@ -252,7 +262,9 @@ TfLiteStatus InitializeTemporaries(TfLiteContext* context, TfLiteNode* node,\n     }\n \n     node->temporaries->data[5] = op_data->scratch_tensor_index + 5;\n-    TfLiteTensor* input_offsets = GetTemporary(context, node, /*index=*/5);\n+    TfLiteTensor* input_offsets;\n+    TF_LITE_ENSURE_OK(\n+        context, GetTemporarySafe(context, node, /*index=*/5, &input_offsets));\n     input_offsets->type = kTfLiteInt32;\n     input_offsets->allocation_type = kTfLiteArenaRw;\n     if (!TfLiteIntArrayEqualsArray(input_offsets->dims, 1, scaling_dims)) {\n@@ -262,7 +274,9 @@ TfLiteStatus InitializeTemporaries(TfLiteContext* context, TfLiteNode* node,\n                                                        input_offsets_size));\n     }\n     node->temporaries->data[6] = op_data->scratch_tensor_index + 6;\n-    TfLiteTensor* row_sums = GetTemporary(context, node, /*index=*/6);\n+    TfLiteTensor* row_sums;\n+    TF_LITE_ENSURE_OK(context,\n+                      GetTemporarySafe(context, node, /*index=*/6, &row_sums));\n     row_sums->type = kTfLiteInt32;\n     row_sums->allocation_type = kTfLiteArenaRwPersistent;\n     int row_sums_dims[1] = {num_weights_matrices * num_units};\n@@ -288,9 +302,15 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   bool adj_x = op_context.params->adj_x;\n   bool adj_y = op_context.params->adj_y;\n \n-  const TfLiteTensor* lhs_data = GetInput(context, node, kInputLHSTensor);\n-  const TfLiteTensor* rhs_data = GetInput(context, node, kInputRHSTensor);\n-  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+  const TfLiteTensor* lhs_data;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kInputLHSTensor, &lhs_data));\n+  const TfLiteTensor* rhs_data;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kInputRHSTensor, &rhs_data));\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetOutputSafe(context, node, kOutputTensor, &output));\n \n   // Note that quantized inference requires that all tensors have their\n   // parameters set. This is usually done during quantized training.\n@@ -502,11 +522,21 @@ TfLiteStatus EvalQuantized(TfLiteContext* context, TfLiteNode* node,\n                            const RuntimeShape& rhs_shape,\n                            const TfLiteTensor* rhs, TfLiteTensor* output) {\n   if (lhs->type == kTfLiteFloat32) {\n-    TfLiteTensor* input_quantized = GetTemporary(context, node, /*index=*/2);\n-    TfLiteTensor* scaling_factors = GetTemporary(context, node, /*index=*/3);\n-    TfLiteTensor* accum_scratch = GetTemporary(context, node, /*index=*/4);\n-    TfLiteTensor* input_offsets = GetTemporary(context, node, /*index=*/5);\n-    TfLiteTensor* row_sums = GetTemporary(context, node, /*index=*/6);\n+    TfLiteTensor* input_quantized;\n+    TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, /*index=*/2,\n+                                                &input_quantized));\n+    TfLiteTensor* scaling_factors;\n+    TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, /*index=*/3,\n+                                                &scaling_factors));\n+    TfLiteTensor* accum_scratch;\n+    TF_LITE_ENSURE_OK(\n+        context, GetTemporarySafe(context, node, /*index=*/4, &accum_scratch));\n+    TfLiteTensor* input_offsets;\n+    TF_LITE_ENSURE_OK(\n+        context, GetTemporarySafe(context, node, /*index=*/5, &input_offsets));\n+    TfLiteTensor* row_sums;\n+    TF_LITE_ENSURE_OK(context,\n+                      GetTemporarySafe(context, node, /*index=*/6, &row_sums));\n     return EvalHybrid<kernel_type>(\n         context, node, data, lhs_shape, lhs, rhs_shape, rhs, input_quantized,\n         scaling_factors, accum_scratch, row_sums, input_offsets, output);\n@@ -524,6 +554,10 @@ TfLiteStatus EvalQuantized(TfLiteContext* context, TfLiteNode* node,\n TfLiteTensor* GetTempRhs(TfLiteContext* context, TfLiteNode* node,\n                          const TfLiteTensor* rhs) {\n   TfLiteTensor* transposed_rhs = GetTemporary(context, node, 1);\n+  if (transposed_rhs == nullptr) {\n+    return nullptr;\n+  }\n+\n   if (rhs->type == kTfLiteInt8) {\n     // Get the quantization params from the RHS tensor.\n     transposed_rhs->params.scale = rhs->params.scale;\n@@ -535,6 +569,10 @@ TfLiteTensor* GetTempRhs(TfLiteContext* context, TfLiteNode* node,\n TfLiteTensor* GetTempLhs(TfLiteContext* context, TfLiteNode* node,\n                          const TfLiteTensor* lhs) {\n   TfLiteTensor* transposed_lhs = GetTemporary(context, node, 0);\n+  if (transposed_lhs == nullptr) {\n+    return nullptr;\n+  }\n+\n   if (lhs->type == kTfLiteInt8) {\n     // Get the quantization params from the LHS tensor.\n     transposed_lhs->params.scale = lhs->params.scale;\n@@ -558,9 +596,15 @@ template <KernelType kernel_type>\n TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n   OpContext op_context(context, node);\n   OpData* op_data = reinterpret_cast<OpData*>(node->user_data);\n-  const TfLiteTensor* lhs = GetInput(context, node, kInputLHSTensor);\n-  const TfLiteTensor* rhs = GetInput(context, node, kInputRHSTensor);\n-  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+  const TfLiteTensor* lhs;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kInputLHSTensor, &lhs));\n+  const TfLiteTensor* rhs;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kInputRHSTensor, &rhs));\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetOutputSafe(context, node, kOutputTensor, &output));\n   RuntimeShape orig_lhs_shape = GetTensorShape(lhs);\n   RuntimeShape orig_rhs_shape = GetTensorShape(rhs);\n "