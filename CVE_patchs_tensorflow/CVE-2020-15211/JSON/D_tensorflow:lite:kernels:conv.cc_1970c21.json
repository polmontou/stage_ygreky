"diff --git a/tensorflow/lite/kernels/conv.cc b/tensorflow/lite/kernels/conv.cc\nindex 1b12945b2f3..5c978f8dbfb 100644\n--- a/tensorflow/lite/kernels/conv.cc\n+++ b/tensorflow/lite/kernels/conv.cc\n@@ -222,8 +222,10 @@ static TfLiteStatus AllocateTemporaryTensorsIfRequired(TfLiteContext* context,\n   OpData* data = reinterpret_cast<OpData*>(node->user_data);\n \n   TF_LITE_ENSURE(context, node->inputs->size >= 2);\n-  const TfLiteTensor* input = GetInput(context, node, 0);\n-  const TfLiteTensor* filter = GetInput(context, node, 1);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n+  const TfLiteTensor* filter;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 1, &filter));\n \n   // If we're using the optimized multithreaded EigenTensor implementation of\n   // convolution, it expects the filter weights to be transposed compared to\n@@ -316,9 +318,12 @@ TfLiteStatus Prepare(KernelType kernel_type, TfLiteContext* context,\n   // Check number of inputs/outputs\n   TF_LITE_ENSURE(context, has_bias || node->inputs->size == 2);\n   TF_LITE_ENSURE_EQ(context, node->outputs->size, 1);\n-  TfLiteTensor* output = GetOutput(context, node, 0);\n-  const TfLiteTensor* input = GetInput(context, node, 0);\n-  const TfLiteTensor* filter = GetInput(context, node, 1);\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n+  const TfLiteTensor* filter;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 1, &filter));\n \n   // Check dimensionality of input, filter\n   TF_LITE_ENSURE_EQ(context, input->dims->size, 4);\n@@ -340,7 +345,7 @@ TfLiteStatus Prepare(KernelType kernel_type, TfLiteContext* context,\n   TF_LITE_ENSURE(context, has_bias);\n \n   if (has_bias) {\n-    bias = GetInput(context, node, 2);\n+    TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 2, &bias));\n     if (input_type == kTfLiteUInt8 || input_type == kTfLiteInt8) {\n       TF_LITE_ENSURE_TYPES_EQ(context, bias->type, kTfLiteInt32);\n       TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);\n@@ -493,8 +498,10 @@ TfLiteStatus Prepare(KernelType kernel_type, TfLiteContext* context,\n   if (is_hybrid) {\n     node->temporaries->data[data->input_quantized_index] =\n         data->input_quantized_id;\n-    TfLiteTensor* input_quantized =\n-        GetTemporary(context, node, data->input_quantized_index);\n+    TfLiteTensor* input_quantized;\n+    TF_LITE_ENSURE_OK(\n+        context, GetTemporarySafe(context, node, data->input_quantized_index,\n+                                  &input_quantized));\n     input_quantized->type = kTfLiteInt8;\n     input_quantized->allocation_type = kTfLiteArenaRw;\n     if (!TfLiteIntArrayEqual(input_quantized->dims, input->dims)) {\n@@ -505,8 +512,10 @@ TfLiteStatus Prepare(KernelType kernel_type, TfLiteContext* context,\n \n     node->temporaries->data[data->scaling_factors_index] =\n         data->scaling_factors_id;\n-    TfLiteTensor* scaling_factors =\n-        GetTemporary(context, node, data->scaling_factors_index);\n+    TfLiteTensor* scaling_factors;\n+    TF_LITE_ENSURE_OK(\n+        context, GetTemporarySafe(context, node, data->scaling_factors_index,\n+                                  &scaling_factors));\n     scaling_factors->type = kTfLiteFloat32;\n     scaling_factors->allocation_type = kTfLiteArenaRw;\n     // Only one scale factor per batch is typically necessary. See optimized\n@@ -522,8 +531,10 @@ TfLiteStatus Prepare(KernelType kernel_type, TfLiteContext* context,\n     }\n \n     node->temporaries->data[data->accum_scratch_index] = data->accum_scratch_id;\n-    TfLiteTensor* accum_scratch =\n-        GetTemporary(context, node, data->accum_scratch_index);\n+    TfLiteTensor* accum_scratch;\n+    TF_LITE_ENSURE_OK(context,\n+                      GetTemporarySafe(context, node, data->accum_scratch_index,\n+                                       &accum_scratch));\n     accum_scratch->type = kTfLiteInt32;\n     accum_scratch->allocation_type = kTfLiteArenaRw;\n     const int scratch_width = batches * out_height * out_width;\n@@ -545,8 +556,10 @@ TfLiteStatus Prepare(KernelType kernel_type, TfLiteContext* context,\n           context, affine_quantization->scale->size,\n           filter->dims->data[affine_quantization->quantized_dimension]);\n       node->temporaries->data[data->input_offset_index] = data->input_offset_id;\n-      TfLiteTensor* input_offsets =\n-          GetTemporary(context, node, data->input_offset_index);\n+      TfLiteTensor* input_offsets;\n+      TF_LITE_ENSURE_OK(\n+          context, GetTemporarySafe(context, node, data->input_offset_index,\n+                                    &input_offsets));\n       input_offsets->type = kTfLiteInt32;\n       input_offsets->allocation_type = kTfLiteArenaRw;\n       // See above comment for the need to allocate for height of inputs.\n@@ -560,8 +573,10 @@ TfLiteStatus Prepare(KernelType kernel_type, TfLiteContext* context,\n                                                          input_offsets_size));\n       }\n       node->temporaries->data[data->row_sums_index] = data->row_sums_id;\n-      TfLiteTensor* row_sums =\n-          GetTemporary(context, node, data->row_sums_index);\n+      TfLiteTensor* row_sums;\n+      TF_LITE_ENSURE_OK(\n+          context,\n+          GetTemporarySafe(context, node, data->row_sums_index, &row_sums));\n       row_sums->type = kTfLiteInt32;\n       row_sums->allocation_type = kTfLiteArenaRwPersistent;\n       // See above comment for the need to allocate for height of inputs.\n@@ -802,23 +817,34 @@ void EvalFloat(TfLiteContext* context, TfLiteNode* node,\n }\n \n template <KernelType kernel_type>\n-void EvalHybridPerChannel(TfLiteContext* context, TfLiteNode* node,\n-                          TfLiteConvParams* params, OpData* data,\n-                          const TfLiteTensor* input, const TfLiteTensor* filter,\n-                          const TfLiteTensor* bias, TfLiteTensor* im2col,\n-                          TfLiteTensor* output) {\n+TfLiteStatus EvalHybridPerChannel(TfLiteContext* context, TfLiteNode* node,\n+                                  TfLiteConvParams* params, OpData* data,\n+                                  const TfLiteTensor* input,\n+                                  const TfLiteTensor* filter,\n+                                  const TfLiteTensor* bias,\n+                                  TfLiteTensor* im2col, TfLiteTensor* output) {\n   float output_activation_min, output_activation_max;\n   CalculateActivationRange(params->activation, &output_activation_min,\n                            &output_activation_max);\n \n   const int input_size = NumElements(input) / SizeOfDimension(input, 0);\n   const int batch_size = SizeOfDimension(input, 0);\n-  int8_t* quantized_input_ptr_batch = GetTensorData<int8_t>(\n-      GetTemporary(context, node, data->input_quantized_index));\n-  float* scaling_factors_ptr = GetTensorData<float>(\n-      GetTemporary(context, node, data->scaling_factors_index));\n-  int32_t* input_offset_ptr = GetTensorData<int32_t>(\n-      GetTemporary(context, node, data->input_offset_index));\n+  TfLiteTensor* quantized_input_tensor;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetTemporarySafe(context, node, data->input_quantized_index,\n+                                     &quantized_input_tensor));\n+  int8_t* quantized_input_ptr_batch =\n+      GetTensorData<int8_t>(quantized_input_tensor);\n+  TfLiteTensor* scaling_factors_tensor;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetTemporarySafe(context, node, data->scaling_factors_index,\n+                                     &scaling_factors_tensor));\n+  float* scaling_factors_ptr = GetTensorData<float>(scaling_factors_tensor);\n+  TfLiteTensor* input_offset_tensor;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetTemporarySafe(context, node, data->input_offset_index,\n+                                     &input_offset_tensor));\n+  int32_t* input_offset_ptr = GetTensorData<int32_t>(input_offset_tensor);\n \n   for (int b = 0; b < batch_size; ++b) {\n     const int offset = b * input_size;\n@@ -859,10 +885,14 @@ void EvalHybridPerChannel(TfLiteContext* context, TfLiteNode* node,\n     case kGenericOptimized:\n     case kMultithreadOptimized:\n     case kCblasOptimized: {\n-      TfLiteTensor* row_sums =\n-          GetTemporary(context, node, data->row_sums_index);\n-      TfLiteTensor* scratch =\n-          GetTemporary(context, node, data->accum_scratch_index);\n+      TfLiteTensor* row_sums;\n+      TF_LITE_ENSURE_OK(\n+          context,\n+          GetTemporarySafe(context, node, data->row_sums_index, &row_sums));\n+      TfLiteTensor* scratch;\n+      TF_LITE_ENSURE_OK(\n+          context,\n+          GetTemporarySafe(context, node, data->accum_scratch_index, &scratch));\n       optimized_ops::HybridConvPerChannel(\n           op_params, scaling_factors_ptr, GetTensorShape(input),\n           quantized_input_ptr_batch, GetTensorShape(filter), filter_ptr,\n@@ -877,14 +907,16 @@ void EvalHybridPerChannel(TfLiteContext* context, TfLiteNode* node,\n       break;\n     }\n   }\n+\n+  return kTfLiteOk;\n }\n \n template <KernelType kernel_type>\n-void EvalHybrid(TfLiteContext* context, TfLiteNode* node,\n-                TfLiteConvParams* params, OpData* data,\n-                const TfLiteTensor* input, const TfLiteTensor* filter,\n-                const TfLiteTensor* bias, TfLiteTensor* im2col,\n-                TfLiteTensor* accum_scratch, TfLiteTensor* output) {\n+TfLiteStatus EvalHybrid(TfLiteContext* context, TfLiteNode* node,\n+                        TfLiteConvParams* params, OpData* data,\n+                        const TfLiteTensor* input, const TfLiteTensor* filter,\n+                        const TfLiteTensor* bias, TfLiteTensor* im2col,\n+                        TfLiteTensor* accum_scratch, TfLiteTensor* output) {\n   float output_activation_min, output_activation_max;\n   CalculateActivationRange(params->activation, &output_activation_min,\n                            &output_activation_max);\n@@ -893,10 +925,17 @@ void EvalHybrid(TfLiteContext* context, TfLiteNode* node,\n   const int batch_size = SizeOfDimension(input, 0);\n \n   const float* input_ptr = GetTensorData<float>(input);\n-  int8_t* quantized_input_ptr_batch = GetTensorData<int8_t>(\n-      GetTemporary(context, node, data->input_quantized_index));\n-  float* scaling_factors_ptr = GetTensorData<float>(\n-      GetTemporary(context, node, data->scaling_factors_index));\n+  TfLiteTensor* quantized_input_tensor;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetTemporarySafe(context, node, data->input_quantized_index,\n+                                     &quantized_input_tensor));\n+  int8_t* quantized_input_ptr_batch =\n+      GetTensorData<int8_t>(quantized_input_tensor);\n+  TfLiteTensor* scaling_factors_tensor;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetTemporarySafe(context, node, data->scaling_factors_index,\n+                                     &scaling_factors_tensor));\n+  float* scaling_factors_ptr = GetTensorData<float>(scaling_factors_tensor);\n \n   // Per-batch input quantization for higher accuracy.\n   {\n@@ -939,6 +978,8 @@ void EvalHybrid(TfLiteContext* context, TfLiteNode* node,\n       break;\n     }\n   }\n+\n+  return kTfLiteOk;\n }\n \n template <KernelType kernel_type, TfLiteType input_type>\n@@ -946,9 +987,12 @@ TfLiteStatus EvalImpl(TfLiteContext* context, TfLiteNode* node) {\n   auto* params = reinterpret_cast<TfLiteConvParams*>(node->builtin_data);\n   OpData* data = reinterpret_cast<OpData*>(node->user_data);\n \n-  TfLiteTensor* output = GetOutput(context, node, 0);\n-  const TfLiteTensor* input = GetInput(context, node, 0);\n-  const TfLiteTensor* filter = GetInput(context, node, 1);\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n+  const TfLiteTensor* filter;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 1, &filter));\n   bool has_bias = node->inputs->size == 3;\n   const TfLiteTensor* bias = has_bias ? GetInput(context, node, 2) : nullptr;\n   TfLiteTensor* im2col =\n@@ -970,14 +1014,17 @@ TfLiteStatus EvalImpl(TfLiteContext* context, TfLiteNode* node) {\n     case kTfLiteFloat32:\n       if (filter->type == kTfLiteUInt8 || filter->type == kTfLiteInt8) {\n         if (data->is_hybrid_per_channel) {\n-          EvalHybridPerChannel<kernel_type>(context, node, params, data, input,\n-                                            filter, bias, im2col, output);\n+          TF_LITE_ENSURE_OK(context, EvalHybridPerChannel<kernel_type>(\n+                                         context, node, params, data, input,\n+                                         filter, bias, im2col, output));\n         } else {\n           TfLiteTensor* accum_scratch =\n               &context->tensors[node->temporaries\n                                     ->data[data->accum_scratch_index]];\n-          EvalHybrid<kernel_type>(context, node, params, data, input, filter,\n-                                  bias, im2col, accum_scratch, output);\n+          TF_LITE_ENSURE_OK(context,\n+                            EvalHybrid<kernel_type>(context, node, params, data,\n+                                                    input, filter, bias, im2col,\n+                                                    accum_scratch, output));\n         }\n       } else {\n         EvalFloat<kernel_type>(context, node, params, data, input, filter, bias,\n@@ -1006,7 +1053,8 @@ TfLiteStatus EvalImpl(TfLiteContext* context, TfLiteNode* node) {\n \n template <KernelType kernel_type>\n TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n-  const TfLiteTensor* input = GetInput(context, node, 0);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n \n   switch (input->type) {\n     case kTfLiteFloat32:"