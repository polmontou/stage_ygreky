"diff --git a/tensorflow/lite/kernels/depthwise_conv.cc b/tensorflow/lite/kernels/depthwise_conv.cc\nindex 961a987cf02..a76853da190 100644\n--- a/tensorflow/lite/kernels/depthwise_conv.cc\n+++ b/tensorflow/lite/kernels/depthwise_conv.cc\n@@ -104,12 +104,17 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   bool hasBias = NumInputs(node) == 3;\n \n   TF_LITE_ENSURE(context, hasBias || NumInputs(node) == 2);\n-  const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n-  const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n+  const TfLiteTensor* filter;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kFilterTensor, &filter));\n   const TfLiteTensor* bias = nullptr;\n \n   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n-  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetOutputSafe(context, node, kOutputTensor, &output));\n \n   TF_LITE_ENSURE_EQ(context, NumDimensions(input), 4);\n   TF_LITE_ENSURE_EQ(context, NumDimensions(filter), 4);\n@@ -132,7 +137,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   TF_LITE_ENSURE_EQ(context, SizeOfDimension(filter, 0), 1);\n \n   if (hasBias) {\n-    bias = GetInput(context, node, kBiasTensor);\n+    TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kBiasTensor, &bias));\n     if (data_type == kTfLiteUInt8 || data_type == kTfLiteInt8) {\n       TF_LITE_ENSURE_TYPES_EQ(context, bias->type, kTfLiteInt32);\n       TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);\n@@ -224,8 +229,10 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n \n     node->temporaries->data[data->input_quantized_index] =\n         data->input_quantized_id;\n-    TfLiteTensor* input_quantized =\n-        GetTemporary(context, node, data->input_quantized_index);\n+    TfLiteTensor* input_quantized;\n+    TF_LITE_ENSURE_OK(\n+        context, GetTemporarySafe(context, node, data->input_quantized_index,\n+                                  &input_quantized));\n     input_quantized->type = kTfLiteInt8;\n     input_quantized->allocation_type = kTfLiteArenaRw;\n     if (!TfLiteIntArrayEqual(input_quantized->dims, input->dims)) {\n@@ -235,8 +242,10 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n     }\n     node->temporaries->data[data->scaling_factors_index] =\n         data->scaling_factors_id;\n-    TfLiteTensor* scaling_factors =\n-        GetTemporary(context, node, data->scaling_factors_index);\n+    TfLiteTensor* scaling_factors;\n+    TF_LITE_ENSURE_OK(\n+        context, GetTemporarySafe(context, node, data->scaling_factors_index,\n+                                  &scaling_factors));\n     scaling_factors->type = kTfLiteFloat32;\n     scaling_factors->allocation_type = kTfLiteArenaRw;\n     const int batch_size = SizeOfDimension(input, 0);\n@@ -248,8 +257,10 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n                                                        scaling_factors_size));\n     }\n     node->temporaries->data[data->input_offset_index] = data->input_offset_id;\n-    TfLiteTensor* input_offsets =\n-        GetTemporary(context, node, data->input_offset_index);\n+    TfLiteTensor* input_offsets;\n+    TF_LITE_ENSURE_OK(context,\n+                      GetTemporarySafe(context, node, data->input_offset_index,\n+                                       &input_offsets));\n     input_offsets->type = kTfLiteInt32;\n     input_offsets->allocation_type = kTfLiteArenaRw;\n     if (!TfLiteIntArrayEqualsArray(input_offsets->dims, 1, scaling_dims)) {\n@@ -446,13 +457,21 @@ TfLiteStatus EvalHybridPerChannel(TfLiteContext* context, TfLiteNode* node,\n                            &output_activation_max);\n   const int input_size = NumElements(input) / SizeOfDimension(input, 0);\n   const int batch_size = SizeOfDimension(input, 0);\n-  const TfLiteTensor* input_quantized =\n-      GetTemporary(context, node, data->input_quantized_index);\n+  TfLiteTensor* input_quantized;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetTemporarySafe(context, node, data->input_quantized_index,\n+                                     &input_quantized));\n   int8_t* quantized_input_ptr_batch = input_quantized->data.int8;\n-  float* scaling_factors_ptr = GetTensorData<float>(\n-      GetTemporary(context, node, data->scaling_factors_index));\n-  int32_t* input_offset_ptr = GetTensorData<int32_t>(\n-      GetTemporary(context, node, data->input_offset_index));\n+  TfLiteTensor* scaling_factors_tensor;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetTemporarySafe(context, node, data->scaling_factors_index,\n+                                     &scaling_factors_tensor));\n+  float* scaling_factors_ptr = GetTensorData<float>(scaling_factors_tensor);\n+  TfLiteTensor* input_offset_tensor;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetTemporarySafe(context, node, data->input_offset_index,\n+                                     &input_offset_tensor));\n+  int32_t* input_offset_ptr = GetTensorData<int32_t>(input_offset_tensor);\n \n   for (int b = 0; b < batch_size; ++b) {\n     const int offset = b * input_size;\n@@ -504,9 +523,14 @@ TfLiteStatus EvalImpl(TfLiteContext* context, TfLiteNode* node) {\n       reinterpret_cast<TfLiteDepthwiseConvParams*>(node->builtin_data);\n   OpData* data = reinterpret_cast<OpData*>(node->user_data);\n \n-  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n-  const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n-  const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetOutputSafe(context, node, kOutputTensor, &output));\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n+  const TfLiteTensor* filter;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kFilterTensor, &filter));\n   const TfLiteTensor* bias =\n       (NumInputs(node) == 3) ? GetInput(context, node, kBiasTensor) : nullptr;\n   TFLITE_DCHECK_EQ(input_type, input->type);\n@@ -547,7 +571,8 @@ TfLiteStatus EvalImpl(TfLiteContext* context, TfLiteNode* node) {\n \n template <KernelType kernel_type>\n TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n-  const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n \n   switch (input->type) {  // Already know in/out types are same.\n     case kTfLiteFloat32:"