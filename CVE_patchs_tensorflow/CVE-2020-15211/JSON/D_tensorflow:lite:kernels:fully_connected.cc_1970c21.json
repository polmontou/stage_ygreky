"diff --git a/tensorflow/lite/kernels/fully_connected.cc b/tensorflow/lite/kernels/fully_connected.cc\nindex 9cbbcae9c51..1ba3932b476 100644\n--- a/tensorflow/lite/kernels/fully_connected.cc\n+++ b/tensorflow/lite/kernels/fully_connected.cc\n@@ -155,13 +155,18 @@ TfLiteStatus PrepareImpl(TfLiteContext* context, TfLiteNode* node) {\n                                                                           : 2;\n   TF_LITE_ENSURE_EQ(context, node->outputs->size, expected_outputs_count);\n \n-  const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n-  const TfLiteTensor* filter = GetInput(context, node, kWeightsTensor);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n+  const TfLiteTensor* filter;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kWeightsTensor, &filter));\n   const TfLiteTensor* bias =\n       (node->inputs->size == 3)\n           ? GetOptionalInputTensor(context, node, kBiasTensor)\n           : nullptr;\n-  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetOutputSafe(context, node, kOutputTensor, &output));\n \n   // Check proper datatype match among all Input Tensors\n   TF_LITE_ENSURE_STATUS(\n@@ -214,7 +219,9 @@ TfLiteStatus PrepareImpl(TfLiteContext* context, TfLiteNode* node) {\n     node->temporaries = TfLiteIntArrayCreate(5);\n     node->temporaries->data[0] = data->scratch_tensor_index;\n \n-    TfLiteTensor* input_quantized = GetTemporary(context, node, /*index=*/0);\n+    TfLiteTensor* input_quantized;\n+    TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, /*index=*/0,\n+                                                &input_quantized));\n     input_quantized->type = filter->type;\n     input_quantized->allocation_type = kTfLiteArenaRw;\n \n@@ -223,7 +230,9 @@ TfLiteStatus PrepareImpl(TfLiteContext* context, TfLiteNode* node) {\n                                                      input_quantized_size));\n \n     node->temporaries->data[1] = data->scratch_tensor_index + 1;\n-    TfLiteTensor* scaling_factors = GetTemporary(context, node, /*index=*/1);\n+    TfLiteTensor* scaling_factors;\n+    TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, /*index=*/1,\n+                                                &scaling_factors));\n     scaling_factors->type = kTfLiteFloat32;\n     scaling_factors->allocation_type = kTfLiteArenaRw;\n \n@@ -236,7 +245,9 @@ TfLiteStatus PrepareImpl(TfLiteContext* context, TfLiteNode* node) {\n     }\n \n     node->temporaries->data[2] = data->scratch_tensor_index + 2;\n-    TfLiteTensor* accum_scratch = GetTemporary(context, node, /*index=*/2);\n+    TfLiteTensor* accum_scratch;\n+    TF_LITE_ENSURE_OK(\n+        context, GetTemporarySafe(context, node, /*index=*/2, &accum_scratch));\n     accum_scratch->type = kTfLiteInt32;\n     accum_scratch->allocation_type = kTfLiteArenaRw;\n     int accum_scratch_dims[2] = {num_units, batch_size};\n@@ -250,7 +261,9 @@ TfLiteStatus PrepareImpl(TfLiteContext* context, TfLiteNode* node) {\n     }\n \n     node->temporaries->data[3] = data->scratch_tensor_index + 3;\n-    TfLiteTensor* input_offsets = GetTemporary(context, node, /*index=*/3);\n+    TfLiteTensor* input_offsets;\n+    TF_LITE_ENSURE_OK(\n+        context, GetTemporarySafe(context, node, /*index=*/3, &input_offsets));\n     input_offsets->type = kTfLiteInt32;\n     input_offsets->allocation_type = kTfLiteArenaRw;\n     if (!TfLiteIntArrayEqualsArray(input_offsets->dims, 1, scaling_dims)) {\n@@ -260,7 +273,9 @@ TfLiteStatus PrepareImpl(TfLiteContext* context, TfLiteNode* node) {\n                                                        input_offsets_size));\n     }\n     node->temporaries->data[4] = data->scratch_tensor_index + 4;\n-    TfLiteTensor* row_sums = GetTemporary(context, node, /*index=*/4);\n+    TfLiteTensor* row_sums;\n+    TF_LITE_ENSURE_OK(context,\n+                      GetTemporarySafe(context, node, /*index=*/4, &row_sums));\n     row_sums->type = kTfLiteInt32;\n     row_sums->allocation_type = kTfLiteArenaRwPersistent;\n     int row_sums_dims[1] = {num_units};\n@@ -300,8 +315,11 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   // Check for supported activation types.\n   auto* params =\n       reinterpret_cast<TfLiteFullyConnectedParams*>(node->builtin_data);\n-  const TfLiteTensor* filter = GetInput(context, node, kWeightsTensor);\n-  const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n+  const TfLiteTensor* filter;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kWeightsTensor, &filter));\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n   const bool is_quantized =\n       ((filter->type == kTfLiteUInt8) || (filter->type == kTfLiteInt8));\n   const bool is_hybrid = is_quantized && (input->type == kTfLiteFloat32);\n@@ -484,11 +502,21 @@ TfLiteStatus EvalQuantized(TfLiteContext* context, TfLiteNode* node,\n   int32_t output_offset = output->params.zero_point;\n   // Only the Pie path supports quantized models and float inputs/outputs.\n   if (input->type == kTfLiteFloat32) {\n-    TfLiteTensor* input_quantized = GetTemporary(context, node, /*index=*/0);\n-    TfLiteTensor* scaling_factors = GetTemporary(context, node, /*index=*/1);\n-    TfLiteTensor* accum_scratch = GetTemporary(context, node, /*index=*/2);\n-    TfLiteTensor* input_offsets = GetTemporary(context, node, /*index=*/3);\n-    TfLiteTensor* row_sums = GetTemporary(context, node, /*index=*/4);\n+    TfLiteTensor* input_quantized;\n+    TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, /*index=*/0,\n+                                                &input_quantized));\n+    TfLiteTensor* scaling_factors;\n+    TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, /*index=*/1,\n+                                                &scaling_factors));\n+    TfLiteTensor* accum_scratch;\n+    TF_LITE_ENSURE_OK(\n+        context, GetTemporarySafe(context, node, /*index=*/2, &accum_scratch));\n+    TfLiteTensor* input_offsets;\n+    TF_LITE_ENSURE_OK(\n+        context, GetTemporarySafe(context, node, /*index=*/3, &input_offsets));\n+    TfLiteTensor* row_sums;\n+    TF_LITE_ENSURE_OK(context,\n+                      GetTemporarySafe(context, node, /*index=*/4, &row_sums));\n     return EvalHybrid(context, node, params, data, input, filter, bias,\n                       input_quantized, scaling_factors, accum_scratch, row_sums,\n                       input_offsets, output);\n@@ -693,13 +721,18 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n       reinterpret_cast<TfLiteFullyConnectedParams*>(node->builtin_data);\n   OpData* data = reinterpret_cast<OpData*>(node->user_data);\n \n-  const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n-  const TfLiteTensor* filter = GetInput(context, node, kWeightsTensor);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n+  const TfLiteTensor* filter;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kWeightsTensor, &filter));\n   const TfLiteTensor* bias =\n       (node->inputs->size == 3)\n           ? GetOptionalInputTensor(context, node, kBiasTensor)\n           : nullptr;\n-  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetOutputSafe(context, node, kOutputTensor, &output));\n \n   switch (filter->type) {\n     case kTfLiteFloat32:\n@@ -708,8 +741,10 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n     case kTfLiteUInt8:\n       if (params->weights_format ==\n           kTfLiteFullyConnectedWeightsFormatShuffled4x16Int8) {\n-        TfLiteTensor* shuffled_input_workspace =\n-            GetOutput(context, node, kShuffledInputWorkspaceTensor);\n+        TfLiteTensor* shuffled_input_workspace;\n+        TF_LITE_ENSURE_OK(\n+            context, GetOutputSafe(context, node, kShuffledInputWorkspaceTensor,\n+                                   &shuffled_input_workspace));\n         return EvalShuffledQuantized<kernel_type>(context, node, params, data,\n                                                   input, filter, bias, output,\n                                                   shuffled_input_workspace);"