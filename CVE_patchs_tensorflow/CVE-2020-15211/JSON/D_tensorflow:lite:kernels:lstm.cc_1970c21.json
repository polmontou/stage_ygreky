"diff --git a/tensorflow/lite/kernels/lstm.cc b/tensorflow/lite/kernels/lstm.cc\nindex 6d67f759ce8..3eb26565bc2 100644\n--- a/tensorflow/lite/kernels/lstm.cc\n+++ b/tensorflow/lite/kernels/lstm.cc\n@@ -149,7 +149,9 @@ TfLiteStatus PopulateQuantizedLstmParams8x8_16(\n   const TfLiteTensor* cell_state =\n       GetVariableInput(context, node, kCellStateTensor);\n   TF_LITE_ENSURE(context, cell_state != nullptr);\n-  const TfLiteTensor* output_tensor = GetOutput(context, node, kOutputTensor);\n+  TfLiteTensor* output_tensor;\n+  TF_LITE_ENSURE_OK(\n+      context, GetOutputSafe(context, node, kOutputTensor, &output_tensor));\n \n   auto* cell_state_params =\n       static_cast<TfLiteAffineQuantization*>(cell_state->quantization.params);\n@@ -173,25 +175,38 @@ TfLiteStatus PopulateQuantizedLstmParams8x8_16(\n   OpData* op_data = static_cast<OpData*>(node->user_data);\n   const bool use_layer_norm = op_data->use_layer_norm;\n \n-  const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n \n   const TfLiteTensor* input_to_input_weights =\n       GetOptionalInputTensor(context, node, kInputToInputWeightsTensor);\n-  const TfLiteTensor* input_to_forget_weights =\n-      GetInput(context, node, kInputToForgetWeightsTensor);\n-  const TfLiteTensor* input_to_cell_weights =\n-      GetInput(context, node, kInputToCellWeightsTensor);\n-  const TfLiteTensor* input_to_output_weights =\n-      GetInput(context, node, kInputToOutputWeightsTensor);\n+  const TfLiteTensor* input_to_forget_weights;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kInputToForgetWeightsTensor,\n+                                 &input_to_forget_weights));\n+  const TfLiteTensor* input_to_cell_weights;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kInputToCellWeightsTensor,\n+                                 &input_to_cell_weights));\n+  const TfLiteTensor* input_to_output_weights;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kInputToOutputWeightsTensor,\n+                                 &input_to_output_weights));\n \n   const TfLiteTensor* recurrent_to_input_weights =\n       GetOptionalInputTensor(context, node, kRecurrentToInputWeightsTensor);\n-  const TfLiteTensor* recurrent_to_forget_weights =\n-      GetInput(context, node, kRecurrentToForgetWeightsTensor);\n-  const TfLiteTensor* recurrent_to_cell_weights =\n-      GetInput(context, node, kRecurrentToCellWeightsTensor);\n-  const TfLiteTensor* recurrent_to_output_weights =\n-      GetInput(context, node, kRecurrentToOutputWeightsTensor);\n+  const TfLiteTensor* recurrent_to_forget_weights;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kRecurrentToForgetWeightsTensor,\n+                                 &recurrent_to_forget_weights));\n+  const TfLiteTensor* recurrent_to_cell_weights;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kRecurrentToCellWeightsTensor,\n+                                 &recurrent_to_cell_weights));\n+  const TfLiteTensor* recurrent_to_output_weights;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kRecurrentToOutputWeightsTensor,\n+                                 &recurrent_to_output_weights));\n \n   const TfLiteTensor* cell_to_input_weights =\n       GetOptionalInputTensor(context, node, kCellToInputWeightsTensor);\n@@ -227,7 +242,9 @@ TfLiteStatus PopulateQuantizedLstmParams8x8_16(\n   std::vector<int32> intermediate_zp;\n   for (int i = 0; i < 4; ++i) {\n     if (use_layer_norm) {\n-      const TfLiteTensor* intermediate = GetIntermediates(context, node, i);\n+      TfLiteTensor* intermediate;\n+      TF_LITE_ENSURE_OK(context,\n+                        GetIntermediatesSafe(context, node, i, &intermediate));\n       auto* params = static_cast<TfLiteAffineQuantization*>(\n           intermediate->quantization.params);\n       intermediate_scale.push_back(params->scale->data[0]);\n@@ -240,7 +257,8 @@ TfLiteStatus PopulateQuantizedLstmParams8x8_16(\n   }\n   // In the absense of projection, hidden becomes otuput and this intermediate\n   // is ignored.\n-  const TfLiteTensor* hidden = GetIntermediates(context, node, 4);\n+  TfLiteTensor* hidden;\n+  TF_LITE_ENSURE_OK(context, GetIntermediatesSafe(context, node, 4, &hidden));\n   auto* hidden_params =\n       static_cast<TfLiteAffineQuantization*>(hidden->quantization.params);\n   intermediate_scale.push_back(hidden_params->scale->data[0]);\n@@ -446,24 +464,37 @@ TfLiteStatus PopulateQuantizedLstmParams8x8_8(\n     TfLiteContext* context, TfLiteNode* node,\n     lstm_eval::IntegerLstmParameter* integer_lstm_param) {\n   // Get all tensors.\n-  const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n   const TfLiteTensor* input_to_input_weights =\n       GetOptionalInputTensor(context, node, kInputToInputWeightsTensor);\n-  const TfLiteTensor* input_to_forget_weights =\n-      GetInput(context, node, kInputToForgetWeightsTensor);\n-  const TfLiteTensor* input_to_cell_weights =\n-      GetInput(context, node, kInputToCellWeightsTensor);\n-  const TfLiteTensor* input_to_output_weights =\n-      GetInput(context, node, kInputToOutputWeightsTensor);\n+  const TfLiteTensor* input_to_forget_weights;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kInputToForgetWeightsTensor,\n+                                 &input_to_forget_weights));\n+  const TfLiteTensor* input_to_cell_weights;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kInputToCellWeightsTensor,\n+                                 &input_to_cell_weights));\n+  const TfLiteTensor* input_to_output_weights;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kInputToOutputWeightsTensor,\n+                                 &input_to_output_weights));\n \n   const TfLiteTensor* recurrent_to_input_weights =\n       GetOptionalInputTensor(context, node, kRecurrentToInputWeightsTensor);\n-  const TfLiteTensor* recurrent_to_forget_weights =\n-      GetInput(context, node, kRecurrentToForgetWeightsTensor);\n-  const TfLiteTensor* recurrent_to_cell_weights =\n-      GetInput(context, node, kRecurrentToCellWeightsTensor);\n-  const TfLiteTensor* recurrent_to_output_weights =\n-      GetInput(context, node, kRecurrentToOutputWeightsTensor);\n+  const TfLiteTensor* recurrent_to_forget_weights;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kRecurrentToForgetWeightsTensor,\n+                                 &recurrent_to_forget_weights));\n+  const TfLiteTensor* recurrent_to_cell_weights;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kRecurrentToCellWeightsTensor,\n+                                 &recurrent_to_cell_weights));\n+  const TfLiteTensor* recurrent_to_output_weights;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kRecurrentToOutputWeightsTensor,\n+                                 &recurrent_to_output_weights));\n \n   const TfLiteTensor* cell_to_input_weights =\n       GetOptionalInputTensor(context, node, kCellToInputWeightsTensor);\n@@ -483,12 +514,15 @@ TfLiteStatus PopulateQuantizedLstmParams8x8_8(\n \n   const TfLiteTensor* input_gate_bias =\n       GetOptionalInputTensor(context, node, kInputGateBiasTensor);\n-  const TfLiteTensor* forget_gate_bias =\n-      GetInput(context, node, kForgetGateBiasTensor);\n-  const TfLiteTensor* cell_gate_bias =\n-      GetInput(context, node, kCellGateBiasTensor);\n-  const TfLiteTensor* output_gate_bias =\n-      GetInput(context, node, kOutputGateBiasTensor);\n+  const TfLiteTensor* forget_gate_bias;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kForgetGateBiasTensor,\n+                                          &forget_gate_bias));\n+  const TfLiteTensor* cell_gate_bias;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kCellGateBiasTensor,\n+                                          &cell_gate_bias));\n+  const TfLiteTensor* output_gate_bias;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kOutputGateBiasTensor,\n+                                          &output_gate_bias));\n \n   const TfLiteTensor* projection_weights =\n       GetOptionalInputTensor(context, node, kProjectionWeightsTensor);\n@@ -774,7 +808,9 @@ TfLiteStatus PopulateQuantizedLstmParams8x8_8(\n   const float cell_clip = params->cell_clip;\n   const float proj_clip = params->proj_clip;\n \n-  const TfLiteTensor* output_tensor = GetOutput(context, node, kOutputTensor);\n+  TfLiteTensor* output_tensor;\n+  TF_LITE_ENSURE_OK(\n+      context, GetOutputSafe(context, node, kOutputTensor, &output_tensor));\n \n   auto* cell_state_params = reinterpret_cast<TfLiteAffineQuantization*>(\n       cell_state->quantization.params);\n@@ -825,8 +861,10 @@ TfLiteStatus CheckInputTensorDimensions(TfLiteContext* context,\n   TF_LITE_ENSURE(context, params->cell_clip >= 0);\n   TF_LITE_ENSURE(context, params->proj_clip >= 0);\n \n-  const TfLiteTensor* input_to_forget_weights =\n-      GetInput(context, node, kInputToForgetWeightsTensor);\n+  const TfLiteTensor* input_to_forget_weights;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kInputToForgetWeightsTensor,\n+                                 &input_to_forget_weights));\n   TF_LITE_ENSURE_EQ(context, input_to_forget_weights->dims->size, 2);\n   TF_LITE_ENSURE_EQ(context, input_to_forget_weights->dims->data[0], n_cell);\n   TF_LITE_ENSURE_EQ(context, input_to_forget_weights->dims->data[1], n_input);\n@@ -845,8 +883,10 @@ TfLiteStatus CheckInputTensorDimensions(TfLiteContext* context,\n                             input_to_forget_weights->type);\n   }\n \n-  const TfLiteTensor* input_to_cell_weights =\n-      GetInput(context, node, kInputToCellWeightsTensor);\n+  const TfLiteTensor* input_to_cell_weights;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kInputToCellWeightsTensor,\n+                                 &input_to_cell_weights));\n   TF_LITE_ENSURE_EQ(context, input_to_cell_weights->dims->size, 2);\n   TF_LITE_ENSURE_EQ(context, input_to_cell_weights->dims->data[0], n_cell);\n   TF_LITE_ENSURE_EQ(context, input_to_cell_weights->dims->data[1], n_input);\n@@ -865,8 +905,10 @@ TfLiteStatus CheckInputTensorDimensions(TfLiteContext* context,\n                             input_to_forget_weights->type);\n   }\n \n-  const TfLiteTensor* recurrent_to_forget_weights =\n-      GetInput(context, node, kRecurrentToForgetWeightsTensor);\n+  const TfLiteTensor* recurrent_to_forget_weights;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kRecurrentToForgetWeightsTensor,\n+                                 &recurrent_to_forget_weights));\n   TF_LITE_ENSURE_EQ(context, recurrent_to_forget_weights->dims->size, 2);\n   TF_LITE_ENSURE_EQ(context, recurrent_to_forget_weights->dims->data[0],\n                     n_cell);\n@@ -875,8 +917,10 @@ TfLiteStatus CheckInputTensorDimensions(TfLiteContext* context,\n   TF_LITE_ENSURE_TYPES_EQ(context, recurrent_to_forget_weights->type,\n                           input_to_forget_weights->type);\n \n-  const TfLiteTensor* recurrent_to_cell_weights =\n-      GetInput(context, node, kRecurrentToCellWeightsTensor);\n+  const TfLiteTensor* recurrent_to_cell_weights;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kRecurrentToCellWeightsTensor,\n+                                 &recurrent_to_cell_weights));\n   TF_LITE_ENSURE_EQ(context, recurrent_to_cell_weights->dims->size, 2);\n   TF_LITE_ENSURE_EQ(context, recurrent_to_cell_weights->dims->data[0], n_cell);\n   TF_LITE_ENSURE_EQ(context, recurrent_to_cell_weights->dims->data[1],\n@@ -948,8 +992,9 @@ TfLiteStatus CheckInputTensorDimensions(TfLiteContext* context,\n     }\n   }\n \n-  const TfLiteTensor* forget_gate_bias =\n-      GetInput(context, node, kForgetGateBiasTensor);\n+  const TfLiteTensor* forget_gate_bias;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kForgetGateBiasTensor,\n+                                          &forget_gate_bias));\n   TF_LITE_ENSURE_EQ(context, forget_gate_bias->dims->size, 1);\n   TF_LITE_ENSURE_EQ(context, forget_gate_bias->dims->data[0], n_cell);\n   if (is_integer) {\n@@ -958,8 +1003,9 @@ TfLiteStatus CheckInputTensorDimensions(TfLiteContext* context,\n     TF_LITE_ENSURE_TYPES_EQ(context, forget_gate_bias->type, kTfLiteFloat32);\n   }\n \n-  const TfLiteTensor* cell_gate_bias =\n-      GetInput(context, node, kCellGateBiasTensor);\n+  const TfLiteTensor* cell_gate_bias;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kCellGateBiasTensor,\n+                                          &cell_gate_bias));\n   TF_LITE_ENSURE_EQ(context, cell_gate_bias->dims->size, 1);\n   TF_LITE_ENSURE_EQ(context, cell_gate_bias->dims->data[0], n_cell);\n   if (is_integer) {\n@@ -968,8 +1014,9 @@ TfLiteStatus CheckInputTensorDimensions(TfLiteContext* context,\n     TF_LITE_ENSURE_TYPES_EQ(context, cell_gate_bias->type, kTfLiteFloat32);\n   }\n \n-  const TfLiteTensor* output_gate_bias =\n-      GetInput(context, node, kOutputGateBiasTensor);\n+  const TfLiteTensor* output_gate_bias;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kOutputGateBiasTensor,\n+                                          &output_gate_bias));\n   TF_LITE_ENSURE_EQ(context, output_gate_bias->dims->size, 1);\n   TF_LITE_ENSURE_EQ(context, output_gate_bias->dims->data[0], n_cell);\n   if (is_integer) {\n@@ -1105,7 +1152,8 @@ TfLiteStatus PrecomputeZeroPointTimesWeightWithBias(\n TfLiteStatus PopulatePrecomputedZPTimesWeightsWithBias(TfLiteContext* context,\n                                                        OpData* op_data,\n                                                        TfLiteNode* node) {\n-  const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n   const TfLiteTensor* output_state =\n       GetVariableInput(context, node, kOutputStateTensor);\n   TF_LITE_ENSURE(context, output_state != nullptr);\n@@ -1115,21 +1163,33 @@ TfLiteStatus PopulatePrecomputedZPTimesWeightsWithBias(TfLiteContext* context,\n \n   const TfLiteTensor* input_to_input_weights =\n       GetOptionalInputTensor(context, node, kInputToInputWeightsTensor);\n-  const TfLiteTensor* input_to_forget_weights =\n-      GetInput(context, node, kInputToForgetWeightsTensor);\n-  const TfLiteTensor* input_to_cell_weights =\n-      GetInput(context, node, kInputToCellWeightsTensor);\n-  const TfLiteTensor* input_to_output_weights =\n-      GetInput(context, node, kInputToOutputWeightsTensor);\n+  const TfLiteTensor* input_to_forget_weights;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kInputToForgetWeightsTensor,\n+                                 &input_to_forget_weights));\n+  const TfLiteTensor* input_to_cell_weights;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kInputToCellWeightsTensor,\n+                                 &input_to_cell_weights));\n+  const TfLiteTensor* input_to_output_weights;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kInputToOutputWeightsTensor,\n+                                 &input_to_output_weights));\n \n   const TfLiteTensor* recurrent_to_input_weights =\n       GetOptionalInputTensor(context, node, kRecurrentToInputWeightsTensor);\n-  const TfLiteTensor* recurrent_to_forget_weights =\n-      GetInput(context, node, kRecurrentToForgetWeightsTensor);\n-  const TfLiteTensor* recurrent_to_cell_weights =\n-      GetInput(context, node, kRecurrentToCellWeightsTensor);\n-  const TfLiteTensor* recurrent_to_output_weights =\n-      GetInput(context, node, kRecurrentToOutputWeightsTensor);\n+  const TfLiteTensor* recurrent_to_forget_weights;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kRecurrentToForgetWeightsTensor,\n+                                 &recurrent_to_forget_weights));\n+  const TfLiteTensor* recurrent_to_cell_weights;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kRecurrentToCellWeightsTensor,\n+                                 &recurrent_to_cell_weights));\n+  const TfLiteTensor* recurrent_to_output_weights;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kRecurrentToOutputWeightsTensor,\n+                                 &recurrent_to_output_weights));\n \n   const TfLiteTensor* projection_weights =\n       GetOptionalInputTensor(context, node, kProjectionWeightsTensor);\n@@ -1254,20 +1314,25 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n \n   // Inferring batch size, number of outputs and number of cells from the\n   // input tensors.\n-  const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n   const bool is_integer = input->type == kTfLiteInt8;\n   TF_LITE_ENSURE(context, input->dims->size > 1);\n   const int n_batch = input->dims->data[0];\n   const int n_input = input->dims->data[1];\n \n-  const TfLiteTensor* input_to_output_weights =\n-      GetInput(context, node, kInputToOutputWeightsTensor);\n+  const TfLiteTensor* input_to_output_weights;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kInputToOutputWeightsTensor,\n+                                 &input_to_output_weights));\n   const int n_cell = input_to_output_weights->dims->data[0];\n   TF_LITE_ENSURE_EQ(context, input_to_output_weights->dims->size, 2);\n   TF_LITE_ENSURE_EQ(context, input_to_output_weights->dims->data[1], n_input);\n \n-  const TfLiteTensor* recurrent_to_output_weights =\n-      GetInput(context, node, kRecurrentToOutputWeightsTensor);\n+  const TfLiteTensor* recurrent_to_output_weights;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kRecurrentToOutputWeightsTensor,\n+                                 &recurrent_to_output_weights));\n   TF_LITE_ENSURE_EQ(context, recurrent_to_output_weights->dims->size, 2);\n   TF_LITE_ENSURE_EQ(context, recurrent_to_output_weights->dims->data[0],\n                     n_cell);\n@@ -1279,7 +1344,9 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n                                           n_cell, use_layer_norm, is_integer));\n \n   // Get the pointer to output, output_state and cell_state tensors.\n-  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetOutputSafe(context, node, kOutputTensor, &output));\n \n   TfLiteTensor* output_state =\n       GetVariableInput(context, node, kOutputStateTensor);\n@@ -1339,7 +1406,9 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   if (!is_integer) {\n     node->temporaries->data[kScratchBuffer] =\n         op_data->scratch_tensor_index + kScratchBuffer;\n-    TfLiteTensor* scratch_buffer = GetTemporary(context, node, kScratchBuffer);\n+    TfLiteTensor* scratch_buffer;\n+    TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, kScratchBuffer,\n+                                                &scratch_buffer));\n     scratch_buffer->type = input->type;\n     scratch_buffer->allocation_type = kTfLiteArenaRw;\n \n@@ -1367,8 +1436,9 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n     // output_state and cell_state tensors.\n     node->temporaries->data[kInputQuantized] =\n         op_data->scratch_tensor_index + kInputQuantized;\n-    TfLiteTensor* input_quantized =\n-        GetTemporary(context, node, kInputQuantized);\n+    TfLiteTensor* input_quantized;\n+    TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, kInputQuantized,\n+                                                &input_quantized));\n     input_quantized->type = input_to_output_weights->type;\n     input_quantized->allocation_type = kTfLiteArenaRw;\n     if (!TfLiteIntArrayEqual(input_quantized->dims, input->dims)) {\n@@ -1378,8 +1448,10 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n     }\n     node->temporaries->data[kOutputStateQuantized] =\n         op_data->scratch_tensor_index + kOutputStateQuantized;\n-    TfLiteTensor* output_state_quantized =\n-        GetTemporary(context, node, kOutputStateQuantized);\n+    TfLiteTensor* output_state_quantized;\n+    TF_LITE_ENSURE_OK(context,\n+                      GetTemporarySafe(context, node, kOutputStateQuantized,\n+                                       &output_state_quantized));\n     output_state_quantized->type = input_to_output_weights->type;\n     output_state_quantized->allocation_type = kTfLiteArenaRw;\n     if (!TfLiteIntArrayEqual(output_state_quantized->dims,\n@@ -1392,8 +1464,10 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n     }\n     node->temporaries->data[kCellStateQuantized] =\n         op_data->scratch_tensor_index + kCellStateQuantized;\n-    TfLiteTensor* cell_state_quantized =\n-        GetTemporary(context, node, kCellStateQuantized);\n+    TfLiteTensor* cell_state_quantized;\n+    TF_LITE_ENSURE_OK(context,\n+                      GetTemporarySafe(context, node, kCellStateQuantized,\n+                                       &cell_state_quantized));\n     cell_state_quantized->type = input_to_output_weights->type;\n     cell_state_quantized->allocation_type = kTfLiteArenaRw;\n     if (!TfLiteIntArrayEqual(cell_state_quantized->dims, cell_state->dims)) {\n@@ -1410,7 +1484,10 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n     // the scaling factor of the matrix).\n     node->temporaries->data[kInputScalingFactors] =\n         op_data->scratch_tensor_index + kInputScalingFactors;\n-    TfLiteTensor* input_sf = GetTemporary(context, node, kInputScalingFactors);\n+    TfLiteTensor* input_sf;\n+    TF_LITE_ENSURE_OK(\n+        context,\n+        GetTemporarySafe(context, node, kInputScalingFactors, &input_sf));\n     input_sf->type = kTfLiteFloat32;\n     input_sf->allocation_type = kTfLiteArenaRw;\n     int scaling_dims[1] = {n_batch};\n@@ -1422,8 +1499,10 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n     }\n     node->temporaries->data[kOutputStateScalingFactors] =\n         op_data->scratch_tensor_index + kOutputStateScalingFactors;\n-    TfLiteTensor* output_state_sf =\n-        GetTemporary(context, node, kOutputStateScalingFactors);\n+    TfLiteTensor* output_state_sf;\n+    TF_LITE_ENSURE_OK(\n+        context, GetTemporarySafe(context, node, kOutputStateScalingFactors,\n+                                  &output_state_sf));\n     output_state_sf->type = kTfLiteFloat32;\n     output_state_sf->allocation_type = kTfLiteArenaRw;\n     if (!TfLiteIntArrayEqualsArray(output_state_sf->dims, 1, scaling_dims)) {\n@@ -1434,8 +1513,10 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n     }\n     node->temporaries->data[kProductScalingFactors] =\n         op_data->scratch_tensor_index + kProductScalingFactors;\n-    TfLiteTensor* prod_scaling_factors =\n-        GetTemporary(context, node, kProductScalingFactors);\n+    TfLiteTensor* prod_scaling_factors;\n+    TF_LITE_ENSURE_OK(context,\n+                      GetTemporarySafe(context, node, kProductScalingFactors,\n+                                       &prod_scaling_factors));\n     prod_scaling_factors->type = kTfLiteFloat32;\n     prod_scaling_factors->allocation_type = kTfLiteArenaRw;\n     if (!TfLiteIntArrayEqualsArray(prod_scaling_factors->dims, 1,\n@@ -1451,8 +1532,10 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n     // this is used for diagonal matrices, only need to store n_cell values.\n     node->temporaries->data[kRecoveredCellWeights] =\n         op_data->scratch_tensor_index + kRecoveredCellWeights;\n-    TfLiteTensor* recovered_cell_weights =\n-        GetTemporary(context, node, kRecoveredCellWeights);\n+    TfLiteTensor* recovered_cell_weights;\n+    TF_LITE_ENSURE_OK(context,\n+                      GetTemporarySafe(context, node, kRecoveredCellWeights,\n+                                       &recovered_cell_weights));\n     recovered_cell_weights->type = kTfLiteFloat32;\n     recovered_cell_weights->allocation_type = kTfLiteArenaRw;\n     int recovered_cell_dims[1] = {n_cell};\n@@ -1468,7 +1551,9 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n     // multiplication before multiplication by scaling factor\n     node->temporaries->data[kAccumScratch] =\n         op_data->scratch_tensor_index + kAccumScratch;\n-    TfLiteTensor* accum_scratch = GetTemporary(context, node, kAccumScratch);\n+    TfLiteTensor* accum_scratch;\n+    TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, kAccumScratch,\n+                                                &accum_scratch));\n     accum_scratch->type = kTfLiteInt32;\n     accum_scratch->allocation_type = kTfLiteArenaRw;\n     int accum_scratch_dims[2] = {n_cell, n_batch};\n@@ -1482,7 +1567,9 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n     }\n     node->temporaries->data[kInputZeroPoints] =\n         op_data->scratch_tensor_index + kInputZeroPoints;\n-    TfLiteTensor* input_zp = GetTemporary(context, node, kInputZeroPoints);\n+    TfLiteTensor* input_zp;\n+    TF_LITE_ENSURE_OK(\n+        context, GetTemporarySafe(context, node, kInputZeroPoints, &input_zp));\n     input_zp->type = kTfLiteFloat32;\n     input_zp->allocation_type = kTfLiteArenaRw;\n     if (!TfLiteIntArrayEqualsArray(input_zp->dims, 1, scaling_dims)) {\n@@ -1493,8 +1580,10 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n     }\n     node->temporaries->data[kOutputStateZeroPoints] =\n         op_data->scratch_tensor_index + kOutputStateZeroPoints;\n-    TfLiteTensor* output_state_zp =\n-        GetTemporary(context, node, kOutputStateZeroPoints);\n+    TfLiteTensor* output_state_zp;\n+    TF_LITE_ENSURE_OK(context,\n+                      GetTemporarySafe(context, node, kOutputStateZeroPoints,\n+                                       &output_state_zp));\n     output_state_zp->type = kTfLiteFloat32;\n     output_state_zp->allocation_type = kTfLiteArenaRw;\n     if (!TfLiteIntArrayEqualsArray(output_state_zp->dims, 1, scaling_dims)) {\n@@ -1516,7 +1605,9 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n       row_sums_rows += ceil(static_cast<float>(n_output) / n_cell);\n     }\n \n-    TfLiteTensor* row_sums = GetTemporary(context, node, kRowSums);\n+    TfLiteTensor* row_sums;\n+    TF_LITE_ENSURE_OK(context,\n+                      GetTemporarySafe(context, node, kRowSums, &row_sums));\n     row_sums->type = kTfLiteInt32;\n     row_sums->allocation_type = kTfLiteArenaRwPersistent;\n     const int row_sums_dims[2] = {row_sums_rows, n_cell};\n@@ -1664,8 +1755,10 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n       for (int scratch_index = 0; scratch_index < 6; ++scratch_index) {\n         node->temporaries->data[scratch_index] =\n             op_data->scratch_tensor_index + scratch_index;\n-        TfLiteTensor* scratch_tensor =\n-            GetTemporary(context, node, scratch_index);\n+        TfLiteTensor* scratch_tensor;\n+        TF_LITE_ENSURE_OK(\n+            context,\n+            GetTemporarySafe(context, node, scratch_index, &scratch_tensor));\n         scratch_tensor->type = kTfLiteInt16;\n         if (scratch_index == 4) {\n           scratch_tensor->type = kTfLiteInt8;\n@@ -1701,8 +1794,10 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n       for (int scratch_index = 0; scratch_index < 8; ++scratch_index) {\n         node->temporaries->data[scratch_index] =\n             op_data->scratch_tensor_index + scratch_index;\n-        TfLiteTensor* scratch_tensor =\n-            GetTemporary(context, node, scratch_index);\n+        TfLiteTensor* scratch_tensor;\n+        TF_LITE_ENSURE_OK(\n+            context,\n+            GetTemporarySafe(context, node, scratch_index, &scratch_tensor));\n         if (scratch_index == 0 || scratch_index == 1) {\n           scratch_tensor->type = kTfLiteInt8;\n         } else {\n@@ -1731,25 +1826,38 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n   const auto* params = static_cast<TfLiteLSTMParams*>(node->builtin_data);\n   OpData* op_data = static_cast<OpData*>(node->user_data);\n \n-  const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n \n   const TfLiteTensor* input_to_input_weights =\n       GetOptionalInputTensor(context, node, kInputToInputWeightsTensor);\n-  const TfLiteTensor* input_to_forget_weights =\n-      GetInput(context, node, kInputToForgetWeightsTensor);\n-  const TfLiteTensor* input_to_cell_weights =\n-      GetInput(context, node, kInputToCellWeightsTensor);\n-  const TfLiteTensor* input_to_output_weights =\n-      GetInput(context, node, kInputToOutputWeightsTensor);\n+  const TfLiteTensor* input_to_forget_weights;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kInputToForgetWeightsTensor,\n+                                 &input_to_forget_weights));\n+  const TfLiteTensor* input_to_cell_weights;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kInputToCellWeightsTensor,\n+                                 &input_to_cell_weights));\n+  const TfLiteTensor* input_to_output_weights;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kInputToOutputWeightsTensor,\n+                                 &input_to_output_weights));\n \n   const TfLiteTensor* recurrent_to_input_weights =\n       GetOptionalInputTensor(context, node, kRecurrentToInputWeightsTensor);\n-  const TfLiteTensor* recurrent_to_forget_weights =\n-      GetInput(context, node, kRecurrentToForgetWeightsTensor);\n-  const TfLiteTensor* recurrent_to_cell_weights =\n-      GetInput(context, node, kRecurrentToCellWeightsTensor);\n-  const TfLiteTensor* recurrent_to_output_weights =\n-      GetInput(context, node, kRecurrentToOutputWeightsTensor);\n+  const TfLiteTensor* recurrent_to_forget_weights;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kRecurrentToForgetWeightsTensor,\n+                                 &recurrent_to_forget_weights));\n+  const TfLiteTensor* recurrent_to_cell_weights;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kRecurrentToCellWeightsTensor,\n+                                 &recurrent_to_cell_weights));\n+  const TfLiteTensor* recurrent_to_output_weights;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kRecurrentToOutputWeightsTensor,\n+                                 &recurrent_to_output_weights));\n \n   const TfLiteTensor* cell_to_input_weights =\n       GetOptionalInputTensor(context, node, kCellToInputWeightsTensor);\n@@ -1769,12 +1877,15 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n \n   const TfLiteTensor* input_gate_bias =\n       GetOptionalInputTensor(context, node, kInputGateBiasTensor);\n-  const TfLiteTensor* forget_gate_bias =\n-      GetInput(context, node, kForgetGateBiasTensor);\n-  const TfLiteTensor* cell_gate_bias =\n-      GetInput(context, node, kCellGateBiasTensor);\n-  const TfLiteTensor* output_gate_bias =\n-      GetInput(context, node, kOutputGateBiasTensor);\n+  const TfLiteTensor* forget_gate_bias;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kForgetGateBiasTensor,\n+                                          &forget_gate_bias));\n+  const TfLiteTensor* cell_gate_bias;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kCellGateBiasTensor,\n+                                          &cell_gate_bias));\n+  const TfLiteTensor* output_gate_bias;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kOutputGateBiasTensor,\n+                                          &output_gate_bias));\n \n   const TfLiteTensor* projection_weights =\n       GetOptionalInputTensor(context, node, kProjectionWeightsTensor);\n@@ -1783,16 +1894,20 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n \n   TfLiteTensor* output_state =\n       GetVariableInput(context, node, kOutputStateTensor);\n-  TF_LITE_ENSURE(context, output_state != nullptr);\n+  TFLITE_DCHECK(output_state != nullptr);\n   TfLiteTensor* cell_state = GetVariableInput(context, node, kCellStateTensor);\n-  TF_LITE_ENSURE(context, cell_state != nullptr);\n+  TFLITE_DCHECK(cell_state != nullptr);\n \n-  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetOutputSafe(context, node, kOutputTensor, &output));\n \n   switch (input_to_output_weights->type) {\n     case kTfLiteFloat32: {\n       // Index the scratch buffers pointers to the global scratch buffer.\n-      TfLiteTensor* scratch_buffer = GetTemporary(context, node, 0);\n+      TfLiteTensor* scratch_buffer;\n+      TF_LITE_ENSURE_OK(context,\n+                        GetTemporarySafe(context, node, 0, &scratch_buffer));\n       return lstm_eval::EvalFloat(\n           input, input_to_input_weights, input_to_forget_weights,\n           input_to_cell_weights, input_to_output_weights,\n@@ -1818,7 +1933,9 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n       const bool is_hybrid = (input->type == kTfLiteFloat32);\n       const bool is_sparse = input_to_output_weights->sparsity != nullptr;\n       if (is_hybrid) {\n-        TfLiteTensor* row_sums = GetTemporary(context, node, kRowSums);\n+        TfLiteTensor* row_sums;\n+        TF_LITE_ENSURE_OK(context,\n+                          GetTemporarySafe(context, node, kRowSums, &row_sums));\n         const int row_sums_size = row_sums->dims->data[0];\n         if (is_sparse) {\n           TfLiteTensor* input_to_input_weights_ledger =\n@@ -1957,12 +2074,24 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n       } else {\n         const int num_intermediate_tensors = node->intermediates->size;\n         if (num_intermediate_tensors == 5) {\n-          TfLiteTensor* scratch0 = GetTemporary(context, node, 0);\n-          TfLiteTensor* scratch1 = GetTemporary(context, node, 1);\n-          TfLiteTensor* scratch2 = GetTemporary(context, node, 2);\n-          TfLiteTensor* scratch3 = GetTemporary(context, node, 3);\n-          TfLiteTensor* scratch4 = GetTemporary(context, node, 4);\n-          TfLiteTensor* scratch5 = GetTemporary(context, node, 5);\n+          TfLiteTensor* scratch0;\n+          TF_LITE_ENSURE_OK(context,\n+                            GetTemporarySafe(context, node, 0, &scratch0));\n+          TfLiteTensor* scratch1;\n+          TF_LITE_ENSURE_OK(context,\n+                            GetTemporarySafe(context, node, 1, &scratch1));\n+          TfLiteTensor* scratch2;\n+          TF_LITE_ENSURE_OK(context,\n+                            GetTemporarySafe(context, node, 2, &scratch2));\n+          TfLiteTensor* scratch3;\n+          TF_LITE_ENSURE_OK(context,\n+                            GetTemporarySafe(context, node, 3, &scratch3));\n+          TfLiteTensor* scratch4;\n+          TF_LITE_ENSURE_OK(context,\n+                            GetTemporarySafe(context, node, 4, &scratch4));\n+          TfLiteTensor* scratch5;\n+          TF_LITE_ENSURE_OK(context,\n+                            GetTemporarySafe(context, node, 5, &scratch5));\n           return lstm_eval::EvalInteger8x8_16(\n               input, input_to_input_weights, input_to_forget_weights,\n               input_to_cell_weights, input_to_output_weights,\n@@ -1978,14 +2107,30 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n               scratch3, scratch4, scratch5,\n               CpuBackendContext::GetFromContext(context));\n         } else {\n-          TfLiteTensor* scratch0 = GetTemporary(context, node, 0);\n-          TfLiteTensor* scratch1 = GetTemporary(context, node, 1);\n-          TfLiteTensor* scratch2 = GetTemporary(context, node, 2);\n-          TfLiteTensor* scratch3 = GetTemporary(context, node, 3);\n-          TfLiteTensor* scratch4 = GetTemporary(context, node, 4);\n-          TfLiteTensor* scratch5 = GetTemporary(context, node, 5);\n-          TfLiteTensor* scratch6 = GetTemporary(context, node, 6);\n-          TfLiteTensor* scratch7 = GetTemporary(context, node, 7);\n+          TfLiteTensor* scratch0;\n+          TF_LITE_ENSURE_OK(context,\n+                            GetTemporarySafe(context, node, 0, &scratch0));\n+          TfLiteTensor* scratch1;\n+          TF_LITE_ENSURE_OK(context,\n+                            GetTemporarySafe(context, node, 1, &scratch1));\n+          TfLiteTensor* scratch2;\n+          TF_LITE_ENSURE_OK(context,\n+                            GetTemporarySafe(context, node, 2, &scratch2));\n+          TfLiteTensor* scratch3;\n+          TF_LITE_ENSURE_OK(context,\n+                            GetTemporarySafe(context, node, 3, &scratch3));\n+          TfLiteTensor* scratch4;\n+          TF_LITE_ENSURE_OK(context,\n+                            GetTemporarySafe(context, node, 4, &scratch4));\n+          TfLiteTensor* scratch5;\n+          TF_LITE_ENSURE_OK(context,\n+                            GetTemporarySafe(context, node, 5, &scratch5));\n+          TfLiteTensor* scratch6;\n+          TF_LITE_ENSURE_OK(context,\n+                            GetTemporarySafe(context, node, 6, &scratch6));\n+          TfLiteTensor* scratch7;\n+          TF_LITE_ENSURE_OK(context,\n+                            GetTemporarySafe(context, node, 7, &scratch7));\n           return lstm_eval::EvalInteger8x8_8(\n               input, input_to_input_weights, input_to_forget_weights,\n               input_to_cell_weights, input_to_output_weights,\n@@ -2046,12 +2191,19 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   TF_LITE_ENSURE(context, node->inputs->size == kInputNum);\n   TF_LITE_ENSURE(context, node->outputs->size == kOutputNum);\n \n-  const TfLiteTensor* input = GetInput(context, node, kInputData);\n-  const TfLiteTensor* prev_activation =\n-      GetInput(context, node, kInputPrevActivation);\n-  const TfLiteTensor* weights = GetInput(context, node, kInputWeights);\n-  const TfLiteTensor* bias = GetInput(context, node, kInputBiases);\n-  const TfLiteTensor* prev_state = GetInput(context, node, kInputPrevState);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputData, &input));\n+  const TfLiteTensor* prev_activation;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputPrevActivation,\n+                                          &prev_activation));\n+  const TfLiteTensor* weights;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kInputWeights, &weights));\n+  const TfLiteTensor* bias;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputBiases, &bias));\n+  const TfLiteTensor* prev_state;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kInputPrevState, &prev_state));\n \n   TF_LITE_ENSURE_EQ(context, input->dims->size, 2);\n   const int num_batches = input->dims->data[0];\n@@ -2073,11 +2225,18 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   TF_LITE_ENSURE_EQ(context, prev_state->dims->data[0], num_batches);\n   TF_LITE_ENSURE_EQ(context, prev_state->dims->data[1], activation_depth);\n \n-  TfLiteTensor* activation_out = GetOutput(context, node, kOutputActivation);\n-  TfLiteTensor* state_out = GetOutput(context, node, kOutputState);\n-  TfLiteTensor* concat_temp = GetOutput(context, node, kOutputConcatTemp);\n-  TfLiteTensor* activation_temp =\n-      GetOutput(context, node, kOutputActivationTemp);\n+  TfLiteTensor* activation_out;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, kOutputActivation,\n+                                           &activation_out));\n+  TfLiteTensor* state_out;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetOutputSafe(context, node, kOutputState, &state_out));\n+  TfLiteTensor* concat_temp;\n+  TF_LITE_ENSURE_OK(\n+      context, GetOutputSafe(context, node, kOutputConcatTemp, &concat_temp));\n+  TfLiteTensor* activation_temp;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, kOutputActivationTemp,\n+                                           &activation_temp));\n \n   TF_LITE_ENSURE_OK(context, context->ResizeTensor(\n                                  context, activation_out,\n@@ -2106,18 +2265,32 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n }\n \n TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n-  const TfLiteTensor* input = GetInput(context, node, kInputData);\n-  const TfLiteTensor* prev_activation =\n-      GetInput(context, node, kInputPrevActivation);\n-  const TfLiteTensor* weights = GetInput(context, node, kInputWeights);\n-  const TfLiteTensor* bias = GetInput(context, node, kInputBiases);\n-  const TfLiteTensor* prev_state = GetInput(context, node, kInputPrevState);\n-\n-  TfLiteTensor* activation_out = GetOutput(context, node, kOutputActivation);\n-  TfLiteTensor* state_out = GetOutput(context, node, kOutputState);\n-  TfLiteTensor* concat_temp = GetOutput(context, node, kOutputConcatTemp);\n-  TfLiteTensor* activation_temp =\n-      GetOutput(context, node, kOutputActivationTemp);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputData, &input));\n+  const TfLiteTensor* prev_activation;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputPrevActivation,\n+                                          &prev_activation));\n+  const TfLiteTensor* weights;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kInputWeights, &weights));\n+  const TfLiteTensor* bias;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputBiases, &bias));\n+  const TfLiteTensor* prev_state;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kInputPrevState, &prev_state));\n+\n+  TfLiteTensor* activation_out;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, kOutputActivation,\n+                                           &activation_out));\n+  TfLiteTensor* state_out;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetOutputSafe(context, node, kOutputState, &state_out));\n+  TfLiteTensor* concat_temp;\n+  TF_LITE_ENSURE_OK(\n+      context, GetOutputSafe(context, node, kOutputConcatTemp, &concat_temp));\n+  TfLiteTensor* activation_temp;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, kOutputActivationTemp,\n+                                           &activation_temp));\n \n   if (input->type == kTfLiteFloat32 &&\n       prev_activation->type == kTfLiteFloat32 &&"