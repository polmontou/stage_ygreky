"diff --git a/tensorflow/lite/kernels/quantize.cc b/tensorflow/lite/kernels/quantize.cc\nindex 1779500e6a2..8f396355777 100644\n--- a/tensorflow/lite/kernels/quantize.cc\n+++ b/tensorflow/lite/kernels/quantize.cc\n@@ -97,8 +97,10 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);\n   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n \n-  const TfLiteTensor* input = GetInput(context, node, 0);\n-  TfLiteTensor* output = GetOutput(context, node, 0);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n \n   // TODO(b/128934713): Add support for fixed-point per-channel quantization.\n   // Currently this only support affine per-layer quantization.\n@@ -141,8 +143,10 @@ template <KernelType kernel_type>\n TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n   OpData* data = static_cast<OpData*>(node->user_data);\n \n-  const TfLiteTensor* input = GetInput(context, node, 0);\n-  TfLiteTensor* output = GetOutput(context, node, 0);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n \n   const RuntimeShape input_shape = GetTensorShape(input);\n   const RuntimeShape output_shape = GetTensorShape(output);"