"diff --git a/tensorflow/lite/kernels/reshape.cc b/tensorflow/lite/kernels/reshape.cc\nindex ab6f0d8577d..2a21fa730bc 100644\n--- a/tensorflow/lite/kernels/reshape.cc\n+++ b/tensorflow/lite/kernels/reshape.cc\n@@ -38,8 +38,11 @@ TfLiteStatus ResizeOutput(TfLiteContext* context, TfLiteNode* node) {\n   std::unique_ptr<TfLiteIntArray, void (*)(TfLiteIntArray*)>\n       scoped_output_shape(output_shape, TfLiteIntArrayFree);\n \n-  const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n-  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetOutputSafe(context, node, kOutputTensor, &output));\n \n   // Tensorflow's Reshape allows one of the shape components to have the\n   // special -1 value, meaning it will be calculated automatically based on the\n@@ -70,6 +73,7 @@ TfLiteStatus ResizeOutput(TfLiteContext* context, TfLiteNode* node) {\n inline TfLiteIntArray* GetOutputShapeFromTensor(TfLiteContext* context,\n                                                 TfLiteNode* node) {\n   const TfLiteTensor* shape = GetInput(context, node, kShapeTensor);\n+  if (shape == nullptr) return nullptr;\n \n   TfLiteIntArray* output_shape = TfLiteIntArrayCreate(shape->dims->data[0]);\n   for (int i = 0; i < output_shape->size; ++i) {\n@@ -103,7 +107,8 @@ inline TfLiteIntArray* GetOutputShapeFromParam(TfLiteContext* context,\n // Check if the shape tensor is valid. Shapes should be int32 vectors.\n inline bool ShapeIsVector(TfLiteContext* context, TfLiteNode* node) {\n   const TfLiteTensor* shape = GetInput(context, node, kShapeTensor);\n-  return (shape->dims->size == 1 && shape->type == kTfLiteInt32);\n+  return (shape != nullptr && shape->dims->size == 1 &&\n+          shape->type == kTfLiteInt32);\n }\n \n TfLiteIntArray* GetOutputShape(TfLiteContext* context, TfLiteNode* node) {\n@@ -122,7 +127,9 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   // calculate their shapes now. String tensors don't benefit from having their\n   // shapes precalculated because the actual memory can only be allocated after\n   // we know all the content.\n-  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetOutputSafe(context, node, kOutputTensor, &output));\n   if (output->type != kTfLiteString) {\n     if (NumInputs(node) == 1 ||\n         IsConstantTensor(GetInput(context, node, kShapeTensor))) {\n@@ -135,8 +142,11 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n }\n \n TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n-  const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n-  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetOutputSafe(context, node, kOutputTensor, &output));\n \n   // There are two ways in which the 'output' can be made dynamic: it could be\n   // a string tensor, or its shape cannot be calculated during Prepare(). In"