"diff --git a/tensorflow/lite/micro/kernels/concatenation.cc b/tensorflow/lite/micro/kernels/concatenation.cc\nindex 636a7636a7b..8127cc322ee 100644\n--- a/tensorflow/lite/micro/kernels/concatenation.cc\n+++ b/tensorflow/lite/micro/kernels/concatenation.cc\n@@ -136,8 +136,12 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   const TfLiteConcatenationParams* params =\n       reinterpret_cast<TfLiteConcatenationParams*>(node->builtin_data);\n \n-  TfLiteType input_type = GetInput(context, node, 0)->type;\n-  TfLiteType output_type = GetOutput(context, node, kOutputTensor)->type;\n+  const TfLiteTensor* input_tensor = GetInput(context, node, 0);\n+  TF_LITE_ENSURE(context, input_tensor != nullptr);\n+  TfLiteType input_type = input_tensor->type;\n+  const TfLiteTensor* output_tensor = GetOutput(context, node, kOutputTensor);\n+  TF_LITE_ENSURE(context, output_tensor != nullptr);\n+  TfLiteType output_type = output_tensor->type;\n \n   // Check activation and input type\n   TF_LITE_ENSURE_EQ(context, params->activation, kTfLiteActNone);\n@@ -156,6 +160,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   // Shapes with dimensions >4 are not yet supported with static allocation.\n   for (int i = 0; i < num_inputs; ++i) {\n     const TfLiteTensor* input = GetInput(context, node, i);\n+    TF_LITE_ENSURE(context, input != nullptr);\n     int num_dimensions = NumDimensions(input);\n \n     if (num_dimensions > 4) {\n@@ -173,6 +178,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   OpData* data = static_cast<OpData*>(node->user_data);\n \n   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+  TF_LITE_ENSURE(context, output != nullptr);\n \n   switch (output_type) {  // Already know in/outtypes are same.\n     case kTfLiteFloat32:\n@@ -199,6 +205,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n       // Store input scale and zero point values in OpParams:\n       for (int i = 0; i < node->inputs->size; ++i) {\n         const TfLiteTensor* t = GetInput(context, node, i);\n+        TF_LITE_ENSURE(context, t != nullptr);\n         input_scales[i] = t->params.scale;\n         input_zero_points[i] = t->params.zero_point;\n       }\n@@ -220,7 +227,9 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n }\n \n TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n-  TfLiteType output_type = GetOutput(context, node, kOutputTensor)->type;\n+  const TfLiteTensor* output_tensor = GetOutput(context, node, kOutputTensor);\n+  TF_LITE_ENSURE(context, output_tensor != nullptr);\n+  TfLiteType output_type = output_tensor->type;\n \n   switch (output_type) {  // Already know in/outtypes are same.\n     case kTfLiteFloat32:"