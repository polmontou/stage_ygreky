"diff --git a/tensorflow/lite/micro/kernels/conv.cc b/tensorflow/lite/micro/kernels/conv.cc\nindex 6601213fc51..ebeb54c64f6 100644\n--- a/tensorflow/lite/micro/kernels/conv.cc\n+++ b/tensorflow/lite/micro/kernels/conv.cc\n@@ -97,10 +97,13 @@ TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteNode* node,\n   // parameters set. This is usually done during quantized training.\n   if (data_type != kTfLiteFloat32) {\n     const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n+    TF_LITE_ENSURE(context, input != nullptr);\n     const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);\n+    TF_LITE_ENSURE(context, filter != nullptr);\n     const TfLiteTensor* bias =\n         GetOptionalInputTensor(context, node, kBiasTensor);\n     TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+    TF_LITE_ENSURE(context, output != nullptr);\n     int output_channels = filter->dims->data[kConvQuantizedDimension];\n \n     TF_LITE_ENSURE_STATUS(tflite::PopulateConvolutionQuantizationParams(\n@@ -127,8 +130,11 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   const auto params = static_cast<const TfLiteConvParams*>(node->builtin_data);\n \n   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+  TF_LITE_ENSURE(context, output != nullptr);\n   const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n+  TF_LITE_ENSURE(context, input != nullptr);\n   const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);\n+  TF_LITE_ENSURE(context, filter != nullptr);\n \n   int input_width = input->dims->data[2];\n   int input_height = input->dims->data[1];"