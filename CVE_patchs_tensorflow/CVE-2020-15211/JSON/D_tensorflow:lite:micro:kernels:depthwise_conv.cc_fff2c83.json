"diff --git a/tensorflow/lite/micro/kernels/depthwise_conv.cc b/tensorflow/lite/micro/kernels/depthwise_conv.cc\nindex 2f6083d56c1..cfb457c2016 100644\n--- a/tensorflow/lite/micro/kernels/depthwise_conv.cc\n+++ b/tensorflow/lite/micro/kernels/depthwise_conv.cc\n@@ -82,10 +82,13 @@ TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteNode* node,\n   // parameters set. This is usually done during quantized training.\n   if (data_type != kTfLiteFloat32) {\n     const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n+    TF_LITE_ENSURE(context, input != nullptr);\n     const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);\n+    TF_LITE_ENSURE(context, filter != nullptr);\n     const TfLiteTensor* bias =\n         GetOptionalInputTensor(context, node, kBiasTensor);\n     TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+    TF_LITE_ENSURE(context, output != nullptr);\n     int num_channels = filter->dims->data[kDepthwiseConvQuantizedDimension];\n \n     return tflite::PopulateConvolutionQuantizationParams(\n@@ -114,8 +117,11 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   OpData* data = static_cast<OpData*>(node->user_data);\n \n   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+  TF_LITE_ENSURE(context, output != nullptr);\n   const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n+  TF_LITE_ENSURE(context, input != nullptr);\n   const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);\n+  TF_LITE_ENSURE(context, filter != nullptr);\n \n   const TfLiteType data_type = input->type;\n   int width = SizeOfDimension(input, 2);"