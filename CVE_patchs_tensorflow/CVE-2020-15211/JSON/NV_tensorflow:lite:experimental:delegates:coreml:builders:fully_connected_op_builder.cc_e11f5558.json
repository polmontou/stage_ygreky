"/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#include \"tensorflow/lite/experimental/delegates/coreml/builders/fully_connected_op_builder.h\"\n\n#include \"tensorflow/lite/c/builtin_op_data.h\"\n#include \"tensorflow/lite/c/common.h\"\n#include \"tensorflow/lite/experimental/delegates/coreml/builders/activation_layer_builder.h\"\n#include \"tensorflow/lite/experimental/delegates/coreml/builders/op_factory.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_ctypes.h\"\n#include \"tensorflow/lite/kernels/kernel_util.h\"\n\nnamespace tflite {\nnamespace delegates {\nnamespace coreml {\nconst char* FullyConnectedOpBuilder::DebugName() {\n  if (!str_debug_name_[0])\n    GetDebugName(\"FullyConnectedOpBuilder\", node_id_, str_debug_name_);\n  return str_debug_name_;\n}\n\nvoid FullyConnectedOpBuilder::SetWeights(TfLiteTensor* weights) {\n  weights_ = weights;\n}\n\nvoid FullyConnectedOpBuilder::SetBias(TfLiteTensor* bias) { bias_ = bias; }\n\nCoreML::Specification::NeuralNetworkLayer* FullyConnectedOpBuilder::Build() {\n  if (layer_ == nullptr) {\n    layer_.reset(new CoreML::Specification::NeuralNetworkLayer);\n  }\n  layer_->set_name(DebugName());\n\n  FillCoreMLWeights();\n  FillCoreMLBias();\n\n  return layer_.release();\n}\n\nvoid FullyConnectedOpBuilder::FillCoreMLWeights() {\n  layer_->mutable_innerproduct()->set_inputchannels(weights_->dims->data[1]);\n  layer_->mutable_innerproduct()->set_outputchannels(weights_->dims->data[0]);\n  if (weights_->type == kTfLiteFloat32) {\n    const float* weights_data = GetTensorData<float>(weights_);\n    std::copy(weights_data, weights_data + NumElements(weights_),\n              google::protobuf::RepeatedFieldBackInserter(layer_->mutable_innerproduct()\n                                                    ->mutable_weights()\n                                                    ->mutable_floatvalue()));\n  } else if (weights_->type == kTfLiteFloat16) {\n    // float16value has type of bytes (std::string)\n    layer_->mutable_innerproduct()\n        ->mutable_weights()\n        ->mutable_float16value()\n        ->assign(weights_->data.raw, weights_->bytes);\n  }\n}\n\nvoid FullyConnectedOpBuilder::FillCoreMLBias() {\n  if (bias_ != nullptr) {\n    layer_->mutable_innerproduct()->set_hasbias(true);\n    if (bias_->type == kTfLiteFloat32) {\n      const float* bias_data = GetTensorData<float>(bias_);\n      std::copy(bias_data, bias_data + NumElements(bias_),\n                google::protobuf::RepeatedFieldBackInserter(layer_->mutable_innerproduct()\n                                                      ->mutable_bias()\n                                                      ->mutable_floatvalue()));\n    } else if (bias_->type == kTfLiteFloat16) {\n      // float16value has type of bytes (std::string)\n      layer_->mutable_innerproduct()\n          ->mutable_bias()\n          ->mutable_float16value()\n          ->assign(bias_->data.raw, bias_->bytes);\n    }\n  }\n}\n\nTfLiteStatus FullyConnectedOpBuilder::PopulateSubgraph(TfLiteContext* context) {\n  const auto* fc_params =\n      reinterpret_cast<const TfLiteFullyConnectedParams*>(builtin_data_);\n  TfLiteFusedActivation activation = fc_params->activation;\n\n  if (activation == kTfLiteActNone) {\n    builder_output_ = AddOutput();\n  } else {\n    ActivationLayerBuilder* activation_builder =\n        reinterpret_cast<ActivationLayerBuilder*>(\n            graph_builder_->AddBuilder(CreateActivationLayerBuilder, nullptr));\n    activation_builder->SetActivation(activation);\n    activation_builder->AddInput(AddOutput());\n    activation_builder->PopulateSubgraph(context);\n    builder_output_ = activation_builder->GetOutput(context);\n  }\n  return kTfLiteOk;\n}\n\nTfLiteStatus FullyConnectedOpBuilder::RegisterInputs(\n    const TfLiteIntArray* inputs, TfLiteContext* context) {\n  const int kInput = 0;\n  const int kWeights = 1;\n  const int kBias = 2;\n  AddInput(inputs->data[kInput]);\n  SetWeights(&context->tensors[inputs->data[kWeights]]);\n  if (inputs->size > 2) {\n    SetBias(&context->tensors[inputs->data[kBias]]);\n  }\n  return kTfLiteOk;\n}\n\nTfLiteStatus FullyConnectedOpBuilder::RegisterOutputs(\n    const TfLiteIntArray* outputs, TfLiteContext* context) {\n  if (outputs->size != 1) {\n    TF_LITE_KERNEL_LOG(context, \"Wrong # of outputs!.\");\n    return kTfLiteError;\n  }\n  TensorID output_tensor = GetOutput(context);\n  if (output_tensor.NodeID() == -1) {\n    TF_LITE_KERNEL_LOG(context, \"Failed to build output tensor.\");\n    return kTfLiteError;\n  }\n  graph_builder_->AddTensorWithID(outputs->data[0], output_tensor);\n  return kTfLiteOk;\n}\n\nOpBuilder* CreateFullyConnectedOpBuilder(GraphBuilder* graph_builder) {\n  return new FullyConnectedOpBuilder(graph_builder);\n}\n\nbool IsFloatType(TfLiteType type) {\n  return type == kTfLiteFloat32 || type == kTfLiteFloat16;\n}\n\nbool IsFullyConnectedOpSupported(const TfLiteRegistration* registration,\n                                 const TfLiteNode* node,\n                                 TfLiteContext* context) {\n  if (node->builtin_data == nullptr) return false;\n  const auto* fc_params =\n      reinterpret_cast<const TfLiteFullyConnectedParams*>(node->builtin_data);\n  const int kInput = 0;\n  const int kWeights = 1;\n  const int kBias = 2;\n\n  if (fc_params->weights_format != kTfLiteFullyConnectedWeightsFormatDefault) {\n    return false;\n  }\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInput, &input));\n  const TfLiteTensor* weights;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kWeights, &weights));\n\n  if (!IsFloatType(input->type)) {\n    return false;\n  }\n  if (!IsFloatType(weights->type) || !IsConstantTensor(weights)) {\n    return false;\n  }\n  // Core ML 2 only supports single-batch fully connected layer, thus dimensions\n  // except the last one should be 1.\n  if (input->dims->data[input->dims->size - 1] != NumElements(input)) {\n    return false;\n  }\n\n  if (node->inputs->size > 2) {\n    const TfLiteTensor* bias;\n    TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kBias, &bias));\n    if (!IsFloatType(bias->type) || !IsConstantTensor(bias)) {\n      return false;\n    }\n  }\n\n  TfLiteFusedActivation activation = fc_params->activation;\n  if (activation == kTfLiteActSignBit) {\n    return false;\n  }\n  return true;\n}\n\n}  // namespace coreml\n}  // namespace delegates\n}  // namespace tflite"