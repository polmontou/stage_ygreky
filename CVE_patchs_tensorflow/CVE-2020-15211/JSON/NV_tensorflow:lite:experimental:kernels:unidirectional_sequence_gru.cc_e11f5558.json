"/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include <limits>\n\n#include \"tensorflow/lite/c/common.h\"\n#include \"tensorflow/lite/experimental/kernels/gru_cell.h\"\n#include \"tensorflow/lite/kernels/cpu_backend_context.h\"\n#include \"tensorflow/lite/kernels/internal/tensor.h\"\n#include \"tensorflow/lite/kernels/kernel_util.h\"\n\nnamespace tflite {\nnamespace ops {\nnamespace experimental {\nnamespace unidirectional_sequence_gru {\nnamespace {\n\nvoid GruImpl(const TfLiteTensor* input, const TfLiteTensor* input_state,\n             const TfLiteTensor* gate_weight, const TfLiteTensor* gate_bias,\n             const TfLiteTensor* candidate_weight,\n             const TfLiteTensor* candidate_bias, TfLiteTensor* output,\n             TfLiteTensor* output_state, TfLiteTensor* activation,\n             TfLiteTensor* concat,\n             tflite::CpuBackendContext* cpu_backend_context) {\n  const int n_time = input->dims->data[0];\n  const int n_batch = input->dims->data[1];\n  const int n_input = input->dims->data[2];\n  const int n_output = output->dims->data[2];\n  const int n_batch_input = n_batch * n_input;\n  const int n_batch_output = n_batch * n_output;\n  const RuntimeShape input_shape({n_batch, n_input});\n  const float* input_data = GetTensorData<float>(input);\n  const RuntimeShape state_shape = GetTensorShape(input_state);\n  const float* input_state_data = GetTensorData<float>(input_state);\n  const RuntimeShape gate_weight_shape = GetTensorShape(gate_weight);\n  const float* gate_weight_data = GetTensorData<float>(gate_weight);\n  const RuntimeShape gate_bias_shape = GetTensorShape(gate_bias);\n  const float* gate_bias_data = GetTensorData<float>(gate_bias);\n  const RuntimeShape candidate_weight_shape = GetTensorShape(candidate_weight);\n  const float* candidate_weight_data = GetTensorData<float>(candidate_weight);\n  const RuntimeShape candidate_bias_shape = GetTensorShape(candidate_bias);\n  const float* candidate_bias_data = GetTensorData<float>(candidate_bias);\n  const RuntimeShape activation_shape = GetTensorShape(activation);\n  const RuntimeShape output_shape = RuntimeShape({n_batch, n_output});\n  float* output_data = GetTensorData<float>(output);\n  float* output_state_data = GetTensorData<float>(output_state);\n  float* activation_data = GetTensorData<float>(activation);\n  const RuntimeShape concat_shape = GetTensorShape(concat);\n  float* concat_data = GetTensorData<float>(concat);\n  tflite::FullyConnectedParams fc_params;\n  fc_params.float_activation_min = std::numeric_limits<float>::lowest();\n  fc_params.float_activation_max = std::numeric_limits<float>::max();\n  for (int i = 0; i < n_time; ++i) {\n    gru_cell::GruCell(\n        input_shape, input_data, state_shape, input_state_data,\n        gate_weight_shape, gate_weight_data, gate_bias_shape, gate_bias_data,\n        candidate_weight_shape, candidate_weight_data, candidate_bias_shape,\n        candidate_bias_data, output_shape, output_data, output_state_data,\n        activation_shape, activation_data, concat_shape, concat_data, fc_params,\n        cpu_backend_context);\n    input_data += n_batch_input;\n    output_data += n_batch_output;\n    input_state_data = output_state_data;\n  }\n}\n\n}  // namespace\n\nenum InputTensor {\n  // Input tensor of size [n_time, n_batch, n_input]\n  kInput = 0,\n  // Input state tensor of size [n_batch, n_output]\n  kInputState = 1,\n  // Gate weight tensor of size [2*n_output, n_input+n_output]\n  kGateWeight = 2,\n  // Gate bias tensor of size [2*n_output]\n  kGateBias = 3,\n  // Candidate weight tensor of size [n_output, n_input+n_output]\n  kCandidateWeight = 4,\n  // Candidate bias tensor of size [n_output]\n  kCandidateBias = 5,\n  kInputNum = 6\n};\n\nenum OutputTensor {\n  // Input tensor of size [n_time, n_batch, n_output]\n  kOutput = 0,\n  // Output state tensor of size [n_batch, n_output]\n  kOutputState = 1,\n  kOutputNum = 2\n};\n\nenum TemporaryTensor {\n  // Scratch buffer for activation of size [n_batch, 2*n_output]\n  kActivation = 0,\n  // Scratch buffer for activation of size [n_batch, n_input+n_output]\n  kConcat = 1,\n  kTemporaryNum = 2\n};\n\nvoid* Init(TfLiteContext* context, const char* buffer, size_t length) {\n  auto* scratch_tensor_index = new int;\n  context->AddTensors(context, kTemporaryNum, scratch_tensor_index);\n  return scratch_tensor_index;\n}\n\nvoid Free(TfLiteContext* context, void* buffer) {\n  delete reinterpret_cast<int*>(buffer);\n}\n\nTfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n  int* scratch_tensor_index = reinterpret_cast<int*>(node->user_data);\n\n  TF_LITE_ENSURE_EQ(context, node->inputs->size, kInputNum);\n  TF_LITE_ENSURE_EQ(context, node->outputs->size, kOutputNum);\n\n  // input's dim = [n_time, n_batch, n_input]\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInput, &input));\n  TF_LITE_ENSURE_EQ(context, input->dims->size, 3);\n  const int n_time = input->dims->data[0];\n  const int n_batch = input->dims->data[1];\n  const int n_input = input->dims->data[2];\n\n  // input_state's dim = [n_batch, n_output]\n  const TfLiteTensor* input_state;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputState, &input_state));\n  TF_LITE_ENSURE_EQ(context, input_state->dims->size, 2);\n  TF_LITE_ENSURE_EQ(context, input_state->dims->data[0], n_batch);\n  const int n_output = input_state->dims->data[1];\n\n  // gate_weight' dim = [2 * n_output, n_input + n_output]\n  const TfLiteTensor* gate_weight;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kGateWeight, &gate_weight));\n  TF_LITE_ENSURE_EQ(context, gate_weight->dims->size, 2);\n  TF_LITE_ENSURE_EQ(context, gate_weight->dims->data[0], 2 * n_output);\n  TF_LITE_ENSURE_EQ(context, gate_weight->dims->data[1], n_input + n_output);\n\n  // gate_bias' dim = [2 * n_output]\n  const TfLiteTensor* gate_bias;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kGateBias, &gate_bias));\n  TF_LITE_ENSURE_EQ(context, gate_bias->dims->size, 1);\n  TF_LITE_ENSURE_EQ(context, gate_bias->dims->data[0], 2 * n_output);\n\n  // candidate_weight' dim = [n_output, n_input + n_output]\n  const TfLiteTensor* candidate_weight;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kCandidateWeight,\n                                          &candidate_weight));\n  TF_LITE_ENSURE_EQ(context, candidate_weight->dims->size, 2);\n  TF_LITE_ENSURE_EQ(context, candidate_weight->dims->data[0], n_output);\n  TF_LITE_ENSURE_EQ(context, candidate_weight->dims->data[1],\n                    n_input + n_output);\n\n  // candidate_bias' dim = [n_output]\n  const TfLiteTensor* candidate_bias;\n  TF_LITE_ENSURE_OK(\n      context, GetInputSafe(context, node, kCandidateBias, &candidate_bias));\n  TF_LITE_ENSURE_EQ(context, candidate_bias->dims->size, 1);\n  TF_LITE_ENSURE_EQ(context, candidate_bias->dims->data[0], n_output);\n\n  // output's dim = [n_time, n_batch, n_output]\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, kOutput, &output));\n  TfLiteIntArray* output_size = TfLiteIntArrayCreate(3);\n  output_size->data[0] = n_time;\n  output_size->data[1] = n_batch;\n  output_size->data[2] = n_output;\n  TF_LITE_ENSURE_OK(context,\n                    context->ResizeTensor(context, output, output_size));\n\n  // output_state's dim = [n_batch, n_output]\n  TfLiteTensor* output_state;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputState, &output_state));\n  TF_LITE_ENSURE_OK(\n      context, context->ResizeTensor(context, output_state,\n                                     TfLiteIntArrayCopy(input_state->dims)));\n\n  TfLiteIntArrayFree(node->temporaries);\n  node->temporaries = TfLiteIntArrayCreate(kTemporaryNum);\n\n  // activation's dim = [n_batch, 2 * n_output]\n  node->temporaries->data[kActivation] = *scratch_tensor_index;\n  TfLiteTensor* activation;\n  TF_LITE_ENSURE_OK(context,\n                    GetTemporarySafe(context, node, kActivation, &activation));\n  activation->type = input->type;\n  activation->allocation_type = kTfLiteArenaRw;\n  TfLiteIntArray* activation_size = TfLiteIntArrayCreate(2);\n  activation_size->data[0] = n_batch;\n  activation_size->data[1] = 2 * n_output;\n  TF_LITE_ENSURE_OK(\n      context, context->ResizeTensor(context, activation, activation_size));\n\n  // concat's dim  = [n_batch, n_input + n_output]\n  node->temporaries->data[kConcat] = (*scratch_tensor_index) + kConcat;\n  TfLiteTensor* concat;\n  TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, kConcat, &concat));\n  concat->type = input->type;\n  concat->allocation_type = kTfLiteArenaRw;\n  TfLiteIntArray* concat_size = TfLiteIntArrayCreate(2);\n  concat_size->data[0] = n_batch;\n  concat_size->data[1] = n_input + n_output;\n  TF_LITE_ENSURE_OK(context,\n                    context->ResizeTensor(context, concat, concat_size));\n\n  return kTfLiteOk;\n}\n\nTfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInput, &input));\n  const TfLiteTensor* input_state;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputState, &input_state));\n  const TfLiteTensor* gate_weight;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kGateWeight, &gate_weight));\n  const TfLiteTensor* gate_bias;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kGateBias, &gate_bias));\n  const TfLiteTensor* candidate_weight;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kCandidateWeight,\n                                          &candidate_weight));\n  const TfLiteTensor* candidate_bias;\n  TF_LITE_ENSURE_OK(\n      context, GetInputSafe(context, node, kCandidateBias, &candidate_bias));\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, kOutput, &output));\n  TfLiteTensor* output_state;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputState, &output_state));\n  TfLiteTensor* activation;\n  TF_LITE_ENSURE_OK(context,\n                    GetTemporarySafe(context, node, kActivation, &activation));\n  TfLiteTensor* concat;\n  TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, kConcat, &concat));\n  auto cpu_backend_context = CpuBackendContext::GetFromContext(context);\n\n  if (gate_weight->type == kTfLiteFloat32) {\n    GruImpl(input, input_state, gate_weight, gate_bias, candidate_weight,\n            candidate_bias, output, output_state, activation, concat,\n            cpu_backend_context);\n  } else {\n    context->ReportError(context,\n                         \"Unsupported combination of data types for GruCell\");\n    return kTfLiteError;\n  }\n\n  return kTfLiteOk;\n}\n\n}  // namespace unidirectional_sequence_gru\n\nTfLiteRegistration* Register_UNIDIRECTIONAL_SEQUENCE_GRU() {\n  static TfLiteRegistration r = {\n      unidirectional_sequence_gru::Init, unidirectional_sequence_gru::Free,\n      unidirectional_sequence_gru::Prepare, unidirectional_sequence_gru::Eval};\n  return &r;\n}\n\n}  // namespace experimental\n}  // namespace ops\n}  // namespace tflite"