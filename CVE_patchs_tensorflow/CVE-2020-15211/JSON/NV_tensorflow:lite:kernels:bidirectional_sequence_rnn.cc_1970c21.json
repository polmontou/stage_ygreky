"/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#include <algorithm>\n#include <cstddef>\n#include <cstdint>\n\n#include \"tensorflow/lite/c/builtin_op_data.h\"\n#include \"tensorflow/lite/c/common.h\"\n#include \"tensorflow/lite/kernels/internal/kernel_utils.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_ctypes.h\"\n#include \"tensorflow/lite/kernels/kernel_util.h\"\n#include \"tensorflow/lite/kernels/op_macros.h\"\n\nnamespace tflite {\nnamespace ops {\nnamespace builtin {\nnamespace bidirectional_sequence_rnn {\n\nnamespace {\n\nstruct OpData {\n  int scratch_tensor_index;\n  bool fw_compute_row_sums = false;\n  bool bw_compute_row_sums = false;\n};\n\n}  // namespace\n\n// LINT.IfChange\n\nconstexpr int kInputTensor = 0;\n// Forward and backward cell tensors.\nconstexpr int kFwWeightsTensor = 1;\nconstexpr int kFwRecurrentWeightsTensor = 2;\nconstexpr int kFwBiasTensor = 3;\nconstexpr int kFwHiddenStateTensor = 4;\nconstexpr int kBwWeightsTensor = 5;\nconstexpr int kBwRecurrentWeightsTensor = 6;\nconstexpr int kBwBiasTensor = 7;\nconstexpr int kBwHiddenStateTensor = 8;\n// Used as auxiliary input and weights when stacking for\n// tf.contrib.rnn.stack_bidirectional_rnn case (with cross links); Used as input\n// to the backward cell when stacking for tf.nn.static_bidirectional_rnn case\n// (without cross links).\nconstexpr int kAuxInputTensor = 9;       // Optional.\nconstexpr int kFwAuxWeightsTensor = 10;  // Optional.\nconstexpr int kBwAuxWeightsTensor = 11;  // Optional.\n// Output tensors.\nconstexpr int kFwOutputTensor = 0;\nconstexpr int kBwOutputTensor = 1;  // Only if merge_outputs is false.\n\n// LINT.ThenChange(//tensorflow/lite/tools/optimize/quantize_weights.cc)\n\n// Temporary tensors.\nenum TemporaryTensor {\n  kInputQuantized = 0,\n  kFwHiddenStateQuantized = 1,\n  kBwHiddenStateQuantized = 2,\n  kScalingFactors = 3,\n  kAccumScratch = 4,\n  kZeroPoints = 5,\n  kFwRowSums = 6,\n  kBwRowSums = 7,\n  kAuxInputQuantized = 8,\n  kNumTemporaryTensors = 9\n};\n\nvoid* Init(TfLiteContext* context, const char* buffer, size_t length) {\n  auto* op_data = new OpData();\n  context->AddTensors(context, kNumTemporaryTensors,\n                      &op_data->scratch_tensor_index);\n  return op_data;\n}\n\nvoid Free(TfLiteContext* context, void* buffer) {\n  delete reinterpret_cast<OpData*>(buffer);\n}\n\nTfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n  const auto* params = reinterpret_cast<TfLiteBidirectionalSequenceRNNParams*>(\n      node->builtin_data);\n\n  // Check we have all the inputs and outputs we need.\n  TF_LITE_ENSURE_EQ(context, node->inputs->size, 12);\n  TF_LITE_ENSURE_EQ(context, node->outputs->size,\n                    params->merge_outputs ? 1 : 2);\n\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n  const TfLiteTensor* fw_input_weights;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kFwWeightsTensor,\n                                          &fw_input_weights));\n  const TfLiteTensor* fw_recurrent_weights;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kFwRecurrentWeightsTensor,\n                                 &fw_recurrent_weights));\n  const TfLiteTensor* fw_bias;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kFwBiasTensor, &fw_bias));\n  const TfLiteTensor* fw_hidden_state;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kFwHiddenStateTensor,\n                                          &fw_hidden_state));\n  const TfLiteTensor* bw_input_weights;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kBwWeightsTensor,\n                                          &bw_input_weights));\n  const TfLiteTensor* bw_recurrent_weights;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kBwRecurrentWeightsTensor,\n                                 &bw_recurrent_weights));\n  const TfLiteTensor* bw_bias;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kBwBiasTensor, &bw_bias));\n  const TfLiteTensor* bw_hidden_state;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kBwHiddenStateTensor,\n                                          &bw_hidden_state));\n\n  const TfLiteTensor* aux_input =\n      GetOptionalInputTensor(context, node, kAuxInputTensor);\n  const TfLiteTensor* fw_aux_input_weights =\n      GetOptionalInputTensor(context, node, kFwAuxWeightsTensor);\n  const TfLiteTensor* bw_aux_input_weights =\n      GetOptionalInputTensor(context, node, kBwAuxWeightsTensor);\n\n  const bool aux_inputs_weights_or_none =\n      ((fw_aux_input_weights != nullptr) &&\n       (bw_aux_input_weights != nullptr)) ||\n      ((fw_aux_input_weights == nullptr) && (bw_aux_input_weights == nullptr));\n  TF_LITE_ENSURE(context, aux_inputs_weights_or_none);\n  const bool has_aux_input = (fw_aux_input_weights != nullptr);\n\n  // Check all the parameters of tensor match within themselves and match the\n  // input configuration.\n  TF_LITE_ENSURE_TYPES_EQ(context, input->type, kTfLiteFloat32);\n\n  TF_LITE_ENSURE_EQ(context, input->dims->size, 3);\n  const bool time_major = params->time_major;\n  const int batch_size =\n      (time_major) ? input->dims->data[1] : input->dims->data[0];\n  const int max_time =\n      (time_major) ? input->dims->data[0] : input->dims->data[1];\n  const int fw_num_units = fw_input_weights->dims->data[0];\n  const int bw_num_units = bw_input_weights->dims->data[0];\n  TF_LITE_ENSURE_EQ(context, input->dims->data[2],\n                    fw_input_weights->dims->data[1]);\n  TF_LITE_ENSURE_EQ(context, input->dims->data[2],\n                    bw_input_weights->dims->data[1]);\n  TF_LITE_ENSURE_EQ(context, fw_input_weights->dims->data[0],\n                    fw_bias->dims->data[0]);\n  TF_LITE_ENSURE_EQ(context, bw_input_weights->dims->data[0],\n                    bw_bias->dims->data[0]);\n  TF_LITE_ENSURE_EQ(context, fw_recurrent_weights->dims->data[0],\n                    fw_bias->dims->data[0]);\n  TF_LITE_ENSURE_EQ(context, bw_recurrent_weights->dims->data[1],\n                    bw_bias->dims->data[0]);\n  TF_LITE_ENSURE_EQ(context, NumDimensions(fw_hidden_state), 2);\n  TF_LITE_ENSURE_EQ(context, fw_hidden_state->dims->data[0], batch_size);\n  TF_LITE_ENSURE_EQ(context, fw_hidden_state->dims->data[1], fw_num_units);\n  TF_LITE_ENSURE_EQ(context, NumDimensions(bw_hidden_state), 2);\n  TF_LITE_ENSURE_EQ(context, bw_hidden_state->dims->data[0], batch_size);\n  TF_LITE_ENSURE_EQ(context, bw_hidden_state->dims->data[1], bw_num_units);\n\n  if (has_aux_input) {\n    // Check that aux_input has the same dimensions (except last) as the input.\n    TF_LITE_ASSERT_EQ(aux_input->dims->data[0], input->dims->data[0]);\n    TF_LITE_ASSERT_EQ(aux_input->dims->data[1], input->dims->data[1]);\n    // Check that aux_input_weights has the same dimensions (except last) as\n    // the input_weights.\n    TF_LITE_ASSERT_EQ(fw_aux_input_weights->dims->data[0], fw_num_units);\n    TF_LITE_ASSERT_EQ(bw_aux_input_weights->dims->data[0], bw_num_units);\n    TF_LITE_ASSERT_EQ(aux_input->dims->data[2],\n                      fw_aux_input_weights->dims->data[1]);\n    TF_LITE_ASSERT_EQ(aux_input->dims->data[2],\n                      bw_aux_input_weights->dims->data[1]);\n  }\n\n  if (IsHybridOp(input, fw_input_weights)) {\n    OpData* op_data = reinterpret_cast<OpData*>(node->user_data);\n    op_data->fw_compute_row_sums = true;\n    op_data->bw_compute_row_sums = true;\n    TfLiteIntArrayFree(node->temporaries);\n    if (has_aux_input) {\n      node->temporaries = TfLiteIntArrayCreate(kNumTemporaryTensors);\n    } else {\n      // No need to create a temporary tensor for the non-existent aux_input.\n      node->temporaries = TfLiteIntArrayCreate(kNumTemporaryTensors - 1);\n    }\n\n    node->temporaries->data[kInputQuantized] =\n        op_data->scratch_tensor_index + kInputQuantized;\n    TfLiteTensor* input_quantized;\n    TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, kInputQuantized,\n                                                &input_quantized));\n    input_quantized->type = fw_input_weights->type;\n    input_quantized->allocation_type = kTfLiteArenaRw;\n    if (!TfLiteIntArrayEqual(input_quantized->dims, input->dims)) {\n      TfLiteIntArray* input_quantized_size = TfLiteIntArrayCopy(input->dims);\n      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_quantized,\n                                                       input_quantized_size));\n    }\n\n    node->temporaries->data[kFwHiddenStateQuantized] =\n        op_data->scratch_tensor_index + kFwHiddenStateQuantized;\n    TfLiteTensor* fw_hidden_state_quantized;\n    TF_LITE_ENSURE_OK(context,\n                      GetTemporarySafe(context, node, kFwHiddenStateQuantized,\n                                       &fw_hidden_state_quantized));\n    fw_hidden_state_quantized->type = fw_input_weights->type;\n    fw_hidden_state_quantized->allocation_type = kTfLiteArenaRw;\n    if (!TfLiteIntArrayEqual(fw_hidden_state_quantized->dims,\n                             fw_hidden_state->dims)) {\n      TfLiteIntArray* fw_hidden_state_quantized_size =\n          TfLiteIntArrayCopy(fw_hidden_state->dims);\n      TF_LITE_ENSURE_OK(\n          context, context->ResizeTensor(context, fw_hidden_state_quantized,\n                                         fw_hidden_state_quantized_size));\n    }\n\n    node->temporaries->data[kBwHiddenStateQuantized] =\n        op_data->scratch_tensor_index + kBwHiddenStateQuantized;\n    TfLiteTensor* bw_hidden_state_quantized;\n    TF_LITE_ENSURE_OK(context,\n                      GetTemporarySafe(context, node, kBwHiddenStateQuantized,\n                                       &bw_hidden_state_quantized));\n    bw_hidden_state_quantized->type = fw_input_weights->type;\n    bw_hidden_state_quantized->allocation_type = kTfLiteArenaRw;\n    if (!TfLiteIntArrayEqual(bw_hidden_state_quantized->dims,\n                             bw_hidden_state->dims)) {\n      TfLiteIntArray* bw_hidden_state_quantized_size =\n          TfLiteIntArrayCopy(bw_hidden_state->dims);\n      TF_LITE_ENSURE_OK(\n          context, context->ResizeTensor(context, bw_hidden_state_quantized,\n                                         bw_hidden_state_quantized_size));\n    }\n\n    // Allocate temporary tensors to store scaling factors of quantization.\n    node->temporaries->data[kScalingFactors] =\n        op_data->scratch_tensor_index + kScalingFactors;\n    TfLiteTensor* scaling_factors;\n    TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, kScalingFactors,\n                                                &scaling_factors));\n    scaling_factors->type = kTfLiteFloat32;\n    scaling_factors->allocation_type = kTfLiteArenaRw;\n    int scaling_dims[1] = {batch_size};\n    if (!TfLiteIntArrayEqualsArray(scaling_factors->dims, 1, scaling_dims)) {\n      TfLiteIntArray* scaling_factors_size = TfLiteIntArrayCreate(1);\n      scaling_factors_size->data[0] = batch_size;\n      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, scaling_factors,\n                                                       scaling_factors_size));\n    }\n    node->temporaries->data[kAccumScratch] =\n        op_data->scratch_tensor_index + kAccumScratch;\n    TfLiteTensor* accum_scratch;\n    TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, kAccumScratch,\n                                                &accum_scratch));\n    accum_scratch->type = kTfLiteInt32;\n    accum_scratch->allocation_type = kTfLiteArenaRw;\n    int accum_scratch_dims[2] = {std::max(fw_num_units, bw_num_units),\n                                 batch_size};\n    if (!TfLiteIntArrayEqualsArray(accum_scratch->dims, 2,\n                                   accum_scratch_dims)) {\n      TfLiteIntArray* accum_scratch_size = TfLiteIntArrayCreate(2);\n      accum_scratch_size->data[0] = accum_scratch_dims[0];\n      accum_scratch_size->data[1] = accum_scratch_dims[1];\n      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, accum_scratch,\n                                                       accum_scratch_size));\n    }\n    node->temporaries->data[kZeroPoints] =\n        op_data->scratch_tensor_index + kZeroPoints;\n    TfLiteTensor* zero_points;\n    TF_LITE_ENSURE_OK(\n        context,\n        GetTemporarySafe(context, node, /*index=*/kZeroPoints, &zero_points));\n    zero_points->type = kTfLiteInt32;\n    zero_points->allocation_type = kTfLiteArenaRw;\n    int zero_points_dims[1] = {batch_size};\n    if (!TfLiteIntArrayEqualsArray(zero_points->dims, 1, zero_points_dims)) {\n      TfLiteIntArray* zero_points_size = TfLiteIntArrayCreate(1);\n      zero_points_size->data[0] = batch_size;\n      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, zero_points,\n                                                       zero_points_size));\n    }\n    const int num_row_sums = has_aux_input ? 3 : 2;\n    node->temporaries->data[kFwRowSums] =\n        op_data->scratch_tensor_index + kFwRowSums;\n    TfLiteTensor* fw_row_sums;\n    TF_LITE_ENSURE_OK(\n        context,\n        GetTemporarySafe(context, node, /*index=*/kFwRowSums, &fw_row_sums));\n    fw_row_sums->type = kTfLiteInt32;\n    fw_row_sums->allocation_type = kTfLiteArenaRwPersistent;\n    int fw_row_sums_dims[2] = {num_row_sums, fw_num_units};\n    if (!TfLiteIntArrayEqualsArray(fw_row_sums->dims, 2, fw_row_sums_dims)) {\n      TfLiteIntArray* fw_row_sums_size = TfLiteIntArrayCreate(2);\n      fw_row_sums_size->data[0] = fw_row_sums_dims[0];\n      fw_row_sums_size->data[1] = fw_row_sums_dims[1];\n      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, fw_row_sums,\n                                                       fw_row_sums_size));\n    }\n    node->temporaries->data[kBwRowSums] =\n        op_data->scratch_tensor_index + kBwRowSums;\n    TfLiteTensor* bw_row_sums;\n    TF_LITE_ENSURE_OK(\n        context,\n        GetTemporarySafe(context, node, /*index=*/kBwRowSums, &bw_row_sums));\n    bw_row_sums->type = kTfLiteInt32;\n    bw_row_sums->allocation_type = kTfLiteArenaRwPersistent;\n    int bw_row_sums_dims[2] = {num_row_sums, bw_num_units};\n    if (!TfLiteIntArrayEqualsArray(bw_row_sums->dims, 2, bw_row_sums_dims)) {\n      TfLiteIntArray* bw_row_sums_size = TfLiteIntArrayCreate(2);\n      bw_row_sums_size->data[0] = bw_row_sums_dims[0];\n      bw_row_sums_size->data[1] = bw_row_sums_dims[1];\n      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, bw_row_sums,\n                                                       bw_row_sums_size));\n    }\n    if (has_aux_input) {\n      node->temporaries->data[kAuxInputQuantized] =\n          op_data->scratch_tensor_index + kAuxInputQuantized;\n      TfLiteTensor* aux_input_quantized;\n      TF_LITE_ENSURE_OK(context,\n                        GetTemporarySafe(context, node, kAuxInputQuantized,\n                                         &aux_input_quantized));\n      aux_input_quantized->type = fw_input_weights->type;\n      aux_input_quantized->allocation_type = kTfLiteArenaRw;\n      if (!TfLiteIntArrayEqual(aux_input_quantized->dims, aux_input->dims)) {\n        TfLiteIntArray* aux_input_quantized_size =\n            TfLiteIntArrayCopy(aux_input->dims);\n        TF_LITE_ENSURE_OK(context,\n                          context->ResizeTensor(context, aux_input_quantized,\n                                                aux_input_quantized_size));\n      }\n    }\n  }\n\n  // Resize outputs.\n  TfLiteTensor* fw_output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kFwOutputTensor, &fw_output));\n  TfLiteIntArray* fw_output_size_array = TfLiteIntArrayCreate(3);\n  fw_output_size_array->data[0] = (time_major) ? max_time : batch_size;\n  fw_output_size_array->data[1] = (time_major) ? batch_size : max_time;\n  fw_output_size_array->data[2] =\n      params->merge_outputs ? fw_num_units + bw_num_units : fw_num_units;\n  TF_LITE_ENSURE_OK(\n      context, context->ResizeTensor(context, fw_output, fw_output_size_array));\n  if (!params->merge_outputs) {\n    TfLiteTensor* bw_output;\n    TF_LITE_ENSURE_OK(\n        context, GetOutputSafe(context, node, kBwOutputTensor, &bw_output));\n    TfLiteIntArray* bw_output_size_array = TfLiteIntArrayCreate(3);\n    bw_output_size_array->data[0] = batch_size;\n    bw_output_size_array->data[1] = max_time;\n    bw_output_size_array->data[2] = bw_num_units;\n    TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, bw_output,\n                                                     bw_output_size_array));\n  }\n\n  return kTfLiteOk;\n}\n\nTfLiteStatus EvalFloat(const TfLiteTensor* input, const TfLiteTensor* bw_input,\n                       const TfLiteTensor* fw_input_weights,\n                       const TfLiteTensor* fw_recurrent_weights,\n                       const TfLiteTensor* fw_bias,\n                       const TfLiteTensor* bw_input_weights,\n                       const TfLiteTensor* bw_recurrent_weights,\n                       const TfLiteTensor* bw_bias,\n                       const TfLiteTensor* aux_input,\n                       const TfLiteTensor* fw_aux_input_weights,\n                       const TfLiteTensor* bw_aux_input_weights,\n                       const TfLiteBidirectionalSequenceRNNParams* params,\n                       TfLiteTensor* fw_hidden_state, TfLiteTensor* fw_output,\n                       TfLiteTensor* bw_hidden_state, TfLiteTensor* bw_output) {\n  const bool time_major = params->time_major;\n  const int batch_size =\n      (time_major) ? input->dims->data[1] : input->dims->data[0];\n  const int max_time =\n      (time_major) ? input->dims->data[0] : input->dims->data[1];\n  const int input_size = input->dims->data[2];\n  const int aux_input_size = (aux_input) ? aux_input->dims->data[2] : 0;\n\n  const int fw_num_units = fw_input_weights->dims->data[0];\n  const float* fw_bias_ptr = GetTensorData<float>(fw_bias);\n  const float* fw_input_weights_ptr = GetTensorData<float>(fw_input_weights);\n  const float* fw_recurrent_weights_ptr =\n      GetTensorData<float>(fw_recurrent_weights);\n\n  const int bw_num_units = bw_input_weights->dims->data[0];\n  const float* bw_bias_ptr = GetTensorData<float>(bw_bias);\n  const float* bw_input_weights_ptr = GetTensorData<float>(bw_input_weights);\n  const float* bw_recurrent_weights_ptr =\n      GetTensorData<float>(bw_recurrent_weights);\n\n  const float* fw_aux_input_weights_ptr =\n      (fw_aux_input_weights != nullptr)\n          ? GetTensorData<float>(fw_aux_input_weights)\n          : nullptr;\n  const float* bw_aux_input_weights_ptr =\n      (bw_aux_input_weights != nullptr)\n          ? GetTensorData<float>(bw_aux_input_weights)\n          : nullptr;\n\n  const int fw_output_step =\n      params->merge_outputs ? fw_num_units + bw_num_units : fw_num_units;\n  const int bw_output_step =\n      params->merge_outputs ? fw_num_units + bw_num_units : bw_num_units;\n  if (time_major) {\n    // Forward cell.\n    float* fw_hidden_state_ptr_batch = GetTensorData<float>(fw_hidden_state);\n    for (int s = 0; s < max_time; s++) {\n      const float* input_ptr_batch =\n          GetTensorData<float>(input) + s * input_size * batch_size;\n      const float* aux_input_ptr_batch =\n          (aux_input != nullptr)\n              ? GetTensorData<float>(aux_input) + s * input_size * batch_size\n              : nullptr;\n      float* output_ptr_batch =\n          GetTensorData<float>(fw_output) + s * fw_output_step * batch_size;\n\n      kernel_utils::RnnBatchStep(\n          input_ptr_batch, fw_input_weights_ptr, aux_input_ptr_batch,\n          fw_aux_input_weights_ptr, fw_recurrent_weights_ptr, fw_bias_ptr,\n          input_size, aux_input_size, fw_num_units, batch_size, fw_output_step,\n          params->activation, fw_hidden_state_ptr_batch, output_ptr_batch);\n    }\n    // Backward cell.\n    float* bw_hidden_state_ptr_batch = GetTensorData<float>(bw_hidden_state);\n    for (int s = max_time - 1; s >= 0; s--) {\n      const float* input_ptr_batch =\n          GetTensorData<float>(bw_input) + s * input_size * batch_size;\n      const float* aux_input_ptr_batch =\n          (aux_input != nullptr)\n              ? GetTensorData<float>(aux_input) + s * input_size * batch_size\n              : nullptr;\n      float* output_ptr_batch =\n          (params->merge_outputs\n               ? GetTensorData<float>(fw_output) + fw_num_units\n               : GetTensorData<float>(bw_output)) +\n          s * bw_output_step * batch_size;\n\n      kernel_utils::RnnBatchStep(\n          input_ptr_batch, bw_input_weights_ptr, aux_input_ptr_batch,\n          bw_aux_input_weights_ptr, bw_recurrent_weights_ptr, bw_bias_ptr,\n          input_size, aux_input_size, bw_num_units, batch_size, bw_output_step,\n          params->activation, bw_hidden_state_ptr_batch, output_ptr_batch);\n    }\n  } else {\n    for (int b = 0; b < batch_size; b++) {\n      // Forward cell.\n      float* fw_hidden_state_ptr_batch =\n          GetTensorData<float>(fw_hidden_state) + b * fw_num_units;\n      float* fw_output_offset =\n          GetTensorData<float>(fw_output) + b * fw_output_step * max_time;\n      for (int s = 0; s < max_time; s++) {\n        const float* input_ptr_batch = GetTensorData<float>(input) +\n                                       b * input_size * max_time +\n                                       s * input_size;\n        const float* aux_input_ptr_batch =\n            (aux_input != nullptr)\n                ? GetTensorData<float>(aux_input) +\n                      b * aux_input_size * max_time + s * aux_input_size\n                : nullptr;\n        float* output_ptr_batch = fw_output_offset + s * fw_output_step;\n\n        kernel_utils::RnnBatchStep(\n            input_ptr_batch, fw_input_weights_ptr, aux_input_ptr_batch,\n            fw_aux_input_weights_ptr, fw_recurrent_weights_ptr, fw_bias_ptr,\n            input_size, aux_input_size, fw_num_units, /*batch_size=*/1,\n            fw_output_step, params->activation, fw_hidden_state_ptr_batch,\n            output_ptr_batch);\n      }\n      // Backward cell.\n      float* bw_hidden_state_ptr_batch =\n          GetTensorData<float>(bw_hidden_state) + b * bw_num_units;\n      float* bw_output_offset =\n          params->merge_outputs\n              ? GetTensorData<float>(fw_output) +\n                    b * bw_output_step * max_time + fw_num_units\n              : GetTensorData<float>(bw_output) + b * bw_output_step * max_time;\n      for (int s = max_time - 1; s >= 0; s--) {\n        const float* input_ptr_batch = GetTensorData<float>(input) +\n                                       b * input_size * max_time +\n                                       s * input_size;\n        const float* aux_input_ptr_batch =\n            (aux_input != nullptr)\n                ? GetTensorData<float>(aux_input) +\n                      b * aux_input_size * max_time + s * aux_input_size\n                : nullptr;\n        float* output_ptr_batch = bw_output_offset + s * bw_output_step;\n\n        kernel_utils::RnnBatchStep(\n            input_ptr_batch, bw_input_weights_ptr, aux_input_ptr_batch,\n            bw_aux_input_weights_ptr, bw_recurrent_weights_ptr, bw_bias_ptr,\n            input_size, aux_input_size, bw_num_units, /*batch_size=*/1,\n            bw_output_step, params->activation, bw_hidden_state_ptr_batch,\n            output_ptr_batch);\n      }\n    }\n  }\n  return kTfLiteOk;\n}\n\nTfLiteStatus EvalHybrid(\n    const TfLiteTensor* input, const TfLiteTensor* bw_input,\n    const TfLiteTensor* fw_input_weights,\n    const TfLiteTensor* fw_recurrent_weights, const TfLiteTensor* fw_bias,\n    const TfLiteTensor* bw_input_weights,\n    const TfLiteTensor* bw_recurrent_weights, const TfLiteTensor* bw_bias,\n    const TfLiteTensor* aux_input, const TfLiteTensor* aux_fw_input_weights,\n    const TfLiteTensor* aux_bw_input_weights,\n    const TfLiteBidirectionalSequenceRNNParams* params,\n    TfLiteTensor* scaling_factors, TfLiteTensor* input_quantized,\n    TfLiteTensor* aux_input_quantized, TfLiteTensor* fw_hidden_state_quantized,\n    TfLiteTensor* fw_hidden_state, TfLiteTensor* fw_output,\n    TfLiteTensor* bw_hidden_state_quantized, TfLiteTensor* bw_hidden_state,\n    TfLiteTensor* bw_output, TfLiteTensor* zero_points,\n    TfLiteTensor* accum_scratch, TfLiteTensor* fw_row_sums,\n    TfLiteTensor* bw_row_sums, bool* fw_compute_row_sums,\n    bool* bw_compute_row_sums) {\n  const bool time_major = params->time_major;\n  const int batch_size =\n      (time_major) ? input->dims->data[1] : input->dims->data[0];\n  const int max_time =\n      (time_major) ? input->dims->data[0] : input->dims->data[1];\n  const int input_size = input->dims->data[2];\n  const int aux_input_size = (aux_input) ? aux_input->dims->data[2] : 0;\n\n  const int fw_num_units = fw_input_weights->dims->data[0];\n  const float* fw_bias_ptr = GetTensorData<float>(fw_bias);\n  const int8_t* fw_input_weights_ptr = GetTensorData<int8_t>(fw_input_weights);\n  float fw_input_weights_scale = fw_input_weights->params.scale;\n  const int8_t* fw_recurrent_weights_ptr =\n      GetTensorData<int8_t>(fw_recurrent_weights);\n  float fw_recurrent_weights_scale = fw_recurrent_weights->params.scale;\n\n  const int bw_num_units = bw_input_weights->dims->data[0];\n  const float* bw_bias_ptr = GetTensorData<float>(bw_bias);\n  const int8_t* bw_input_weights_ptr = GetTensorData<int8_t>(bw_input_weights);\n  float bw_input_weights_scale = bw_input_weights->params.scale;\n  const int8_t* bw_recurrent_weights_ptr =\n      GetTensorData<int8_t>(bw_recurrent_weights);\n  float bw_recurrent_weights_scale = bw_recurrent_weights->params.scale;\n\n  // Set the auxiliary pointers and scales if needed.\n  const int8_t* aux_fw_input_weights_ptr = nullptr;\n  float aux_fw_input_weights_scale = 0.0f;\n  const int8_t* aux_bw_input_weights_ptr = nullptr;\n  float aux_bw_input_weights_scale = 0.0f;\n  int8_t* aux_quantized_input_ptr = nullptr;\n  if (aux_input_size > 0) {\n    aux_fw_input_weights_ptr = GetTensorData<int8_t>(aux_fw_input_weights);\n    aux_fw_input_weights_scale = aux_fw_input_weights->params.scale;\n    aux_bw_input_weights_ptr = GetTensorData<int8_t>(aux_bw_input_weights);\n    aux_bw_input_weights_scale = aux_bw_input_weights->params.scale;\n    aux_quantized_input_ptr = GetTensorData<int8_t>(aux_input_quantized);\n  }\n\n  // Initialize temporary storage for quantized values.\n  int8_t* quantized_input_ptr = GetTensorData<int8_t>(input_quantized);\n  int8_t* fw_quantized_hidden_state_ptr =\n      GetTensorData<int8_t>(fw_hidden_state_quantized);\n  int8_t* bw_quantized_hidden_state_ptr =\n      GetTensorData<int8_t>(bw_hidden_state_quantized);\n  float* scaling_factors_ptr = GetTensorData<float>(scaling_factors);\n  int32_t* accum_scratch_ptr = GetTensorData<int32_t>(accum_scratch);\n  int32_t* zero_points_ptr = nullptr;\n  int32_t* fw_row_sums_ptr = nullptr;\n  int32_t* bw_row_sums_ptr = nullptr;\n  if (params->asymmetric_quantize_inputs) {\n    zero_points_ptr = GetTensorData<int32_t>(zero_points);\n    fw_row_sums_ptr = GetTensorData<int32_t>(fw_row_sums);\n    bw_row_sums_ptr = GetTensorData<int32_t>(bw_row_sums);\n  }\n  const int fw_output_step =\n      params->merge_outputs ? fw_num_units + bw_num_units : fw_num_units;\n  const int bw_output_step =\n      params->merge_outputs ? fw_num_units + bw_num_units : bw_num_units;\n\n  if (time_major) {\n    for (int t = 0; t < max_time; t++) {\n      // Forward cell.\n      float* fw_hidden_state_ptr_batch = GetTensorData<float>(fw_hidden_state);\n      for (int s = 0; s < max_time; s++) {\n        const float* input_ptr_batch =\n            GetTensorData<float>(input) + s * input_size * batch_size;\n        const float* aux_input_ptr_batch =\n            (aux_input != nullptr)\n                ? GetTensorData<float>(aux_input) + s * input_size * batch_size\n                : nullptr;\n        float* output_ptr_batch =\n            GetTensorData<float>(fw_output) + s * fw_output_step * batch_size;\n\n        kernel_utils::RnnBatchStep(\n            input_ptr_batch, fw_input_weights_ptr, fw_input_weights_scale,\n            aux_input_ptr_batch, aux_fw_input_weights_ptr,\n            aux_fw_input_weights_scale, fw_recurrent_weights_ptr,\n            fw_recurrent_weights_scale, fw_bias_ptr, input_size, aux_input_size,\n            fw_num_units, batch_size, fw_output_step, params->activation,\n            quantized_input_ptr, aux_quantized_input_ptr,\n            fw_quantized_hidden_state_ptr, scaling_factors_ptr,\n            fw_hidden_state_ptr_batch, output_ptr_batch,\n            params->asymmetric_quantize_inputs, zero_points_ptr,\n            accum_scratch_ptr, fw_row_sums_ptr, fw_compute_row_sums);\n      }\n      // Backward cell.\n      float* bw_hidden_state_ptr_batch = GetTensorData<float>(bw_hidden_state);\n      for (int s = max_time - 1; s >= 0; s--) {\n        const float* input_ptr_batch =\n            GetTensorData<float>(bw_input) + s * input_size * batch_size;\n        const float* aux_input_ptr_batch =\n            (aux_input != nullptr)\n                ? GetTensorData<float>(aux_input) + s * input_size * batch_size\n                : nullptr;\n        float* output_ptr_batch =\n            (params->merge_outputs\n                 ? GetTensorData<float>(fw_output) + fw_num_units\n                 : GetTensorData<float>(bw_output)) +\n            s * bw_output_step * batch_size;\n\n        kernel_utils::RnnBatchStep(\n            input_ptr_batch, bw_input_weights_ptr, bw_input_weights_scale,\n            aux_input_ptr_batch, aux_bw_input_weights_ptr,\n            aux_bw_input_weights_scale, bw_recurrent_weights_ptr,\n            bw_recurrent_weights_scale, bw_bias_ptr, input_size, aux_input_size,\n            bw_num_units, batch_size, bw_output_step, params->activation,\n            quantized_input_ptr, aux_quantized_input_ptr,\n            bw_quantized_hidden_state_ptr, scaling_factors_ptr,\n            bw_hidden_state_ptr_batch, output_ptr_batch,\n            params->asymmetric_quantize_inputs, zero_points_ptr,\n            accum_scratch_ptr, bw_row_sums_ptr, bw_compute_row_sums);\n      }\n    }\n  } else {\n    for (int b = 0; b < batch_size; b++) {\n      // Forward cell.\n      float* fw_hidden_state_ptr_batch =\n          GetTensorData<float>(fw_hidden_state) + b * fw_num_units;\n      float* fw_output_offset =\n          GetTensorData<float>(fw_output) + b * fw_output_step * max_time;\n      for (int s = 0; s < max_time; s++) {\n        const float* input_ptr_batch = GetTensorData<float>(input) +\n                                       b * input_size * max_time +\n                                       s * input_size;\n        const float* aux_input_ptr_batch =\n            (aux_input != nullptr)\n                ? GetTensorData<float>(aux_input) + b * input_size * max_time +\n                      s * input_size\n                : nullptr;\n        float* output_ptr_batch = fw_output_offset + s * fw_output_step;\n\n        kernel_utils::RnnBatchStep(\n            input_ptr_batch, fw_input_weights_ptr, fw_input_weights_scale,\n            aux_input_ptr_batch, aux_fw_input_weights_ptr,\n            aux_fw_input_weights_scale, fw_recurrent_weights_ptr,\n            fw_recurrent_weights_scale, fw_bias_ptr, input_size, aux_input_size,\n            fw_num_units, /*batch_size=*/1, fw_output_step, params->activation,\n            quantized_input_ptr, aux_quantized_input_ptr,\n            fw_quantized_hidden_state_ptr, scaling_factors_ptr,\n            fw_hidden_state_ptr_batch, output_ptr_batch,\n            params->asymmetric_quantize_inputs, zero_points_ptr,\n            accum_scratch_ptr, fw_row_sums_ptr, fw_compute_row_sums);\n      }\n      // Backward cell.\n      float* bw_hidden_state_ptr_batch =\n          GetTensorData<float>(bw_hidden_state) + b * bw_num_units;\n      float* bw_output_offset =\n          params->merge_outputs\n              ? GetTensorData<float>(fw_output) +\n                    b * bw_output_step * max_time + fw_num_units\n              : GetTensorData<float>(bw_output) + b * bw_output_step * max_time;\n      for (int s = max_time - 1; s >= 0; s--) {\n        const float* input_ptr_batch = GetTensorData<float>(input) +\n                                       b * input_size * max_time +\n                                       s * input_size;\n        const float* aux_input_ptr_batch =\n            (aux_input != nullptr)\n                ? GetTensorData<float>(aux_input) + b * input_size * max_time +\n                      s * input_size\n                : nullptr;\n        float* output_ptr_batch = bw_output_offset + s * bw_output_step;\n\n        kernel_utils::RnnBatchStep(\n            input_ptr_batch, bw_input_weights_ptr, bw_input_weights_scale,\n            aux_input_ptr_batch, aux_bw_input_weights_ptr,\n            aux_bw_input_weights_scale, bw_recurrent_weights_ptr,\n            bw_recurrent_weights_scale, bw_bias_ptr, input_size, aux_input_size,\n            bw_num_units, /*batch_size=*/1, bw_output_step, params->activation,\n            quantized_input_ptr, aux_quantized_input_ptr,\n            bw_quantized_hidden_state_ptr, scaling_factors_ptr,\n            bw_hidden_state_ptr_batch, output_ptr_batch,\n            params->asymmetric_quantize_inputs, zero_points_ptr,\n            accum_scratch_ptr, bw_row_sums_ptr, bw_compute_row_sums);\n      }\n    }\n  }\n  return kTfLiteOk;\n}\n\nTfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n  const auto* params = reinterpret_cast<TfLiteBidirectionalSequenceRNNParams*>(\n      node->builtin_data);\n\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n  const TfLiteTensor* fw_input_weights;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kFwWeightsTensor,\n                                          &fw_input_weights));\n  const TfLiteTensor* fw_recurrent_weights;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kFwRecurrentWeightsTensor,\n                                 &fw_recurrent_weights));\n  const TfLiteTensor* fw_bias;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kFwBiasTensor, &fw_bias));\n  const TfLiteTensor* bw_input_weights;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kBwWeightsTensor,\n                                          &bw_input_weights));\n  const TfLiteTensor* bw_recurrent_weights;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kBwRecurrentWeightsTensor,\n                                 &bw_recurrent_weights));\n  const TfLiteTensor* bw_bias;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kBwBiasTensor, &bw_bias));\n\n  // Get auxiliary inputs.\n  const TfLiteTensor* aux_input =\n      GetOptionalInputTensor(context, node, kAuxInputTensor);\n  const TfLiteTensor* fw_aux_input_weights =\n      GetOptionalInputTensor(context, node, kFwAuxWeightsTensor);\n  const TfLiteTensor* bw_aux_input_weights =\n      GetOptionalInputTensor(context, node, kBwAuxWeightsTensor);\n\n  TfLiteTensor* fw_hidden_state =\n      GetVariableInput(context, node, kFwHiddenStateTensor);\n  TFLITE_DCHECK(fw_hidden_state != nullptr);\n  TfLiteTensor* bw_hidden_state =\n      GetVariableInput(context, node, kBwHiddenStateTensor);\n  TFLITE_DCHECK(bw_hidden_state != nullptr);\n\n  TfLiteTensor* fw_output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kFwOutputTensor, &fw_output));\n  TfLiteTensor* bw_output = params->merge_outputs\n                                ? nullptr\n                                : GetOutput(context, node, kBwOutputTensor);\n\n  const bool has_previous_bw_output = (aux_input != nullptr);\n  const bool use_aux_input = (fw_aux_input_weights != nullptr);\n\n  // We want to cover the following cases:\n  //\n  // If not stacking (not connected after other bidi lstms):\n  //   both fw & bw will just use `input`; aux_input will be null.\n  //\n  // If stacking with cross_links, TensorFlow equivalent\n  // (tf.contrib.rnn.stack_bidirectional_rnn):\n  //   both fw & bw will use `input`, but aux_input will be none null.\n  //   Note, this time, whether connected after other bidi lstms both works.\n  //\n  // If stacking without cross_links, but connected after other bidi lstms,\n  // TensorFlow equivalent (tf.nn.static_bidirectional_rnn):\n  //   fw will use `input`, bw will use aux_input, and the `real aux_input`\n  //   will be null.\n\n  const bool non_stacking_mode = !use_aux_input && has_previous_bw_output;\n  const TfLiteTensor* bw_input = non_stacking_mode ? aux_input : input;\n  const TfLiteTensor* real_aux_input = non_stacking_mode ? nullptr : aux_input;\n\n  switch (fw_input_weights->type) {\n    case kTfLiteFloat32:\n      return EvalFloat(input, bw_input, fw_input_weights, fw_recurrent_weights,\n                       fw_bias, bw_input_weights, bw_recurrent_weights, bw_bias,\n                       real_aux_input, fw_aux_input_weights,\n                       bw_aux_input_weights, params, fw_hidden_state, fw_output,\n                       bw_hidden_state, bw_output);\n    case kTfLiteUInt8:\n    case kTfLiteInt8: {\n      TfLiteTensor* input_quantized;\n      TF_LITE_ENSURE_OK(\n          context,\n          GetTemporarySafe(context, node, kInputQuantized, &input_quantized));\n      TfLiteTensor* fw_hidden_state_quantized;\n      TF_LITE_ENSURE_OK(context,\n                        GetTemporarySafe(context, node, kFwHiddenStateQuantized,\n                                         &fw_hidden_state_quantized));\n      TfLiteTensor* bw_hidden_state_quantized;\n      TF_LITE_ENSURE_OK(context,\n                        GetTemporarySafe(context, node, kBwHiddenStateQuantized,\n                                         &bw_hidden_state_quantized));\n      TfLiteTensor* scaling_factors;\n      TF_LITE_ENSURE_OK(\n          context,\n          GetTemporarySafe(context, node, kScalingFactors, &scaling_factors));\n      TfLiteTensor* zero_points;\n      TF_LITE_ENSURE_OK(\n          context, GetTemporarySafe(context, node, kZeroPoints, &zero_points));\n      TfLiteTensor* accum_scratch;\n      TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, kAccumScratch,\n                                                  &accum_scratch));\n      TfLiteTensor* fw_row_sums;\n      TF_LITE_ENSURE_OK(\n          context, GetTemporarySafe(context, node, kFwRowSums, &fw_row_sums));\n      TfLiteTensor* bw_row_sums;\n      TF_LITE_ENSURE_OK(\n          context, GetTemporarySafe(context, node, kBwRowSums, &bw_row_sums));\n      TfLiteTensor* aux_input_quantized =\n          use_aux_input ? GetTemporary(context, node, kAuxInputQuantized)\n                        : nullptr;\n      auto* op_data = reinterpret_cast<OpData*>(node->user_data);\n      return EvalHybrid(\n          input, bw_input, fw_input_weights, fw_recurrent_weights, fw_bias,\n          bw_input_weights, bw_recurrent_weights, bw_bias, real_aux_input,\n          fw_aux_input_weights, bw_aux_input_weights, params, scaling_factors,\n          input_quantized, aux_input_quantized, fw_hidden_state_quantized,\n          fw_hidden_state, fw_output, bw_hidden_state_quantized,\n          bw_hidden_state, bw_output, zero_points, accum_scratch, fw_row_sums,\n          bw_row_sums, &op_data->fw_compute_row_sums,\n          &op_data->bw_compute_row_sums);\n    }\n    default:\n      context->ReportError(context, \"Type not currently supported.\");\n      return kTfLiteError;\n  }\n  return kTfLiteOk;\n}\n\n}  // namespace bidirectional_sequence_rnn\n\nTfLiteRegistration* Register_BIDIRECTIONAL_SEQUENCE_RNN() {\n  static TfLiteRegistration r = {\n      bidirectional_sequence_rnn::Init, bidirectional_sequence_rnn::Free,\n      bidirectional_sequence_rnn::Prepare, bidirectional_sequence_rnn::Eval};\n  return &r;\n}\n\n}  // namespace builtin\n}  // namespace ops\n}  // namespace tflite"