"/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/lite/kernels/internal/optimized/integer_ops/depthwise_conv.h\"\n\n#include <stddef.h>\n#include <stdint.h>\n\n#include <vector>\n\n#include \"tensorflow/lite/c/builtin_op_data.h\"\n#include \"tensorflow/lite/c/common.h\"\n#include \"tensorflow/lite/kernels/cpu_backend_context.h\"\n#include \"tensorflow/lite/kernels/internal/compatibility.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/cpu_check.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/integer_ops/depthwise_conv_hybrid.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/neon_check.h\"\n#include \"tensorflow/lite/kernels/internal/quantization_util.h\"\n#include \"tensorflow/lite/kernels/internal/reference/depthwiseconv_float.h\"\n#include \"tensorflow/lite/kernels/internal/reference/depthwiseconv_uint8.h\"\n#include \"tensorflow/lite/kernels/internal/reference/integer_ops/depthwise_conv.h\"\n#include \"tensorflow/lite/kernels/internal/tensor.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_ctypes.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_utils.h\"\n#include \"tensorflow/lite/kernels/internal/types.h\"\n#include \"tensorflow/lite/kernels/kernel_util.h\"\n#include \"tensorflow/lite/kernels/padding.h\"\n\nnamespace tflite {\nnamespace ops {\nnamespace builtin {\nnamespace depthwise_conv {\n\nconstexpr int kInputTensor = 0;\nconstexpr int kFilterTensor = 1;\nconstexpr int kBiasTensor = 2;\nconstexpr int kOutputTensor = 0;\n\n// This file has three implementation of DepthwiseConv.\nenum KernelType {\n  kReference,\n  kGenericOptimized,  // Neon-free\n  kNeonOptimized,\n};\n\nconst int kTensorNotAllocated = -1;\n\nstruct OpData {\n  TfLitePaddingValues padding;\n  // The scaling factor from input to output (aka the 'real multiplier') can\n  // be represented as a fixed point multiplier plus a left shift.\n  int32_t output_multiplier;\n  int output_shift;\n  // The range of the fused activation layer. For example for kNone and\n  // uint8_t these would be 0 and 255.\n  int32_t output_activation_min;\n  int32_t output_activation_max;\n\n  // Per channel output multiplier and shift.\n  std::vector<int32_t> per_channel_output_multiplier;\n  std::vector<int> per_channel_output_shift;\n\n  // Hybrid per channel temporary tensors.\n  int input_quantized_id = kTensorNotAllocated;\n  int scaling_factors_id = kTensorNotAllocated;\n  int input_offset_id = kTensorNotAllocated;\n  int32_t input_quantized_index;\n  int32_t scaling_factors_index;\n  int32_t input_offset_index;\n};\n\nvoid* Init(TfLiteContext* context, const char* buffer, size_t length) {\n  // This is a builtin op, so we don't use the contents in 'buffer', if any.\n  // Instead, we allocate a new object to carry information from Prepare() to\n  // Eval().\n  return new OpData;\n}\n\nvoid Free(TfLiteContext* context, void* buffer) {\n  delete reinterpret_cast<OpData*>(buffer);\n}\n\nTfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n  auto* params =\n      reinterpret_cast<TfLiteDepthwiseConvParams*>(node->builtin_data);\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n\n  // TODO(ahentz): use could use GetOptionalInputTensor() here, but we need to\n  // decide whether we are OK with optional tensors being completely absent, as\n  // opposed to having -1 as their index.\n  bool hasBias = NumInputs(node) == 3;\n\n  TF_LITE_ENSURE(context, hasBias || NumInputs(node) == 2);\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n  const TfLiteTensor* filter;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kFilterTensor, &filter));\n  const TfLiteTensor* bias = nullptr;\n\n  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n\n  TF_LITE_ENSURE_EQ(context, NumDimensions(input), 4);\n  TF_LITE_ENSURE_EQ(context, NumDimensions(filter), 4);\n\n  const TfLiteType data_type = input->type;\n\n  const TfLiteType filter_type = filter->type;\n  const bool is_hybrid =\n      data_type == kTfLiteFloat32 && filter_type == kTfLiteInt8;\n  TF_LITE_ENSURE(context,\n                 data_type == kTfLiteFloat32 || data_type == kTfLiteUInt8 ||\n                     data_type == kTfLiteInt8 || data_type == kTfLiteInt16);\n  TF_LITE_ENSURE_TYPES_EQ(context, output->type, data_type);\n  if (!is_hybrid) {\n    TF_LITE_ENSURE(context,\n                   filter->type == data_type || data_type == kTfLiteInt16);\n  }\n\n  // Filter in DepthwiseConv is expected to be [1, H, W, O].\n  TF_LITE_ENSURE_EQ(context, SizeOfDimension(filter, 0), 1);\n\n  if (hasBias) {\n    TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kBiasTensor, &bias));\n    if (data_type == kTfLiteUInt8 || data_type == kTfLiteInt8) {\n      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, kTfLiteInt32);\n      TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);\n    } else if (data_type == kTfLiteInt16) {\n      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, kTfLiteInt64);\n      TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);\n      TF_LITE_ENSURE_EQ(context, input->params.zero_point, 0);\n      TF_LITE_ENSURE_EQ(context, output->params.zero_point, 0);\n    } else {\n      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, data_type);\n    }\n    TF_LITE_ENSURE_EQ(context, NumDimensions(bias), 1);\n    TF_LITE_ENSURE_EQ(context, SizeOfDimension(filter, 3),\n                      SizeOfDimension(bias, 0));\n  }\n\n  int channels_out = SizeOfDimension(filter, 3);\n  int width = SizeOfDimension(input, 2);\n  int height = SizeOfDimension(input, 1);\n  int filter_width = SizeOfDimension(filter, 2);\n  int filter_height = SizeOfDimension(filter, 1);\n  int batches = SizeOfDimension(input, 0);\n\n  // Matching GetWindowedOutputSize in TensorFlow.\n  auto padding = params->padding;\n  int out_width, out_height;\n\n  data->padding = ComputePaddingHeightWidth(\n      params->stride_height, params->stride_width,\n      params->dilation_height_factor, params->dilation_width_factor, height,\n      width, filter_height, filter_width, padding, &out_height, &out_width);\n\n  // Note that quantized inference requires that all tensors have their\n  // parameters set. This is usually done during quantized training or\n  // calibration.\n  if (data_type != kTfLiteFloat32) {\n    TF_LITE_ENSURE_EQ(context, filter->quantization.type,\n                      kTfLiteAffineQuantization);\n    const auto* affine_quantization =\n        reinterpret_cast<TfLiteAffineQuantization*>(\n            filter->quantization.params);\n    TF_LITE_ENSURE(context, affine_quantization);\n    TF_LITE_ENSURE(context, affine_quantization->scale);\n    TF_LITE_ENSURE(context, (affine_quantization->scale->size == 1 ||\n                             affine_quantization->scale->size == channels_out));\n\n    data->per_channel_output_multiplier.resize(channels_out);\n    data->per_channel_output_shift.resize(channels_out);\n    TF_LITE_ENSURE_STATUS(tflite::PopulateConvolutionQuantizationParams(\n        context, input, filter, bias, output, params->activation,\n        &data->output_multiplier, &data->output_shift,\n        &data->output_activation_min, &data->output_activation_max,\n        data->per_channel_output_multiplier.data(),\n        data->per_channel_output_shift.data(), channels_out));\n  }\n\n  if (is_hybrid) {\n    const auto* affine_quantization =\n        reinterpret_cast<TfLiteAffineQuantization*>(\n            filter->quantization.params);\n    TF_LITE_ENSURE(context, affine_quantization);\n    TF_LITE_ENSURE(context, affine_quantization->scale);\n    TF_LITE_ENSURE_EQ(\n        context, affine_quantization->scale->size,\n        filter->dims->data[affine_quantization->quantized_dimension]);\n\n    int temporaries_count = 0;\n    data->input_quantized_index = temporaries_count;\n    if (data->input_quantized_id == kTensorNotAllocated) {\n      TF_LITE_ENSURE_OK(\n          context, context->AddTensors(context, 1, &data->input_quantized_id));\n    }\n    ++temporaries_count;\n    data->scaling_factors_index = temporaries_count;\n    if (data->scaling_factors_id == kTensorNotAllocated) {\n      TF_LITE_ENSURE_OK(\n          context, context->AddTensors(context, 1, &data->scaling_factors_id));\n    }\n    ++temporaries_count;\n    data->input_offset_index = temporaries_count;\n    if (data->input_offset_id == kTensorNotAllocated) {\n      TF_LITE_ENSURE_OK(\n          context, context->AddTensors(context, 1, &data->input_offset_id));\n    }\n    ++temporaries_count;\n\n    TfLiteIntArrayFree(node->temporaries);\n    node->temporaries = TfLiteIntArrayCreate(temporaries_count);\n\n    node->temporaries->data[data->input_quantized_index] =\n        data->input_quantized_id;\n    TfLiteTensor* input_quantized;\n    TF_LITE_ENSURE_OK(\n        context, GetTemporarySafe(context, node, data->input_quantized_index,\n                                  &input_quantized));\n    input_quantized->type = kTfLiteInt8;\n    input_quantized->allocation_type = kTfLiteArenaRw;\n    if (!TfLiteIntArrayEqual(input_quantized->dims, input->dims)) {\n      TfLiteIntArray* input_quantized_size = TfLiteIntArrayCopy(input->dims);\n      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_quantized,\n                                                       input_quantized_size));\n    }\n    node->temporaries->data[data->scaling_factors_index] =\n        data->scaling_factors_id;\n    TfLiteTensor* scaling_factors;\n    TF_LITE_ENSURE_OK(\n        context, GetTemporarySafe(context, node, data->scaling_factors_index,\n                                  &scaling_factors));\n    scaling_factors->type = kTfLiteFloat32;\n    scaling_factors->allocation_type = kTfLiteArenaRw;\n    const int batch_size = SizeOfDimension(input, 0);\n    int scaling_dims[1] = {batch_size};\n    if (!TfLiteIntArrayEqualsArray(scaling_factors->dims, 1, scaling_dims)) {\n      TfLiteIntArray* scaling_factors_size = TfLiteIntArrayCreate(1);\n      scaling_factors_size->data[0] = batch_size;\n      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, scaling_factors,\n                                                       scaling_factors_size));\n    }\n    node->temporaries->data[data->input_offset_index] = data->input_offset_id;\n    TfLiteTensor* input_offsets;\n    TF_LITE_ENSURE_OK(context,\n                      GetTemporarySafe(context, node, data->input_offset_index,\n                                       &input_offsets));\n    input_offsets->type = kTfLiteInt32;\n    input_offsets->allocation_type = kTfLiteArenaRw;\n    if (!TfLiteIntArrayEqualsArray(input_offsets->dims, 1, scaling_dims)) {\n      TfLiteIntArray* input_offsets_size = TfLiteIntArrayCreate(1);\n      input_offsets_size->data[0] = batch_size;\n      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_offsets,\n                                                       input_offsets_size));\n    }\n  }\n\n  TfLiteIntArray* outputSize = TfLiteIntArrayCreate(4);\n  outputSize->data[0] = batches;\n  outputSize->data[1] = out_height;\n  outputSize->data[2] = out_width;\n  outputSize->data[3] = channels_out;\n  return context->ResizeTensor(context, output, outputSize);\n}\n\nTfLiteStatus ComputeDepthMultiplier(TfLiteContext* context,\n                                    const TfLiteTensor* input,\n                                    const TfLiteTensor* filter,\n                                    int16* depth_multiplier) {\n  int num_filter_channels = SizeOfDimension(filter, 3);\n  int num_input_channels = SizeOfDimension(input, 3);\n  TF_LITE_ENSURE_EQ(context, num_filter_channels % num_input_channels, 0);\n\n  *depth_multiplier = num_filter_channels / num_input_channels;\n  return kTfLiteOk;\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus EvalFloat(TfLiteContext* context, TfLiteNode* node,\n                       TfLiteDepthwiseConvParams* params, OpData* data,\n                       const TfLiteTensor* input, const TfLiteTensor* filter,\n                       const TfLiteTensor* bias, TfLiteTensor* output) {\n  float output_activation_min, output_activation_max;\n  CalculateActivationRange(params->activation, &output_activation_min,\n                           &output_activation_max);\n\n  DepthwiseParams op_params;\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = data->padding.width;\n  op_params.padding_values.height = data->padding.height;\n  op_params.stride_width = params->stride_width;\n  op_params.stride_height = params->stride_height;\n  op_params.dilation_width_factor = params->dilation_width_factor;\n  op_params.dilation_height_factor = params->dilation_height_factor;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n  TF_LITE_ENSURE_STATUS(ComputeDepthMultiplier(context, input, filter,\n                                               &op_params.depth_multiplier));\n  if (kernel_type == kReference) {\n    reference_ops::DepthwiseConv(\n        op_params, GetTensorShape(input), GetTensorData<float>(input),\n        GetTensorShape(filter), GetTensorData<float>(filter),\n        GetTensorShape(bias), GetTensorData<float>(bias),\n        GetTensorShape(output), GetTensorData<float>(output));\n  } else {\n    optimized_ops::DepthwiseConv<float, float>(\n        op_params, GetTensorShape(input), GetTensorData<float>(input),\n        GetTensorShape(filter), GetTensorData<float>(filter),\n        GetTensorShape(bias), GetTensorData<float>(bias),\n        GetTensorShape(output), GetTensorData<float>(output),\n        CpuBackendContext::GetFromContext(context));\n  }\n  return kTfLiteOk;\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus EvalQuantized(TfLiteContext* context, TfLiteNode* node,\n                           TfLiteDepthwiseConvParams* params, OpData* data,\n                           const TfLiteTensor* input,\n                           const TfLiteTensor* filter, const TfLiteTensor* bias,\n                           TfLiteTensor* output) {\n  auto input_offset = -input->params.zero_point;\n  auto filter_offset = -filter->params.zero_point;\n  auto output_offset = output->params.zero_point;\n\n  DepthwiseParams op_params;\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = data->padding.width;\n  op_params.padding_values.height = data->padding.height;\n  op_params.stride_width = params->stride_width;\n  op_params.stride_height = params->stride_height;\n  op_params.dilation_width_factor = params->dilation_width_factor;\n  op_params.dilation_height_factor = params->dilation_height_factor;\n  op_params.input_offset = input_offset;\n  op_params.weights_offset = filter_offset;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = data->output_multiplier;\n  op_params.output_shift = -data->output_shift;\n  op_params.quantized_activation_min = data->output_activation_min;\n  op_params.quantized_activation_max = data->output_activation_max;\n  TF_LITE_ENSURE_STATUS(ComputeDepthMultiplier(context, input, filter,\n                                               &op_params.depth_multiplier));\n  if (kernel_type == kReference) {\n    reference_ops::DepthwiseConv(\n        op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),\n        GetTensorShape(filter), GetTensorData<uint8_t>(filter),\n        GetTensorShape(bias), GetTensorData<int32_t>(bias),\n        GetTensorShape(output), GetTensorData<uint8_t>(output));\n  } else {\n    optimized_ops::DepthwiseConv<uint8, int32>(\n        op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),\n        GetTensorShape(filter), GetTensorData<uint8_t>(filter),\n        GetTensorShape(bias), GetTensorData<int32_t>(bias),\n        GetTensorShape(output), GetTensorData<uint8_t>(output),\n        CpuBackendContext::GetFromContext(context));\n  }\n  return kTfLiteOk;\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus EvalQuantizedPerChannel(TfLiteContext* context, TfLiteNode* node,\n                                     TfLiteDepthwiseConvParams* params,\n                                     OpData* data, const TfLiteTensor* input,\n                                     const TfLiteTensor* filter,\n                                     const TfLiteTensor* bias,\n                                     TfLiteTensor* output) {\n  DepthwiseParams op_params;\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = data->padding.width;\n  op_params.padding_values.height = data->padding.height;\n  op_params.stride_width = params->stride_width;\n  op_params.stride_height = params->stride_height;\n  op_params.dilation_width_factor = params->dilation_width_factor;\n  op_params.dilation_height_factor = params->dilation_height_factor;\n  op_params.input_offset = -input->params.zero_point;\n  op_params.weights_offset = 0;\n  op_params.output_offset = output->params.zero_point;\n  op_params.quantized_activation_min = data->output_activation_min;\n  op_params.quantized_activation_max = data->output_activation_max;\n  TF_LITE_ENSURE_STATUS(ComputeDepthMultiplier(context, input, filter,\n                                               &op_params.depth_multiplier));\n\n  if (kernel_type == kReference) {\n    reference_integer_ops::DepthwiseConvPerChannel(\n        op_params, data->per_channel_output_multiplier.data(),\n        data->per_channel_output_shift.data(), GetTensorShape(input),\n        GetTensorData<int8>(input), GetTensorShape(filter),\n        GetTensorData<int8>(filter), GetTensorShape(bias),\n        GetTensorData<int32>(bias), GetTensorShape(output),\n        GetTensorData<int8>(output));\n  } else {\n    optimized_integer_ops::DepthwiseConvPerChannel(\n        op_params, data->per_channel_output_multiplier.data(),\n        data->per_channel_output_shift.data(), GetTensorShape(input),\n        GetTensorData<int8>(input), GetTensorShape(filter),\n        GetTensorData<int8>(filter), GetTensorShape(bias),\n        GetTensorData<int32>(bias), GetTensorShape(output),\n        GetTensorData<int8>(output),\n        CpuBackendContext::GetFromContext(context));\n  }\n  return kTfLiteOk;\n}\n\nTfLiteStatus EvalQuantizedPerChannel16x8(\n    const TfLiteDepthwiseConvParams* params, const OpData* data,\n    const TfLiteTensor* input, const TfLiteTensor* filter,\n    const TfLiteTensor* bias, TfLiteTensor* output) {\n  DepthwiseParams op_params;\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = data->padding.width;\n  op_params.padding_values.height = data->padding.height;\n  op_params.stride_width = params->stride_width;\n  op_params.stride_height = params->stride_height;\n  op_params.dilation_width_factor = params->dilation_width_factor;\n  op_params.dilation_height_factor = params->dilation_height_factor;\n  op_params.depth_multiplier = params->depth_multiplier;\n  op_params.weights_offset = 0;\n  op_params.quantized_activation_min = data->output_activation_min;\n  op_params.quantized_activation_max = data->output_activation_max;\n\n  reference_integer_ops::DepthwiseConvPerChannel(\n      op_params, data->per_channel_output_multiplier.data(),\n      data->per_channel_output_shift.data(), GetTensorShape(input),\n      GetTensorData<int16>(input), GetTensorShape(filter),\n      GetTensorData<int8>(filter), GetTensorShape(bias),\n      GetTensorData<std::int64_t>(bias), GetTensorShape(output),\n      GetTensorData<int16>(output));\n\n  return kTfLiteOk;\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus EvalHybridPerChannel(TfLiteContext* context, TfLiteNode* node,\n                                  TfLiteDepthwiseConvParams* params,\n                                  OpData* data, const TfLiteTensor* input,\n                                  const TfLiteTensor* filter,\n                                  const TfLiteTensor* bias,\n                                  TfLiteTensor* output) {\n  float output_activation_min, output_activation_max;\n  CalculateActivationRange(params->activation, &output_activation_min,\n                           &output_activation_max);\n  const int input_size = NumElements(input) / SizeOfDimension(input, 0);\n  const int batch_size = SizeOfDimension(input, 0);\n  TfLiteTensor* input_quantized;\n  TF_LITE_ENSURE_OK(context,\n                    GetTemporarySafe(context, node, data->input_quantized_index,\n                                     &input_quantized));\n  int8_t* quantized_input_ptr_batch = input_quantized->data.int8;\n  TfLiteTensor* scaling_factors_tensor;\n  TF_LITE_ENSURE_OK(context,\n                    GetTemporarySafe(context, node, data->scaling_factors_index,\n                                     &scaling_factors_tensor));\n  float* scaling_factors_ptr = GetTensorData<float>(scaling_factors_tensor);\n  TfLiteTensor* input_offset_tensor;\n  TF_LITE_ENSURE_OK(context,\n                    GetTemporarySafe(context, node, data->input_offset_index,\n                                     &input_offset_tensor));\n  int32_t* input_offset_ptr = GetTensorData<int32_t>(input_offset_tensor);\n\n  for (int b = 0; b < batch_size; ++b) {\n    const int offset = b * input_size;\n    tensor_utils::AsymmetricQuantizeFloats(\n        GetTensorData<float>(input) + offset, input_size,\n        quantized_input_ptr_batch + offset, &scaling_factors_ptr[b],\n        &input_offset_ptr[b]);\n  }\n\n  DepthwiseParams op_params;\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = data->padding.width;\n  op_params.padding_values.height = data->padding.height;\n  op_params.stride_width = params->stride_width;\n  op_params.stride_height = params->stride_height;\n  op_params.dilation_width_factor = params->dilation_width_factor;\n  op_params.dilation_height_factor = params->dilation_height_factor;\n  op_params.depth_multiplier = params->depth_multiplier;\n\n  op_params.weights_offset = 0;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n  const auto* affine_quantization =\n      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params);\n  if (kernel_type == kReference) {\n    reference_integer_ops::DepthwiseConvHybridPerChannel(\n        op_params, scaling_factors_ptr, GetTensorShape(input),\n        quantized_input_ptr_batch, GetTensorShape(filter),\n        GetTensorData<int8>(filter), GetTensorShape(bias),\n        GetTensorData<float>(bias), GetTensorShape(output),\n        GetTensorData<float>(output), affine_quantization->scale->data,\n        input_offset_ptr);\n  } else {\n    optimized_integer_ops::DepthwiseConvHybridPerChannel(\n        op_params, scaling_factors_ptr, GetTensorShape(input),\n        quantized_input_ptr_batch, GetTensorShape(filter),\n        GetTensorData<int8>(filter), GetTensorShape(bias),\n        GetTensorData<float>(bias), GetTensorShape(output),\n        GetTensorData<float>(output), affine_quantization->scale->data,\n        input_offset_ptr, CpuBackendContext::GetFromContext(context));\n  }\n\n  return kTfLiteOk;\n}\n\ntemplate <KernelType kernel_type, TfLiteType input_type>\nTfLiteStatus EvalImpl(TfLiteContext* context, TfLiteNode* node) {\n  auto* params =\n      reinterpret_cast<TfLiteDepthwiseConvParams*>(node->builtin_data);\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n  const TfLiteTensor* filter;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kFilterTensor, &filter));\n  const TfLiteTensor* bias =\n      (NumInputs(node) == 3) ? GetInput(context, node, kBiasTensor) : nullptr;\n  TFLITE_DCHECK_EQ(input_type, input->type);\n\n  switch (input_type) {  // Already know in/out types are same.\n    case kTfLiteFloat32:\n      if (filter->type == kTfLiteFloat32) {\n        return EvalFloat<kernel_type>(context, node, params, data, input,\n                                      filter, bias, output);\n      } else if (filter->type == kTfLiteInt8) {\n        return EvalHybridPerChannel<kernel_type>(context, node, params, data,\n                                                 input, filter, bias, output);\n      } else {\n        TF_LITE_KERNEL_LOG(\n            context, \"Type %s with filter type %s not currently supported.\",\n            TfLiteTypeGetName(input->type), TfLiteTypeGetName(filter->type));\n        return kTfLiteError;\n      }\n      break;\n    case kTfLiteUInt8:\n      return EvalQuantized<kernel_type>(context, node, params, data, input,\n                                        filter, bias, output);\n      break;\n    case kTfLiteInt8:\n      return EvalQuantizedPerChannel<kernel_type>(context, node, params, data,\n                                                  input, filter, bias, output);\n      break;\n    case kTfLiteInt16:\n      return EvalQuantizedPerChannel16x8(params, data, input, filter, bias,\n                                         output);\n      break;\n    default:\n      context->ReportError(context, \"Type %d not currently supported.\",\n                           input->type);\n      return kTfLiteError;\n  }\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n\n  switch (input->type) {  // Already know in/out types are same.\n    case kTfLiteFloat32:\n      return EvalImpl<kernel_type, kTfLiteFloat32>(context, node);\n    case kTfLiteUInt8:\n      return EvalImpl<kernel_type, kTfLiteUInt8>(context, node);\n    case kTfLiteInt8:\n      return EvalImpl<kernel_type, kTfLiteInt8>(context, node);\n    case kTfLiteInt16:\n      return EvalImpl<kernel_type, kTfLiteInt16>(context, node);\n    default:\n      context->ReportError(context, \"Type %d not currently supported.\",\n                           input->type);\n      return kTfLiteError;\n  }\n}\n\n}  // namespace depthwise_conv\n\nTfLiteRegistration* Register_DEPTHWISE_CONVOLUTION_REF() {\n  static TfLiteRegistration r = {\n      depthwise_conv::Init, depthwise_conv::Free, depthwise_conv::Prepare,\n      depthwise_conv::Eval<depthwise_conv::kReference>};\n  return &r;\n}\n\nTfLiteRegistration* Register_DEPTHWISE_CONVOLUTION_GENERIC_OPT() {\n  static TfLiteRegistration r = {\n      depthwise_conv::Init, depthwise_conv::Free, depthwise_conv::Prepare,\n      depthwise_conv::Eval<depthwise_conv::kGenericOptimized>};\n  return &r;\n}\n\nTfLiteRegistration* Register_DEPTHWISE_CONVOLUTION_NEON_OPT() {\n  static TfLiteRegistration r = {\n      depthwise_conv::Init, depthwise_conv::Free, depthwise_conv::Prepare,\n      depthwise_conv::Eval<depthwise_conv::kNeonOptimized>};\n  return &r;\n}\n\nTfLiteRegistration* Register_DEPTHWISE_CONVOLUTION_NEON_OPT_UINT8() {\n  static TfLiteRegistration r = {\n      depthwise_conv::Init, depthwise_conv::Free, depthwise_conv::Prepare,\n      depthwise_conv::EvalImpl<depthwise_conv::kNeonOptimized, kTfLiteUInt8>};\n  return &r;\n}\n\nTfLiteRegistration* Register_DEPTHWISE_CONV_2D() {\n#ifdef USE_NEON\n  return Register_DEPTHWISE_CONVOLUTION_NEON_OPT();\n#else\n  return Register_DEPTHWISE_CONVOLUTION_GENERIC_OPT();\n#endif\n}\n\n// Warning: Clients using this variant are responsible for ensuring that their\n// models only need the UINT8 type. TFLite's op registration mechanism doesn't\n// yet allow for more nuanced registration mechanisms.\nTfLiteRegistration* Register_DEPTHWISE_CONV_2D_UINT8() {\n#ifdef USE_NEON\n  return Register_DEPTHWISE_CONVOLUTION_NEON_OPT_UINT8();\n#else\n  return Register_DEPTHWISE_CONV_2D();\n#endif\n}\n\n}  // namespace builtin\n}  // namespace ops\n}  // namespace tflite"