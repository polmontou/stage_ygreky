"/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include <stdint.h>\n#include <stdlib.h>\n\n#include <cmath>\n#include <limits>\n\n#include \"tensorflow/lite/c/common.h\"\n#include \"tensorflow/lite/kernels/internal/quantization_util.h\"\n#include \"tensorflow/lite/kernels/internal/reference/reference_ops.h\"\n#include \"tensorflow/lite/kernels/internal/tensor.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_ctypes.h\"\n#include \"tensorflow/lite/kernels/kernel_util.h\"\n#include \"tensorflow/lite/kernels/op_macros.h\"\n\nnamespace tflite {\nnamespace ops {\nnamespace builtin {\nnamespace elementwise {\nnamespace {\n\nconstexpr char kAbsName[] = \"Abs\";\nconstexpr char kSinName[] = \"Sin\";\nconstexpr char kCosName[] = \"Cos\";\nconstexpr char kLogName[] = \"Log\";\nconstexpr char kSqrtName[] = \"Sqrt\";\nconstexpr char kRsqrtName[] = \"Rsqrt\";\nconstexpr char kSquareName[] = \"Square\";\nconstexpr char kNotName[] = \"Not\";\n\nstruct OpData {\n  int32_t multiplier;\n  int32_t shift;\n  int input_offset;\n  int output_offset;\n};\n\nbool IsNumericSupportedType(const TfLiteType type) {\n  return type == kTfLiteFloat32;\n}\n\nbool IsLogicalSupportedType(const TfLiteType type) {\n  return type == kTfLiteBool;\n}\n\nbool IsAbsSupportedType(const TfLiteType type) {\n  return type == kTfLiteFloat32 || type == kTfLiteInt8;\n}\n\ntypedef bool (*IsSupportedType)(TfLiteType);\ntemplate <IsSupportedType is_supported_type, const char* op_name>\nTfLiteStatus GenericPrepare(TfLiteContext* context, TfLiteNode* node) {\n  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);\n  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n  TF_LITE_ENSURE_TYPES_EQ(context, input->type, output->type);\n  if (!is_supported_type(input->type)) {\n    TF_LITE_UNSUPPORTED_TYPE(context, input->type, op_name);\n  }\n  return context->ResizeTensor(context, output,\n                               TfLiteIntArrayCopy(input->dims));\n}\n\nTfLiteStatus AbsPrepare(TfLiteContext* context, TfLiteNode* node) {\n  TF_LITE_ENSURE_EQ(\n      context, (GenericPrepare<IsAbsSupportedType, kAbsName>(context, node)),\n      kTfLiteOk);\n  const TfLiteTensor* input = GetInput(context, node, 0);\n  if (input->type == kTfLiteInt8) {\n    TfLiteTensor* output = GetOutput(context, node, 0);\n    auto* op_data = static_cast<OpData*>(node->user_data);\n    TF_LITE_ENSURE_EQ(context, input->quantization.type,\n                      kTfLiteAffineQuantization);\n    TF_LITE_ENSURE_EQ(context, output->quantization.type,\n                      kTfLiteAffineQuantization);\n    const auto* input_params =\n        reinterpret_cast<TfLiteAffineQuantization*>(input->quantization.params);\n    const auto* output_params = reinterpret_cast<TfLiteAffineQuantization*>(\n        output->quantization.params);\n    TF_LITE_ENSURE(context, input_params != nullptr);\n    TF_LITE_ENSURE(context, input_params->scale != nullptr);\n    TF_LITE_ENSURE(context, input_params->scale->size > 0);\n    TF_LITE_ENSURE(context, input_params->zero_point->size > 0);\n    TF_LITE_ENSURE(context, output_params != nullptr);\n    TF_LITE_ENSURE(context, output_params->scale != nullptr);\n    TF_LITE_ENSURE(context, output_params->scale->size > 0);\n    TF_LITE_ENSURE(context, output_params->zero_point->size > 0);\n    op_data->input_offset = input_params->zero_point->data[0];\n    op_data->output_offset = output_params->zero_point->data[0];\n    const float input_scale = input_params->scale->data[0];\n    const float output_scale = output_params->scale->data[0];\n    double scale = input_scale / output_scale;\n    QuantizeMultiplier(scale, &op_data->multiplier, &op_data->shift);\n  }\n  return kTfLiteOk;\n}\n\ntemplate <typename T>\ninline TfLiteStatus EvalImpl(TfLiteContext* context, TfLiteNode* node,\n                             std::function<T(T)> func,\n                             TfLiteType expected_type) {\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n  TF_LITE_ENSURE_TYPES_EQ(context, input->type, expected_type);\n  const int64_t num_elements = NumElements(input);\n  const T* in_data = GetTensorData<T>(input);\n  T* out_data = GetTensorData<T>(output);\n  for (int64_t i = 0; i < num_elements; ++i) {\n    out_data[i] = func(in_data[i]);\n  }\n  return kTfLiteOk;\n}\n\ninline TfLiteStatus EvalNumeric(TfLiteContext* context, TfLiteNode* node,\n                                float float_func(float)) {\n  return EvalImpl<float>(context, node, float_func, kTfLiteFloat32);\n}\n\ninline TfLiteStatus EvalLogical(TfLiteContext* context, TfLiteNode* node,\n                                bool bool_func(bool)) {\n  return EvalImpl<bool>(context, node, bool_func, kTfLiteBool);\n}\n\nvoid* AbsInit(TfLiteContext* context, const char* buffer, size_t length) {\n  return new OpData();\n}\n\nvoid AbsFree(TfLiteContext* context, void* buffer) {\n  delete static_cast<OpData*>(buffer);\n}\n\nTfLiteStatus AbsEval(TfLiteContext* context, TfLiteNode* node) {\n  const TfLiteType type = GetInput(context, node, 0)->type;\n  switch (type) {\n    case kTfLiteFloat32:\n      return EvalImpl<float>(context, node, std::abs<float>, type);\n    case kTfLiteInt8: {\n      const auto* op_data = static_cast<const OpData*>(node->user_data);\n      const int kMinInt8 = std::numeric_limits<int8_t>::min();\n      const int kMaxInt8 = std::numeric_limits<int8_t>::max();\n      std::function<int8_t(int8_t)> func = [&](int8_t i) {\n        const int32_t value = std::abs(i - op_data->input_offset);\n        return std::min(\n            std::max(op_data->output_offset +\n                         MultiplyByQuantizedMultiplier(\n                             value, op_data->multiplier, op_data->shift),\n                     kMinInt8),\n            kMaxInt8);\n      };\n      return EvalImpl<int8_t>(context, node, func, type);\n    }\n    default:\n      TF_LITE_KERNEL_LOG(context, \"Current data type %s is not supported.\",\n                         TfLiteTypeGetName(type));\n      return kTfLiteError;\n  }\n}\n\nTfLiteStatus SinEval(TfLiteContext* context, TfLiteNode* node) {\n  return EvalNumeric(context, node, std::sin);\n}\n\nTfLiteStatus CosEval(TfLiteContext* context, TfLiteNode* node) {\n  return EvalNumeric(context, node, std::cos);\n}\n\nTfLiteStatus LogEval(TfLiteContext* context, TfLiteNode* node) {\n  return EvalNumeric(context, node, std::log);\n}\n\nTfLiteStatus SqrtEval(TfLiteContext* context, TfLiteNode* node) {\n  return EvalNumeric(context, node, std::sqrt);\n}\n\nTfLiteStatus RsqrtEval(TfLiteContext* context, TfLiteNode* node) {\n  return EvalNumeric(context, node, [](float f) { return 1.f / std::sqrt(f); });\n}\n\nTfLiteStatus SquareEval(TfLiteContext* context, TfLiteNode* node) {\n  return EvalNumeric(context, node, [](float f) { return f * f; });\n}\n\nTfLiteStatus LogicalNotEval(TfLiteContext* context, TfLiteNode* node) {\n  return EvalLogical(context, node, [](bool v) { return !v; });\n}\n\n}  // namespace\n}  // namespace elementwise\n\nTfLiteRegistration* Register_ABS() {\n  static TfLiteRegistration r = {elementwise::AbsInit, elementwise::AbsFree,\n                                 elementwise::AbsPrepare, elementwise::AbsEval};\n  return &r;\n}\n\nTfLiteRegistration* Register_SIN() {\n  static TfLiteRegistration r = {\n      /*init=*/nullptr, /*free=*/nullptr,\n      elementwise::GenericPrepare<elementwise::IsNumericSupportedType,\n                                  elementwise::kSinName>,\n      elementwise::SinEval};\n  return &r;\n}\n\nTfLiteRegistration* Register_COS() {\n  static TfLiteRegistration r = {\n      /*init=*/nullptr, /*free=*/nullptr,\n      elementwise::GenericPrepare<elementwise::IsNumericSupportedType,\n                                  elementwise::kCosName>,\n      elementwise::CosEval};\n  return &r;\n}\n\nTfLiteRegistration* Register_LOG() {\n  static TfLiteRegistration r = {\n      /*init=*/nullptr, /*free=*/nullptr,\n      elementwise::GenericPrepare<elementwise::IsNumericSupportedType,\n                                  elementwise::kLogName>,\n      elementwise::LogEval};\n  return &r;\n}\n\nTfLiteRegistration* Register_SQRT() {\n  static TfLiteRegistration r = {\n      /*init=*/nullptr, /*free=*/nullptr,\n      elementwise::GenericPrepare<elementwise::IsNumericSupportedType,\n                                  elementwise::kSqrtName>,\n      elementwise::SqrtEval};\n  return &r;\n}\n\nTfLiteRegistration* Register_RSQRT() {\n  static TfLiteRegistration r = {\n      /*init=*/nullptr, /*free=*/nullptr,\n      elementwise::GenericPrepare<elementwise::IsNumericSupportedType,\n                                  elementwise::kRsqrtName>,\n      elementwise::RsqrtEval};\n  return &r;\n}\n\nTfLiteRegistration* Register_SQUARE() {\n  static TfLiteRegistration r = {\n      /*init=*/nullptr, /*free=*/nullptr,\n      elementwise::GenericPrepare<elementwise::IsNumericSupportedType,\n                                  elementwise::kSquareName>,\n      elementwise::SquareEval};\n  return &r;\n}\n\nTfLiteRegistration* Register_LOGICAL_NOT() {\n  static TfLiteRegistration r = {\n      /*init=*/nullptr, /*free=*/nullptr,\n      elementwise::GenericPrepare<elementwise::IsLogicalSupportedType,\n                                  elementwise::kNotName>,\n      elementwise::LogicalNotEval};\n  return &r;\n}\n\n}  // namespace builtin\n}  // namespace ops\n}  // namespace tflite"