"/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include <stddef.h>\n#include <stdint.h>\n\n#include <vector>\n\n#include \"tensorflow/lite/c/builtin_op_data.h\"\n#include \"tensorflow/lite/c/common.h\"\n#include \"tensorflow/lite/kernels/cpu_backend_context.h\"\n#include \"tensorflow/lite/kernels/internal/compatibility.h\"\n// NOLINTNEXTLINE - This header file should't go to the top.\n#include \"tensorflow/lite/kernels/internal/optimized/integer_ops/transpose_conv.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/optimized_ops.h\"\n// NOLINTNEXTLINE - This header file should't go to the top.\n#include \"tensorflow/lite/kernels/internal/reference/integer_ops/transpose_conv.h\"\n#include \"tensorflow/lite/kernels/internal/reference/reference_ops.h\"\n#include \"tensorflow/lite/kernels/internal/tensor.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_ctypes.h\"\n#include \"tensorflow/lite/kernels/internal/types.h\"\n#include \"tensorflow/lite/kernels/kernel_util.h\"\n#include \"tensorflow/lite/kernels/padding.h\"\n\nnamespace tflite {\nnamespace ops {\nnamespace builtin {\nnamespace transpose_conv {\n\n// This file has 2 implementation of TransposeConv.\nenum KernelType {\n  kReference,\n  kGenericOptimized,  // Neon-free\n};\n\nconstexpr int kOutputShapeTensor = 0;\nconstexpr int kWeightsTensor = 1;\nconstexpr int kDataInputTensor = 2;\nconstexpr int kBiasTensor = 3;\nconstexpr int kOutputTensor = 0;\n\nconst int kTensorNotAllocated = -1;\n\nstruct OpData {\n  // IDs are the arbitrary identifiers used by TF Lite to identify and access\n  // memory buffers.\n  int col2im_id = kTensorNotAllocated;\n  int transposed_weights_id = kTensorNotAllocated;\n  int scratch_tensor_id = kTensorNotAllocated;\n\n  // col2im is the temporary tensor allocated and used in optimized path for\n  // storing col2im data:gemm result for input_matrix x filter_matrix.\n  int32_t col2im_index;\n\n  // TfLiteConverter will transpose weights from HWOI to OHWI order.\n  // In optimized path, we will transpose them back to HWOI, this temporary\n  // tensor is allocated for storing transposed weights.\n  int32_t transposed_weights_index;\n\n  // Scratch tensor is used in the quantized path for storing accumulation\n  // results.\n  int32_t scratch_tensor_index;\n\n  TfLitePaddingValues padding;\n  // The scaling factor from input to output (aka the 'real multiplier') can\n  // be represented as a fixed point multiplier plus a left shift.\n  int32_t output_multiplier;\n  int output_shift;\n\n  // Per channel output multiplier and shift.\n  // TODO(b/144846950): Add channel dimension index for the kernel to be more\n  // flexible.\n  std::vector<int32_t> per_channel_output_multiplier;\n  std::vector<int32_t> per_channel_output_shift;\n\n  // The range of the fused activation layer. For example for kNone and\n  // uint8_t these would be 0 and 255.\n  int32_t output_activation_min;\n  int32_t output_activation_max;\n\n  bool has_col2im = false;\n  bool weights_are_transposed = false;\n};\n\nvoid* Init(TfLiteContext* context, const char* buffer, size_t length) {\n  return new OpData;\n}\n\nvoid Free(TfLiteContext* context, void* buffer) {\n  delete reinterpret_cast<OpData*>(buffer);\n}\n\nTfLiteStatus ResizeTensor(TfLiteContext* context,\n                          const TfLiteTensor* shape_tensor,\n                          TfLiteTensor* tensor_to_resize) {\n  // Currently only support int32 for output shape.\n  if (shape_tensor->type != kTfLiteInt32) {\n    TF_LITE_KERNEL_LOG(context, \"Output shape is %s, not int32.\",\n                       TfLiteTypeGetName(shape_tensor->type));\n    return kTfLiteError;\n  }\n\n  TfLiteIntArray* shape = TfLiteIntArrayCreate(NumElements(shape_tensor));\n  for (int i = 0; i < shape->size; ++i) {\n    shape->data[i] = GetTensorData<int32_t>(shape_tensor)[i];\n  }\n\n  return context->ResizeTensor(context, tensor_to_resize, shape);\n}\n\n// Allocate temporary tensors if necessary.\ntemplate <KernelType kernel_type>\nstatic TfLiteStatus AllocateTemporaryTensorsIfRequired(TfLiteContext* context,\n                                                       TfLiteType input_type,\n                                                       TfLiteType weights_type,\n                                                       TfLiteNode* node) {\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n  int temporaries_count = 0;\n\n  // Allocate col2im tensor. Currently it's only used for optimized kernels.\n  if (kernel_type == kGenericOptimized) {\n    if (data->col2im_id == kTensorNotAllocated) {\n      context->AddTensors(context, 1, &data->col2im_id);\n    }\n    data->col2im_index = temporaries_count;\n    data->has_col2im = true;\n    ++temporaries_count;\n  }\n\n  // Allocate transposed_weights tensor. Currently it's only used for optimized\n  // float kernels.\n  if (kernel_type == kGenericOptimized) {\n    if (data->transposed_weights_id == kTensorNotAllocated) {\n      context->AddTensors(context, 1, &data->transposed_weights_id);\n    }\n    data->transposed_weights_index = temporaries_count;\n    data->weights_are_transposed = true;\n    ++temporaries_count;\n  }\n\n  // Allocate scratch buffer tensor\n  if (input_type == kTfLiteUInt8 || input_type == kTfLiteInt8 ||\n      input_type == kTfLiteInt16) {\n    if (data->scratch_tensor_id == kTensorNotAllocated) {\n      context->AddTensors(context, 1, &data->scratch_tensor_id);\n    }\n    data->scratch_tensor_index = temporaries_count;\n    ++temporaries_count;\n  }\n\n  TfLiteIntArrayFree(node->temporaries);\n  node->temporaries = TfLiteIntArrayCreate(temporaries_count);\n\n  return kTfLiteOk;\n}\n\nTfLiteStatus ResizeCol2ImTensor(TfLiteContext* context,\n                                const TfLiteTensor* output_shape,\n                                const TfLiteTensor* weights,\n                                const TfLiteTensor* input,\n                                TfLiteTensor* col2im) {\n  if (output_shape->type != kTfLiteInt32) {\n    TF_LITE_KERNEL_LOG(context, \"col2im shape is %s, not int32.\",\n                       TfLiteTypeGetName(output_shape->type));\n    return kTfLiteError;\n  }\n  TF_LITE_ENSURE_EQ(context, NumElements(output_shape), 4);\n  TfLiteIntArray* col2im_shape_array = TfLiteIntArrayCreate(2);\n  const RuntimeShape& input_shape = GetTensorShape(input);\n  const RuntimeShape& weights_shape = GetTensorShape(weights);\n  col2im_shape_array->data[0] = input_shape.Dims(1) * input_shape.Dims(2);\n  col2im_shape_array->data[1] =\n      weights_shape.Dims(0) * weights_shape.Dims(1) * weights_shape.Dims(2);\n\n  col2im->type = input->type == kTfLiteFloat32 ? kTfLiteFloat32 : kTfLiteInt32;\n  col2im->allocation_type = kTfLiteDynamic;\n  return context->ResizeTensor(context, col2im, col2im_shape_array);\n}\n\nTfLiteStatus ResizeAndTransposeWeights(TfLiteContext* context,\n                                       const TfLiteTensor* weights,\n                                       TfLiteTensor* transposed_weights) {\n  TfLiteIntArray* transposed_weights_shape_array = TfLiteIntArrayCreate(4);\n  const RuntimeShape& input_shape = GetTensorShape(weights);\n  transposed_weights_shape_array->data[0] = input_shape.Dims(1);\n  transposed_weights_shape_array->data[1] = input_shape.Dims(2);\n  transposed_weights_shape_array->data[2] = input_shape.Dims(0);\n  transposed_weights_shape_array->data[3] = input_shape.Dims(3);\n\n  transposed_weights->type = weights->type;\n  transposed_weights->allocation_type = kTfLiteDynamic;\n  TF_LITE_ENSURE_STATUS(context->ResizeTensor(context, transposed_weights,\n                                              transposed_weights_shape_array));\n\n  // Transpose the weights from from OHWI order to HWOI order.\n  TransposeParams transpose_params;\n  transpose_params.perm_count = 4;\n  transpose_params.perm[0] = 1;\n  transpose_params.perm[1] = 2;\n  transpose_params.perm[2] = 0;\n  transpose_params.perm[3] = 3;\n\n  if (weights->type == kTfLiteFloat32) {\n    optimized_ops::Transpose(transpose_params, input_shape,\n                             GetTensorData<float>(weights),\n                             GetTensorShape(transposed_weights),\n                             GetTensorData<float>(transposed_weights));\n  } else if (weights->type == kTfLiteUInt8) {\n    optimized_ops::Transpose(transpose_params, input_shape,\n                             GetTensorData<uint8>(weights),\n                             GetTensorShape(transposed_weights),\n                             GetTensorData<uint8>(transposed_weights));\n  } else if (weights->type == kTfLiteInt8) {\n    // int16 transpose_conv also with int8 weights\n    optimized_ops::Transpose(transpose_params, input_shape,\n                             GetTensorData<int8>(weights),\n                             GetTensorShape(transposed_weights),\n                             GetTensorData<int8>(transposed_weights));\n  } else {\n    TF_LITE_KERNEL_LOG(\n        context,\n        \"Only float32, uint8, int8, int16 is supported currently, got %s.\",\n        TfLiteTypeGetName(weights->type));\n    return kTfLiteError;\n  }\n\n  return kTfLiteOk;\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n\n  bool has_bias = NumInputs(node) == 4;\n\n  // Sanity checks on op\n  TF_LITE_ENSURE(context, has_bias || NumInputs(node) == 3);\n  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n\n  // Retrieve tensors\n  const TfLiteTensor* output_shape;\n  TF_LITE_ENSURE_OK(\n      context, GetInputSafe(context, node, kOutputShapeTensor, &output_shape));\n  const TfLiteTensor* weights;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kWeightsTensor, &weights));\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kDataInputTensor, &input));\n  const TfLiteTensor* bias = nullptr;\n\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n\n  // Tensor sanity checks\n  TF_LITE_ENSURE_EQ(context, NumDimensions(output_shape), 1);\n  TF_LITE_ENSURE_EQ(context, NumDimensions(input), 4);\n  TF_LITE_ENSURE_EQ(context, NumDimensions(weights), 4);\n  TF_LITE_ENSURE(context,\n                 input->type == kTfLiteFloat32 || input->type == kTfLiteUInt8 ||\n                     input->type == kTfLiteInt8 || input->type == kTfLiteInt16);\n\n  if (has_bias) {\n    bias = GetOptionalInputTensor(context, node, kBiasTensor);\n    if (bias) {\n      if (input->type == kTfLiteUInt8 || input->type == kTfLiteInt8) {\n        TF_LITE_ENSURE_TYPES_EQ(context, bias->type, kTfLiteInt32);\n        if (input->type == kTfLiteInt8) {\n          TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);\n        }\n      } else if (input->type == kTfLiteInt16) {\n        TF_LITE_ENSURE_EQ(context, bias->type, kTfLiteInt64);\n        TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);\n      } else {\n        TF_LITE_ENSURE_TYPES_EQ(context, bias->type, input->type);\n      }\n      TF_LITE_ENSURE_EQ(context, NumElements(bias),\n                        SizeOfDimension(weights, 0));\n    }\n  }\n\n  if (input->type == kTfLiteInt16) {\n    TF_LITE_ENSURE_EQ(context, weights->type, kTfLiteInt8);\n    TF_LITE_ENSURE_EQ(context, input->params.zero_point, 0);\n    TF_LITE_ENSURE_EQ(context, output->params.zero_point, 0);\n  } else {\n    TF_LITE_ENSURE_TYPES_EQ(context, weights->type, input->type);\n  }\n  TF_LITE_ENSURE_TYPES_EQ(context, output->type, input->type);\n  // Ensure that weights and inputs have the same channel dimension.\n  // Note: TOCO will reorder weights in the following format: OHWI.\n  TF_LITE_ENSURE_EQ(context, SizeOfDimension(input, 3),\n                    SizeOfDimension(weights, 3));\n\n  // Allocate col2Im, transposed_weights & scratch Tensor.\n  TF_LITE_ENSURE_STATUS(AllocateTemporaryTensorsIfRequired<kernel_type>(\n      context, input->type, weights->type, node));\n\n  OpData* user_data = reinterpret_cast<OpData*>(node->user_data);\n  TfLiteTensor* col2im = nullptr;\n  if (data->has_col2im) {\n    node->temporaries->data[data->col2im_index] = data->col2im_id;\n    TF_LITE_ENSURE_OK(\n        context,\n        GetTemporarySafe(context, node, user_data->col2im_index, &col2im));\n  }\n\n  if (!IsConstantTensor(output_shape)) {\n    // Defer resizing until Eval().\n    SetTensorToDynamic(output);\n    if (data->has_col2im) {\n      SetTensorToDynamic(col2im);\n    }\n  } else {\n    TF_LITE_ENSURE_STATUS(ResizeTensor(context, output_shape, output));\n    if (data->has_col2im) {\n      TF_LITE_ENSURE_STATUS(\n          ResizeCol2ImTensor(context, output_shape, weights, input, col2im));\n    }\n  }\n\n  if (data->weights_are_transposed) {\n    node->temporaries->data[data->transposed_weights_index] =\n        data->transposed_weights_id;\n    TfLiteTensor* transposed_weights;\n    TF_LITE_ENSURE_OK(\n        context,\n        GetTemporarySafe(context, node, user_data->transposed_weights_index,\n                         &transposed_weights));\n    if (!IsConstantTensor(weights)) {\n      SetTensorToDynamic(transposed_weights);\n    } else {\n      ResizeAndTransposeWeights(context, weights, transposed_weights);\n    }\n  }\n\n  if (input->type == kTfLiteUInt8 || input->type == kTfLiteInt8 ||\n      input->type == kTfLiteInt16) {\n    node->temporaries->data[data->scratch_tensor_index] =\n        data->scratch_tensor_id;\n    TfLiteTensor* scratch_buffer;\n    TF_LITE_ENSURE_OK(\n        context, GetTemporarySafe(context, node, data->scratch_tensor_index,\n                                  &scratch_buffer));\n    if (input->type == kTfLiteInt16) {\n      scratch_buffer->type = kTfLiteInt64;\n    } else {\n      scratch_buffer->type = kTfLiteInt32;\n    }\n\n    scratch_buffer->allocation_type = kTfLiteDynamic;\n    if (!IsConstantTensor(output_shape)) {\n      SetTensorToDynamic(scratch_buffer);\n    } else {\n      TF_LITE_ENSURE_STATUS(\n          ResizeTensor(context, output_shape, scratch_buffer));\n    }\n\n    TF_LITE_ENSURE_EQ(context, weights->quantization.type,\n                      kTfLiteAffineQuantization);\n    const auto* affine_quantization =\n        reinterpret_cast<TfLiteAffineQuantization*>(\n            weights->quantization.params);\n    TF_LITE_ENSURE(context, affine_quantization);\n    TF_LITE_ENSURE(context, affine_quantization->scale);\n    const int number_channel = affine_quantization->scale->size;\n    data->per_channel_output_multiplier.resize(number_channel);\n    data->per_channel_output_shift.resize(number_channel);\n    TF_LITE_ENSURE_STATUS(tflite::PopulateConvolutionQuantizationParams(\n        context, input, weights, bias, output, kTfLiteActNone,\n        &data->output_multiplier, &data->output_shift,\n        &data->output_activation_min, &data->output_activation_max,\n        data->per_channel_output_multiplier.data(),\n        data->per_channel_output_shift.data()));\n  }\n\n  return kTfLiteOk;\n}\n\ntemplate <KernelType kernel_type>\nvoid EvalFloat(TfLiteContext* context, const TfLiteTransposeConvParams* params,\n               const OpData* data, const TfLiteTensor* input,\n               const TfLiteTensor* weights, const TfLiteTensor* bias,\n               const TfLiteTensor* transposed_weights, TfLiteTensor* col2im,\n               TfLiteTensor* output) {\n  tflite::ConvParams op_params;\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = data->padding.width;\n  op_params.padding_values.height = data->padding.height;\n  op_params.padding_values.width_offset = data->padding.width_offset;\n  op_params.padding_values.height_offset = data->padding.height_offset;\n  op_params.stride_width = params->stride_width;\n  op_params.stride_height = params->stride_height;\n\n  switch (kernel_type) {\n    case kReference: {\n      reference_ops::TransposeConv(\n          op_params, GetTensorShape(input), GetTensorData<float>(input),\n          GetTensorShape(weights), GetTensorData<float>(weights),\n          GetTensorShape(bias), GetTensorData<float>(bias),\n          GetTensorShape(output), GetTensorData<float>(output),\n          GetTensorShape(col2im), GetTensorData<float>(col2im));\n      break;\n    }\n    case kGenericOptimized: {\n      optimized_ops::TransposeConvV2(\n          op_params, GetTensorShape(input), GetTensorData<float>(input),\n          GetTensorShape(transposed_weights),\n          GetTensorData<float>(transposed_weights), GetTensorShape(bias),\n          GetTensorData<float>(bias), GetTensorShape(output),\n          GetTensorData<float>(output), GetTensorShape(col2im),\n          GetTensorData<float>(col2im),\n          CpuBackendContext::GetFromContext(context));\n      break;\n    }\n  }\n}\n\ntemplate <KernelType kernel_type>\nvoid EvalQuantized(TfLiteContext* context,\n                   const TfLiteTransposeConvParams* params, OpData* data,\n                   const TfLiteTensor* input, const TfLiteTensor* weights,\n                   const TfLiteTensor* transposed_weights,\n                   const TfLiteTensor* bias, TfLiteTensor* col2im,\n                   TfLiteTensor* output, TfLiteTensor* scratch_buffer) {\n  int32_t input_offset = -input->params.zero_point;\n  int32_t filter_offset = -weights->params.zero_point;\n  int32_t output_offset = output->params.zero_point;\n\n  tflite::ConvParams op_params;\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = data->padding.width;\n  op_params.padding_values.height = data->padding.height;\n  op_params.padding_values.width_offset = data->padding.width_offset;\n  op_params.padding_values.height_offset = data->padding.height_offset;\n  op_params.stride_width = params->stride_width;\n  op_params.stride_height = params->stride_height;\n  op_params.input_offset = input_offset;\n  op_params.output_offset = output_offset;\n  op_params.weights_offset = filter_offset;\n  op_params.output_multiplier = data->output_multiplier;\n  op_params.output_shift = -data->output_shift;\n  op_params.quantized_activation_min = data->output_activation_min;\n  op_params.quantized_activation_max = data->output_activation_max;\n\n  switch (kernel_type) {\n    case kReference: {\n      reference_ops::TransposeConv(\n          op_params, GetTensorShape(input), GetTensorData<uint8>(input),\n          GetTensorShape(weights), GetTensorData<uint8>(weights),\n          GetTensorShape(bias), GetTensorData<int32_t>(bias),\n          GetTensorShape(output), GetTensorData<uint8>(output),\n          GetTensorShape(col2im), GetTensorData<uint8>(col2im),\n          GetTensorData<int32_t>(scratch_buffer));\n      break;\n    }\n    case kGenericOptimized: {\n      optimized_ops::TransposeConvV2(\n          op_params, GetTensorShape(input), GetTensorData<uint8>(input),\n          GetTensorShape(transposed_weights),\n          GetTensorData<uint8>(transposed_weights), GetTensorShape(bias),\n          GetTensorData<int32>(bias), GetTensorShape(output),\n          GetTensorData<uint8>(output), GetTensorShape(col2im),\n          GetTensorData<int32>(col2im), GetTensorData<int32>(scratch_buffer),\n          CpuBackendContext::GetFromContext(context));\n      break;\n    }\n  }\n}\n\ntemplate <KernelType kernel_type>\nvoid EvalQuantizedPerChannel(\n    TfLiteContext* context, const TfLiteTransposeConvParams* params,\n    OpData* data, const TfLiteTensor* input, const TfLiteTensor* weights,\n    const TfLiteTensor* transposed_weights, const TfLiteTensor* bias,\n    TfLiteTensor* col2im, TfLiteTensor* output, TfLiteTensor* scratch_buffer) {\n  tflite::ConvParams op_params;\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = data->padding.width;\n  op_params.padding_values.height = data->padding.height;\n  op_params.padding_values.width_offset = data->padding.width_offset;\n  op_params.padding_values.height_offset = data->padding.height_offset;\n  op_params.stride_width = params->stride_width;\n  op_params.stride_height = params->stride_height;\n  // Need to flip the sign of input offset to add it directly to the quantized\n  // buffer.\n  op_params.input_offset = -input->params.zero_point;\n  op_params.output_offset = output->params.zero_point;\n  op_params.quantized_activation_min = data->output_activation_min;\n  op_params.quantized_activation_max = data->output_activation_max;\n\n  switch (kernel_type) {\n    case kReference: {\n      reference_integer_ops::TransposeConv(\n          op_params, data->per_channel_output_multiplier.data(),\n          data->per_channel_output_shift.data(), GetTensorShape(input),\n          GetTensorData<int8>(input), GetTensorShape(weights),\n          GetTensorData<int8>(weights), GetTensorShape(bias),\n          GetTensorData<int32>(bias), GetTensorShape(output),\n          GetTensorData<int8>(output), GetTensorShape(col2im),\n          GetTensorData<int8>(col2im), GetTensorData<int32_t>(scratch_buffer));\n      break;\n    }\n    case kGenericOptimized: {\n      optimized_integer_ops::TransposeConvV2(\n          op_params, data->per_channel_output_multiplier.data(),\n          data->per_channel_output_shift.data(), GetTensorShape(input),\n          GetTensorData<int8>(input), GetTensorShape(transposed_weights),\n          GetTensorData<int8>(transposed_weights), GetTensorShape(bias),\n          GetTensorData<int32>(bias), GetTensorShape(output),\n          GetTensorData<int8>(output), GetTensorShape(col2im),\n          GetTensorData<int32>(col2im), GetTensorData<int32>(scratch_buffer),\n          CpuBackendContext::GetFromContext(context));\n      break;\n    }\n  }\n}\n\nvoid EvalQuantizedPerChannel16x8(\n    TfLiteContext* context, const TfLiteTransposeConvParams* params,\n    OpData* data, const TfLiteTensor* input, const TfLiteTensor* weights,\n    const TfLiteTensor* transposed_weights, const TfLiteTensor* bias,\n    TfLiteTensor* col2im, TfLiteTensor* output, TfLiteTensor* scratch_buffer) {\n  tflite::ConvParams op_params;\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = data->padding.width;\n  op_params.padding_values.height = data->padding.height;\n  op_params.padding_values.width_offset = data->padding.width_offset;\n  op_params.padding_values.height_offset = data->padding.height_offset;\n  op_params.stride_width = params->stride_width;\n  op_params.stride_height = params->stride_height;\n  // Need to flip the sign of input offset to add it directly to the quantized\n  // buffer.\n  op_params.input_offset = -input->params.zero_point;\n  op_params.output_offset = output->params.zero_point;\n  op_params.quantized_activation_min = data->output_activation_min;\n  op_params.quantized_activation_max = data->output_activation_max;\n\n  // Need to add optimized kernel\n  reference_integer_ops::TransposeConv(\n      op_params, data->per_channel_output_multiplier.data(),\n      data->per_channel_output_shift.data(), GetTensorShape(input),\n      GetTensorData<int16>(input), GetTensorShape(weights),\n      GetTensorData<int8>(weights), GetTensorShape(bias),\n      GetTensorData<int64_t>(bias), GetTensorShape(output),\n      GetTensorData<int16>(output), GetTensorShape(col2im),\n      GetTensorData<int8>(col2im), GetTensorData<int64_t>(scratch_buffer));\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n  // Retrieve tensors (All should be allocated by now)\n  const TfLiteTensor* output_shape;\n  TF_LITE_ENSURE_OK(\n      context, GetInputSafe(context, node, kOutputShapeTensor, &output_shape));\n  const TfLiteTensor* weights;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kWeightsTensor, &weights));\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kDataInputTensor, &input));\n  const TfLiteTensor* bias =\n      (NumInputs(node) == 4)\n          ? GetOptionalInputTensor(context, node, kBiasTensor)\n          : nullptr;\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n  TfLiteTensor* col2im = data->has_col2im\n                             ? GetTemporary(context, node, data->col2im_index)\n                             : nullptr;\n  TfLiteTensor* transposed_weights =\n      data->weights_are_transposed\n          ? GetTemporary(context, node, data->transposed_weights_index)\n          : nullptr;\n  const auto* params =\n      reinterpret_cast<TfLiteTransposeConvParams*>(node->builtin_data);\n\n  // Resize any deferred dynamic tensors\n  if (IsDynamicTensor(output)) {\n    TF_LITE_ENSURE_OK(context, ResizeTensor(context, output_shape, output));\n  }\n  if (data->has_col2im && IsDynamicTensor(col2im)) {\n    TF_LITE_ENSURE_OK(context, ResizeCol2ImTensor(context, output_shape,\n                                                  weights, input, col2im));\n  }\n\n  // Get height and width of the output image.\n  const int width = SizeOfDimension(output, 2);\n  const int height = SizeOfDimension(output, 1);\n  const int filter_width = SizeOfDimension(weights, 2);\n  const int filter_height = SizeOfDimension(weights, 1);\n\n  int unused_output_height, unused_output_width;\n  data->padding = ComputePaddingHeightWidth(\n      params->stride_height, params->stride_width, 1, 1, height, width,\n      filter_height, filter_width, params->padding, &unused_output_height,\n      &unused_output_width);\n\n  // Currently support float32, uint8, int8, int16.\n  switch (input->type) {\n    case kTfLiteFloat32: {\n      // Only for GenericOptimized path, we use transposed weights.\n      if (data->weights_are_transposed) {\n        if (!IsConstantTensor(weights)) {\n          ResizeAndTransposeWeights(context, weights, transposed_weights);\n        }\n      }\n      EvalFloat<kernel_type>(context, params, data, input, weights, bias,\n                             transposed_weights, col2im, output);\n      break;\n    }\n    case kTfLiteUInt8: {\n      TfLiteTensor* scratch_buffer;\n      TF_LITE_ENSURE_OK(\n          context, GetTemporarySafe(context, node, data->scratch_tensor_index,\n                                    &scratch_buffer));\n      if (IsDynamicTensor(scratch_buffer)) {\n        TF_LITE_ENSURE_OK(context,\n                          ResizeTensor(context, output_shape, scratch_buffer));\n      }\n      if (data->weights_are_transposed) {\n        if (!IsConstantTensor(weights)) {\n          ResizeAndTransposeWeights(context, weights, transposed_weights);\n        }\n      }\n      EvalQuantized<kernel_type>(context, params, data, input, weights,\n                                 transposed_weights, bias, col2im, output,\n                                 scratch_buffer);\n      break;\n    }\n    case kTfLiteInt8: {\n      TfLiteTensor* scratch_buffer;\n      TF_LITE_ENSURE_OK(\n          context, GetTemporarySafe(context, node, data->scratch_tensor_index,\n                                    &scratch_buffer));\n      if (IsDynamicTensor(scratch_buffer)) {\n        TF_LITE_ENSURE_OK(context,\n                          ResizeTensor(context, output_shape, scratch_buffer));\n      }\n      if (data->weights_are_transposed && !IsConstantTensor(weights)) {\n        ResizeAndTransposeWeights(context, weights, transposed_weights);\n      }\n      EvalQuantizedPerChannel<kernel_type>(context, params, data, input,\n                                           weights, transposed_weights, bias,\n                                           col2im, output, scratch_buffer);\n      break;\n    }\n    case kTfLiteInt16: {\n      TfLiteTensor* scratch_buffer;\n      TF_LITE_ENSURE_OK(\n          context, GetTemporarySafe(context, node, data->scratch_tensor_index,\n                                    &scratch_buffer));\n      if (IsDynamicTensor(scratch_buffer)) {\n        TF_LITE_ENSURE_OK(context,\n                          ResizeTensor(context, output_shape, scratch_buffer));\n      }\n      if (data->weights_are_transposed && !IsConstantTensor(weights)) {\n        ResizeAndTransposeWeights(context, weights, transposed_weights);\n      }\n      EvalQuantizedPerChannel16x8(context, params, data, input, weights,\n                                  transposed_weights, bias, col2im, output,\n                                  scratch_buffer);\n      break;\n    }\n    default:\n      context->ReportError(context, \"Type '%s' is not currently supported.\",\n                           TfLiteTypeGetName(input->type));\n      return kTfLiteError;\n  }\n  return kTfLiteOk;\n}\n\n}  // namespace transpose_conv\n\nTfLiteRegistration* Register_TRANSPOSECONV_REF() {\n  static TfLiteRegistration r = {\n      transpose_conv::Init, transpose_conv::Free,\n      transpose_conv::Prepare<transpose_conv::kReference>,\n      transpose_conv::Eval<transpose_conv::kReference>};\n  return &r;\n}\n\nTfLiteRegistration* Register_TRANSPOSECONV_GENERIC_OPT() {\n  static TfLiteRegistration r = {\n      transpose_conv::Init, transpose_conv::Free,\n      transpose_conv::Prepare<transpose_conv::kGenericOptimized>,\n      transpose_conv::Eval<transpose_conv::kGenericOptimized>};\n  return &r;\n}\n\nTfLiteRegistration* Register_TRANSPOSE_CONV() {\n  return Register_TRANSPOSECONV_GENERIC_OPT();\n}\n\n}  // namespace builtin\n}  // namespace ops\n}  // namespace tflite"