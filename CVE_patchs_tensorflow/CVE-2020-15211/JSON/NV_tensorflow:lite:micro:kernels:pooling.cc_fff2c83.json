"/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#include \"tensorflow/lite/kernels/internal/reference/pooling.h\"\n\n#include \"tensorflow/lite/c/builtin_op_data.h\"\n#include \"tensorflow/lite/kernels/internal/reference/integer_ops/pooling.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_ctypes.h\"\n#include \"tensorflow/lite/kernels/kernel_util.h\"\n#include \"tensorflow/lite/kernels/padding.h\"\n#include \"tensorflow/lite/micro/kernels/kernel_util.h\"\n\nnamespace tflite {\nnamespace ops {\nnamespace micro {\nnamespace pooling {\n\nnamespace {\n\nconstexpr int kInputTensor = 0;\nconstexpr int kOutputTensor = 0;\n\nstruct OpData {\n  TfLitePaddingValues padding;\n  int32_t activation_min;\n  int32_t activation_max;\n  float activation_min_f32;\n  float activation_max_f32;\n};\n\nTfLiteStatus CalculateOpData(const TfLiteContext* context,\n                             const TfLitePoolParams* params,\n                             const TfLiteTensor* input,\n                             const TfLiteTensor* output, OpData* data) {\n  // input: batch, height, width, channel\n  int height = SizeOfDimension(input, 1);\n  int width = SizeOfDimension(input, 2);\n\n  int out_height, out_width;\n\n  data->padding = ComputePaddingHeightWidth(\n      params->stride_height, params->stride_width,\n      /*dilation_rate_height=*/1,\n      /*dilation_rate_width=*/1, height, width, params->filter_height,\n      params->filter_width, params->padding, &out_height, &out_width);\n\n  return kTfLiteOk;\n}\n\nvoid AverageEvalFloat(const TfLiteContext* context, const TfLiteNode* node,\n                      const TfLitePoolParams* params, const OpData* data,\n                      const TfLiteEvalTensor* input, TfLiteEvalTensor* output) {\n  PoolParams op_params;\n  op_params.stride_height = params->stride_height;\n  op_params.stride_width = params->stride_width;\n  op_params.filter_height = params->filter_height;\n  op_params.filter_width = params->filter_width;\n  op_params.padding_values.height = data->padding.height;\n  op_params.padding_values.width = data->padding.width;\n  op_params.float_activation_min = data->activation_min_f32;\n  op_params.float_activation_max = data->activation_max_f32;\n  reference_ops::AveragePool(op_params, tflite::micro::GetTensorShape(input),\n                             tflite::micro::GetTensorData<float>(input),\n                             tflite::micro::GetTensorShape(output),\n                             tflite::micro::GetTensorData<float>(output));\n}\n\nvoid AverageEvalQuantized(TfLiteContext* context, const TfLiteNode* node,\n                          const TfLitePoolParams* params, const OpData* data,\n                          const TfLiteEvalTensor* input,\n                          TfLiteEvalTensor* output) {\n  TFLITE_DCHECK(input->type == kTfLiteUInt8 || input->type == kTfLiteInt8);\n\n  PoolParams op_params;\n  op_params.stride_height = params->stride_height;\n  op_params.stride_width = params->stride_width;\n  op_params.filter_height = params->filter_height;\n  op_params.filter_width = params->filter_width;\n  op_params.padding_values.height = data->padding.height;\n  op_params.padding_values.width = data->padding.width;\n  op_params.quantized_activation_min = data->activation_min;\n  op_params.quantized_activation_max = data->activation_max;\n\n  if (input->type == kTfLiteUInt8) {\n    reference_ops::AveragePool(op_params, tflite::micro::GetTensorShape(input),\n                               tflite::micro::GetTensorData<uint8_t>(input),\n                               tflite::micro::GetTensorShape(output),\n                               tflite::micro::GetTensorData<uint8_t>(output));\n  } else {\n    reference_integer_ops::AveragePool(\n        op_params, tflite::micro::GetTensorShape(input),\n        tflite::micro::GetTensorData<int8_t>(input),\n        tflite::micro::GetTensorShape(output),\n        tflite::micro::GetTensorData<int8_t>(output));\n  }\n}\n\nvoid MaxEvalFloat(TfLiteContext* context, TfLiteNode* node,\n                  TfLitePoolParams* params, const OpData* data,\n                  const TfLiteEvalTensor* input, TfLiteEvalTensor* output) {\n  tflite::PoolParams op_params;\n  op_params.stride_height = params->stride_height;\n  op_params.stride_width = params->stride_width;\n  op_params.filter_height = params->filter_height;\n  op_params.filter_width = params->filter_width;\n  op_params.padding_values.height = data->padding.height;\n  op_params.padding_values.width = data->padding.width;\n  op_params.float_activation_min = data->activation_min_f32;\n  op_params.float_activation_max = data->activation_max_f32;\n  reference_ops::MaxPool(op_params, tflite::micro::GetTensorShape(input),\n                         tflite::micro::GetTensorData<float>(input),\n                         tflite::micro::GetTensorShape(output),\n                         tflite::micro::GetTensorData<float>(output));\n}\n\nvoid MaxEvalQuantized(TfLiteContext* context, TfLiteNode* node,\n                      TfLitePoolParams* params, const OpData* data,\n                      const TfLiteEvalTensor* input, TfLiteEvalTensor* output) {\n  tflite::PoolParams op_params;\n  op_params.stride_height = params->stride_height;\n  op_params.stride_width = params->stride_width;\n  op_params.filter_height = params->filter_height;\n  op_params.filter_width = params->filter_width;\n  op_params.padding_values.height = data->padding.height;\n  op_params.padding_values.width = data->padding.width;\n  op_params.quantized_activation_min = data->activation_min;\n  op_params.quantized_activation_max = data->activation_max;\n\n  if (input->type == kTfLiteUInt8) {\n    reference_ops::MaxPool(op_params, tflite::micro::GetTensorShape(input),\n                           tflite::micro::GetTensorData<uint8_t>(input),\n                           tflite::micro::GetTensorShape(output),\n                           tflite::micro::GetTensorData<uint8_t>(output));\n  } else {\n    reference_integer_ops::MaxPool(\n        op_params, tflite::micro::GetTensorShape(input),\n        tflite::micro::GetTensorData<int8_t>(input),\n        tflite::micro::GetTensorShape(output),\n        tflite::micro::GetTensorData<int8_t>(output));\n  }\n}\n}  // namespace\n\nTfLiteStatus AverageEval(TfLiteContext* context, TfLiteNode* node) {\n  TFLITE_DCHECK(node->builtin_data != nullptr);\n  auto* params = reinterpret_cast<TfLitePoolParams*>(node->builtin_data);\n\n  TFLITE_DCHECK(node->user_data != nullptr);\n  const OpData* data = static_cast<const OpData*>(node->user_data);\n\n  const TfLiteEvalTensor* input =\n      tflite::micro::GetEvalInput(context, node, kInputTensor);\n  TfLiteEvalTensor* output =\n      tflite::micro::GetEvalOutput(context, node, kOutputTensor);\n\n  // Inputs and outputs share the same type, guaranteed by the converter.\n  switch (input->type) {\n    case kTfLiteFloat32:\n      AverageEvalFloat(context, node, params, data, input, output);\n      break;\n    case kTfLiteUInt8:\n    case kTfLiteInt8:\n      AverageEvalQuantized(context, node, params, data, input, output);\n      break;\n    default:\n      TF_LITE_KERNEL_LOG(context, \"Input type %s is not currently supported\",\n                         TfLiteTypeGetName(input->type));\n      return kTfLiteError;\n  }\n  return kTfLiteOk;\n}\n\nTfLiteStatus MaxEval(TfLiteContext* context, TfLiteNode* node) {\n  TFLITE_DCHECK(node->builtin_data != nullptr);\n  auto* params = reinterpret_cast<TfLitePoolParams*>(node->builtin_data);\n\n  TFLITE_DCHECK(node->user_data != nullptr);\n  const OpData* data = static_cast<const OpData*>(node->user_data);\n\n  const TfLiteEvalTensor* input =\n      tflite::micro::GetEvalInput(context, node, kInputTensor);\n  TfLiteEvalTensor* output =\n      tflite::micro::GetEvalOutput(context, node, kOutputTensor);\n\n  switch (input->type) {\n    case kTfLiteFloat32:\n      MaxEvalFloat(context, node, params, data, input, output);\n      break;\n    case kTfLiteUInt8:\n    case kTfLiteInt8:\n      MaxEvalQuantized(context, node, params, data, input, output);\n      break;\n    default:\n      TF_LITE_KERNEL_LOG(context, \"Type %s not currently supported.\",\n                         TfLiteTypeGetName(input->type));\n      return kTfLiteError;\n  }\n  return kTfLiteOk;\n}\n\nvoid* Init(TfLiteContext* context, const char* buffer, size_t length) {\n  TFLITE_DCHECK(context->AllocatePersistentBuffer != nullptr);\n  return context->AllocatePersistentBuffer(context, sizeof(OpData));\n}\n\nTfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n  TFLITE_DCHECK(node->builtin_data != nullptr);\n  auto* params = reinterpret_cast<TfLitePoolParams*>(node->builtin_data);\n\n  TFLITE_DCHECK(node->user_data != nullptr);\n  OpData* data = static_cast<OpData*>(node->user_data);\n\n  const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n  TF_LITE_ENSURE(context, input != nullptr);\n  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n  TF_LITE_ENSURE(context, output != nullptr);\n\n  TF_LITE_ENSURE_STATUS(CalculateOpData(context, params, input, output, data));\n\n  if (input->type == kTfLiteFloat32) {\n    CalculateActivationRange(params->activation, &data->activation_min_f32,\n                             &data->activation_max_f32);\n  } else if (input->type == kTfLiteInt8 || input->type == kTfLiteUInt8) {\n    CalculateActivationRangeQuantized(context, params->activation, output,\n                                      &data->activation_min,\n                                      &data->activation_max);\n  }\n\n  return kTfLiteOk;\n}\n\n}  // namespace pooling\n\nTfLiteRegistration Register_AVERAGE_POOL_2D() {\n  return {/*init=*/pooling::Init,\n          /*free=*/nullptr,\n          /*prepare=*/pooling::Prepare,\n          /*invoke=*/pooling::AverageEval,\n          /*profiling_string=*/nullptr,\n          /*builtin_code=*/0,\n          /*custom_name=*/nullptr,\n          /*version=*/0};\n}\n\nTfLiteRegistration Register_MAX_POOL_2D() {\n  return {/*init=*/pooling::Init,\n          /*free=*/nullptr,\n          /*prepare=*/pooling::Prepare,\n          /*invoke=*/pooling::MaxEval,\n          /*profiling_string=*/nullptr,\n          /*builtin_code=*/0,\n          /*custom_name=*/nullptr,\n          /*version=*/0};\n}\n\n}  // namespace micro\n}  // namespace ops\n}  // namespace tflite"