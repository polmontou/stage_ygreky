"/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/lite/delegates/gpu/common/model_builder.h\"\n\n#include <algorithm>\n#include <any>\n#include <cstdint>\n#include <map>\n#include <memory>\n#include <optional>\n#include <set>\n#include <string>\n#include <utility>\n#include <variant>\n#include <vector>\n\n#include \"absl/base/attributes.h\"\n#include \"absl/container/flat_hash_map.h\"\n#include \"absl/strings/str_cat.h\"\n#include \"absl/strings/str_join.h\"\n#include \"absl/strings/string_view.h\"\n#include \"tensorflow/lite/builtin_ops.h\"\n#include \"tensorflow/lite/c/builtin_op_data.h\"\n#include \"tensorflow/lite/c/common.h\"\n#include \"tensorflow/lite/delegates/gpu/common/custom_parsers.h\"\n#include \"tensorflow/lite/delegates/gpu/common/data_type.h\"\n#include \"tensorflow/lite/delegates/gpu/common/lstm_parser.h\"\n#include \"tensorflow/lite/delegates/gpu/common/model.h\"\n#include \"tensorflow/lite/delegates/gpu/common/model_builder_helper.h\"\n#include \"tensorflow/lite/delegates/gpu/common/model_transformer.h\"\n#include \"tensorflow/lite/delegates/gpu/common/object_reader.h\"\n#include \"tensorflow/lite/delegates/gpu/common/operations.h\"\n#include \"tensorflow/lite/delegates/gpu/common/shape.h\"\n#include \"tensorflow/lite/delegates/gpu/common/status.h\"\n#include \"tensorflow/lite/delegates/gpu/common/tensor.h\"\n#include \"tensorflow/lite/delegates/gpu/common/transformations/model_transformations.h\"\n#include \"tensorflow/lite/delegates/utils.h\"\n#include \"tensorflow/lite/kernels/internal/reference/dequantize.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_ctypes.h\"\n#include \"tensorflow/lite/kernels/kernel_util.h\"\n#include \"tensorflow/lite/util.h\"\n\nnamespace tflite {\nnamespace gpu {\nnamespace {\n\nabsl::Status CheckTensorIsAvailable(const TfLiteContext* context,\n                                    const TfLiteNode* tflite_node, int idx) {\n  // If tensor id is in range, it's guaranteed that it'll be available.\n  if (idx >= tflite_node->inputs->size) {\n    return absl::OutOfRangeError(\n        absl::StrCat(\"Requested index goes beyond array size: \", idx, \" vs \",\n                     idx, tflite_node->inputs->size));\n  }\n  return absl::OkStatus();\n}\n\n// A parser responsible for parsing TFLite operation and adding it to a graph.\nclass TFLiteOperationParser {\n public:\n  virtual ~TFLiteOperationParser() = default;\n\n  // Parses TFLite operation. This method allows expanding fused operations\n  // into more than one node.\n  virtual absl::Status Parse(const TfLiteNode* tflite_node,\n                             const TfLiteRegistration* registration,\n                             GraphFloat32* graph, ObjectReader* reader) = 0;\n\n  // Verifies whether passed tflite node may be built by GPU delegate or not.\n  virtual absl::Status IsSupported(const TfLiteContext* context,\n                                   const TfLiteNode* tflite_node,\n                                   const TfLiteRegistration* registration) = 0;\n\n  // Return the value ids in the graph that correspond to the updated values of\n  // the variable input tensor.\n  virtual absl::flat_hash_map<int, ValueId>\n  GetNewValueIdsForVariableInputNodes() {\n    return absl::flat_hash_map<int, ValueId>();\n  }\n};\n\nHW ToHW(int32_t h, int32_t w) { return HW(h > 0 ? h : 1, w > 0 ? w : 1); }\n\ntemplate <typename AttrT>\nvoid UpdatePadding(const TfLitePadding& padding, const BHWC& input_shape,\n                   AttrT* attr) {\n  if (padding == kTfLitePaddingSame) {\n    attr->padding = CalculateSamePadding(input_shape, *attr);\n  } else {\n    attr->padding.prepended = HW(0, 0);\n    attr->padding.appended = HW(0, 0);\n  }\n}\n\nabsl::Status GetFullyConnectedAttributes(int weights_tensor_id,\n                                         int bias_tensor_id,\n                                         ObjectReader* reader,\n                                         FullyConnectedAttributes* attr) {\n  Tensor<HW, DataType::FLOAT32> weights;\n  RETURN_IF_ERROR(reader->ReadTensor(weights_tensor_id, &weights));\n  attr->weights.data = std::move(weights.data);\n  attr->weights.id = weights.id;\n  attr->weights.shape.h = 1;\n  attr->weights.shape.w = 1;\n  attr->weights.shape.o = weights.shape.h;\n  attr->weights.shape.i = weights.shape.w;\n  reader->ReadTensor(bias_tensor_id, &attr->bias).IgnoreError();  // optional\n  return absl::OkStatus();\n}\n\ntemplate <typename ParamsT>\nabsl::Status RetrieveBuiltinData(const TfLiteNode* tflite_node,\n                                 const ParamsT** tf_options) {\n  *tf_options = static_cast<const ParamsT*>(tflite_node->builtin_data);\n  if (!*tf_options) {\n    return absl::InternalError(\"Unable to retrieve builtin_data.\");\n  }\n  return absl::OkStatus();\n}\n\ntemplate <typename ParamsT>\nabsl::Status RetrieveCustomInitialData(const TfLiteNode* tflite_node,\n                                       const ParamsT** tf_options) {\n  *tf_options = static_cast<const ParamsT*>(tflite_node->custom_initial_data);\n  if (!*tf_options) {\n    return absl::InternalError(\"Unable to retrieve custom_initial_data.\");\n  }\n  return absl::OkStatus();\n}\n\nabsl::Status CheckMaxSupportedOpVersion(const TfLiteRegistration* registration,\n                                        int max_version) {\n  const int op_version = registration->version;\n  if (op_version > max_version) {\n    return absl::UnimplementedError(\n        absl::StrCat(\"Max version supported: \", max_version,\n                     \". Requested version \", op_version, \".\"));\n  }\n  return absl::OkStatus();\n}\n\nabsl::Status CheckKernels(int kernel_h, int kernel_w) {\n  if (kernel_h <= 0 || kernel_w <= 0) {\n    return absl::InvalidArgumentError(\n        absl::StrCat(\"Incorrect kernel values: kernel_height = \", kernel_h,\n                     \", kernel_width = \", kernel_w));\n  }\n  return absl::OkStatus();\n}\n\nabsl::Status CheckStrides(int strides_h, int strides_w) {\n  if (strides_h <= 0 || strides_w <= 0) {\n    return absl::InvalidArgumentError(\n        absl::StrCat(\"Incorrect stride values: stride_height = \", strides_h,\n                     \", stride_width = \", strides_w));\n  }\n  return absl::OkStatus();\n}\n\nabsl::Status CheckDilation(int dilation_h, int dilation_w) {\n  if (dilation_h <= 0 || dilation_w <= 0) {\n    return absl::InvalidArgumentError(absl::StrCat(\n        \"Incorrect dilation values: dilation_factor = \", dilation_h,\n        \", dilation_factor = \", dilation_w));\n  }\n  return absl::OkStatus();\n}\n\nabsl::Status CheckStridesAndDilation(int strides_h, int strides_w,\n                                     int dilation_h, int dilation_w) {\n  RETURN_IF_ERROR(CheckStrides(strides_h, strides_w));\n  RETURN_IF_ERROR(CheckDilation(dilation_h, dilation_w));\n  return absl::OkStatus();\n}\n\nabsl::Status CheckKernelsAndStrides(int kernel_h, int kernel_w, int strides_h,\n                                    int strides_w) {\n  RETURN_IF_ERROR(CheckKernels(kernel_h, kernel_w));\n  RETURN_IF_ERROR(CheckStrides(strides_h, strides_w));\n  return absl::OkStatus();\n}\n\n// Creates a simple node that holds tensor value.\nabsl::Status NewConstNode(TensorFloat32 t, GraphFloat32* graph, Value** value) {\n  ConstTensorAttributes attr;\n  attr.tensor = std::move(t);\n  Node* node = graph->NewNode();\n  node->operation.attributes = attr;\n  node->operation.type = ToString(OperationType::CONST);\n  *value = graph->NewValue();\n  RETURN_IF_ERROR(graph->SetProducer(node->id, (*value)->id));\n  // Keep data inside this tensor.\n  (*value)->tensor.ref = attr.tensor.id;\n  (*value)->tensor.type = attr.tensor.kType;\n  (*value)->tensor.shape = attr.tensor.shape;\n  return absl::OkStatus();\n}\n\nabsl::Status ParsePoolingAttributes(const TfLitePoolParams* tf_options,\n                                    const BHWC& input_shape,\n                                    Pooling2DAttributes* attr) {\n  attr->kernel = ToHW(tf_options->filter_height, tf_options->filter_width);\n  attr->strides = ToHW(tf_options->stride_height, tf_options->stride_width);\n  UpdatePadding(tf_options->padding, input_shape, attr);\n  return absl::OkStatus();\n}\n\nabsl::Status ParseInputsWithConstTensor(Node* node, ObjectReader* reader,\n                                        TensorOrScalar* tensor_or_scalar) {\n  const std::string& opname = node->operation.type;\n\n  // Determine runtime/constant tensors.\n  const TfLiteTensor* input0 = reader->GetInputTensor(0);\n  if (!input0) {\n    return absl::InvalidArgumentError(\"Couldn't get the 1st input tensor for \" +\n                                      opname);\n  }\n  const TfLiteTensor* input1 = reader->GetInputTensor(1);\n  if (!input1) {\n    return absl::InvalidArgumentError(\"Couldn't get the 2nd input tensor for \" +\n                                      opname);\n  }\n  const bool constant_tensor0 = IsConstantTensor(input0);\n  const bool constant_tensor1 = IsConstantTensor(input1);\n  if (constant_tensor0 && constant_tensor1) {\n    return absl::InvalidArgumentError(\"No runtime input tensors for \" + opname);\n  }\n  const bool runtime_tensor0 = !constant_tensor0;\n  const bool runtime_tensor1 = !constant_tensor1;\n\n  if (runtime_tensor0 && runtime_tensor1) {\n    RETURN_IF_ERROR(reader->AddInput(node, 0));\n    RETURN_IF_ERROR(reader->AddInput(node, 1));\n  } else {\n    int runtime_tensor = 0;\n    int constant_tensor = 1;\n    TfLiteIntArray* constant_dims = input1->dims;\n    if (constant_tensor0 && runtime_tensor1) {\n      runtime_tensor = 1;\n      constant_tensor = 0;\n      constant_dims = input0->dims;\n    }\n    RETURN_IF_ERROR(reader->AddInput(node, runtime_tensor));\n    if (constant_dims->size <= 0 || NumElements(constant_dims) == 1) {\n      Tensor<Scalar, DataType::FLOAT32> tensor;\n      RETURN_IF_ERROR(reader->ReadTensor(constant_tensor, &tensor));\n      *tensor_or_scalar = tensor.data[0];\n    } else {\n      if (CheckIfLinearConvertible(constant_dims).ok()) {\n        Tensor<Linear, DataType::FLOAT32> tensor;\n        RETURN_IF_ERROR(reader->ReadTensor(constant_tensor, &tensor));\n        *tensor_or_scalar = std::move(tensor);\n      } else {\n        Tensor<HWC, DataType::FLOAT32> tensor;\n        RETURN_IF_ERROR(reader->ReadTensor(constant_tensor, &tensor));\n        *tensor_or_scalar = std::move(tensor);\n      }\n    }\n  }\n  return absl::OkStatus();\n}\n\nclass AddOperationParser : public TFLiteOperationParser {\n public:\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    RETURN_IF_ERROR(CheckMaxSupportedOpVersion(registration, 2));\n    if (tflite_node->inputs->size != 2) {\n      return absl::UnimplementedError(\"ADD requires two input tensors.\");\n    }\n    // TODO(eignasheva): Add shapes check.\n\n    const TfLiteAddParams* tf_options;\n    return RetrieveBuiltinData(tflite_node, &tf_options);\n  }\n\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    // TFLite currently only supports 2 input ADDs.  Thus, the logic below only\n    // considers 2 input cases.  The underlying GPU shader programs can accept\n    // more inputs, but the logic below would have to be expanded.\n\n    Node* node = graph->NewNode();\n    node->operation.type = ToString(OperationType::ADD);\n    RETURN_IF_ERROR(reader->AddOutputs(node));\n    ElementwiseAttributes attr;\n    RETURN_IF_ERROR(ParseInputsWithConstTensor(node, reader, &attr.param));\n    node->operation.attributes = std::move(attr);\n    const TfLiteAddParams* tf_options;\n    RETURN_IF_ERROR(RetrieveBuiltinData(tflite_node, &tf_options));\n    return MaybeFuseActivation(tf_options->activation, graph, node);\n  }\n};\n\nclass BatchedMatMulOperationParser : public TFLiteOperationParser {\n public:\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    return CheckInputsOutputs(context, tflite_node,\n                              /*runtime_inputs=*/2, /*outputs=*/1);\n  }\n\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    Node* node = graph->NewNode();\n    node->operation.type = ToString(OperationType::BATCHED_MATMUL);\n    RETURN_IF_ERROR(reader->AddInput(node, 0));\n    RETURN_IF_ERROR(reader->AddInput(node, 1));\n    RETURN_IF_ERROR(reader->AddOutputs(node));\n    return absl::OkStatus();\n  }\n};\n\nclass ConcatenationOperationParser : public TFLiteOperationParser {\n public:\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    RETURN_IF_ERROR(CheckMaxSupportedOpVersion(registration, 2));\n\n    // TODO(eignasheva): add proper tensor availability checking\n    // for (uint32_t idx = 0; idx < tflite_node->inputs->size; ++idx) {\n    //   RETURN_IF_ERROR(CheckTensorIsAvailable(context, tflite_node, idx));\n    // }\n    // TODO(eignasheva): add axis checking.\n    const TfLiteConcatenationParams* tf_options;\n    RETURN_IF_ERROR(RetrieveBuiltinData(tflite_node, &tf_options));\n    return absl::OkStatus();\n  }\n\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    ConcatAttributes attr;\n    // Read inputs first to make sure const node is added to a graph before\n    // concat node to ensure topological order.\n    std::vector<const Value*> inputs;\n    for (uint32_t idx = 0; idx < tflite_node->inputs->size; ++idx) {\n      Value* value;\n      const auto status = reader->ReadValue(idx, &value);\n      if (status.ok()) {\n        inputs.push_back(value);\n      } else {\n        TensorFloat32 tensor;\n        RETURN_IF_ERROR(reader->ReadTensor(idx, &tensor));\n        Value* value;\n        RETURN_IF_ERROR(NewConstNode(std::move(tensor), graph, &value));\n        inputs.push_back(value);\n      }\n    }\n\n    Node* node = graph->NewNode();\n    node->operation.type = ToString(OperationType::CONCAT);\n    RETURN_IF_ERROR(reader->AddOutputs(node));\n    for (const Value* input : inputs) {\n      RETURN_IF_ERROR(graph->AddConsumer(node->id, input->id));\n    }\n\n    std::vector<BHWC> input_shapes;\n    for (auto input : graph->FindInputs(node->id)) {\n      input_shapes.push_back(input->tensor.shape);\n    }\n    RETURN_IF_ERROR(SetAxis(input_shapes, &attr.axis));\n\n    // Guess axis.\n    BHWC output_shape = graph->FindOutputs(node->id)[0]->tensor.shape;\n    for (auto input : graph->FindInputs(node->id)) {\n      if (input->tensor.shape.h != output_shape.h) {\n        attr.axis = Axis::HEIGHT;\n        break;\n      }\n      if (input->tensor.shape.w != output_shape.w) {\n        attr.axis = Axis::WIDTH;\n        break;\n      }\n      if (input->tensor.shape.c != output_shape.c) {\n        attr.axis = Axis::CHANNELS;\n        break;\n      }\n    }\n    const TfLiteConcatenationParams* tf_options;\n    RETURN_IF_ERROR(RetrieveBuiltinData(tflite_node, &tf_options));\n    RETURN_IF_ERROR(MaybeFuseActivation(tf_options->activation, graph, node));\n    node->operation.attributes = attr;\n    return absl::OkStatus();\n  }\n\n private:\n  absl::Status SetAxis(const std::vector<BHWC>& input_shapes, Axis* axis) {\n    *axis = Axis::BATCH;\n    for (int i = 1; i < input_shapes.size(); i++) {\n      if (input_shapes[0].h != input_shapes[i].h &&\n          input_shapes[0].w != input_shapes[i].w &&\n          input_shapes[0].c != input_shapes[i].c) {\n        *axis = Axis::HEIGHT;\n        break;\n      }\n    }\n    if (*axis == Axis::BATCH) return absl::OkStatus();\n    for (int i = 1; i < input_shapes.size(); i++) {\n      if (input_shapes[0].b != input_shapes[i].b &&\n          input_shapes[0].w != input_shapes[i].w &&\n          input_shapes[0].c != input_shapes[i].c) {\n        *axis = Axis::WIDTH;\n        break;\n      }\n    }\n    if (*axis == Axis::HEIGHT) return absl::OkStatus();\n    for (int i = 1; i < input_shapes.size(); i++) {\n      if (input_shapes[0].b != input_shapes[i].b &&\n          input_shapes[0].h != input_shapes[i].h &&\n          input_shapes[0].c != input_shapes[i].c) {\n        *axis = Axis::CHANNELS;\n        break;\n      }\n    }\n    if (*axis == Axis::WIDTH) return absl::OkStatus();\n    for (int i = 1; i < input_shapes.size(); i++) {\n      if (input_shapes[0].b != input_shapes[i].b &&\n          input_shapes[0].w != input_shapes[i].w &&\n          input_shapes[0].h != input_shapes[i].h) {\n        return absl::UnimplementedError(\n            \"Can concatenate tensors only by batch, height, width, or \"\n            \"channels.\");\n      }\n    }\n    return absl::OkStatus();\n  }\n};\n\nclass Conv2DOperationParser : public TFLiteOperationParser {\n public:\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    RETURN_IF_ERROR(CheckMaxSupportedOpVersion(registration, 5));\n    const int runtime_inputs =\n        GetNumberOfRuntimeInputsForNode(context, tflite_node);\n    if (runtime_inputs > 2) {\n      return absl::InternalError(\n          absl::StrCat(\"Expected 1 or 2 input tensor(s), but node has \",\n                       runtime_inputs, \" runtime inputs.\"));\n    }\n    const int runtime_outputs = NumOutputs(tflite_node);\n    if (runtime_outputs != 1) {\n      return absl::InternalError(\n          absl::StrCat(\"Expected 1 output tensor(s), but node has \",\n                       runtime_outputs, \" runtime outputs.\"));\n    }\n    if (runtime_inputs == 1) {\n      RETURN_IF_ERROR(CheckTensorIsAvailable(context, tflite_node, 1));\n    }\n    const TfLiteConvParams* tf_options;\n    RETURN_IF_ERROR(RetrieveBuiltinData(tflite_node, &tf_options));\n    RETURN_IF_ERROR(CheckStridesAndDilation(\n        tf_options->stride_height, tf_options->stride_width,\n        tf_options->dilation_height_factor, tf_options->dilation_width_factor));\n    return IsActivationSupported(tf_options->activation);\n  }\n\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    Node* node = graph->NewNode();\n    node->operation.type = ToString(OperationType::CONVOLUTION_2D);\n    RETURN_IF_ERROR(reader->AddInput(node, 0));\n    RETURN_IF_ERROR(reader->AddOutputs(node));\n\n    Convolution2DAttributes attr;\n    const int runtime_inputs = reader->GetNumberOfRuntimeInputs();\n    if (runtime_inputs == 2) {\n      RETURN_IF_ERROR(reader->AddInput(node, 1));\n    } else {  // runtime_inputs == 1;\n      RETURN_IF_ERROR(reader->ReadTensor(1, &attr.weights));\n    }\n    reader->ReadTensor(2, &attr.bias).IgnoreError();  // bias is optional\n\n    const TfLiteConvParams* tf_options;\n    RETURN_IF_ERROR(RetrieveBuiltinData(tflite_node, &tf_options));\n    attr.strides = ToHW(tf_options->stride_height, tf_options->stride_width);\n    attr.dilations = HW(tf_options->dilation_height_factor,\n                        tf_options->dilation_width_factor);\n    UpdatePadding(tf_options->padding,\n                  graph->FindInputs(node->id)[0]->tensor.shape, &attr);\n    RETURN_IF_ERROR(MaybeFuseActivation(tf_options->activation, graph, node));\n    node->operation.attributes = std::move(attr);\n    return absl::OkStatus();\n  }\n};\n\nclass Convolution2DTransposeBiasParser : public TFLiteOperationParser {\n public:\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    RETURN_IF_ERROR(CheckTensorIsAvailable(context, tflite_node, 1));\n    const TfLiteTransposeConvParams* tf_options;\n    RETURN_IF_ERROR(RetrieveCustomInitialData(tflite_node, &tf_options));\n    RETURN_IF_ERROR(\n        CheckStrides(tf_options->stride_height, tf_options->stride_width));\n    return absl::OkStatus();\n  }\n\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    auto* node = graph->NewNode();\n    node->operation.type = ToString(OperationType::CONVOLUTION_TRANSPOSED);\n    RETURN_IF_ERROR(reader->AddInput(node, 0));\n    RETURN_IF_ERROR(reader->AddOutputs(node));\n\n    const TfLiteTransposeConvParams* tf_options;\n    auto status = RetrieveCustomInitialData(tflite_node, &tf_options);\n\n    ConvolutionTransposedAttributes attr;\n    attr.stride = status.ok()\n                      ? HW(tf_options->stride_height, tf_options->stride_width)\n                      : HW(1, 1);\n\n    RETURN_IF_ERROR(reader->ReadTensor(1, &attr.weights));\n    reader->ReadTensor(2, &attr.bias).IgnoreError();  // bias is optional\n\n    UpdatePadding(status.ok() ? tf_options->padding : kTfLitePaddingUnknown,\n                  graph->FindInputs(node->id)[0]->tensor.shape, &attr);\n\n    node->operation.attributes = std::move(attr);\n    return absl::OkStatus();\n  }\n};\n\nclass DepthwiseConvolutionOperationParser : public TFLiteOperationParser {\n public:\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    RETURN_IF_ERROR(CheckMaxSupportedOpVersion(registration, 6));\n    RETURN_IF_ERROR(CheckInputsOutputs(context, tflite_node,\n                                       /*runtime_inputs=*/1, /*outputs=*/1));\n    RETURN_IF_ERROR(CheckTensorIsAvailable(context, tflite_node, 1));\n    const TfLiteDepthwiseConvParams* tf_options;\n    RETURN_IF_ERROR(RetrieveBuiltinData(tflite_node, &tf_options));\n    RETURN_IF_ERROR(CheckStridesAndDilation(\n        tf_options->stride_height, tf_options->stride_width,\n        tf_options->dilation_height_factor, tf_options->dilation_width_factor));\n    RETURN_IF_ERROR(IsActivationSupported(tf_options->activation));\n\n    const int depth_multiplier = tf_options->depth_multiplier;\n    const auto* input = context->tensors + tflite_node->inputs->data[0];\n    const auto* filter = context->tensors + tflite_node->inputs->data[1];\n    const auto* bias = tflite_node->inputs->size > 2\n                           ? context->tensors + tflite_node->inputs->data[2]\n                           : nullptr;\n    const auto* output = context->tensors + tflite_node->outputs->data[0];\n    if (!input->dims || input->dims->size != 4) {\n      return absl::InvalidArgumentError(\"input.dims.size != 4\");\n    }\n    if (!filter->dims || filter->dims->size != 4) {\n      return absl::InvalidArgumentError(\"filter.dims.size != 4\");\n    }\n    if (!output->dims || output->dims->size != 4) {\n      return absl::InvalidArgumentError(\"output.dims.size != 4\");\n    }\n    if (input->dims->data[0] != output->dims->data[0]) {\n      return absl::InvalidArgumentError(\"input.b != output.b\");\n    }\n    const int input_depth = input->dims->data[3];\n    const int output_depth = output->dims->data[3];\n    if (filter->dims->data[3] != output_depth) {\n      return absl::InvalidArgumentError(\"filter.i != output.c\");\n    }\n    if (output_depth != input_depth * depth_multiplier) {\n      return absl::InvalidArgumentError(\n          \"output.c != input.c * depth_multiplier\");\n    }\n    if (bias && NumElements(bias) != output_depth) {\n      return absl::InvalidArgumentError(\"bias.size != output.c\");\n    }\n    if (depth_multiplier != 1 && input_depth != 1) {\n      return absl::UnimplementedError(\"depth_multiplier != 1 && input.c != 1\");\n    }\n    return absl::OkStatus();\n  }\n\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    Node* node = graph->NewNode();\n    node->operation.type = ToString(OperationType::DEPTHWISE_CONVOLUTION);\n    RETURN_IF_ERROR(reader->AddInput(node, 0));\n    RETURN_IF_ERROR(reader->AddOutputs(node));\n\n    DepthwiseConvolution2DAttributes attr;\n    RETURN_IF_ERROR(reader->ReadTensor(1, &attr.weights));\n    reader->ReadTensor(2, &attr.bias).IgnoreError();  // bias is optional\n    const TfLiteDepthwiseConvParams* tf_options;\n    RETURN_IF_ERROR(RetrieveBuiltinData(tflite_node, &tf_options));\n    attr.strides = ToHW(tf_options->stride_height, tf_options->stride_width);\n    attr.dilations = HW(std::max(1, tf_options->dilation_height_factor),\n                        std::max(1, tf_options->dilation_width_factor));\n    UpdatePadding(tf_options->padding,\n                  graph->FindInputs(node->id)[0]->tensor.shape, &attr);\n    RETURN_IF_ERROR(MaybeFuseActivation(tf_options->activation, graph, node));\n    const int depth_multiplier = tf_options->depth_multiplier;\n    if (depth_multiplier != 1) {\n      const TfLiteTensor* input = reader->GetInputTensor(0);\n      const TfLiteTensor* filter = reader->GetInputTensor(1);\n      const TfLiteTensor* output = reader->GetOutputTensor(0);\n      TransposeWeights(input, filter, output, depth_multiplier, &attr);\n    }\n    node->operation.attributes = std::move(attr);\n    return absl::OkStatus();\n  }\n\n private:\n  // TFLite CPU stores weights as:\n  //   [1, kernel_height, kernel_width, input_depth * depth_multiplier]\n  // TFLite GPU stores weights as:\n  //   [depth_multiplier, kernel_height, kernel_width, input_depth]\n  static void TransposeWeights(const TfLiteTensor* input,\n                               const TfLiteTensor* filter,\n                               const TfLiteTensor* output, int depth_multiplier,\n                               DepthwiseConvolution2DAttributes* attr) {\n    const int input_depth = input->dims->data[3];\n    const int filter_height = filter->dims->data[1];\n    const int filter_width = filter->dims->data[2];\n    const int output_depth = output->dims->data[3];\n    Tensor<OHWI, DataType::FLOAT32> weights;\n    weights.id = attr->weights.id;\n    weights.shape =\n        OHWI(output_depth, filter_height, filter_width, input_depth);\n    weights.data.resize(weights.shape.DimensionsProduct());\n    float* dst = &weights.data[0];\n    for (int j = 0; j < output_depth; ++j) {\n      const float* src = attr->weights.data.data() + j;\n      for (int i = 0; i < filter_height * filter_width; ++i) {\n        *dst = *src;\n        dst++;\n        src += output_depth;\n      }\n    }\n    attr->weights = std::move(weights);\n  }\n};\n\nclass DequantizeOperationParser : public TFLiteOperationParser {\n public:\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    RETURN_IF_ERROR(CheckMaxSupportedOpVersion(registration, 2));\n    RETURN_IF_ERROR(CheckInputsOutputs(context, tflite_node,\n                                       /*runtime_inputs=*/1, /*outputs=*/1));\n    return absl::OkStatus();\n  }\n\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    // 'Dequantize' is rewritten as QuantizeAndDequantize since we are dealing\n    // with floating-point versions of the original tensors.\n    Node* node = graph->NewNode();\n    node->operation.type = ToString(OperationType::QUANTIZE_AND_DEQUANTIZE);\n    RETURN_IF_ERROR(reader->AddInput(node, 0));\n    RETURN_IF_ERROR(reader->AddOutputs(node));\n\n    // Quantization attributes should already be present in the input tensor.\n    auto input_value = graph->FindInputs(node->id)[0];\n    if (!input_value->quant_params) {\n      return absl::InvalidArgumentError(\n          \"Encountered Dequantize input with no quant params\");\n    }\n    QuantizeAndDequantizeAttributes attr;\n    attr.min = input_value->quant_params.value().min;\n    attr.max = input_value->quant_params.value().max;\n    attr.scale = input_value->quant_params.value().scale;\n\n    node->operation.attributes = attr;\n    return absl::OkStatus();\n  }\n};\n\nclass ElementwiseOperationParser : public TFLiteOperationParser {\n public:\n  explicit ElementwiseOperationParser(OperationType operation_type)\n      : operation_type_(operation_type) {}\n\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    RETURN_IF_ERROR(CheckMaxSupportedOpVersion(registration, 2));\n    if (IsOneArgumentOperation()) {\n      RETURN_IF_ERROR(CheckInputsConstsOutputs(context, tflite_node,\n                                               /*runtime_inputs=*/1,\n                                               /*const_inputs=*/0,\n                                               /*outputs=*/1));\n      // For some elementwise operations (currently only for SUB operation)\n      // second condition may be false. But it's worth checking the next case\n      // with const input, which may be supported.\n    } else if (IsTwoArgumentOperation() &&\n               CheckInputsConstsOutputs(context, tflite_node,\n                                        /*runtime_inputs=*/2,\n                                        /*const_inputs=*/0,\n                                        /*outputs=*/1)\n                   .ok()) {\n    } else if (IsTwoArgumentOperationWithConst()) {\n      RETURN_IF_ERROR(CheckInputsConstsOutputs(context, tflite_node,\n                                               /*runtime_inputs=*/1,\n                                               /*const_inputs=*/1,\n                                               /*outputs=*/1));\n    } else {\n      return absl::InvalidArgumentError(\n          \"Op can only handle 1 or 2 operand(s).\");\n    }\n    TfLiteFusedActivation activation;\n    RETURN_IF_ERROR(GetActivation(tflite_node, &activation));\n    return IsActivationSupported(activation);\n  }\n\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    Node* node = graph->NewNode();\n    node->operation.type = ToString(operation_type_);\n\n    if (IsOneArgumentOperation()) {\n      RETURN_IF_ERROR(reader->VerifyInputsConstsOutputs(tflite_node,\n                                                        /*runtime_inputs=*/1,\n                                                        /*const_inputs=*/0,\n                                                        /*outputs=*/1));\n\n      RETURN_IF_ERROR(reader->AddInput(node, 0));\n    } else if (IsTwoArgumentOperation() &&\n               reader\n                   ->VerifyInputsConstsOutputs(tflite_node,\n                                               /*runtime_inputs=*/2,\n                                               /*const_inputs=*/0,\n                                               /*outputs=*/1)\n                   .ok()) {\n      if (tflite_node->inputs->size != 2) {\n        return absl::InvalidArgumentError(\"Applies only two input tensors\");\n      }\n      RETURN_IF_ERROR(reader->AddInput(node, 0));\n      RETURN_IF_ERROR(reader->AddInput(node, 1));\n\n      TfLiteFusedActivation activation = kTfLiteActNone;\n      switch (operation_type_) {\n        case OperationType::SUB: {\n          const TfLiteSubParams* tf_options;\n          if (RetrieveBuiltinData(tflite_node, &tf_options).ok()) {\n            activation = tf_options->activation;\n          }\n          break;\n        }\n        case OperationType::DIV: {\n          const TfLiteDivParams* tf_options;\n          if (RetrieveBuiltinData(tflite_node, &tf_options).ok()) {\n            activation = tf_options->activation;\n          }\n          break;\n        }\n        default:\n          // No activation expected.\n          activation = kTfLiteActNone;\n      }\n\n      if (activation) {\n        RETURN_IF_ERROR(MaybeFuseActivation(activation, graph, node));\n      }\n    } else if (IsTwoArgumentOperationWithConst()) {\n      RETURN_IF_ERROR(reader->VerifyInputsConstsOutputs(tflite_node,\n                                                        /*runtime_inputs=*/1,\n                                                        /*const_inputs=*/1,\n                                                        /*outputs=*/1));\n      ElementwiseAttributes attr;\n      RETURN_IF_ERROR(ParseInputsWithConstTensor(node, reader, &attr.param));\n      attr.runtime_tensor_is_second =\n          IsConstantTensor(reader->GetInputTensor(0));\n      node->operation.attributes = std::move(attr);\n    } else {\n      return absl::InvalidArgumentError(\"Incorrect operation type passed\");\n    }\n\n    return reader->AddOutputs(node);\n  }\n\n private:\n  absl::Status GetActivation(const TfLiteNode* tflite_node,\n                             TfLiteFusedActivation* activation) const {\n    if (operation_type_ == OperationType::DIV) {\n      const TfLiteDivParams* tf_options;\n      auto status = RetrieveBuiltinData(tflite_node, &tf_options);\n      *activation = status.ok() ? tf_options->activation : kTfLiteActNone;\n      return absl::OkStatus();\n    }\n    if (operation_type_ == OperationType::SUB) {\n      const TfLiteSubParams* tf_options;\n      auto status = RetrieveBuiltinData(tflite_node, &tf_options);\n      *activation = status.ok() ? tf_options->activation : kTfLiteActNone;\n      return absl::OkStatus();\n    }\n\n    // Return kTfLiteActNone as other ops either do not have TfLiteXxxParams or\n    // TfLiteXxxParams.activation.\n    *activation = kTfLiteActNone;\n    return absl::OkStatus();\n  }\n\n  bool IsOneArgumentOperation() const {\n    switch (operation_type_) {\n      case OperationType::ABS:\n      case OperationType::COPY:\n      case OperationType::COS:\n      case OperationType::ELU:\n      case OperationType::EXP:\n      case OperationType::LOG:\n      case OperationType::NEG:\n      case OperationType::RSQRT:\n      case OperationType::SIGMOID:\n      case OperationType::SIN:\n      case OperationType::SQRT:\n      case OperationType::SQUARE:\n      case OperationType::TANH:\n        return true;\n      default:\n        return false;\n    }\n  }\n\n  bool IsTwoArgumentOperation() const {\n    switch (operation_type_) {\n      case OperationType::DIV:\n      case OperationType::MAXIMUM:\n      case OperationType::MINIMUM:\n      case OperationType::POW:\n      case OperationType::SQUARED_DIFF:\n      case OperationType::SUB:\n        return true;\n      default:\n        return false;\n    }\n  }\n\n  bool IsTwoArgumentOperationWithConst() const {\n    switch (operation_type_) {\n      case OperationType::DIV:\n      case OperationType::MAXIMUM:\n      case OperationType::MINIMUM:\n      case OperationType::POW:\n      case OperationType::SQUARED_DIFF:\n      case OperationType::SUB:\n        return true;\n      default:\n        return false;\n    }\n  }\n\n  OperationType operation_type_;\n};\n\nclass FullyConnectedOperationParser : public TFLiteOperationParser {\n public:\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    RETURN_IF_ERROR(CheckMaxSupportedOpVersion(registration, 4));\n    const TfLiteFullyConnectedParams* tf_options;\n    RETURN_IF_ERROR(RetrieveBuiltinData(tflite_node, &tf_options));\n    if (tf_options->weights_format !=\n        kTfLiteFullyConnectedWeightsFormatDefault) {\n      return absl::UnimplementedError(\n          \"Unsupported FullyConnected weights format.\");\n    }\n    if (GetNumberOfRuntimeInputsForNode(context, tflite_node) > 2) {\n      return absl::UnimplementedError(\n          \"FullyConnected doesn't support more than 2 runtime inputs.\");\n    }\n    // TODO(eignasheva): check input shape\n    return absl::OkStatus();\n  }\n\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    const TfLiteFullyConnectedParams* tf_options;\n    RETURN_IF_ERROR(RetrieveBuiltinData(tflite_node, &tf_options));\n\n    if (reader->GetNumberOfRuntimeInputs() == 2) {\n      // Create Convolution2D, so as it supports runtime weights.\n      Node* node = graph->NewNode();\n      node->operation.type = ToString(OperationType::CONVOLUTION_2D);\n      RETURN_IF_ERROR(reader->AddInput(node, 0));\n      RETURN_IF_ERROR(reader->AddInput(node, 1));\n      RETURN_IF_ERROR(reader->AddOutputs(node));\n\n      Convolution2DAttributes attr;\n      reader->ReadTensor(2, &attr.bias).IgnoreError();  // bias is optional\n\n      attr.strides = HW(1, 1);\n      attr.dilations = HW(1, 1);\n      attr.padding.appended = HW(0, 0);\n      attr.padding.prepended = HW(0, 0);\n      RETURN_IF_ERROR(MaybeFuseActivation(tf_options->activation, graph, node));\n      node->operation.attributes = std::move(attr);\n      return absl::OkStatus();\n    }\n    Node* node = graph->NewNode();\n    RETURN_IF_ERROR(reader->AddInput(node, 0));\n\n    if (tf_options->weights_format !=\n        kTfLiteFullyConnectedWeightsFormatDefault) {\n      return absl::UnimplementedError(\n          \"Unsupported FullyConnected weights format.\");\n    }\n\n    FullyConnectedAttributes attr;\n    RETURN_IF_ERROR(GetFullyConnectedAttributes(1, 2, reader, &attr));\n\n    Tensor<HW, DataType::FLOAT32> weights;\n    RETURN_IF_ERROR(reader->ReadTensor(1, &weights));\n    auto input = graph->FindInputs(node->id)[0];\n    int batch_size = input->tensor.shape.b;\n    if (input->tensor.shape.DimensionsProduct() / batch_size !=\n        weights.shape.w) {\n      return absl::UnimplementedError(\n          \"Amount of input data should match weights width\");\n    }\n\n    Node* conv = node;\n    if (input->tensor.shape.h != 1 || input->tensor.shape.w != 1) {\n      auto& reshape = node;\n      conv = graph->NewNode();  // reset conv pointer!\n      Value* reshaped_value = graph->NewValue();\n      reshaped_value->tensor.type = DataType::FLOAT32;\n      reshaped_value->tensor.shape =\n          BHWC(input->tensor.shape.b, 1, 1, weights.shape.w);\n      RETURN_IF_ERROR(graph->SetProducer(reshape->id, reshaped_value->id));\n      reshape->operation.type = ToString(OperationType::RESHAPE);\n      ReshapeAttributes attr;\n      attr.new_shape = reshaped_value->tensor.shape;\n      reshape->operation.attributes = attr;\n      RETURN_IF_ERROR(graph->AddConsumer(conv->id, reshaped_value->id));\n    }\n\n    conv->operation.type = ToString(OperationType::FULLY_CONNECTED);\n    conv->operation.attributes = std::move(attr);\n    absl::Status result = reader->AddOutputs(conv);\n    RETURN_IF_ERROR(MaybeFuseActivation(tf_options->activation, graph, conv));\n\n    return result;\n  }\n};\n\nclass HardSwishOperationParser : public TFLiteOperationParser {\n public:\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration*) final {\n    return CheckInputsOutputs(context, tflite_node, /*runtime_inputs=*/1,\n                              /*outputs=*/1);\n  }\n\n  absl::Status Parse(const TfLiteNode*, const TfLiteRegistration*,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    Node* node = graph->NewNode();\n    node->operation.type = ToString(OperationType::HARD_SWISH);\n    RETURN_IF_ERROR(reader->AddInput(node, 0));\n    return reader->AddOutputs(node);\n  }\n};\n\n// Basic LSTM Cell:\n//\n//  1name = name is at input  index 1\n//  name1 = name is at output index 1\n//\n//    0input     1prev_activ\n//       \\        /\n//        [[concat]]\n//             \\\n//       concat_temp2  2weights  3biases\n//              \\      /        /\n//             [[fully-connected]]\n//               \\\n//         activ_temp3    4prev_state\n//                 \\      /\n//                 [[LSTM]]\n//                 /      \\\n//           new_state1    activation0\n//\n// For full LSTM cells, see this blog post:\n// https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n// In addition to Peephole connections and Combined Input Forget Gates (CIFG)\n// described in that post, this code also adds the following optional features:\n// - Configurable activations (sigmoid or TANH)\n// - L2 Normalization of gates: https://arxiv.org/abs/1607.06450\n// - Output projection:\n//     https://www.isca-speech.org/archive/interspeech_2014/i14_0338.html\n// - Configurable clipping of cell state and output state.\nclass LSTMOperationParser : public TFLiteOperationParser {\n public:\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    RETURN_IF_ERROR(CheckMaxSupportedOpVersion(registration, 3));\n    const TfLiteLSTMParams* tf_options;\n    RETURN_IF_ERROR(RetrieveBuiltinData(tflite_node, &tf_options));\n    switch (tf_options->kernel_type) {\n      case kTfLiteLSTMFullKernel: {\n        const int inputs = NumInputs(tflite_node);\n        if (inputs != 20 && inputs != 24) {\n          return absl::InternalError(\n              absl::StrCat(\"Expected 20 or 24 input tensors, but node has \",\n                           inputs, \" input(s).\"));\n        }\n        const int runtime_outputs = NumOutputs(tflite_node);\n        if (runtime_outputs != 1) {\n          return absl::InternalError(\n              absl::StrCat(\"Expected 1 output tensor, but node has \",\n                           runtime_outputs, \" output(s).\"));\n        }\n        return CheckFullParameters(tf_options);\n      }\n      case kTfLiteLSTMBasicKernel:\n        RETURN_IF_ERROR(\n            CheckInputsConstsOutputs(context, tflite_node, /*runtime_inputs=*/3,\n                                     /*const_inputs=*/2, /*outputs=*/4));\n        return CheckBasicParameters(tf_options);\n    }\n  }\n\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    const TfLiteLSTMParams* tf_options;\n    RETURN_IF_ERROR(RetrieveBuiltinData(tflite_node, &tf_options));\n    switch (tf_options->kernel_type) {\n      case kTfLiteLSTMFullKernel:\n        return ParseFull(tflite_node, registration, graph, reader, tf_options);\n      case kTfLiteLSTMBasicKernel:\n        return ParseBasic(tflite_node, registration, graph, reader, tf_options);\n    }\n  }\n\n  absl::flat_hash_map<int, ValueId> GetNewValueIdsForVariableInputNodes()\n      final {\n    return new_variable_input_value_map_;\n  }\n\n private:\n  absl::Status ParseBasic(const TfLiteNode* tflite_node,\n                          const TfLiteRegistration* registration,\n                          GraphFloat32* graph, ObjectReader* reader,\n                          const TfLiteLSTMParams* tf_options) {\n    if (tflite_node->inputs->size != 5) {\n      return absl::InvalidArgumentError(\"LSTM should have 5 input tensors\");\n    }\n    if (tflite_node->outputs->size != 4) {\n      return absl::InvalidArgumentError(\"LSTM should have 4 output tensors\");\n    }\n    RETURN_IF_ERROR(CheckBasicParameters(tf_options));\n\n    Node* concat_node = graph->NewNode();\n    concat_node->operation.type = ToString(OperationType::CONCAT);\n    ConcatAttributes concat_attr;\n    concat_attr.axis = Axis::CHANNELS;\n    concat_node->operation.attributes = concat_attr;\n\n    Node* fc_node = graph->NewNode();\n    fc_node->operation.type = ToString(OperationType::FULLY_CONNECTED);\n    FullyConnectedAttributes fc_attr;\n    RETURN_IF_ERROR(GetFullyConnectedAttributes(2, 3, reader, &fc_attr));\n    fc_node->operation.attributes = std::move(fc_attr);\n\n    Node* lstm_node = graph->NewNode();\n    lstm_node->operation.type = ToString(OperationType::LSTM);\n    LstmAttributes lstm_attr;\n    lstm_attr.kernel_type = LstmKernelType::BASIC;\n    lstm_node->operation.attributes = lstm_attr;\n\n    Value* concat_temp;\n    int concat_tensor_idx = tflite_node->outputs->data[2];\n    RETURN_IF_ERROR(\n        reader->ReadValueByTensorIdx(concat_tensor_idx, &concat_temp));\n    Value* activ_temp;\n    int activ_tensor_idx = tflite_node->outputs->data[3];\n    RETURN_IF_ERROR(\n        reader->ReadValueByTensorIdx(activ_tensor_idx, &activ_temp));\n\n    RETURN_IF_ERROR(reader->AddInput(concat_node, 0));  // input\n    RETURN_IF_ERROR(reader->AddInput(concat_node, 1));  // prev_activ\n    RETURN_IF_ERROR(graph->SetProducer(concat_node->id, concat_temp->id));\n\n    RETURN_IF_ERROR(graph->AddConsumer(fc_node->id, concat_temp->id));\n    RETURN_IF_ERROR(graph->SetProducer(fc_node->id, activ_temp->id));\n\n    RETURN_IF_ERROR(graph->AddConsumer(lstm_node->id, activ_temp->id));\n    RETURN_IF_ERROR(reader->AddInput(lstm_node, 4));   // prev_state\n    RETURN_IF_ERROR(reader->AddOutput(lstm_node, 1));  // new_state\n    RETURN_IF_ERROR(reader->AddOutput(lstm_node, 0));  // activation\n\n    return absl::OkStatus();\n  }\n\n  absl::Status CheckBasicParameters(const TfLiteLSTMParams* tf_options) {\n    if (tf_options->activation != kTfLiteActTanh) {\n      return absl::UnimplementedError(\"Only TANH activation is supported.\");\n    }\n    if (tf_options->cell_clip != 0.0f) {\n      return absl::UnimplementedError(\"cell_clip is not supported.\");\n    }\n    if (tf_options->proj_clip != 0.0f) {\n      return absl::UnimplementedError(\"proj_clip is not supported.\");\n    }\n    return absl::OkStatus();\n  }\n\n  absl::Status ParseFull(const TfLiteNode* tflite_node,\n                         const TfLiteRegistration* registration,\n                         GraphFloat32* graph, ObjectReader* reader,\n                         const TfLiteLSTMParams* tf_options) {\n    // Invoke full LSTM parser\n    RETURN_IF_ERROR(ParseLSTMAttributes(tflite_node, registration, graph,\n                                        reader, tf_options,\n                                        &new_variable_input_value_map_));\n    return absl::OkStatus();\n  }\n\n  absl::Status CheckFullParameters(const TfLiteLSTMParams* tf_options) {\n    if (tf_options->activation != kTfLiteActSigmoid &&\n        tf_options->activation != kTfLiteActTanh) {\n      return absl::UnimplementedError(\n          \"Only sigmoid or tanh activation is supported.\");\n    }\n\n    return absl::OkStatus();\n  }\n\n  absl::flat_hash_map<int, ValueId> new_variable_input_value_map_;\n};\n\nclass MulOperationParser : public TFLiteOperationParser {\n public:\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    RETURN_IF_ERROR(CheckMaxSupportedOpVersion(registration, 3));\n    if (tflite_node->inputs->size != 2) {\n      return absl::UnimplementedError(\"MUL requires two input tensors.\");\n    }\n    auto input0 = tflite::GetInput(context, tflite_node, 0);\n    auto input1 = tflite::GetInput(context, tflite_node, 1);\n    if (input0->dims->size == input1->dims->size) {\n      // this code checks that at least one input of Mul not smaller in all\n      // dimensions. Sometimes Mul used for matrix-vector multiplication that we\n      // currently don't support. For example input0 HWC(1, 256, 1), input1\n      // HWC(1, 1, 256) -> output HWC (1, 256, 256). In this case it can be\n      // replaced with Convolution operation.\n      bool first_has_smaller_dim = false;\n      bool second_has_smaller_dim = false;\n      for (int i = 0; i < input0->dims->size; ++i) {\n        if (input0->dims->data[i] < input1->dims->data[i]) {\n          first_has_smaller_dim = true;\n        }\n        if (input1->dims->data[i] < input0->dims->data[i]) {\n          second_has_smaller_dim = true;\n        }\n      }\n      if (first_has_smaller_dim && second_has_smaller_dim) {\n        return absl::UnimplementedError(\n            \"MUL requires one tensor that not less than second in all \"\n            \"dimensions.\");\n      }\n    }\n    const TfLiteMulParams* tf_options;\n    RETURN_IF_ERROR(RetrieveBuiltinData(tflite_node, &tf_options));\n    return IsActivationSupported(tf_options->activation);\n  }\n\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    const TfLiteTensor* input0 = reader->GetInputTensor(0);\n    if (!input0) {\n      return absl::InvalidArgumentError(\n          \"Couldn't get the 1st input tensor for MUL.\");\n    }\n    const TfLiteTensor* input1 = reader->GetInputTensor(1);\n    if (!input1) {\n      return absl::InvalidArgumentError(\n          \"Couldn't get the 2nd input tensor for MUL.\");\n    }\n    const bool constant_tensor0 = IsConstantTensor(input0);\n    const bool constant_tensor1 = IsConstantTensor(input1);\n    if (constant_tensor0 && constant_tensor1) {\n      return absl::InvalidArgumentError(\"No runtime input tensors for MUL.\");\n    }\n    const bool runtime_tensor0 = !constant_tensor0;\n    const bool runtime_tensor1 = !constant_tensor1;\n\n    Node* node = graph->NewNode();\n    node->operation.type = ToString(OperationType::MUL);\n    RETURN_IF_ERROR(reader->AddOutputs(node));\n\n    // Determine runtime/constant tensors.\n    if (runtime_tensor0 && runtime_tensor1) {\n      if (input0 == input1) {\n        // replace MUL(A, A) with POW(A, 2.0)\n        // TODO(b/166831113): Support the same inputs for operations.\n        node->operation.type = ToString(OperationType::POW);\n        ElementwiseAttributes attr;\n        attr.param = 2.0f;\n        node->operation.attributes = std::move(attr);\n        return reader->AddInput(node, 0);\n      }\n\n      // The \"larger\" input tensor must be bound to 1st input and the \"smaller\"\n      // input tensor must be bound to 2nd input.\n      BHWC shape0;\n      RETURN_IF_ERROR(ExtractTensorShape(*input0, &shape0));\n      BHWC shape1;\n      RETURN_IF_ERROR(ExtractTensorShape(*input1, &shape1));\n      int input_tensor0 = 0;\n      int input_tensor1 = 1;\n      if (shape0.h <= shape1.h && shape0.w <= shape1.w &&\n          shape0.c == shape1.c) {\n        input_tensor0 = 1;\n        input_tensor1 = 0;\n      }\n      RETURN_IF_ERROR(reader->AddInput(node, input_tensor0));\n      RETURN_IF_ERROR(reader->AddInput(node, input_tensor1));\n    } else {\n      ElementwiseAttributes attr;\n      RETURN_IF_ERROR(ParseInputsWithConstTensor(node, reader, &attr.param));\n      node->operation.attributes = std::move(attr);\n    }\n\n    const TfLiteMulParams* tf_options;\n    RETURN_IF_ERROR(RetrieveBuiltinData(tflite_node, &tf_options));\n    return MaybeFuseActivation(tf_options->activation, graph, node);\n  }\n};\n\nclass PackOperationParser : public TFLiteOperationParser {\n public:\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    const TfLitePackParams* tf_options;\n    return RetrieveBuiltinData(tflite_node, &tf_options);\n  }\n\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    if (tflite_node->inputs->size == 1) {\n      // Pack with single input can be replaced with Reshape\n      Node* node = graph->NewNode();\n      node->operation.type = ToString(OperationType::RESHAPE);\n      RETURN_IF_ERROR(reader->AddInput(node, 0));\n      RETURN_IF_ERROR(reader->AddOutputs(node));\n      // New shape comes from output shape.\n      ReshapeAttributes attr;\n      attr.new_shape = graph->FindOutputs(node->id)[0]->tensor.shape;\n      node->operation.attributes = attr;\n      return absl::OkStatus();\n    } else {\n      // Pack with few inputs can be replaced with Concat\n      const TfLitePackParams* tf_options;\n      RETURN_IF_ERROR(RetrieveBuiltinData(tflite_node, &tf_options));\n\n      // Read inputs first to make sure const node is added to a graph before\n      // concat node to ensure topological order.\n      std::vector<const Value*> inputs;\n      for (uint32_t idx = 0; idx < tflite_node->inputs->size; ++idx) {\n        Value* value;\n        const auto status = reader->ReadValue(idx, &value);\n        if (status.ok()) {\n          inputs.push_back(value);\n        } else {\n          TensorFloat32 tensor;\n          RETURN_IF_ERROR(reader->ReadTensor(idx, &tensor));\n          Value* value;\n          RETURN_IF_ERROR(NewConstNode(std::move(tensor), graph, &value));\n          inputs.push_back(value);\n        }\n      }\n\n      Node* node = graph->NewNode();\n      node->operation.type = ToString(OperationType::CONCAT);\n      RETURN_IF_ERROR(reader->AddOutputs(node));\n      for (const Value* input : inputs) {\n        RETURN_IF_ERROR(graph->AddConsumer(node->id, input->id));\n      }\n      const TfLiteTensor* output = reader->GetOutputTensor(0);\n      ConcatAttributes attr;\n      RETURN_IF_ERROR(\n          ExtractAxisFromIndex(*output, tf_options->axis, &attr.axis));\n      node->operation.attributes = attr;\n      return absl::OkStatus();\n    }\n  }\n};\n\nclass PReLUOperationParser : public TFLiteOperationParser {\n public:\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    RETURN_IF_ERROR(CheckMaxSupportedOpVersion(registration, 1));\n    // TODO(eignasheva): add params check\n    return absl::OkStatus();\n  }\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    Node* node = graph->NewNode();\n    node->operation.type = ToString(OperationType::PRELU);\n    RETURN_IF_ERROR(reader->AddInput(node, 0));\n    auto input_shape = graph->FindInputs(node->id)[0]->tensor.shape;\n\n    PReLUAttributes attr;\n    Tensor<Linear, DataType::FLOAT32> linear_alpha;\n    absl::Status status = reader->ReadTensor(1, &linear_alpha);\n    if (status.ok()) {\n      if (linear_alpha.shape.v != input_shape.c) {\n        return absl::InvalidArgumentError(\n            \"Linear alpha shape does not match the number of input channels.\");\n      }\n      attr.alpha = std::move(linear_alpha);\n    } else {\n      Tensor<HWC, DataType::FLOAT32> hwc_alpha;\n      RETURN_IF_ERROR(reader->ReadTensor(1, &hwc_alpha));\n      if (hwc_alpha.shape.h != input_shape.h ||\n          hwc_alpha.shape.w != input_shape.w ||\n          hwc_alpha.shape.c != input_shape.c) {\n        return absl::InvalidArgumentError(\n            \"Alpha shape does not match input shape.\");\n      }\n      attr.alpha = std::move(hwc_alpha);\n    }\n    node->operation.attributes = std::move(attr);\n    return reader->AddOutputs(node);\n  }\n};\n\nclass PadOperationParser : public TFLiteOperationParser {\n public:\n  explicit PadOperationParser(bool mirror_pad) : mirror_pad_(mirror_pad) {}\n\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    if (mirror_pad_) {\n      const TfLiteMirrorPaddingParams* tf_options;\n      RETURN_IF_ERROR(RetrieveBuiltinData(tflite_node, &tf_options));\n      if (tf_options->mode !=\n          TfLiteMirrorPaddingMode::kTfLiteMirrorPaddingReflect) {\n        return absl::InvalidArgumentError(\n            \"Only Reflective padding is supported for Mirror Pad operation.\");\n      }\n    }\n    RETURN_IF_ERROR(CheckMaxSupportedOpVersion(registration, 2));\n    RETURN_IF_ERROR(CheckInputsOutputs(context, tflite_node,\n                                       /*runtime_inputs=*/1, /*outputs=*/1));\n    RETURN_IF_ERROR(CheckTensorIsAvailable(context, tflite_node, 1));\n    auto pad_tensor = tflite::GetInput(context, tflite_node, 1);\n    if (pad_tensor->dims->size != 2) {\n      return absl::InvalidArgumentError(absl::StrCat(\n          \"Invalid paddings tensor dimension: expected 2 dim, got \",\n          pad_tensor->dims->size, \" dim\"));\n    }\n    bool supported =\n        pad_tensor->dims->data[0] == 3 || pad_tensor->dims->data[0] == 4;\n    if (!supported || pad_tensor->dims->data[1] != 2) {\n      return absl::InvalidArgumentError(absl::StrCat(\n          \"Invalid paddings tensor shape: expected 4x2 or 3x2, got \",\n          pad_tensor->dims->data[0], \"x\", pad_tensor->dims->data[1]));\n    }\n    return absl::OkStatus();\n  }\n\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    Node* node = graph->NewNode();\n    node->operation.type = ToString(OperationType::PAD);\n    RETURN_IF_ERROR(reader->AddInput(node, 0));\n    RETURN_IF_ERROR(reader->AddOutputs(node));\n\n    PadAttributes attr;\n    if (mirror_pad_) {\n      attr.type = PaddingContentType::REFLECT;\n    } else /*zero pad*/ {\n      attr.type = PaddingContentType::ZEROS;\n    }\n\n    Tensor<HW, DataType::INT32> paddings;\n    RETURN_IF_ERROR(reader->ReadTensor(1, &paddings));\n\n    if (paddings.shape.h == 4 && paddings.shape.w == 2) {\n      // 4x2 tensor with paddings.\n      attr.prepended = BHWC(paddings.data[0], paddings.data[2],\n                            paddings.data[4], paddings.data[6]);\n      attr.appended = BHWC(paddings.data[1], paddings.data[3], paddings.data[5],\n                           paddings.data[7]);\n    } else if (paddings.shape.h == 3 && paddings.shape.w == 2) {\n      // 3x2 tensor with paddings.\n      attr.prepended =\n          BHWC(1, paddings.data[0], paddings.data[2], paddings.data[4]);\n      attr.appended =\n          BHWC(1, paddings.data[1], paddings.data[3], paddings.data[5]);\n    } else {\n      // It shouldn't fail here since it's checked at IsSupported().\n      return absl::InvalidArgumentError(\n          \"Paddings tensor has unexpected shape.\");\n    }\n    node->operation.attributes = attr;\n    return absl::OkStatus();\n  }\n\n private:\n  bool mirror_pad_ = false;\n};\n\nclass Pooling2DOperationParser : public TFLiteOperationParser {\n public:\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    RETURN_IF_ERROR(CheckMaxSupportedOpVersion(registration, 2));\n    const TfLitePoolParams* tf_options;\n    auto status = RetrieveCustomInitialData(tflite_node, &tf_options);\n    if (status.ok()) {  // custom case with indices as a second output\n      RETURN_IF_ERROR(CheckInputsOutputs(context, tflite_node,\n                                         /*runtime_inputs=*/1,\n                                         /*outputs=*/2));\n    } else {  // common pooling with 1 output\n      RETURN_IF_ERROR(RetrieveBuiltinData(tflite_node, &tf_options));\n      RETURN_IF_ERROR(CheckInputsOutputs(context, tflite_node,\n                                         /*runtime_inputs=*/1,\n                                         /*outputs=*/1));\n    }\n    RETURN_IF_ERROR(CheckKernelsAndStrides(\n        tf_options->filter_height, tf_options->filter_width,\n        tf_options->stride_height, tf_options->stride_width));\n    return IsActivationSupported(tf_options->activation);\n  }\n\n public:\n  explicit Pooling2DOperationParser(PoolingType type) : type_(type) {}\n\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    Node* node = graph->NewNode();\n    node->operation.type = ToString(OperationType::POOLING_2D);\n    RETURN_IF_ERROR(reader->AddInput(node, 0));\n    RETURN_IF_ERROR(reader->AddOutput(node, 0));\n\n    Pooling2DAttributes attr;\n    attr.type = type_;\n\n    auto input_shape = graph->FindInputs(node->id)[0]->tensor.shape;\n\n    // check whether there are custom options encoded. It happens if operation\n    // is MaxPoolingWithArgmax2D. There is no way to read\n    // tflite_node->builtin_code, so, simply check whether custom data is\n    // available.\n    const TfLitePoolParams* tf_options;\n    if (!RetrieveCustomInitialData(tflite_node, &tf_options).ok()) {\n      RETURN_IF_ERROR(RetrieveBuiltinData(tflite_node, &tf_options));\n    }\n\n    RETURN_IF_ERROR(MaybeFuseActivation(tf_options->activation, graph, node));\n    // Second output is optional. It is not required, it but must be added after\n    // MaybeAddFusedActivation function is called\n    reader->AddOutput(node, 1).IgnoreError();\n\n    // First output is the result of pooling operation, while second output is\n    // indices used for pooling.\n    auto outputs = graph->FindOutputs(node->id);\n    attr.output_indices = outputs.size() == 2;\n    if (attr.output_indices) {\n      // Fix data type for output indices. In the model it is set as float32.\n      outputs[1]->tensor.type = DataType::INT32;\n    }\n    RETURN_IF_ERROR(ParsePoolingAttributes(tf_options, input_shape, &attr));\n    node->operation.attributes = attr;\n    return absl::OkStatus();\n  }\n\n private:\n  const PoolingType type_;\n};\n\nclass ReduceOperationParser : public TFLiteOperationParser {\n public:\n  explicit ReduceOperationParser(OperationType operation_type)\n      : operation_type_(operation_type) {}\n\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    RETURN_IF_ERROR(CheckInputsOutputs(context, tflite_node,\n                                       /*runtime_inputs=*/1, /*outputs=*/1));\n    auto* axes = &context->tensors[tflite_node->inputs->data[1]];\n    if (axes->allocation_type != kTfLiteMmapRo || axes->type != kTfLiteInt32) {\n      return absl::UnimplementedError(\n          \"Reduce has unsupported tensor for axes.\");\n    }\n    if (tflite::NumElements(axes) != 1) {\n      return absl::UnimplementedError(\n          \"Supported reduce in single dimensions only.\");\n    }\n    return absl::OkStatus();\n  }\n\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    Node* node = graph->NewNode();\n    node->operation.type = ToString(operation_type_);\n    RETURN_IF_ERROR(reader->AddInput(node, 0));\n    RETURN_IF_ERROR(reader->AddOutputs(node));\n\n    const TfLiteReducerParams* tf_options;\n    RETURN_IF_ERROR(RetrieveBuiltinData(tflite_node, &tf_options));\n\n    Tensor<Scalar, DataType::INT32> axes;\n    RETURN_IF_ERROR(reader->ReadTensor(1, &axes));\n    const TfLiteTensor* input = reader->GetInputTensor(0);\n    ReduceAttributes attr;\n    RETURN_IF_ERROR(ExtractAxisFromIndex(*input, axes.data[0], &attr.axis));\n    node->operation.attributes = attr;\n    return absl::OkStatus();\n  }\n\n private:\n  const OperationType operation_type_;\n};\n\nclass QuantizeOperationParser : public TFLiteOperationParser {\n public:\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    RETURN_IF_ERROR(CheckMaxSupportedOpVersion(registration, 2));\n    RETURN_IF_ERROR(CheckInputsOutputs(context, tflite_node,\n                                       /*runtime_inputs=*/1, /*outputs=*/1));\n    return absl::OkStatus();\n  }\n\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    // 'Quantize' is rewritten as QuantizeAndDequantize since we are dealing\n    // with floating-point versions of the original tensors.\n    Node* node = graph->NewNode();\n    node->operation.type = ToString(OperationType::QUANTIZE_AND_DEQUANTIZE);\n    RETURN_IF_ERROR(reader->AddInput(node, 0));\n    RETURN_IF_ERROR(reader->AddOutputs(node));\n\n    // Quantization attributes should already be present in the output tensor.\n    auto output_value = graph->FindOutputs(node->id)[0];\n    if (!output_value->quant_params) {\n      return absl::InvalidArgumentError(\n          \"Encountered Quantize output with no quant params\");\n    }\n    QuantizeAndDequantizeAttributes attr;\n    attr.min = output_value->quant_params.value().min;\n    attr.max = output_value->quant_params.value().max;\n    attr.scale = output_value->quant_params.value().scale;\n\n    node->operation.attributes = attr;\n    return absl::OkStatus();\n  }\n};\n\nclass ReLUOperationParser : public TFLiteOperationParser {\n public:\n  explicit ReLUOperationParser(int clip) : clip_(clip) {}\n\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    RETURN_IF_ERROR(CheckMaxSupportedOpVersion(registration, 2));\n    return absl::OkStatus();\n  }\n\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    Node* node = graph->NewNode();\n    node->operation.type = ToString(OperationType::RELU);\n    RETURN_IF_ERROR(reader->AddInput(node, 0));\n\n    ReLUAttributes attr;\n    const TfLiteLeakyReluParams* tf_options;\n    auto status = RetrieveBuiltinData(tflite_node, &tf_options);\n    attr.alpha = status.ok() ? tf_options->alpha : 0;\n    attr.clip = clip_;\n    node->operation.attributes = attr;\n    return reader->AddOutputs(node);\n  }\n\n private:\n  const int clip_;\n};\n\nclass ReshapeOperationParser : public TFLiteOperationParser {\n public:\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    RETURN_IF_ERROR(CheckMaxSupportedOpVersion(registration, 1));\n    RETURN_IF_ERROR(CheckInputsOutputs(context, tflite_node,\n                                       /*runtime_inputs=*/1, /*outputs=*/1));\n    // TODO(eignasheva): add shape checking\n    return absl::OkStatus();\n  }\n\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    Node* node = graph->NewNode();\n    node->operation.type = ToString(OperationType::RESHAPE);\n    RETURN_IF_ERROR(reader->AddInput(node, 0));\n    RETURN_IF_ERROR(reader->AddOutputs(node));\n    // Here we may have extra inputs. Other tensors were supposed to\n    // define new shape, but in TFLite these are ignored.\n    // TODO(akulik): check that shapes match?\n\n    // New shape comes from output shape.\n    ReshapeAttributes attr;\n    attr.new_shape = graph->FindOutputs(node->id)[0]->tensor.shape;\n    node->operation.attributes = attr;\n    return absl::OkStatus();\n  }\n};\n\nclass Resize2DOperationParser : public TFLiteOperationParser {\n public:\n  explicit Resize2DOperationParser(SamplingType sampling_type)\n      : sampling_type_(sampling_type) {}\n\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    RETURN_IF_ERROR(CheckMaxSupportedOpVersion(registration, 3));\n    RETURN_IF_ERROR(CheckInputsOutputs(context, tflite_node,\n                                       /*runtime_inputs=*/1, /*outputs=*/1));\n\n    RETURN_IF_ERROR(CheckOnlyUpsamplingIsSupported(context, tflite_node));\n    bool align_corners;\n    RETURN_IF_ERROR(GetAlignCornersValue(tflite_node, &align_corners));\n    bool half_pixel_centers;\n    RETURN_IF_ERROR(GetHalfPixelCentersValue(tflite_node, &half_pixel_centers));\n    return absl::OkStatus();\n  }\n\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    Node* node = graph->NewNode();\n    node->operation.type = ToString(OperationType::RESIZE);\n    RETURN_IF_ERROR(reader->AddInput(node, 0));\n    RETURN_IF_ERROR(reader->AddOutputs(node));\n    // Here we may have extra inputs. Other tensors were supposed to\n    // define new shape, but in TFLite these are ignored.\n\n    Resize2DAttributes attr;\n    RETURN_IF_ERROR(GetAlignCornersValue(tflite_node, &attr.align_corners));\n    RETURN_IF_ERROR(\n        GetHalfPixelCentersValue(tflite_node, &attr.half_pixel_centers));\n    attr.type = sampling_type_;\n    attr.new_shape.CopyAllDefinedAxis(\n        graph->FindOutputs(node->id)[0]->tensor.shape);\n    node->operation.attributes = attr;\n    return absl::OkStatus();\n  }\n\n private:\n  absl::Status GetAlignCornersValue(const TfLiteNode* tflite_node,\n                                    bool* align_corners) {\n    switch (sampling_type_) {\n      case SamplingType::BILINEAR:\n        return GetAlignCornersValueForType<TfLiteResizeBilinearParams>(\n            tflite_node, align_corners);\n      case SamplingType::NEAREST:\n        return GetAlignCornersValueForType<TfLiteResizeNearestNeighborParams>(\n            tflite_node, align_corners);\n      case SamplingType::UNKNOWN:\n        return absl::InternalError(\"Sampling type is not specified\");\n    }\n    return absl::OkStatus();\n  }\n\n  template <class T>\n  absl::Status GetAlignCornersValueForType(const TfLiteNode* tflite_node,\n                                           bool* align_corners) {\n    const T* tf_options;\n    RETURN_IF_ERROR(RetrieveBuiltinData(tflite_node, &tf_options));\n    *align_corners = tf_options->align_corners;\n    return absl::OkStatus();\n  }\n\n  absl::Status GetHalfPixelCentersValue(const TfLiteNode* tflite_node,\n                                        bool* half_pixel_centers) {\n    if (sampling_type_ == SamplingType::BILINEAR) {\n      const TfLiteResizeBilinearParams* tf_options;\n      RETURN_IF_ERROR(RetrieveBuiltinData(tflite_node, &tf_options));\n      if (tf_options->align_corners && tf_options->half_pixel_centers) {\n        return absl::InternalError(\n            \"If half_pixel_centers is True, align_corners must be False.\");\n      }\n      *half_pixel_centers = tf_options->half_pixel_centers;\n    } else {\n      const TfLiteResizeNearestNeighborParams* tf_options;\n      RETURN_IF_ERROR(RetrieveBuiltinData(tflite_node, &tf_options));\n      *half_pixel_centers = tf_options->half_pixel_centers;\n    }\n    return absl::OkStatus();\n  }\n\n  absl::Status CheckOnlyUpsamplingIsSupported(const TfLiteContext* context,\n                                              const TfLiteNode* tflite_node) {\n    const auto* input = context->tensors + tflite_node->inputs->data[0];\n    const auto* output = context->tensors + tflite_node->outputs->data[0];\n\n    if (!input->dims || input->dims->size != 4) {\n      return absl::InvalidArgumentError(\"input.dims.size != 4\");\n    }\n    if (!output->dims || output->dims->size != 4) {\n      return absl::InvalidArgumentError(\"output.dims.size != 4\");\n    }\n    if (output->dims->data[1] < input->dims->data[1] ||\n        output->dims->data[2] < input->dims->data[2]) {\n      return absl::InvalidArgumentError(absl::StrCat(\n          \"Only upsampling is supported, received output h,w = \",\n          output->dims->data[1], \",\", output->dims->data[2],\n          \" input h,w = \", input->dims->data[1], \",\", input->dims->data[2]));\n    }\n    return absl::OkStatus();\n  }\n\n  SamplingType sampling_type_ = SamplingType::UNKNOWN;\n};\n\nclass SliceOperationParser : public TFLiteOperationParser {\n public:\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    RETURN_IF_ERROR(CheckMaxSupportedOpVersion(registration, 2));\n    return absl::OkStatus();\n  }\n\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    Node* node = graph->NewNode();\n    node->operation.type = ToString(OperationType::SLICE);\n    RETURN_IF_ERROR(reader->AddOutputs(node));\n    Value* input;\n    RETURN_IF_ERROR(reader->ReadValue(0, &input));\n    RETURN_IF_ERROR(graph->AddConsumer(node->id, input->id));\n\n    SliceAttributes attr;\n    attr.strides = BHWC(1, 1, 1, 1);\n    Tensor<Linear, DataType::INT32> starts, sizes;\n    RETURN_IF_ERROR(reader->ReadTensor(1, &starts));\n    RETURN_IF_ERROR(reader->ReadTensor(2, &sizes));\n    if (starts.data.size() != sizes.data.size()) {\n      return absl::InvalidArgumentError(\"Starts amount != sizes amount.\");\n    }\n    const auto& in_shape = input->tensor.shape;\n    if (starts.data.size() == 4) {\n      sizes.data[0] =\n          sizes.data[0] != -1 ? sizes.data[0] : in_shape.b - starts.data[0];\n      sizes.data[1] =\n          sizes.data[1] != -1 ? sizes.data[1] : in_shape.h - starts.data[1];\n      sizes.data[2] =\n          sizes.data[2] != -1 ? sizes.data[2] : in_shape.w - starts.data[2];\n      sizes.data[3] =\n          sizes.data[3] != -1 ? sizes.data[3] : in_shape.c - starts.data[3];\n      attr.starts =\n          BHWC(starts.data[0], starts.data[1], starts.data[2], starts.data[3]);\n      attr.ends =\n          BHWC(starts.data[0] + sizes.data[0], starts.data[1] + sizes.data[1],\n               starts.data[2] + sizes.data[2], starts.data[3] + sizes.data[3]);\n    } else if (starts.data.size() == 3) {\n      sizes.data[0] =\n          sizes.data[0] != -1 ? sizes.data[0] : in_shape.h - starts.data[0];\n      sizes.data[1] =\n          sizes.data[1] != -1 ? sizes.data[1] : in_shape.w - starts.data[1];\n      sizes.data[2] =\n          sizes.data[2] != -1 ? sizes.data[2] : in_shape.c - starts.data[2];\n      attr.starts = BHWC(0, starts.data[0], starts.data[1], starts.data[2]);\n      attr.ends =\n          BHWC(in_shape.b, starts.data[0] + sizes.data[0],\n               starts.data[1] + sizes.data[1], starts.data[2] + sizes.data[2]);\n    } else {\n      return absl::UnimplementedError(\n          \"Slicing is supported for 3 or 4 dimensional tensors only.\");\n    }\n    RETURN_IF_ERROR(UpdateIfNegative(in_shape, &attr));\n\n    auto out_shape = graph->FindOutputs(node->id)[0]->tensor.shape;\n    if ((attr.ends.b - attr.starts.b) != out_shape.b) {\n      return absl::UnimplementedError(\"Output batch don't match\");\n    }\n    if ((attr.ends.h - attr.starts.h) != out_shape.h) {\n      return absl::UnimplementedError(\"Output height doesn't match\");\n    }\n    if ((attr.ends.w - attr.starts.w) != out_shape.w) {\n      return absl::UnimplementedError(\"Output width doesn't match\");\n    }\n    if ((attr.ends.c - attr.starts.c) != out_shape.c) {\n      return absl::UnimplementedError(\"Output channels don't match\");\n    }\n    node->operation.attributes = attr;\n    return absl::OkStatus();\n  }\n\n private:\n  absl::Status UpdateIfNegative(const BHWC& input_shape,\n                                SliceAttributes* attr) {\n    if (attr->ends.h < 0) {\n      attr->ends.h = input_shape.h + attr->ends.h;\n    }\n    if (attr->ends.w < 0) {\n      attr->ends.w = input_shape.w + attr->ends.w;\n    }\n    if (attr->ends.c < 0) {\n      attr->ends.c = input_shape.c + attr->ends.c;\n    }\n    if (attr->ends.b < 0) {\n      attr->ends.b = input_shape.b + attr->ends.b;\n    }\n    return absl::OkStatus();\n  }\n};\n\nclass SoftmaxOperationParser : public TFLiteOperationParser {\n public:\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    RETURN_IF_ERROR(CheckMaxSupportedOpVersion(registration, 2));\n    RETURN_IF_ERROR(CheckInputsOutputs(context, tflite_node,\n                                       /*runtime_inputs=*/1, /*outputs=*/1));\n    const TfLiteSoftmaxParams* tf_options;\n    RETURN_IF_ERROR(RetrieveBuiltinData(tflite_node, &tf_options));\n    if (tf_options->beta != 1) {\n      // TODO(eignasheva): figure out, what's wrong with softmax.\n      return absl::UnimplementedError(\"Softmax.beta != 1 is not supported.\");\n    }\n    return absl::OkStatus();\n  }\n\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    Node* node = graph->NewNode();\n    node->operation.type = ToString(OperationType::SOFTMAX);\n    RETURN_IF_ERROR(reader->AddInput(node, 0));\n    RETURN_IF_ERROR(reader->AddOutputs(node));\n\n    const TfLiteSoftmaxParams* tf_options;\n    RETURN_IF_ERROR(RetrieveBuiltinData(tflite_node, &tf_options));\n    if (tf_options->beta != 1) {\n      // there is multiply by scalar operation fused in softmax. Make a layer\n      // out of it before softmax.\n      return absl::UnimplementedError(\"Softmax.beta != 1 is not supported.\");\n      // auto mul_node = reader->NewPassthroughNode(node);\n      // mul_node->operation.type = ToString(OperationType::MUL);\n    }\n    SoftmaxAttributes attr;\n    attr.axis = Axis::CHANNELS;  // always by channels\n    node->operation.attributes = attr;\n    return absl::OkStatus();\n  }\n};\n\nclass SpaceToDepthOperationParser : public TFLiteOperationParser {\n public:\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    RETURN_IF_ERROR(CheckMaxSupportedOpVersion(registration, 2));\n    RETURN_IF_ERROR(CheckInputsOutputs(context, tflite_node,\n                                       /*runtime_inputs=*/1, /*outputs=*/1));\n    // TODO(impjdi): Dims check.\n    const TfLiteSpaceToDepthParams* s2d_params;\n    RETURN_IF_ERROR(RetrieveBuiltinData(tflite_node, &s2d_params));\n    if (s2d_params->block_size == 1) {\n      return absl::InvalidArgumentError(\n          \"SPACE_TO_DEPTH block_size = 1 is a no-op.\");\n    }\n    if (s2d_params->block_size < 1) {\n      return absl::InvalidArgumentError(\n          \"SPACE_TO_DEPTH block_size must be > 1.\");\n    }\n    return absl::OkStatus();\n  }\n\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    Node* node = graph->NewNode();\n    node->operation.type = ToString(OperationType::SPACE_TO_DEPTH);\n    RETURN_IF_ERROR(reader->AddInput(node, 0));\n    RETURN_IF_ERROR(reader->AddOutputs(node));\n    const TfLiteSpaceToDepthParams* tf_options;\n    RETURN_IF_ERROR(RetrieveBuiltinData(tflite_node, &tf_options));\n    SpaceToDepthAttributes attr;\n    attr.block_size = tf_options->block_size;\n    node->operation.attributes = attr;\n    return absl::OkStatus();\n  }\n};\n\nclass StridedSliceOperationParser : public TFLiteOperationParser {\n public:\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    RETURN_IF_ERROR(CheckMaxSupportedOpVersion(registration, 2));\n    const TfLiteStridedSliceParams* tf_options;\n    RETURN_IF_ERROR(RetrieveBuiltinData(tflite_node, &tf_options));\n    RETURN_IF_ERROR(CheckOptionsSupport(tf_options));\n    return absl::OkStatus();\n  }\n\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    Node* node = graph->NewNode();\n    node->operation.type = ToString(OperationType::SLICE);\n    RETURN_IF_ERROR(reader->AddOutputs(node));\n    Value* input;\n    RETURN_IF_ERROR(reader->ReadValue(0, &input));\n    RETURN_IF_ERROR(graph->AddConsumer(node->id, input->id));\n\n    Tensor<Linear, DataType::INT32> tmp;\n    RETURN_IF_ERROR(reader->ReadTensor(1, &tmp));\n\n    bool read_without_batch = tmp.data.size() == 3;\n    bool read_with_batch = tmp.data.size() == 4;\n    if (!read_without_batch && !read_with_batch) {\n      return absl::UnimplementedError(\n          \"Slicing is supported for 3 or 4 dimensional tensors only.\");\n    }\n\n    const TfLiteStridedSliceParams* tf_options;\n    RETURN_IF_ERROR(RetrieveBuiltinData(tflite_node, &tf_options));\n    RETURN_IF_ERROR(CheckOptionsSupport(tf_options));\n\n    auto out_shape = graph->FindOutputs(node->id)[0]->tensor.shape;\n\n    SliceAttributes attr;\n    if (read_without_batch) {\n      RETURN_IF_ERROR(ReadAttribsWithoutBatch(reader, tf_options,\n                                              input->tensor.shape, &attr));\n    }\n    if (read_with_batch) {\n      RETURN_IF_ERROR(\n          ReadAttribsWithBatch(reader, tf_options, input->tensor.shape, &attr));\n    }\n    if (attr.strides.b == 0 || attr.strides.h == 0 || attr.strides.w == 0 ||\n        attr.strides.c == 0) {\n      return absl::InvalidArgumentError(\"stride values must be non-zero\");\n    }\n    if (attr.strides.b < 0 || attr.strides.h < 0 || attr.strides.w < 0 ||\n        attr.strides.c < 0) {\n      return absl::UnimplementedError(\"Reverse slices are not supported.\");\n    }\n    if ((attr.ends.b - attr.starts.b + attr.strides.b - 1) / attr.strides.b !=\n        out_shape.b) {\n      return absl::UnimplementedError(\"Output batch don't match\");\n    }\n    if ((attr.ends.h - attr.starts.h + attr.strides.h - 1) / attr.strides.h !=\n        out_shape.h) {\n      return absl::UnimplementedError(\"Output height doesn't match\");\n    }\n    if ((attr.ends.w - attr.starts.w + attr.strides.w - 1) / attr.strides.w !=\n        out_shape.w) {\n      return absl::UnimplementedError(\"Output width doesn't match\");\n    }\n    if ((attr.ends.c - attr.starts.c + attr.strides.c - 1) / attr.strides.c !=\n        out_shape.c) {\n      return absl::UnimplementedError(\"Output channels don't match\");\n    }\n    node->operation.attributes = attr;\n    return absl::OkStatus();\n  }\n\n private:\n  absl::Status UpdateWithMask(const TfLiteStridedSliceParams* tf_options,\n                              const BHWC& input_shape, int ignore_b,\n                              int ignore_h, int ignore_w, int ignore_c,\n                              SliceAttributes* attr) {\n    if (tf_options->begin_mask & ignore_h) {\n      attr->starts.h = 0;\n    }\n    if (tf_options->begin_mask & ignore_w) {\n      attr->starts.w = 0;\n    }\n    if (tf_options->begin_mask & ignore_c) {\n      attr->starts.c = 0;\n    }\n    if (tf_options->begin_mask & ignore_b) {\n      attr->starts.b = 0;\n    }\n\n    if (tf_options->end_mask & ignore_h) {\n      attr->ends.h = input_shape.h;\n    }\n    if (tf_options->end_mask & ignore_w) {\n      attr->ends.w = input_shape.w;\n    }\n    if (tf_options->end_mask & ignore_c) {\n      attr->ends.c = input_shape.c;\n    }\n    if (tf_options->end_mask & ignore_b) {\n      attr->ends.b = input_shape.b;\n    }\n    return absl::OkStatus();\n  }\n\n  absl::Status UpdateIfNegative(const BHWC& input_shape,\n                                SliceAttributes* attr) {\n    if (attr->ends.h < 0) {\n      attr->ends.h = input_shape.h + attr->ends.h;\n    }\n    if (attr->ends.w < 0) {\n      attr->ends.w = input_shape.w + attr->ends.w;\n    }\n    if (attr->ends.c < 0) {\n      attr->ends.c = input_shape.c + attr->ends.c;\n    }\n    if (attr->ends.b < 0) {\n      attr->ends.b = input_shape.b + attr->ends.b;\n    }\n    return absl::OkStatus();\n  }\n\n  absl::Status ReadAttribsWithBatch(const ObjectReader* reader,\n                                    const TfLiteStridedSliceParams* tf_options,\n                                    const BHWC& input_shape,\n                                    SliceAttributes* attr) {\n    auto read_bhwc = [&](int tensor_index, BHWC* bhwc) -> absl::Status {\n      Tensor<Linear, DataType::INT32> t;\n      RETURN_IF_ERROR(reader->ReadTensor(tensor_index, &t));\n      *bhwc = BHWC(t.data[0], t.data[1], t.data[2], t.data[3]);\n      return absl::OkStatus();\n    };\n\n    RETURN_IF_ERROR(read_bhwc(1, &attr->starts));\n    RETURN_IF_ERROR(read_bhwc(2, &attr->ends));\n    RETURN_IF_ERROR(read_bhwc(3, &attr->strides));\n    RETURN_IF_ERROR(UpdateIfNegative(input_shape, attr));\n    RETURN_IF_ERROR(UpdateWithMask(tf_options, input_shape, 1, 2, 4, 8, attr));\n    return absl::OkStatus();\n  }\n\n  absl::Status ReadAttribsWithoutBatch(\n      const ObjectReader* reader, const TfLiteStridedSliceParams* tf_options,\n      const BHWC& input_shape, SliceAttributes* attr) {\n    auto read_hwc = [&](int tensor_index, BHWC* bhwc) -> absl::Status {\n      Tensor<Linear, DataType::INT32> t;\n      RETURN_IF_ERROR(reader->ReadTensor(tensor_index, &t));\n      *bhwc = BHWC(0, t.data[0], t.data[1], t.data[2]);\n      return absl::OkStatus();\n    };\n\n    RETURN_IF_ERROR(read_hwc(1, &attr->starts));\n    RETURN_IF_ERROR(read_hwc(2, &attr->ends));\n    RETURN_IF_ERROR(read_hwc(3, &attr->strides));\n    RETURN_IF_ERROR(UpdateIfNegative(input_shape, attr));\n    RETURN_IF_ERROR(UpdateWithMask(tf_options, input_shape, 0, 1, 2, 4, attr));\n    attr->starts.b = 0;\n    attr->ends.b = input_shape.b;\n    attr->strides.b = 1;\n    return absl::OkStatus();\n  }\n  absl::Status CheckOptionsSupport(const TfLiteStridedSliceParams* tf_options) {\n    if (tf_options->ellipsis_mask) {\n      return absl::UnimplementedError(\"Slice does not support ellipsis_mask.\");\n    }\n    if (tf_options->new_axis_mask) {\n      return absl::UnimplementedError(\"Slice does not support new_axis_mask.\");\n    }\n    if (tf_options->shrink_axis_mask) {\n      return absl::UnimplementedError(\n          \"Slice does not support shrink_axis_mask parameter. \");\n    }\n    return absl::OkStatus();\n  }\n};\n\nclass TransposeConvOperationParser : public TFLiteOperationParser {\n public:\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    RETURN_IF_ERROR(CheckMaxSupportedOpVersion(registration, 2));\n    RETURN_IF_ERROR(CheckTensorIsAvailable(context, tflite_node, 1));\n    const TfLiteTransposeConvParams* tf_options;\n    RETURN_IF_ERROR(RetrieveBuiltinData(tflite_node, &tf_options));\n    RETURN_IF_ERROR(\n        CheckStrides(tf_options->stride_height, tf_options->stride_width));\n    return absl::OkStatus();\n  }\n\n  // TFLite's TRANSPOSE_CONV expects 3 input (output shape, weights, and input)\n  // and allows configurable padding & stride.\n  // TODO(impjdi): Translate output_shape to attr.adjacent.\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    auto* node = graph->NewNode();\n    node->operation.type = ToString(OperationType::CONVOLUTION_TRANSPOSED);\n    Value* input;\n    RETURN_IF_ERROR(reader->ReadValue(2, &input));\n    RETURN_IF_ERROR(graph->AddConsumer(node->id, input->id));\n    RETURN_IF_ERROR(reader->AddOutputs(node));\n\n    const TfLiteTransposeConvParams* tf_options;\n    RETURN_IF_ERROR(RetrieveBuiltinData(tflite_node, &tf_options));\n\n    ConvolutionTransposedAttributes attr;\n    attr.stride = tf_options\n                      ? HW(tf_options->stride_height, tf_options->stride_width)\n                      : HW(1, 1);\n    RETURN_IF_ERROR(reader->ReadTensor(1, &attr.weights));\n\n    // TFLite does not support bias.\n\n    UpdatePadding(tf_options->padding,\n                  graph->FindInputs(node->id)[0]->tensor.shape, &attr);\n    node->operation.attributes = std::move(attr);\n    return absl::OkStatus();\n  }\n};\n\nclass TransposeOperationParser : public TFLiteOperationParser {\n public:\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    RETURN_IF_ERROR(CheckMaxSupportedOpVersion(registration, 2));\n    RETURN_IF_ERROR(CheckInputsOutputs(context, tflite_node,\n                                       /*runtime_inputs=*/1, /*outputs=*/1));\n    return absl::OkStatus();\n  }\n\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    Node* node = graph->NewNode();\n    node->operation.type = ToString(OperationType::TRANSPOSE);\n    RETURN_IF_ERROR(reader->AddInput(node, 0));\n    RETURN_IF_ERROR(reader->AddOutputs(node));\n\n    TransposeAttributes attr;\n    Tensor<Linear, DataType::INT32> perm;\n    RETURN_IF_ERROR(reader->ReadTensor(1, &perm));\n    std::map<Axis, int> axis_to_index = {{Axis::BATCH, 0},\n                                         {Axis::HEIGHT, 1},\n                                         {Axis::WIDTH, 2},\n                                         {Axis::CHANNELS, 3}};\n    if (perm.data.size() == 4) {\n      attr.perm = BHWC(perm.data[0], perm.data[1], perm.data[2], perm.data[3]);\n    } else if (perm.data.size() == 3) {\n      std::vector<Axis> index_to_axis = {Axis::BATCH, Axis::WIDTH,\n                                         Axis::CHANNELS};\n      attr.perm.b = axis_to_index[index_to_axis[perm.data[0]]];\n      attr.perm.h = 1;\n      attr.perm.w = axis_to_index[index_to_axis[perm.data[1]]];\n      attr.perm.c = axis_to_index[index_to_axis[perm.data[2]]];\n    } else if (perm.data.size() == 2) {\n      std::vector<Axis> index_to_axis = {Axis::BATCH, Axis::CHANNELS};\n      attr.perm.b = axis_to_index[index_to_axis[perm.data[0]]];\n      attr.perm.h = 1;\n      attr.perm.w = 2;\n      attr.perm.c = axis_to_index[index_to_axis[perm.data[1]]];\n    } else {\n      return absl::InvalidArgumentError(\n          \"Permutation for transpose is invalid.\");\n    }\n\n    node->operation.attributes = attr;\n    return absl::OkStatus();\n  }\n};\n\nclass Unpooling2DOperationParser : public TFLiteOperationParser {\n public:\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    RETURN_IF_ERROR(CheckInputsOutputs(context, tflite_node,\n                                       /*runtime_inputs=*/2, /*outputs=*/1));\n    const TfLitePoolParams* tf_options;\n    RETURN_IF_ERROR(RetrieveCustomInitialData(tflite_node, &tf_options));\n    RETURN_IF_ERROR(CheckKernelsAndStrides(\n        tf_options->filter_height, tf_options->filter_width,\n        tf_options->stride_height, tf_options->stride_width));\n    return absl::OkStatus();\n  }\n\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    Node* node = graph->NewNode();\n    node->operation.type = ToString(OperationType::MAX_UNPOOLING_2D);\n    RETURN_IF_ERROR(reader->AddInput(node, 0));\n    RETURN_IF_ERROR(reader->AddInput(node, 1));\n    RETURN_IF_ERROR(reader->AddOutputs(node));\n    auto input_shape = graph->FindInputs(node->id)[0]->tensor.shape;\n    MaxUnpooling2DAttributes attr;\n\n    const TfLitePoolParams* tf_options;\n    RETURN_IF_ERROR(RetrieveCustomInitialData(tflite_node, &tf_options));\n\n    attr.kernel = ToHW(tf_options->filter_height, tf_options->filter_width);\n    attr.strides = ToHW(tf_options->stride_height, tf_options->stride_width);\n    UpdatePadding(tf_options->padding, input_shape, &attr);\n\n    node->operation.attributes = attr;\n\n    auto output_value = graph->FindOutputs(node->id)[0];\n    output_value->tensor.shape = CalculateOutputShape(input_shape, attr);\n    return absl::OkStatus();\n  }\n};\n\n// TODO(impjdi): BATCH_TO_SPACE/SPACE_TO_BATCH shouldn't be supported.\nclass BatchToSpaceOperationParser : public TFLiteOperationParser {\n public:\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    return absl::OkStatus();\n  }\n\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    auto* node = graph->NewNode();\n    node->operation.type = ToString(OperationType::BATCH_TO_SPACE);\n    RETURN_IF_ERROR(reader->AddInput(node, 0));\n    RETURN_IF_ERROR(reader->AddOutputs(node));\n\n    BatchToSpaceAttributes bs_attr;\n    Tensor<Linear, DataType::INT32> block;\n    RETURN_IF_ERROR(reader->ReadTensor(1, &block));\n    if (block.shape.v != 2) {\n      return absl::InternalError(\"Space has to be HxW.\");\n    }\n    bs_attr.block.h = block.data[0];\n    bs_attr.block.w = block.data[1];\n\n    Tensor<HW, DataType::INT32> crop;\n    RETURN_IF_ERROR(reader->ReadTensor(2, &crop));\n    auto crop_shape = crop.shape;\n    if (crop_shape.h != 2 && crop_shape.w != 2) {\n      return absl::InternalError(\"Space has to be HxW.\");\n    }\n\n    bs_attr.crop.prepended.h = crop.data[0];\n    bs_attr.crop.prepended.w = crop.data[2];\n\n    bs_attr.crop.appended.h = crop.data[1];\n    bs_attr.crop.appended.w = crop.data[3];\n\n    node->operation.attributes = std::move(bs_attr);\n    return absl::OkStatus();\n  }\n};\n\nclass SpaceToBatchOperationParser : public TFLiteOperationParser {\n public:\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    return absl::OkStatus();\n  }\n\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    auto* node = graph->NewNode();\n    node->operation.type = ToString(OperationType::SPACE_TO_BATCH);\n    RETURN_IF_ERROR(reader->AddInput(node, 0));\n    RETURN_IF_ERROR(reader->AddOutputs(node));\n    SpaceToBatchAttributes sb_attr;\n    Tensor<Linear, DataType::INT32> block;\n    RETURN_IF_ERROR(reader->ReadTensor(1, &block));\n    if (block.shape.v != 2) {\n      return absl::InternalError(\"Space has to be HxW.\");\n    }\n    sb_attr.block.h = block.data[0];\n    sb_attr.block.w = block.data[1];\n\n    Tensor<HW, DataType::INT32> padding;\n    RETURN_IF_ERROR(reader->ReadTensor(2, &padding));\n    auto padding_shape = padding.shape;\n\n    if (padding_shape.h != 2 && padding_shape.w != 2) {\n      return absl::InternalError(\"Space has to be HxW.\");\n    }\n\n    sb_attr.padding.prepended.h = padding.data[0];\n    sb_attr.padding.prepended.w = padding.data[2];\n\n    sb_attr.padding.appended.h = padding.data[1];\n    sb_attr.padding.appended.w = padding.data[3];\n\n    node->operation.attributes = std::move(sb_attr);\n    return absl::OkStatus();\n  }\n};\n\nclass RoIToTransformMatrixOperationParser : public TFLiteOperationParser {\n public:\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    RETURN_IF_ERROR(CheckMaxSupportedOpVersion(registration, 2));\n    RETURN_IF_ERROR(CheckInputsOutputs(context, tflite_node,\n                                       /*runtime_inputs=*/1, /*outputs=*/1));\n    return absl::OkStatus();\n  }\n\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    Node* node = graph->NewNode();\n    RETURN_IF_ERROR(reader->AddInput(node, 0));  // bbox\n    RETURN_IF_ERROR(reader->AddOutputs(node));\n\n    std::string op_name = \"roi_to_transform_matrix\";\n    node->operation.type = op_name;\n    BHWC output_shape;\n    RETURN_IF_ERROR(ParseCustomAttributes(\n        op_name, registration->version, tflite_node->custom_initial_data,\n        tflite_node->custom_initial_data_size, &(node->operation.attributes),\n        &output_shape));\n\n    auto output_value = graph->FindOutputs(node->id)[0];\n    output_value->tensor.shape = output_shape;\n    return absl::OkStatus();\n  }\n};\n\nclass TransformTensorBilinearOperationParser : public TFLiteOperationParser {\n public:\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    RETURN_IF_ERROR(CheckMaxSupportedOpVersion(registration, 2));\n    RETURN_IF_ERROR(CheckInputsOutputs(context, tflite_node,\n                                       /*runtime_inputs=*/2, /*outputs=*/1));\n    return absl::OkStatus();\n  }\n\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    Node* node = graph->NewNode();\n    RETURN_IF_ERROR(reader->AddInput(node, 0));  // data\n    RETURN_IF_ERROR(reader->AddInput(node, 1));  // bbox\n    RETURN_IF_ERROR(reader->AddOutputs(node));\n\n    std::string op_name = \"transform_tensor_bilinear\";\n    node->operation.type = op_name;\n    BHWC output_shape;\n    RETURN_IF_ERROR(ParseCustomAttributes(\n        op_name, registration->version, tflite_node->custom_initial_data,\n        tflite_node->custom_initial_data_size, &(node->operation.attributes),\n        &output_shape));\n\n    auto output_value = graph->FindOutputs(node->id)[0];\n\n    output_value->tensor.shape =\n        BHWC(1, output_shape.h, output_shape.w,\n             graph->FindInputs(node->id)[0]->tensor.shape.c);\n    return absl::OkStatus();\n  }\n};\n\nclass TransformLandmarksOperationParser : public TFLiteOperationParser {\n public:\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    RETURN_IF_ERROR(CheckMaxSupportedOpVersion(registration, 2));\n    RETURN_IF_ERROR(CheckInputsOutputs(context, tflite_node,\n                                       /*runtime_inputs=*/2, /*outputs=*/1));\n    return absl::OkStatus();\n  }\n\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    Node* node = graph->NewNode();\n    RETURN_IF_ERROR(reader->AddInput(node, 0));  // data\n    RETURN_IF_ERROR(reader->AddInput(node, 1));  // bbox\n    RETURN_IF_ERROR(reader->AddOutputs(node));\n    std::string op_name = \"transform_landmarks\";\n    node->operation.type = op_name;\n    BHWC output_shape = graph->FindOutputs(node->id)[0]->tensor.shape;\n    RETURN_IF_ERROR(ParseCustomAttributes(\n        op_name, registration->version, tflite_node->custom_initial_data,\n        tflite_node->custom_initial_data_size, &(node->operation.attributes),\n        &output_shape));\n\n    auto output_value = graph->FindOutputs(node->id)[0];\n\n    output_value->tensor.shape = graph->FindInputs(node->id)[0]->tensor.shape;\n    return absl::OkStatus();\n  }\n};\n\nclass Landmarks2TransformMatrixOperationParser : public TFLiteOperationParser {\n public:\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    RETURN_IF_ERROR(CheckMaxSupportedOpVersion(registration, 2));\n    return CheckInputsOutputs(context, tflite_node, /*runtime_inputs=*/1,\n                              /*outputs=*/1);\n  }\n\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    Node* node = graph->NewNode();\n    RETURN_IF_ERROR(reader->AddInput(node, 0));  // landmarks\n    RETURN_IF_ERROR(reader->AddOutputs(node));   // transform matrix\n\n    const std::string op_name = \"landmarks_to_transform_matrix\";\n    node->operation.type = op_name;\n    BHWC output_shape;\n    RETURN_IF_ERROR(ParseCustomAttributes(\n        op_name, registration->version, tflite_node->custom_initial_data,\n        tflite_node->custom_initial_data_size, &(node->operation.attributes),\n        &output_shape));\n\n    auto output_value = graph->FindOutputs(node->id)[0];\n    output_value->tensor.shape = output_shape;\n    return absl::OkStatus();\n  }\n};\n\nclass AlignmentPointsToTransformMatrixOperationParser\n    : public TFLiteOperationParser {\n public:\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    return CheckInputsOutputs(context, tflite_node, /*runtime_inputs=*/1,\n                              /*outputs=*/1);\n  }\n\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    Node* node = graph->NewNode();\n    RETURN_IF_ERROR(reader->AddInput(node, 0));  // alignment points\n    RETURN_IF_ERROR(reader->AddOutputs(node));   // transform matrix\n\n    const std::string op_name = \"alignment_points_to_transform_matrix\";\n    node->operation.type = op_name;\n    BHWC output_shape;\n    RETURN_IF_ERROR(ParseCustomAttributes(\n        op_name, registration->version, tflite_node->custom_initial_data,\n        tflite_node->custom_initial_data_size, &(node->operation.attributes),\n        &output_shape));\n\n    auto output_value = graph->FindOutputs(node->id)[0];\n    output_value->tensor.shape = output_shape;\n    return absl::OkStatus();\n  }\n\n private:\n};\n\nclass MeanOperationParser : public TFLiteOperationParser {\n public:\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    RETURN_IF_ERROR(CheckInputsOutputs(context, tflite_node,\n                                       /*runtime_inputs=*/1,\n                                       /*outputs=*/1));\n\n    // Simple mechanism to check if MEAN is to be performed only on HW plane.\n    auto* axes = &context->tensors[tflite_node->inputs->data[1]];\n    if (axes->allocation_type != kTfLiteMmapRo || axes->type != kTfLiteInt32) {\n      return absl::UnimplementedError(\"Mean has unsupported tensor for axes\");\n    }\n    auto* axes_data = axes->data.i32;\n    const bool is_hw_mean = tflite::NumElements(axes) == 2 &&\n                            ((axes_data[0] == 1 && axes_data[1] == 2) ||\n                             (axes_data[0] == 2 && axes_data[1] == 1));\n    if (!is_hw_mean) {\n      return absl::UnimplementedError(\"Mean operation supports only HW plane\");\n    }\n    return absl::OkStatus();\n  }\n\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    auto* node = graph->NewNode();\n    node->operation.type = ToString(OperationType::MEAN);\n    RETURN_IF_ERROR(reader->AddInput(node, 0));\n    RETURN_IF_ERROR(reader->AddOutputs(node));\n\n    MeanAttributes attr;\n    Tensor<Linear, DataType::INT32> channel;\n    RETURN_IF_ERROR(reader->ReadTensor(1, &channel));\n    for (int i = 0; i < channel.data.size(); i++) {\n      std::string unsupported;\n      switch (channel.data[i]) {\n        case 1:\n          attr.dims.insert(Axis::HEIGHT);\n          break;\n        case 2:\n          attr.dims.insert(Axis::WIDTH);\n          break;\n        case 0:\n          unsupported = unsupported.empty() ? \"batch\" : unsupported;\n          ABSL_FALLTHROUGH_INTENDED;\n        case 3:\n          unsupported = unsupported.empty() ? \"channels\" : unsupported;\n          ABSL_FALLTHROUGH_INTENDED;\n        default:\n          return absl::UnimplementedError(\n              absl::StrCat(\"Unsupported mean dimension: \", unsupported));\n      }\n    }\n    node->operation.attributes = attr;\n    return absl::OkStatus();\n  }\n};\n\nclass UnsupportedOperationParser : public TFLiteOperationParser {\n public:\n  absl::Status IsSupported(const TfLiteContext* context,\n                           const TfLiteNode* tflite_node,\n                           const TfLiteRegistration* registration) final {\n    return absl::UnimplementedError(\"Operation is not supported.\");\n  }\n\n  absl::Status Parse(const TfLiteNode* tflite_node,\n                     const TfLiteRegistration* registration,\n                     GraphFloat32* graph, ObjectReader* reader) final {\n    return absl::UnimplementedError(\"Operation is not supported.\");\n  }\n};\n\nstd::unique_ptr<TFLiteOperationParser> NewOperationParser(\n    const TfLiteRegistration* registration, bool allow_quant_ops = false) {\n  const auto builtin_code = registration->builtin_code;\n  switch (builtin_code) {\n    case kTfLiteBuiltinAbs:\n      return std::make_unique<ElementwiseOperationParser>(OperationType::ABS);\n    case kTfLiteBuiltinAdd:\n      return std::make_unique<AddOperationParser>();\n    case kTfLiteBuiltinAveragePool2d:\n      return std::make_unique<Pooling2DOperationParser>(PoolingType::AVERAGE);\n    case kTfLiteBuiltinBatchMatmul:\n      return std::make_unique<BatchedMatMulOperationParser>();\n    case kTfLiteBuiltinConcatenation:\n      return std::make_unique<ConcatenationOperationParser>();\n    case kTfLiteBuiltinConv2d:\n      return std::make_unique<Conv2DOperationParser>();\n    case kTfLiteBuiltinCos:\n      return std::make_unique<ElementwiseOperationParser>(OperationType::COS);\n    case kTfLiteBuiltinDepthwiseConv2d:\n      return std::make_unique<DepthwiseConvolutionOperationParser>();\n    case kTfLiteBuiltinDequantize:\n      if (allow_quant_ops) {\n        return std::make_unique<DequantizeOperationParser>();\n      }\n      break;\n    case kTfLiteBuiltinDiv:\n      return std::make_unique<ElementwiseOperationParser>(OperationType::DIV);\n    case kTfLiteBuiltinElu:\n      return std::make_unique<ElementwiseOperationParser>(OperationType::ELU);\n    case kTfLiteBuiltinExp:\n      return std::make_unique<ElementwiseOperationParser>(OperationType::EXP);\n    case kTfLiteBuiltinFullyConnected:\n      return std::make_unique<FullyConnectedOperationParser>();\n    case kTfLiteBuiltinHardSwish:\n      return std::make_unique<HardSwishOperationParser>();\n    case kTfLiteBuiltinLogistic:\n      return std::make_unique<ElementwiseOperationParser>(\n          OperationType::SIGMOID);\n    case kTfLiteBuiltinLog:\n      return std::make_unique<ElementwiseOperationParser>(OperationType::LOG);\n    case kTfLiteBuiltinLstm:\n      return std::make_unique<LSTMOperationParser>();\n    case kTfLiteBuiltinMaximum:\n      return std::make_unique<ElementwiseOperationParser>(\n          OperationType::MAXIMUM);\n    case kTfLiteBuiltinMaxPool2d:\n      return std::make_unique<Pooling2DOperationParser>(PoolingType::MAX);\n    case kTfLiteBuiltinMean:\n      return std::make_unique<MeanOperationParser>();\n    case kTfLiteBuiltinMinimum:\n      return std::make_unique<ElementwiseOperationParser>(\n          OperationType::MINIMUM);\n    case kTfLiteBuiltinMirrorPad:\n      return std::make_unique<PadOperationParser>(/*mirror_pad=*/true);\n    case kTfLiteBuiltinMul:\n      return std::make_unique<MulOperationParser>();\n    case kTfLiteBuiltinNeg:\n      return std::make_unique<ElementwiseOperationParser>(OperationType::NEG);\n    case kTfLiteBuiltinPack:\n      return std::make_unique<PackOperationParser>();\n    case kTfLiteBuiltinPad:\n      return std::make_unique<PadOperationParser>(/*mirror_pad=*/false);\n    case kTfLiteBuiltinPow:\n      return std::make_unique<ElementwiseOperationParser>(OperationType::POW);\n    case kTfLiteBuiltinReduceMax:\n      return std::make_unique<ReduceOperationParser>(\n          OperationType::REDUCE_MAXIMUM);\n    case kTfLiteBuiltinReduceMin:\n      return std::make_unique<ReduceOperationParser>(\n          OperationType::REDUCE_MINIMUM);\n    case kTfLiteBuiltinReduceProd:\n      return std::make_unique<ReduceOperationParser>(\n          OperationType::REDUCE_PRODUCT);\n    case kTfLiteBuiltinQuantize:\n      if (allow_quant_ops) {\n        return std::make_unique<QuantizeOperationParser>();\n      }\n      break;\n    case kTfLiteBuiltinRelu:\n      return std::make_unique<ReLUOperationParser>(0);\n    case kTfLiteBuiltinRelu6:\n      return std::make_unique<ReLUOperationParser>(6);\n    case kTfLiteBuiltinLeakyRelu:\n      return std::make_unique<ReLUOperationParser>(0);\n    case kTfLiteBuiltinPrelu:\n      return std::make_unique<PReLUOperationParser>();\n    case kTfLiteBuiltinReshape:\n      return std::make_unique<ReshapeOperationParser>();\n    case kTfLiteBuiltinResizeBilinear:\n      return std::make_unique<Resize2DOperationParser>(SamplingType::BILINEAR);\n    case kTfLiteBuiltinResizeNearestNeighbor:\n      return std::make_unique<Resize2DOperationParser>(SamplingType::NEAREST);\n    case kTfLiteBuiltinRsqrt:\n      return std::make_unique<ElementwiseOperationParser>(OperationType::RSQRT);\n    case kTfLiteBuiltinSin:\n      return std::make_unique<ElementwiseOperationParser>(OperationType::SIN);\n    case kTfLiteBuiltinSlice:\n      return std::make_unique<SliceOperationParser>();\n    case kTfLiteBuiltinSoftmax:\n      return std::make_unique<SoftmaxOperationParser>();\n    case kTfLiteBuiltinSpaceToDepth:\n      return std::make_unique<SpaceToDepthOperationParser>();\n    case kTfLiteBuiltinSqrt:\n      return std::make_unique<ElementwiseOperationParser>(OperationType::SQRT);\n    case kTfLiteBuiltinSquare:\n      return std::make_unique<ElementwiseOperationParser>(\n          OperationType::SQUARE);\n    case kTfLiteBuiltinSquaredDifference:\n      return std::make_unique<ElementwiseOperationParser>(\n          OperationType::SQUARED_DIFF);\n    case kTfLiteBuiltinStridedSlice:\n      return std::make_unique<StridedSliceOperationParser>();\n    case kTfLiteBuiltinSub:\n      return std::make_unique<ElementwiseOperationParser>(OperationType::SUB);\n    case kTfLiteBuiltinSum:\n      return std::make_unique<ReduceOperationParser>(OperationType::REDUCE_SUM);\n    case kTfLiteBuiltinTanh:\n      return std::make_unique<ElementwiseOperationParser>(OperationType::TANH);\n    case kTfLiteBuiltinTranspose:\n      return std::make_unique<TransposeOperationParser>();\n    case kTfLiteBuiltinTransposeConv:\n      return std::make_unique<TransposeConvOperationParser>();\n\n    case kTfLiteBuiltinCustom:\n      const absl::string_view custom_name = registration->custom_name;\n      if (custom_name == \"Convolution2DTransposeBias\") {\n        return std::make_unique<Convolution2DTransposeBiasParser>();\n      }\n      if (custom_name == \"MaxPoolingWithArgmax2D\") {\n        return std::make_unique<Pooling2DOperationParser>(PoolingType::MAX);\n      }\n      if (custom_name == \"MaxUnpooling2D\") {\n        return std::make_unique<Unpooling2DOperationParser>();\n      }\n      if (custom_name == \"RoIToTransformMatrix\") {\n        return std::make_unique<RoIToTransformMatrixOperationParser>();\n      }\n      if (custom_name == \"TransformTensor\" /*for version 1*/ ||\n          custom_name == \"TransformTensorBilinear\" /*for version 2*/) {\n        return std::make_unique<TransformTensorBilinearOperationParser>();\n      }\n      if (custom_name == \"TransformLandmarks\") {\n        return std::make_unique<TransformLandmarksOperationParser>();\n      }\n      if (custom_name == \"Landmarks2TransformMatrix\" ||\n          custom_name == \"Landmarks2TransformMatrixV2\") {\n        return std::make_unique<Landmarks2TransformMatrixOperationParser>();\n      }\n      if (custom_name == \"AlignmentPointsToTransformMatrix\") {\n        return std::make_unique<\n            AlignmentPointsToTransformMatrixOperationParser>();\n      }\n      break;\n  }\n  return std::make_unique<UnsupportedOperationParser>();\n}\n\nabsl::Status IsSupported(const TfLiteContext* context, TfLiteNode* node,\n                         const TfLiteRegistration* registration,\n                         bool allow_quant_ops = false) {\n  return NewOperationParser(registration, allow_quant_ops)\n      ->IsSupported(context, node, registration);\n}\n\nbool IsAllAllowedTensors(TfLiteContext* context,\n                         const TfLiteIntArray* tensor_indices,\n                         bool allow_quant_ops = false) {\n  for (int i = 0; i < tensor_indices->size; ++i) {\n    int tensor_idx = tensor_indices->data[i];\n    if (tensor_idx == kTfLiteOptionalTensor) continue;\n    const TfLiteTensor* t = &context->tensors[tensor_idx];\n    bool type_supported =\n        (t->type == kTfLiteFloat32 || t->type == kTfLiteFloat16);\n    if (allow_quant_ops) {\n      // Since we only check non-constant tensors, type cannot be Int32.\n      type_supported =\n          type_supported || t->type == kTfLiteInt8 || t->type == kTfLiteUInt8;\n    }\n    if (t->allocation_type == kTfLiteArenaRw && !type_supported) {\n      return false;\n    }\n  }\n  return true;\n}\n}  // namespace\n\n// TODO(impjdi): Check number of input/output tensors and their dimensions.\n// TODO(impjdi): Check ops' parameters.\nTfLiteIntArray* GetOpsToReplace(TfLiteContext* context, bool allow_quant_ops,\n                                int max_delegated_partitions) {\n  delegates::IsNodeSupportedFn node_supported_fn =\n      [=](TfLiteContext* context, TfLiteNode* node,\n          TfLiteRegistration* registration,\n          std::string* unsupported_details) -> bool {\n    const auto status =\n        IsSupported(context, node, registration, allow_quant_ops);\n    if (!status.ok()) {\n      if (unsupported_details) {\n        *unsupported_details = std::string(status.message());\n      }\n      return false;\n    }\n\n    if (!IsAllAllowedTensors(context, node->inputs, allow_quant_ops) ||\n        !IsAllAllowedTensors(context, node->outputs, allow_quant_ops)) {\n      if (unsupported_details) {\n        *unsupported_details =\n            \"OP is supported, but tensor type isn't matched!\";\n      }\n      return false;\n    }\n    return true;\n  };\n\n  delegates::FP16GraphPartitionHelper partition_helper(context,\n                                                       node_supported_fn);\n  std::set<std::string> unsupported_nodes_info;\n  if (partition_helper.Partition(&unsupported_nodes_info) != kTfLiteOk) {\n    return TfLiteIntArrayCreate(0);\n  }\n\n  // By default, we simply get 1st largest partition as 'max_delegate_partions'\n  // is set to 1 by default.\n  std::vector<int> ops_to_replace =\n      partition_helper.GetNodesOfFirstNLargestPartitions(\n          max_delegated_partitions);\n\n  if (!unsupported_nodes_info.empty()) {\n    std::string unsupported = absl::StrJoin(unsupported_nodes_info, \"\\n\");\n    std::string error_message = absl::StrCat(\n        \"Following operations are not supported by GPU delegate:\\n\",\n        unsupported, \"\\n\");\n    if (!ops_to_replace.empty()) {\n      absl::StrAppend(\n          &error_message, ops_to_replace.size(),\n          \" operations will run on the GPU, and the remaining \",\n          partition_helper.num_total_nodes() - ops_to_replace.size());\n    } else {\n      absl::StrAppend(&error_message,\n                      \"No operations will run on the GPU, and all \",\n                      partition_helper.num_total_nodes());\n    }\n    absl::StrAppend(&error_message, \" operations will run on the CPU.\");\n    TF_LITE_KERNEL_LOG(context, error_message.c_str());\n  }\n  return ConvertVectorToTfLiteIntArray(ops_to_replace);\n}\n\n// Creates inputs and outputs passed by io_tensors parameters in the resulting\n// graph. We force it to make sure that delegated subgraph has same order of\n// inputs and outputs with the original one. When delegated model is built from\n// the tflite model representation tensors are created lazily, so there is no\n// guarantee that the order will match the source model tensors order.\nabsl::Status PrecreateIOTensors(\n    TfLiteContext* context, GraphFloat32* graph, TfLiteIntArray* io_tensors,\n    absl::flat_hash_map<int, int>* quant_conversion_map,\n    absl::flat_hash_map<int, Value*>* tensor_to_value) {\n  for (int i = 0; i < io_tensors->size; ++i) {\n    const int tensor_index = io_tensors->data[i];\n    const TfLiteTensor& tflite_tensor = context->tensors[tensor_index];\n    if (tflite::IsConstantTensor(&tflite_tensor)) continue;\n    RETURN_IF_ERROR(ObjectReader::ReadNonConstantTensor(\n        context, tensor_to_value, quant_conversion_map, graph, tensor_index));\n  }\n  return absl::OkStatus();\n}\n\nabsl::Status CopyVariableTensorOutputs(\n    TfLiteNode* tflite_node, TfLiteRegistration* registration,\n    GraphFloat32* graph, ObjectReader& reader,\n    const absl::flat_hash_map<int, ValueId>& new_variable_tensor_values) {\n  absl::flat_hash_map<int, ValueId> new_variable_tensor_values_copy(\n      new_variable_tensor_values);\n  // Retrieve the final value id for the variable input tensors.\n  for (int i = 0; i < tflite_node->inputs->size; i++) {\n    int tensor_idx = tflite_node->inputs->data[i];\n    Value* value;\n    if (!reader.ReadValueByTensorIdx(tensor_idx, &value).ok()) continue;\n    if (value->tensor.is_variable_input) {\n      if (new_variable_tensor_values_copy.find(i) ==\n          new_variable_tensor_values_copy.end()) {\n        return absl::InvalidArgumentError(\n            absl::StrCat(GetOpNameByRegistration(*registration),\n                         \" did not provide a new value for the variable input \"\n                         \"tensor with index \",\n                         tensor_idx));\n      } else {\n        Node* node = graph->NewNode();\n        node->operation.type = ToString(OperationType::COPY);\n        RETURN_IF_ERROR(graph->AddConsumer(\n            node->id, new_variable_tensor_values_copy.at(i)));\n        RETURN_IF_ERROR(reader.AddUpdate(node, i));\n        new_variable_tensor_values_copy.erase(\n            new_variable_tensor_values_copy.find(i));\n      }\n    }\n  }\n  if (!new_variable_tensor_values_copy.empty()) {\n    return absl::InvalidArgumentError(\n        \"More input variable tensors asked to be copied than present on the \"\n        \"node\");\n  }\n  return absl::OkStatus();\n}\n\nabsl::Status BuildModel(TfLiteContext* context,\n                        const TfLiteDelegateParams* delegate_params,\n                        GraphFloat32* graph,\n                        absl::flat_hash_map<int, int>* quant_conversion_map) {\n  std::vector<std::unique_ptr<TFLiteOperationParser>> operations;\n  std::vector<int> tflite_nodes;\n  for (int i = 0; i < delegate_params->nodes_to_replace->size; ++i) {\n    TfLiteNode* tflite_node = nullptr;\n    TfLiteRegistration* registration = nullptr;\n    RETURN_IF_ERROR(GetNodeAndRegistration(\n        context, delegate_params->nodes_to_replace->data[i], &tflite_node,\n        &registration));\n    if (registration->builtin_code == kTfLiteBuiltinDequantize &&\n        context->tensors[tflite_node->inputs->data[0]].type ==\n            TfLiteType::kTfLiteFloat16) {\n      // Ignore Fp16 Dequantize nodes.\n      continue;\n    }\n    auto op_parser = NewOperationParser(\n        registration, /*allow_quant_ops=*/quant_conversion_map != nullptr);\n    if (!op_parser) {\n      return absl::UnimplementedError(\n          absl::StrCat(\"Operation \", registration->builtin_code, \"(\",\n                       registration->custom_name,\n                       \") is not supported by TFLite GPU Delegate.\"));\n    }\n    operations.push_back(std::move(op_parser));\n    tflite_nodes.push_back(i);\n  }\n  absl::flat_hash_map<int, Value*> tensor_to_value;\n  std::vector<ValueId> variable_inputs_to_value_id;\n  RETURN_IF_ERROR(PrecreateIOTensors(context, graph,\n                                     delegate_params->input_tensors,\n                                     quant_conversion_map, &tensor_to_value));\n  RETURN_IF_ERROR(PrecreateIOTensors(context, graph,\n                                     delegate_params->output_tensors,\n                                     quant_conversion_map, &tensor_to_value));\n  for (int i = 0; i < operations.size(); ++i) {\n    TfLiteNode* tflite_node;\n    TfLiteRegistration* registration;\n    RETURN_IF_ERROR(GetNodeAndRegistration(\n        context, delegate_params->nodes_to_replace->data[tflite_nodes[i]],\n        &tflite_node, &registration));\n    ObjectReader reader(graph, context, tflite_node, &tensor_to_value,\n                        quant_conversion_map);\n    const auto status =\n        operations[i]->Parse(tflite_node, registration, graph, &reader);\n    if (!status.ok()) {\n      return absl::InternalError(absl::StrCat(\n          GetOpNameByRegistration(*registration), \": \", status.message()));\n    }\n\n    absl::flat_hash_map<int, ValueId> new_value_for_variable_input_tensors =\n        operations[i]->GetNewValueIdsForVariableInputNodes();\n\n    RETURN_IF_ERROR(\n        CopyVariableTensorOutputs(tflite_node, registration, graph, reader,\n                                  new_value_for_variable_input_tensors));\n  }\n\n  // Variable input tensors expect to be unchanged throughout model execution.\n  // They need to be an output of the graph in order to have them unchanged.\n  for (auto value_id : variable_inputs_to_value_id) {\n    if (!graph->IsGraphOutput(value_id)) {\n      return absl::InvalidArgumentError(\n          absl::StrCat(\"Variable input tensors must be a graph output. Value \",\n                       value_id, \" is not a graph output\"));\n    }\n  }\n  return absl::OkStatus();\n}\n\nabsl::Status BuildFinalModel(\n    TfLiteContext* context, const TfLiteDelegateParams* delegate_params,\n    GraphFloat32* graph, absl::flat_hash_map<int, int>* quant_conversion_map) {\n  RETURN_IF_ERROR(\n      BuildModel(context, delegate_params, graph, quant_conversion_map));\n\n  // Apply general transformations on the graph.\n  NullTransformationReporter reporter;\n  ModelTransformer transformer(graph, &reporter);\n  if (!ApplyModelTransformations(&transformer)) {\n    return absl::InternalError(\"Graph transformations failed\");\n  }\n  return absl::OkStatus();\n}\n\n}  // namespace gpu\n}  // namespace tflite"