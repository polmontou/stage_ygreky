"/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#include \"tensorflow/lite/kernels/internal/reference/quantize.h\"\n\n#include <cstddef>\n#include <cstdint>\n\n#include \"tensorflow/lite/c/common.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/optimized_ops.h\"\n#include \"tensorflow/lite/kernels/internal/quantization_util.h\"\n#include \"tensorflow/lite/kernels/internal/reference/reference_ops.h\"\n#include \"tensorflow/lite/kernels/internal/reference/requantize.h\"\n#include \"tensorflow/lite/kernels/internal/tensor.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_ctypes.h\"\n#include \"tensorflow/lite/kernels/internal/types.h\"\n#include \"tensorflow/lite/kernels/kernel_util.h\"\n\nnamespace tflite {\nnamespace ops {\nnamespace builtin {\nnamespace quantize {\n\n// This file has two implementation of Quantize.\nenum KernelType {\n  kReference,\n  kGenericOptimized,\n};\n\nstruct OpData {\n  int32_t output_multiplier;\n  int output_shift;\n};\n\nnamespace {\ntemplate <KernelType kernel_type, typename output_type>\nstatic inline void AffineQuantize(const tflite::QuantizationParams& op_params,\n                                  const RuntimeShape& input_shape,\n                                  const float* input_data,\n                                  const RuntimeShape& output_shape,\n                                  output_type* output_data) {\n  if (kernel_type == kReference) {\n    reference_ops::AffineQuantize(op_params, input_shape, input_data,\n                                  output_shape, output_data);\n  } else {\n    optimized_ops::AffineQuantize(op_params, input_shape, input_data,\n                                  output_shape, output_data);\n  }\n}\n\ntemplate <KernelType kernel_type, typename input_type, typename output_type>\nstatic inline void Requantize(const input_type* input_data, int32_t size,\n                              int32_t effective_scale_multiplier,\n                              int32_t effective_scale_shift,\n                              int32_t input_zeropoint, int32_t output_zeropoint,\n                              output_type* output_data) {\n  if (kernel_type == kReference) {\n    reference_ops::Requantize(input_data, size, effective_scale_multiplier,\n                              effective_scale_shift, input_zeropoint,\n                              output_zeropoint, output_data);\n  } else {\n    optimized_ops::Requantize(input_data, size, effective_scale_multiplier,\n                              effective_scale_shift, input_zeropoint,\n                              output_zeropoint, output_data);\n  }\n}\n\nvoid ReportError(TfLiteContext* context, TfLiteType input_type,\n                 TfLiteType output_type) {\n  context->ReportError(\n      context, \"Input type %s with Output type %s is not currently supported.\",\n      TfLiteTypeGetName(input_type), TfLiteTypeGetName(output_type));\n}\n}  // namespace\n\nvoid* Init(TfLiteContext* context, const char* buffer, size_t length) {\n  return new OpData;\n}\n\nvoid Free(TfLiteContext* context, void* buffer) {\n  delete static_cast<OpData*>(buffer);\n}\n\nTfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n  OpData* data = static_cast<OpData*>(node->user_data);\n  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);\n  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n\n  const TfLiteTensor* input = GetInput(context, node, 0);\n  TfLiteTensor* output = GetOutput(context, node, 0);\n\n  // TODO(b/128934713): Add support for fixed-point per-channel quantization.\n  // Currently this only support affine per-layer quantization.\n  TF_LITE_ENSURE_EQ(context, output->quantization.type,\n                    kTfLiteAffineQuantization);\n  const auto* affine_quantization =\n      static_cast<TfLiteAffineQuantization*>(output->quantization.params);\n  TF_LITE_ENSURE(context, affine_quantization);\n  TF_LITE_ENSURE(context, affine_quantization->scale);\n  TF_LITE_ENSURE(context, affine_quantization->scale->size == 1);\n\n  if (input->type == kTfLiteFloat32) {\n    // Quantize use case.\n    TF_LITE_ENSURE(context, output->type == kTfLiteUInt8 ||\n                                output->type == kTfLiteInt8 ||\n                                output->type == kTfLiteInt16);\n  } else {\n    // Requantize use case.\n    if (input->type == kTfLiteInt16) {\n      TF_LITE_ENSURE(\n          context, output->type == kTfLiteInt8 || output->type == kTfLiteInt16);\n    } else {\n      TF_LITE_ENSURE(context,\n                     input->type == kTfLiteInt8 || input->type == kTfLiteUInt8);\n      TF_LITE_ENSURE(\n          context, output->type == kTfLiteUInt8 || output->type == kTfLiteInt8);\n    }\n    const double effective_output_scale =\n        static_cast<double>(input->params.scale) /\n        static_cast<double>(output->params.scale);\n    QuantizeMultiplier(effective_output_scale, &data->output_multiplier,\n                       &data->output_shift);\n  }\n\n  return context->ResizeTensor(context, output,\n                               TfLiteIntArrayCopy(input->dims));\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n  OpData* data = static_cast<OpData*>(node->user_data);\n\n  const TfLiteTensor* input = GetInput(context, node, 0);\n  TfLiteTensor* output = GetOutput(context, node, 0);\n\n  const RuntimeShape input_shape = GetTensorShape(input);\n  const RuntimeShape output_shape = GetTensorShape(output);\n\n  switch (input->type) {\n    case kTfLiteFloat32: {\n      // Float to int8, uint8, int16.\n      tflite::QuantizationParams op_params;\n      op_params.zero_point = output->params.zero_point;\n      op_params.scale = output->params.scale;\n      const float* input_data = GetTensorData<float>(input);\n      switch (output->type) {\n        case kTfLiteInt8:\n          AffineQuantize<kernel_type>(op_params, input_shape, input_data,\n                                      output_shape,\n                                      GetTensorData<int8_t>(output));\n          return kTfLiteOk;\n        case kTfLiteUInt8:\n          AffineQuantize<kernel_type>(op_params, input_shape, input_data,\n                                      output_shape,\n                                      GetTensorData<uint8_t>(output));\n          return kTfLiteOk;\n        case kTfLiteInt16:\n          AffineQuantize<kernel_type>(op_params, input_shape, input_data,\n                                      output_shape,\n                                      GetTensorData<int16_t>(output));\n          return kTfLiteOk;\n        default:\n          ReportError(context, input->type, output->type);\n          return kTfLiteError;\n      }\n    }\n    case kTfLiteInt16: {\n      // int16 to int8 or int16.\n      switch (output->type) {\n        case kTfLiteInt8:\n          Requantize<kernel_type>(GetTensorData<int16_t>(input),\n                                  MatchingFlatSize(input_shape, output_shape),\n                                  data->output_multiplier, data->output_shift,\n                                  input->params.zero_point,\n                                  output->params.zero_point,\n                                  GetTensorData<int8_t>(output));\n          return kTfLiteOk;\n        case kTfLiteInt16:\n          Requantize<kernel_type>(GetTensorData<int16_t>(input),\n                                  MatchingFlatSize(input_shape, output_shape),\n                                  data->output_multiplier, data->output_shift,\n                                  input->params.zero_point,\n                                  output->params.zero_point,\n                                  GetTensorData<int16_t>(output));\n          return kTfLiteOk;\n        default:\n          ReportError(context, input->type, output->type);\n          return kTfLiteError;\n      }\n    }\n    case kTfLiteInt8: {\n      // int8 to int8, uint8.\n      const int32_t size = MatchingFlatSize(input_shape, output_shape);\n      const int8_t* input_data = GetTensorData<int8_t>(input);\n      switch (output->type) {\n        case kTfLiteInt8:\n          Requantize<kernel_type>(input_data, size, data->output_multiplier,\n                                  data->output_shift, input->params.zero_point,\n                                  output->params.zero_point,\n                                  GetTensorData<int8_t>(output));\n          return kTfLiteOk;\n        case kTfLiteUInt8:\n          Requantize<kernel_type>(input_data, size, data->output_multiplier,\n                                  data->output_shift, input->params.zero_point,\n                                  output->params.zero_point,\n                                  GetTensorData<uint8_t>(output));\n          return kTfLiteOk;\n        default:\n          ReportError(context, input->type, output->type);\n          return kTfLiteError;\n      }\n    }\n    case kTfLiteUInt8: {\n      // uint8 to int8, uint8.\n      const int32_t size = MatchingFlatSize(input_shape, output_shape);\n      const uint8_t* input_data = GetTensorData<uint8_t>(input);\n      switch (output->type) {\n        case kTfLiteInt8:\n          Requantize<kernel_type>(input_data, size, data->output_multiplier,\n                                  data->output_shift, input->params.zero_point,\n                                  output->params.zero_point,\n                                  GetTensorData<int8_t>(output));\n          return kTfLiteOk;\n        case kTfLiteUInt8:\n          Requantize<kernel_type>(input_data, size, data->output_multiplier,\n                                  data->output_shift, input->params.zero_point,\n                                  output->params.zero_point,\n                                  GetTensorData<uint8_t>(output));\n          return kTfLiteOk;\n        default:\n          ReportError(context, input->type, output->type);\n          return kTfLiteError;\n      }\n    }\n    default:\n      ReportError(context, input->type, output->type);\n      return kTfLiteError;\n  }\n}\n\n}  // namespace quantize\n\n// This Op (QUANTIZE) quantizes the input and produces quantized output.\n// The input can be either float or quantized. If the input is float,\n// AffineQuantize takes scale and zero point and quantize the float value to\n// quantized output, in int8 or uint8 format. If the input is quantized value,\n// the op requantize the input (of a certain type, with a given scale and zero\n// point) to the output of the same or different type with a same or different\n// scale and zero point.\nTfLiteRegistration* Register_QUANTIZE_OPT() {\n  static TfLiteRegistration r = {quantize::Init, quantize::Free,\n                                 quantize::Prepare,\n                                 quantize::Eval<quantize::kGenericOptimized>};\n  return &r;\n}\n\nTfLiteRegistration* Register_QUANTIZE_REF() {\n  static TfLiteRegistration r = {quantize::Init, quantize::Free,\n                                 quantize::Prepare,\n                                 quantize::Eval<quantize::kReference>};\n  return &r;\n}\n\nTfLiteRegistration* Register_QUANTIZE() { return Register_QUANTIZE_OPT(); }\n\n}  // namespace builtin\n}  // namespace ops\n}  // namespace tflite"