"/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include <math.h>\n#include <stdint.h>\n#include <stdlib.h>\n\n#include <functional>\n#include <type_traits>\n\n#include \"tensorflow/lite/c/common.h\"\n#include \"tensorflow/lite/kernels/internal/reference/reference_ops.h\"\n#include \"tensorflow/lite/kernels/internal/tensor.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_ctypes.h\"\n#include \"tensorflow/lite/kernels/kernel_util.h\"\n\nnamespace tflite {\nnamespace ops {\nnamespace builtin {\nnamespace range {\nnamespace {\n\nconstexpr int kStartTensor = 0;\nconstexpr int kLimitTensor = 1;\nconstexpr int kDeltaTensor = 2;\nconstexpr int kOutputTensor = 0;\n\ntemplate <typename T>\nTfLiteStatus GetSize(TfLiteContext* context, T start, T limit, T delta,\n                     int* size) {\n  TF_LITE_ENSURE(context, !std::equal_to<T>()(delta, 0));\n  TF_LITE_ENSURE(\n      context, (start >= limit && delta < 0) || (start <= limit && delta > 0));\n  *size =\n      (std::is_integral<T>::value\n           ? ((std::abs(limit - start) + std::abs(delta) - 1) / std::abs(delta))\n           : std::ceil(std::abs((limit - start) / delta)));\n  return kTfLiteOk;\n}\n\nTfLiteStatus ResizeOutput(TfLiteContext* context, const TfLiteTensor* start,\n                          const TfLiteTensor* limit, const TfLiteTensor* delta,\n                          TfLiteTensor* output) {\n  // The output will always be a 1-d array.\n  int size = 0;\n  switch (start->type) {\n    case kTfLiteInt32: {\n      TF_LITE_ENSURE_OK(context,\n                        GetSize(context, *GetTensorData<int32_t>(start),\n                                *GetTensorData<int32_t>(limit),\n                                *GetTensorData<int32_t>(delta), &size));\n      break;\n    }\n    case kTfLiteFloat32: {\n      TF_LITE_ENSURE_OK(context, GetSize(context, *GetTensorData<float>(start),\n                                         *GetTensorData<float>(limit),\n                                         *GetTensorData<float>(delta), &size));\n      break;\n    }\n    default: {\n      context->ReportError(context, \"Unknown data type: %d\", start->type);\n      return kTfLiteError;\n    }\n  }\n  TfLiteIntArray* output_shape_array = TfLiteIntArrayCreate(1);\n  output_shape_array->data[0] = size;\n  return context->ResizeTensor(context, output, output_shape_array);\n}\n\nTfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n  TF_LITE_ENSURE_EQ(context, NumInputs(node), 3);\n  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n\n  const TfLiteTensor* start = GetInput(context, node, kStartTensor);\n  const TfLiteTensor* limit = GetInput(context, node, kLimitTensor);\n  const TfLiteTensor* delta = GetInput(context, node, kDeltaTensor);\n  // Make sure all the inputs are scalars.\n  TF_LITE_ENSURE_EQ(context, NumDimensions(start), 0);\n  TF_LITE_ENSURE_EQ(context, NumDimensions(limit), 0);\n  TF_LITE_ENSURE_EQ(context, NumDimensions(delta), 0);\n\n  // Currently only supports int32 and float.\n  // TODO(b/117912892): Support quantization as well.\n  const auto dtype = start->type;\n  if (dtype != kTfLiteFloat32 && dtype != kTfLiteInt32) {\n    context->ReportError(context, \"Unknown index output data type: %s\",\n                         TfLiteTypeGetName(dtype));\n    return kTfLiteError;\n  }\n\n  TF_LITE_ENSURE_TYPES_EQ(context, limit->type, dtype);\n  TF_LITE_ENSURE_TYPES_EQ(context, delta->type, dtype);\n\n  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n  output->type = dtype;\n\n  if (IsConstantTensor(start) && IsConstantTensor(limit) &&\n      IsConstantTensor(delta)) {\n    return ResizeOutput(context, start, limit, delta, output);\n  }\n\n  SetTensorToDynamic(output);\n  return kTfLiteOk;\n}\n\ntemplate <typename T>\nvoid EvalImpl(const TfLiteTensor* start, const TfLiteTensor* delta,\n              TfLiteTensor* output) {\n  const T start_value = *GetTensorData<T>(start);\n  const T delta_value = *GetTensorData<T>(delta);\n  T* output_data = GetTensorData<T>(output);\n  const int num_elements = NumElements(output);\n  T value = start_value;\n  for (int i = 0; i < num_elements; ++i) {\n    output_data[i] = value;\n    value += delta_value;\n  }\n}\n\nTfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n  const TfLiteTensor* start = GetInput(context, node, kStartTensor);\n  const TfLiteTensor* limit = GetInput(context, node, kLimitTensor);\n  const TfLiteTensor* delta = GetInput(context, node, kDeltaTensor);\n\n  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n\n  if (IsDynamicTensor(output)) {\n    TF_LITE_ENSURE_OK(context,\n                      ResizeOutput(context, start, limit, delta, output));\n  }\n\n  switch (output->type) {\n    case kTfLiteInt32: {\n      EvalImpl<int32_t>(start, delta, output);\n      break;\n    }\n    case kTfLiteFloat32: {\n      EvalImpl<float>(start, delta, output);\n      break;\n    }\n    default: {\n      context->ReportError(context, \"Unsupported data type: %d\", output->type);\n      return kTfLiteError;\n    }\n  }\n  return kTfLiteOk;\n}\n\n}  // namespace\n}  // namespace range\n\nTfLiteRegistration* Register_RANGE() {\n  static TfLiteRegistration r = {nullptr, nullptr, range::Prepare, range::Eval};\n  return &r;\n}\n\n}  // namespace builtin\n}  // namespace ops\n}  // namespace tflite"