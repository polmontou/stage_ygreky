"/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/lite/kernels/internal/reference/hard_swish.h\"\n\n#include \"tensorflow/lite/c/builtin_op_data.h\"\n#include \"tensorflow/lite/c/common.h\"\n#include \"tensorflow/lite/kernels/internal/common.h\"\n#include \"tensorflow/lite/kernels/internal/quantization_util.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_ctypes.h\"\n#include \"tensorflow/lite/kernels/internal/types.h\"\n#include \"tensorflow/lite/kernels/kernel_util.h\"\n#include \"tensorflow/lite/kernels/op_macros.h\"\n#include \"tensorflow/lite/micro/kernels/kernel_util.h\"\n#include \"tensorflow/lite/micro/micro_utils.h\"\n\nnamespace tflite {\nnamespace ops {\nnamespace micro {\nnamespace hard_swish {\n\nconstexpr int kInputTensor = 0;\nconstexpr int kOutputTensor = 0;\n\nvoid* HardSwishInit(TfLiteContext* context, const char* buffer, size_t length) {\n  TFLITE_DCHECK(context->AllocatePersistentBuffer != nullptr);\n  return context->AllocatePersistentBuffer(context, sizeof(HardSwishParams));\n}\n\nTfLiteStatus HardSwishPrepare(TfLiteContext* context, TfLiteNode* node) {\n  TFLITE_DCHECK(node->user_data != nullptr);\n  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);\n  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n\n  const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n\n  if (input->type == kTfLiteUInt8 || input->type == kTfLiteInt8) {\n    HardSwishParams* params = static_cast<HardSwishParams*>(node->user_data);\n\n    params->input_zero_point = input->params.zero_point;\n    params->output_zero_point = output->params.zero_point;\n\n    const float input_scale = input->params.scale;\n    const float hires_input_scale = (1.0f / 128.0f) * input_scale;\n    const float reluish_scale = 3.0f / 32768.0f;\n    const float output_scale = output->params.scale;\n\n    const double output_multiplier =\n        static_cast<double>(hires_input_scale / output_scale);\n    int32_t output_multiplier_fixedpoint_int32;\n    QuantizeMultiplier(output_multiplier, &output_multiplier_fixedpoint_int32,\n                       &params->output_multiplier_exponent);\n    DownScaleInt32ToInt16Multiplier(\n        output_multiplier_fixedpoint_int32,\n        &params->output_multiplier_fixedpoint_int16);\n\n    TF_LITE_ENSURE(context, params->output_multiplier_exponent <= 0);\n\n    const double reluish_multiplier =\n        static_cast<double>(hires_input_scale / reluish_scale);\n    int32_t reluish_multiplier_fixedpoint_int32;\n    QuantizeMultiplier(reluish_multiplier, &reluish_multiplier_fixedpoint_int32,\n                       &params->reluish_multiplier_exponent);\n    DownScaleInt32ToInt16Multiplier(\n        reluish_multiplier_fixedpoint_int32,\n        &params->reluish_multiplier_fixedpoint_int16);\n  }\n\n  return kTfLiteOk;\n}\n\nTfLiteStatus HardSwishEval(TfLiteContext* context, TfLiteNode* node) {\n  const TfLiteEvalTensor* input =\n      tflite::micro::GetEvalInput(context, node, kInputTensor);\n  TfLiteEvalTensor* output =\n      tflite::micro::GetEvalOutput(context, node, kOutputTensor);\n  HardSwishParams* params = static_cast<HardSwishParams*>(node->user_data);\n\n  switch (input->type) {\n    case kTfLiteFloat32: {\n      tflite::reference_ops::HardSwish<float>(\n          tflite::micro::GetTensorShape(input),\n          tflite::micro::GetTensorData<float>(input),\n          tflite::micro::GetTensorShape(output),\n          tflite::micro::GetTensorData<float>(output));\n    } break;\n    case kTfLiteUInt8: {\n      tflite::reference_ops::HardSwish<uint8_t>(\n          *params, tflite::micro::GetTensorShape(input),\n          tflite::micro::GetTensorData<uint8_t>(input),\n          tflite::micro::GetTensorShape(output),\n          tflite::micro::GetTensorData<uint8_t>(output));\n    } break;\n    case kTfLiteInt8: {\n      tflite::reference_ops::HardSwish<int8_t>(\n          *params, tflite::micro::GetTensorShape(input),\n          tflite::micro::GetTensorData<int8_t>(input),\n          tflite::micro::GetTensorShape(output),\n          tflite::micro::GetTensorData<int8_t>(output));\n    } break;\n    default: {\n      TF_LITE_KERNEL_LOG(\n          context,\n          \"Only float32/int8_t/uint8_t are supported currently, got %s\",\n          TfLiteTypeGetName(input->type));\n      return kTfLiteError;\n    }\n  }\n  return kTfLiteOk;\n}\n\n}  // namespace hard_swish\n\nTfLiteRegistration Register_HARD_SWISH() {\n  return {/*init=*/hard_swish::HardSwishInit,\n          /*free=*/nullptr,\n          /*prepare=*/hard_swish::HardSwishPrepare,\n          /*invoke=*/hard_swish::HardSwishEval,\n          /*profiling_string=*/nullptr,\n          /*builtin_code=*/0,\n          /*custom_name=*/nullptr,\n          /*version=*/0};\n}\n\n}  // namespace micro\n}  // namespace ops\n}  // namespace tflite"