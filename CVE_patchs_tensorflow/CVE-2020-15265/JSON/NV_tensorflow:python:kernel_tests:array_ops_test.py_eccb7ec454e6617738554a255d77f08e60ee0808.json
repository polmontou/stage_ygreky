"# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for array_ops.\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\nimport time\nimport unittest\n\nfrom absl.testing import parameterized\nimport numpy as np\n\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.python.client import session\nfrom tensorflow.python.eager import backprop\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_spec\nfrom tensorflow.python.framework import test_ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gen_array_ops\nfrom tensorflow.python.ops import gradient_checker_v2\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import list_ops\nfrom tensorflow.python.ops import map_fn\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import resource_variable_ops\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.platform import test as test_lib\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass BatchMatrixTransposeTest(test_util.TensorFlowTestCase):\n\n  def testNonBatchMatrix(self):\n    matrix = [[1, 2, 3], [4, 5, 6]]  # Shape (2, 3)\n    expected_transposed = [[1, 4], [2, 5], [3, 6]]  # Shape (3, 2)\n    transposed = array_ops.matrix_transpose(matrix)\n    self.assertEqual((3, 2), transposed.get_shape())\n    self.assertAllEqual(expected_transposed, transposed)\n\n  def testConjugate(self):\n    m = [[1 + 1j, 2 + 2j, 3 + 3j], [4 + 4j, 5 + 5j, 6 + 6j]]\n    expected_transposed = [[1 - 1j, 4 - 4j], [2 - 2j, 5 - 5j], [3 - 3j, 6 - 6j]]\n    matrix = ops.convert_to_tensor(m)\n    transposed = array_ops.matrix_transpose(matrix, conjugate=True)\n    self.assertEqual((3, 2), transposed.get_shape())\n    self.assertAllEqual(expected_transposed, transposed)\n\n  def testBatchMatrix(self):\n    matrix_0 = [[1, 2, 3], [4, 5, 6]]\n    matrix_0_t = [[1, 4], [2, 5], [3, 6]]\n    matrix_1 = [[11, 22, 33], [44, 55, 66]]\n    matrix_1_t = [[11, 44], [22, 55], [33, 66]]\n    batch_matrix = [matrix_0, matrix_1]  # Shape (2, 2, 3)\n    expected_transposed = [matrix_0_t, matrix_1_t]  # Shape (2, 3, 2)\n    transposed = array_ops.matrix_transpose(batch_matrix)\n    self.assertEqual((2, 3, 2), transposed.get_shape())\n    self.assertAllEqual(expected_transposed, transposed)\n\n  def testNonBatchMatrixDynamicallyDefined(self):\n    # needs explicit `constant` because lists are not automatically\n    # converted to sensors when applying `transpose` below\n    matrix = constant_op.constant([[1, 2, 3], [4, 5, 6]])  # Shape (2, 3)\n    expected_transposed = [[1, 4], [2, 5], [3, 6]]  # Shape (3, 2)\n\n    @def_function.function(input_signature=[\n        tensor_spec.TensorSpec(shape=None, dtype=dtypes.int32)\n    ])\n    def transpose(matrix):\n      self.assertIs(matrix.shape.ndims, None)\n      return array_ops.matrix_transpose(matrix)\n\n    self.assertAllEqual(expected_transposed, transpose(matrix))\n\n  def testBatchMatrixDynamicallyDefined(self):\n    matrix_0 = [[1, 2, 3], [4, 5, 6]]\n    matrix_0_t = [[1, 4], [2, 5], [3, 6]]\n    matrix_1 = [[11, 22, 33], [44, 55, 66]]\n    matrix_1_t = [[11, 44], [22, 55], [33, 66]]\n    # needs explicit `constant` because lists are not automatically\n    # converted to sensors when applying `transpose` below\n    batch_matrix = constant_op.constant([matrix_0, matrix_1])  # Shape (2, 2, 3)\n    expected_transposed = [matrix_0_t, matrix_1_t]  # Shape (2, 3, 2)\n\n    @def_function.function(input_signature=[\n        tensor_spec.TensorSpec(shape=None, dtype=dtypes.int32)\n    ])\n    def transpose(matrix):\n      self.assertIs(matrix.shape.ndims, None)\n      return array_ops.matrix_transpose(matrix)\n\n    self.assertAllEqual(expected_transposed, transpose(batch_matrix))\n\n  def testTensorWithStaticRankLessThanTwoRaisesBecauseNotAMatrix(self):\n    vector = [1, 2, 3]\n    with self.assertRaisesRegex(ValueError, \"should be a \"):\n      array_ops.matrix_transpose(vector)\n\n\nclass BooleanMaskTest(test_util.TensorFlowTestCase):\n\n  def setUp(self):\n    self.rng = np.random.RandomState(42)\n\n  def CheckVersusNumpy(self, ndims_mask, arr_shape, make_mask=None, axis=None):\n    \"\"\"Check equivalence between boolean_mask and numpy masking.\"\"\"\n    if make_mask is None:\n      make_mask = lambda shape: self.rng.randint(0, 2, size=shape).astype(bool)\n    arr = np.random.rand(*arr_shape)\n    mask = make_mask(arr_shape[:ndims_mask])\n    if axis is not None:\n      mask = make_mask(arr_shape[axis:ndims_mask + axis])\n    if axis is None or axis == 0:\n      masked_arr = arr[mask]\n    elif axis == 1:\n      masked_arr = arr[:, mask]\n    elif axis == 2:\n      masked_arr = arr[:, :, mask]\n    with self.cached_session():\n      masked_tensor = array_ops.boolean_mask(arr, mask, axis=axis)\n\n      # Leading dimension size of masked_tensor is always unknown until runtime\n      # since we don't how many elements will be kept.\n      leading = 1 if axis is None else axis + 1\n      self.assertAllEqual(masked_tensor.get_shape()[leading:],\n                          masked_arr.shape[leading:])\n\n      self.assertAllClose(masked_arr, masked_tensor)\n\n  @test_util.run_deprecated_v1\n  def testMaskDim1ArrDim2Axis1(self):\n    ndims_mask = 1\n    for arr_shape in [(1, 1), (2, 2), (2, 5)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape, axis=1)\n\n  @test_util.run_deprecated_v1\n  def testMaskDim2ArrDim2Axis1(self):\n    ndims_mask = 2\n    for arr_shape in [(1, 1), (2, 2), (2, 5)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape, axis=1)\n\n  @test_util.run_deprecated_v1\n  def testMaskDim1ArrDim1(self):\n    ndims_mask = 1\n    for arr_shape in [(1,), (2,), (3,), (10,)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape)\n\n  @test_util.run_deprecated_v1\n  def testMaskDim1ArrDim2(self):\n    ndims_mask = 1\n    for arr_shape in [(1, 1), (2, 2), (2, 5)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape)\n\n  @test_util.run_deprecated_v1\n  def testMaskDim2ArrDim2(self):\n    ndims_mask = 2\n    for arr_shape in [(1, 1), (2, 2), (2, 5)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape)\n\n  @test_util.run_deprecated_v1\n  def testMaskDim2ArrDim3(self):\n    ndims_mask = 2\n    for arr_shape in [(1, 1, 1), (1, 2, 2), (2, 2, 1)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape)\n\n  @test_util.run_deprecated_v1\n  def testEmptyInput2D(self):\n    mask = np.array([True, False])\n    arr = np.array([[], []]).astype(np.float32)\n    numpy_result = arr[mask]\n    tf_result = array_ops.boolean_mask(arr, mask)\n    self.assertAllEqual(numpy_result.shape[1:], tf_result.get_shape()[1:])\n    with self.cached_session():\n      self.assertAllClose(numpy_result, tf_result)\n\n  @test_util.run_deprecated_v1\n  def testEmptyInput1D(self):\n    mask = np.array([]).astype(bool)\n    arr = np.array([]).astype(np.float32)\n    numpy_result = arr[mask]\n    tf_result = array_ops.boolean_mask(arr, mask)\n    self.assertAllEqual(numpy_result.shape[1:], tf_result.get_shape()[1:])\n    with self.cached_session():\n      self.assertAllClose(numpy_result, tf_result)\n\n  @test_util.run_deprecated_v1\n  def testEmptyOutput(self):\n    make_mask = lambda shape: np.zeros(shape, dtype=bool)\n    for ndims_mask in range(1, 4):\n      for ndims_arr in range(ndims_mask, ndims_mask + 3):\n        for _ in range(3):\n          with self.subTest(ndims_mask=ndims_mask, ndims_arr=ndims_arr, _=_):\n            arr_shape = np.random.randint(1, 5, size=ndims_arr)\n            self.CheckVersusNumpy(ndims_mask, arr_shape, make_mask=make_mask)\n\n  @test_util.run_deprecated_v1\n  def testWorksWithDimensionsEqualToNoneDuringGraphBuild(self):\n    # The rank of the mask tensor must be specified. This is explained\n    # in the docstring as well.\n    with self.cached_session() as sess:\n      ph_tensor = array_ops.placeholder(dtypes.int32, shape=None)\n      ph_mask = array_ops.placeholder(dtypes.bool, shape=[None])\n\n      arr = np.array([[1, 2], [3, 4]])\n      mask = np.array([False, True])\n\n      masked_tensor = sess.run(\n          array_ops.boolean_mask(ph_tensor, ph_mask),\n          feed_dict={\n              ph_tensor: arr,\n              ph_mask: mask\n          })\n      np.testing.assert_allclose(masked_tensor, arr[mask])\n\n  @test_util.run_deprecated_v1\n  def testMaskDimensionsSetToNoneRaises(self):\n    # The rank of the mask tensor must be specified. This is explained\n    # in the docstring as well.\n    with self.cached_session():\n      tensor = array_ops.placeholder(dtypes.int32, shape=[None, 2])\n      mask = array_ops.placeholder(dtypes.bool, shape=None)\n      with self.assertRaisesRegex(ValueError, \"dimensions must be specified\"):\n        array_ops.boolean_mask(tensor, mask)\n\n  def testMaskHasMoreDimsThanTensorRaises(self):\n    mask = [[True, True], [False, False]]\n    tensor = [1, 2, 3, 4]\n    with self.cached_session():\n      with self.assertRaisesRegex(ValueError, \"incompatible\"):\n        array_ops.boolean_mask(tensor, mask).eval()\n\n  def testMaskIsScalarRaises(self):\n    mask = True\n    tensor = 1\n    with self.cached_session():\n      with self.assertRaisesRegex(ValueError, \"mask.*scalar\"):\n        array_ops.boolean_mask(tensor, mask).eval()\n\n  def testMaskShapeDifferentThanFirstPartOfTensorShapeRaises(self):\n    mask = [True, True, True]\n    tensor = [[1, 2], [3, 4]]\n    with self.cached_session():\n      with self.assertRaisesRegex(ValueError, \"incompatible\"):\n        array_ops.boolean_mask(tensor, mask).eval()\n\n  @test_util.run_deprecated_v1\n  def testStringMask(self):\n    # Reproduces b/111171330, where the optimized boolean_mask graph would\n    # be incorrectly placed on GPU.\n    with ops.Graph().as_default():\n      tile_placeholder = array_ops.placeholder(dtypes.int32, [2])\n      string_tensor = array_ops.tile([[\"hello\"]], tile_placeholder)\n      bool_tensor = array_ops.tile([[True]], tile_placeholder)\n      masked_tensor = array_ops.boolean_mask(string_tensor, bool_tensor)\n      config = config_pb2.ConfigProto()\n      config.graph_options.rewrite_options.shape_optimization = 1\n      config.gpu_options.per_process_gpu_memory_fraction = 0.3\n      with session.Session(config=config) as sess:\n        result = sess.run(masked_tensor, feed_dict={tile_placeholder: [2, 2]})\n        self.assertAllEqual([b\"hello\", b\"hello\", b\"hello\", b\"hello\"], result)\n\n  def testMaskWithAxisTensor(self):\n\n    @def_function.function(autograph=False)\n    def f():\n      return array_ops.boolean_mask([1, 2, 3], [True, False, True],\n                                    axis=constant_op.constant(\n                                        0, dtype=dtypes.int32))\n\n    self.assertAllEqual(self.evaluate(f()), [1, 3])\n\n  def testMaskWithAxisNonConstTensor(self):\n\n    @def_function.function(\n        autograph=False,\n        input_signature=[\n            tensor_spec.TensorSpec(shape=None, dtype=dtypes.int32)\n        ])\n    def f(axis):\n      return array_ops.boolean_mask([1, 2, 3], [True, False, True], axis=axis)\n\n    self.assertAllEqual(\n        self.evaluate(f(constant_op.constant(0, dtype=dtypes.int32))), [1, 3])\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass OperatorShapeTest(test_util.TensorFlowTestCase):\n\n  def testExpandScalar(self):\n    scalar = \"hello\"\n    scalar_expanded = array_ops.expand_dims(scalar, [0])\n    self.assertEqual(scalar_expanded.get_shape(), (1,))\n\n  def testSqueezeScalar(self):\n    scalar = \"hello\"\n    scalar_squeezed = array_ops.squeeze(scalar, ())\n    self.assertEqual(scalar_squeezed.get_shape(), ())\n\n  def testSqueezeMatrix(self):\n    matrix = [[1, 2, 3]]\n    matrix_squeezed = array_ops.squeeze(matrix, [0])\n    self.assertEqual(matrix_squeezed.get_shape(), (3))\n\n    with self.assertRaisesRegex(\n        Exception, \"Can not squeeze dim.1., expected a dimension of 1, got 3\"):\n      matrix_squeezed = array_ops.squeeze(matrix, [1])\n\n  def testSqueezeScalarDim(self):\n    matrix = [[1, 2, 3]]\n    matrix_squeezed = array_ops.squeeze(matrix, 0)\n    self.assertEqual(matrix_squeezed.get_shape(), (3))\n\n  def testExpandDimsWithNonScalarDim(self):\n    with self.assertRaisesRegex(Exception,\n                                \"must be a tensor with a single value\"):\n      array_ops.expand_dims(1, axis=[0, 1])\n\n\nclass ReverseV2Test(test_util.TensorFlowTestCase):\n\n  @test_util.run_deprecated_v1\n  def testReverse0DimAuto(self):\n    x_np = 4\n    for use_gpu in [False, True]:\n      with self.subTest(use_gpu=use_gpu):\n        with self.cached_session(use_gpu=use_gpu):\n          x_tf = array_ops.reverse_v2(x_np, []).eval()\n          self.assertAllEqual(x_tf, x_np)\n\n  def _reverse1DimAuto(self, np_dtype):\n    x_np = np.array([1, 200, 3, 40, 5], dtype=np_dtype)\n\n    for use_gpu in [False, True]:\n      for axis_dtype in [dtypes.int32, dtypes.int64]:\n        with self.subTest(use_gpu=use_gpu, axis_dtype=axis_dtype):\n          with self.cached_session(use_gpu=use_gpu):\n            x_tf = array_ops.reverse_v2(\n                x_np, constant_op.constant([0], dtype=axis_dtype)).eval()\n            self.assertAllEqual(x_tf, np.asarray(x_np)[::-1])\n\n  def _reverse2DimAuto(self, np_dtype):\n    x_np = np.array([[1, 200, 3], [4, 5, 60]], dtype=np_dtype)\n\n    for reverse_f in [array_ops.reverse_v2, array_ops.reverse]:\n      for use_gpu in [False, True]:\n        for axis_dtype in [dtypes.int32, dtypes.int64]:\n          with self.subTest(\n              reverse_f=reverse_f, use_gpu=use_gpu, axis_dtype=axis_dtype):\n            with self.cached_session(use_gpu=use_gpu):\n              x_tf_1 = reverse_f(x_np,\n                                 constant_op.constant([0],\n                                                      dtype=axis_dtype)).eval()\n              x_tf_2 = reverse_f(x_np,\n                                 constant_op.constant([-2],\n                                                      dtype=axis_dtype)).eval()\n              x_tf_3 = reverse_f(x_np,\n                                 constant_op.constant([1],\n                                                      dtype=axis_dtype)).eval()\n              x_tf_4 = reverse_f(x_np,\n                                 constant_op.constant([-1],\n                                                      dtype=axis_dtype)).eval()\n              x_tf_5 = reverse_f(x_np,\n                                 constant_op.constant([1, 0],\n                                                      dtype=axis_dtype)).eval()\n              self.assertAllEqual(x_tf_1, np.asarray(x_np)[::-1, :])\n              self.assertAllEqual(x_tf_2, np.asarray(x_np)[::-1, :])\n              self.assertAllEqual(x_tf_3, np.asarray(x_np)[:, ::-1])\n              self.assertAllEqual(x_tf_4, np.asarray(x_np)[:, ::-1])\n              self.assertAllEqual(x_tf_5, np.asarray(x_np)[::-1, ::-1])\n\n  # This test covers the axis validation in the shape function\n  # (no eval())\n  @test_util.run_deprecated_v1\n  def testInvalidAxis(self):\n    x_np = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32)\n    with self.assertRaisesRegex(ValueError, \"is out of valid range\"):\n      array_ops.reverse_v2(x_np, [-30])\n    with self.assertRaisesRegex(ValueError, \"is out of valid range\"):\n      array_ops.reverse_v2(x_np, [2])\n    with self.assertRaisesRegex(ValueError, \"axis 0 specified more than once\"):\n      array_ops.reverse_v2(x_np, [0, -2])\n\n  # This is the version of reverse that uses axis indices rather than\n  # bool tensors\n  # TODO(b/32254538): Change this test to use array_ops.reverse\n  #\n  # Note: this test passes placeholder as constant axis is validated\n  # in shape function (see testInvalidAxis)\n  @test_util.run_deprecated_v1\n  def testInvalid(self):\n    x_np = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32)\n    axis = array_ops.placeholder(dtypes.int32)\n    with self.cached_session():\n      with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n                                  \"is out of.*range\"):\n        array_ops.reverse_v2(x_np, axis).eval(feed_dict={axis: [-30]})\n      with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n                                  \"is out of.*range\"):\n        array_ops.reverse_v2(x_np, axis).eval(feed_dict={axis: [2]})\n      with self.assertRaisesRegex(\n          errors_impl.InvalidArgumentError,\n          \"(axis 0 specified more than once|canonicalized axis 0 was repeated.)\"\n      ):\n        array_ops.reverse_v2(x_np, axis).eval(feed_dict={axis: [0, -2]})\n\n  @test_util.run_deprecated_v1\n  def testReverse1DimAuto(self):\n    for dtype in [\n        np.uint8, np.int8, np.uint16, np.int16, np.int32, np.int64, np.bool,\n        np.float16, np.float32, np.float64, np.complex64, np.complex128,\n        np.array(b\"\").dtype.type\n    ]:\n      self._reverse1DimAuto(dtype)\n\n  @test_util.run_deprecated_v1\n  def testReverse2DimAuto(self):\n    for dtype in [\n        np.uint8, np.int8, np.uint16, np.int16, np.int32, np.int64, np.bool,\n        np.float16, np.float32, np.float64, np.complex64, np.complex128,\n        np.array(b\"\").dtype.type\n    ]:\n      self._reverse2DimAuto(dtype)\n\n  @test_util.run_deprecated_v1\n  def testUnknownDims(self):\n    reverse_v2 = array_ops.reverse_v2\n    data_t = array_ops.placeholder(dtypes.float32)\n    axis_known_t = array_ops.placeholder(dtypes.int32, shape=[3])\n    reverse_known_t = reverse_v2(data_t, axis_known_t)\n    # Unlike V1 we cannot know this anymore\n    self.assertEqual(None, reverse_known_t.get_shape().ndims)\n\n    axis_unknown_t = array_ops.placeholder(dtypes.int32)\n    reverse_unknown_t = reverse_v2(data_t, axis_unknown_t)\n    self.assertIs(None, reverse_unknown_t.get_shape().ndims)\n\n    data_2d_t = array_ops.placeholder(dtypes.float32, shape=[None, None])\n    axis_2d_t = array_ops.placeholder(dtypes.int32, shape=[3])\n    reverse_2d_t = reverse_v2(data_2d_t, axis_2d_t)\n    self.assertEqual(2, reverse_2d_t.get_shape().ndims)\n\n  @test_util.run_deprecated_v1\n  def testReverseRowsOf3Channels(self):\n    \"\"\"Tests optimized code for reversing rows with last dim size = 3.\"\"\"\n    with self.session(use_gpu=True):\n      for reverse_f in [array_ops.reverse_v2, array_ops.reverse]:\n        for outer_size in (1, 2):\n          for middle_size in list(range(50)) + [100000]:\n            with self.subTest(\n                reverse_f=reverse_f,\n                outer_size=outer_size,\n                middle_size=middle_size):\n              x_np = np.reshape(\n                  np.arange(outer_size * middle_size * 3, dtype=np.float32),\n                  newshape=(outer_size, middle_size, 3))\n              x_tf = reverse_f(x_np, [1]).eval()\n              np_answer = x_np[:, ::-1, :]\n              self.assertAllEqual(x_tf, np_answer)\n\n  @test_util.run_deprecated_v1\n  def testReverseRowsOf4Channels(self):\n    with self.session(use_gpu=True):\n      for reverse_f in [array_ops.reverse_v2, array_ops.reverse]:\n        for outer_size in (1, 2):\n          for middle_size in list(range(50)) + [100000]:\n            with self.subTest(\n                reverse_f=reverse_f,\n                outer_size=outer_size,\n                middle_size=middle_size):\n              x_np = np.reshape(\n                  np.arange(outer_size * middle_size * 4, dtype=np.float32),\n                  newshape=(outer_size, middle_size, 4))\n              x_tf = reverse_f(x_np, [1]).eval()\n              np_answer = x_np[:, ::-1, :]\n              self.assertAllEqual(x_tf, np_answer)\n\n  @test_util.run_deprecated_v1\n  def testReverseColumnsOf3Channels(self):\n    with self.session(use_gpu=True):\n      for reverse_f in [array_ops.reverse_v2, array_ops.reverse]:\n        for outer_size in list(range(50)) + [100000]:\n          for middle_size in (1, 2):\n            with self.subTest(\n                reverse_f=reverse_f,\n                outer_size=outer_size,\n                middle_size=middle_size):\n              x_np = np.reshape(\n                  np.arange(outer_size * middle_size * 3, dtype=np.float32),\n                  newshape=(outer_size, middle_size, 3))\n              x_tf = reverse_f(x_np, [0]).eval()\n              np_answer = x_np[::-1, :, :]\n              self.assertAllEqual(x_tf, np_answer)\n\n  def testReverseInvalidShape(self):\n    x = np.ndarray(shape=[0, 1, 1])\n    v = array_ops.reverse_v2(x, axis=[1])\n    self.assertAllEqual(self.evaluate(v), v)\n\n\nclass MeshgridTest(test_util.TensorFlowTestCase):\n\n  def _compareDiff(self, x, y, use_gpu):\n    for index in (\"ij\", \"xy\"):\n      numpy_out = np.meshgrid(x, y, indexing=index)\n      tf_out = array_ops.meshgrid(x, y, indexing=index)\n      with self.cached_session(use_gpu=use_gpu):\n        for xx, yy in zip(numpy_out, tf_out):\n          self.assertAllEqual(xx, yy)\n\n  def _compareDiffType(self, n, np_dtype, use_gpu):\n    inputs = []\n    for index in (\"ij\", \"xy\"):\n      for _ in range(n):\n        x = np.linspace(-10, 10, 5).astype(np_dtype)\n        if np_dtype in (np.complex64, np.complex128):\n          x += 1j\n        inputs.append(x)\n      numpy_out = np.meshgrid(*inputs, indexing=index)\n      with self.cached_session(use_gpu=use_gpu):\n        tf_out = array_ops.meshgrid(*inputs, indexing=index)\n        for x_np, x_tf in zip(numpy_out, tf_out):\n          self.assertAllEqual(x_np, x_tf)\n\n  @test_util.run_deprecated_v1\n  def testCompare(self):\n    for t in (np.float16, np.float32, np.float64, np.int32, np.int64,\n              np.complex64, np.complex128):\n      with self.subTest(t=t):\n        self._compareDiffType(2, t, False)\n        self._compareDiffType(3, t, False)\n\n        x = [1, 2, 3]\n        y = [4, 5]\n\n        a = [[1, 1], [1, 1]]\n\n        self._compareDiff(x, y, False)\n        self._compareDiff(x, a, False)\n\n\nclass StridedSliceChecker(object):\n  \"\"\"Check a given tensor against the numpy result.\"\"\"\n\n  REF_TENSOR = np.arange(1, 19, dtype=np.float32).reshape(3, 2, 3)\n  REF_TENSOR_ALIGNED = np.arange(1, 97, dtype=np.float32).reshape(3, 4, 8)\n\n  def __init__(self, test, x, tensor_type=dtypes.int32, check_type_infer=True):\n    self.x_np = np.array(x).astype(tensor_type.as_numpy_dtype)\n    if tensor_type.is_bool:\n      self.x_np = np.array(x % 3).astype(np.bool)\n    # Give the value a non-zero imaginary component for complex types.\n    if tensor_type.is_complex:\n      self.x_np -= 1j * self.x_np\n    self.test = test\n    self.x = constant_op.constant(self.x_np, dtype=tensor_type)\n    self.check_type_infer = check_type_infer\n\n  def __getitem__(self, spec):\n    op = self.x.__getitem__(spec)\n\n    def eval_if_tensor(x):\n      try:\n        return x.eval()\n      except AttributeError:\n        return x\n\n    if isinstance(spec, bool) or \\\n      (isinstance(spec, ops.Tensor) and spec.dtype == dtypes.bool) or \\\n      (isinstance(spec, np.ndarray) and spec.dtype == bool) or \\\n      (isinstance(spec, (list, tuple)) and np.asarray(spec).dtype == bool):\n      tensor = op.eval()\n      np_spec = eval_if_tensor(spec)\n      self.test.assertAllEqual(self.x_np[np_spec], tensor)\n      return tensor\n\n    if not isinstance(spec, (list, tuple)):\n      spec = [spec]\n\n    tensor = op.eval()\n\n    # Make a numpy spec that pre-evals the tensors\n    np_specs = []\n\n    for s in spec:\n      if isinstance(s, slice):\n        start = eval_if_tensor(s.start)\n        stop = eval_if_tensor(s.stop)\n        step = eval_if_tensor(s.step)\n        np_specs.append(slice(start, stop, step))\n      else:\n        np_specs.append(eval_if_tensor(s))\n\n    self.test.assertAllEqual(self.x_np[tuple(np_specs)], tensor)\n    if self.check_type_infer:\n      self.test.assertAllEqual(tensor.shape, op.get_shape())\n    return tensor\n\n\nSTRIDED_SLICE_TYPES = [\n    dtypes.int32, dtypes.int64, dtypes.int16, dtypes.int8, dtypes.float32,\n    dtypes.float64, dtypes.complex64, dtypes.complex128, dtypes.bool\n]\n\n\nclass StridedSliceTest(test_util.TensorFlowTestCase):\n  \"\"\"Test the strided slice operation with variants of slices.\"\"\"\n\n  @test_util.run_deprecated_v1\n  def test_basic_slice(self):\n    for tensor_type in STRIDED_SLICE_TYPES:\n      with self.subTest(tensor_type=tensor_type):\n        with self.cached_session(use_gpu=True):\n          checker = StridedSliceChecker(\n              self, StridedSliceChecker.REF_TENSOR, tensor_type=tensor_type)\n          _ = checker[:, :, :]\n          # Various ways of representing identity slice\n          _ = checker[:, :, :]\n          _ = checker[::, ::, ::]\n          _ = checker[::1, ::1, ::1]\n          # Not zero slice\n          _ = checker[::1, ::5, ::2]\n          # Reverse in each dimension independently\n          _ = checker[::-1, :, :]\n          _ = checker[:, ::-1, :]\n          _ = checker[:, :, ::-1]\n          ## negative index tests i.e. n-2 in first component\n          _ = checker[-2::-1, :, ::1]\n          # negative index tests i.e. n-2 in first component, non-unit stride\n          _ = checker[-2::-1, :, ::2]\n\n          # Check rank-0 examples\n          checker2 = StridedSliceChecker(self, 5, tensor_type=tensor_type)\n          _ = checker2[None]\n          _ = checker2[...]\n          _ = checker2[tuple()]\n\n  def testInt64GPU(self):\n    if not test_util.is_gpu_available():\n      self.skipTest(\"No GPU available\")\n\n    with test_util.force_gpu():\n      x = constant_op.constant([1., 2., 3.])\n      begin = constant_op.constant([2], dtype=dtypes.int64)\n      end = constant_op.constant([3], dtype=dtypes.int64)\n      strides = constant_op.constant([1], dtype=dtypes.int64)\n      s = array_ops.strided_slice(x, begin, end, strides)\n      self.assertAllEqual([3.], self.evaluate(s))\n\n  @test_util.assert_no_new_pyobjects_executing_eagerly\n  @test_util.assert_no_garbage_created\n  def testTensorSliceEagerMemory(self):\n    with context.eager_mode():\n      inputs = constant_op.constant([[[1], [2], [3], [4]]],\n                                    dtype=dtypes.float32)\n      # Tests that slicing an EagerTensor doesn't leak memory\n      inputs[0]  # pylint: disable=pointless-statement\n\n  @test_util.assert_no_new_pyobjects_executing_eagerly\n  @test_util.assert_no_garbage_created\n  def testVariableSliceEagerMemory(self):\n    with context.eager_mode():\n      v = variables.Variable([1., 2.])\n      v[0]  # pylint: disable=pointless-statement\n\n  @test_util.run_deprecated_v1\n  def testDegenerateSlices(self):\n    with self.session(use_gpu=True):\n      checker = StridedSliceChecker(self, StridedSliceChecker.REF_TENSOR)\n      # degenerate by offering a forward interval with a negative stride\n      _ = checker[0:-1:-1, :, :]\n      # degenerate with a reverse interval with a positive stride\n      _ = checker[-1:0, :, :]\n      # empty interval in every dimension\n      _ = checker[-1:0, 2:2, 2:3:-1]\n      # empty first dimension only (used to break for aligned tensors).\n      checker = StridedSliceChecker(self,\n                                    StridedSliceChecker.REF_TENSOR_ALIGNED)\n      _ = checker[1:0]\n\n  @test_util.run_deprecated_v1\n  def testSliceWithUndefinedDimension(self):\n    t = constant_op.constant([1, 2, 3])\n    d = tensor_shape.Dimension(None)\n    self.assertAllEqual(t[d:d:d], t)\n\n  @test_util.run_deprecated_v1\n  def testEllipsis(self):\n    with self.session(use_gpu=True):\n      raw = [[[[[1, 2], [3, 4], [5, 6]]], [[[7, 8], [9, 10], [11, 12]]]]]\n      checker = StridedSliceChecker(self, raw)\n\n      _ = checker[0:]\n      # implicit ellipsis\n      _ = checker[0:, ...]\n      # ellipsis alone\n      _ = checker[...]\n      # ellipsis at end\n      _ = checker[0:1, ...]\n      # ellipsis at begin\n      _ = checker[..., 0:1]\n      # ellipsis at middle\n      _ = checker[0:1, ..., 0:1]\n      # multiple ellipses not allowed\n      with self.assertRaisesRegex(ValueError, \"Multiple ellipses\"):\n        _ = checker[..., :, ...].eval()\n\n  @test_util.run_deprecated_v1\n  def testShrink(self):\n    with self.session(use_gpu=True):\n      raw = [[[[[1, 2, 4, 5], [5, 6, 7, 8], [9, 10, 11, 12]]],\n              [[[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]]]]\n      checker = StridedSliceChecker(self, raw)\n      _ = checker[:, :, :, :, 3]\n      _ = checker[..., 3]\n      _ = checker[:, 0]\n      _ = checker[:, :, 0]\n\n  @test_util.run_deprecated_v1\n  def testBothNewAxisAndShrink(self):\n    with self.session(use_gpu=True):\n      ones = array_ops.placeholder(shape=[2, 2], dtype=dtypes.int16)\n      self.assertAllEqual(\n          ones[array_ops.newaxis, :,\n               0].eval(feed_dict={ones: [[1, 1], [1, 1]]}), [[1, 1]])\n\n  @test_util.run_deprecated_v1\n  def testTensorIndexing(self):\n    with self.session(use_gpu=True):\n      raw = [[[[[1, 2, 4, 5], [5, 6, 7, 8], [9, 10, 11, 12]]],\n              [[[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]]]]\n      checker = StridedSliceChecker(self, raw, check_type_infer=False)\n      bar = constant_op.constant(2)\n      bar2 = constant_op.constant(3)\n      _ = checker[..., bar:bar2]\n      _ = checker[..., bar]\n      _ = checker[..., 3]\n      _ = checker[..., 2**64 // 2**63]  # Test longs in Python 2\n\n  def testTensorIndexingTypeError(self):\n    with self.session(use_gpu=True):\n      checker = StridedSliceChecker(self, StridedSliceChecker.REF_TENSOR)\n      expected = re.escape(array_ops._SLICE_TYPE_ERROR)\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[\"foo\"]\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[constant_op.constant(\"foo\")]\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[0.0]\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[constant_op.constant(0.0)]\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[constant_op.constant([1, 2, 3])]\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[[2.1, -0.7, 1.5]]\n\n  @test_util.run_deprecated_v1\n  def testExpand(self):\n    with self.session(use_gpu=True):\n      raw = [[[[[1, 2, 4, 5], [5, 6, 7, 8], [9, 10, 11, 12]]],\n              [[[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]]]]\n      checker = StridedSliceChecker(self, raw)\n      # new axis (followed by implicit ellipsis)\n      _ = checker[np.newaxis]\n      # newaxis after ellipsis\n      _ = checker[..., np.newaxis]\n      # newaxis in between ellipsis and explicit range\n      _ = checker[..., np.newaxis, :]\n      _ = checker[:, ..., np.newaxis, :, :]\n      # Reverse final dimension with new axis\n      _ = checker[:, :, np.newaxis, :, 2::-1]\n      # Ellipsis in middle of two newaxis\n      _ = checker[np.newaxis, ..., np.newaxis]\n\n  @test_util.run_deprecated_v1\n  def testExpandVariable(self):\n    with self.session(use_gpu=True):\n      x = variables.Variable(7, dtype=dtypes.int32)\n      self.evaluate(x.initializer)\n      y = x[None].eval()\n      self.assertEqual(y.shape, (1,))\n      self.assertAllEqual(y, (7,))\n\n  @test_util.run_deprecated_v1\n  def testOptimizedCases(self):\n    with self.session(use_gpu=True):\n      checker = StridedSliceChecker(self,\n                                    StridedSliceChecker.REF_TENSOR_ALIGNED)\n      # Identity\n      _ = checker[:]\n      # Identity\n      _ = checker[...]\n      # Identity\n      _ = checker[np.newaxis, ..., np.newaxis]\n      # First axis slice\n      _ = checker[1:]\n      # First axis slice\n      _ = checker[np.newaxis, 1:]\n\n  @test_util.run_v1_only(\"currently failing on v2\")\n  def testMasks(self):\n    with self.session(use_gpu=True):\n      scalar = np.array(0)\n      # Test tensor type mask\n      checker = StridedSliceChecker(self, StridedSliceChecker.REF_TENSOR)\n      _ = checker[checker.x > 2]\n      _ = checker[checker.x <= 5]\n      _ = checker[ops.convert_to_tensor(scalar)]\n\n      # Test numpy array type mask\n      raw = np.array([[[[[1, 2, 4, 5], [5, 6, 7, 8], [9, 10, 11, 12]]],\n                       [[[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23,\n                                                              24]]]]])\n      checker1 = StridedSliceChecker(self, raw)\n      _ = checker1[raw >= 4]\n      _ = checker1[raw < 19]\n      _ = checker1[scalar]\n\n      # Test boolean and non boolean cases\n      mask = np.array([True, False, True])\n      raw1 = np.array([[1, 2, 4, 5], [5, 6, 7, 8], [9, 10, 11, 12]])\n      checker2 = StridedSliceChecker(self, raw1)\n      _ = checker2[mask]\n      _ = checker2[ops.convert_to_tensor(mask)]\n\n\nclass StridedSliceShapeChecker(object):\n\n  def __init__(self, x):\n    self.x = x\n\n  def __getitem__(self, spec):\n    op = self.x.__getitem__(spec)\n    return op.get_shape()\n\n\nclass StridedSliceShapeTest(test_util.TensorFlowTestCase):\n  \"\"\"Test the shape inference of StridedSliceShapes.\"\"\"\n\n  @test_util.run_deprecated_v1\n  def testUnknown(self):\n    with self.session(use_gpu=True):\n      uncertain_tensor = array_ops.placeholder(dtypes.float32)\n      a = StridedSliceShapeChecker(uncertain_tensor)\n      a_slice_shape = a[...]\n      self.assertAllEqual(a_slice_shape.ndims, None)\n\n  def tensorShapeEqual(self, x, y):\n    self.assertTrue(x is not None and y is not None or x is None and y is None)\n    self.assertEqual(x.as_list(), y.as_list())\n\n  @test_util.run_deprecated_v1\n  def testTensorShapeUncertain(self):\n    with self.session(use_gpu=True):\n      uncertain_tensor = array_ops.placeholder(\n          dtypes.float32, shape=(5, None, 7))\n      a = StridedSliceShapeChecker(uncertain_tensor)\n      self.tensorShapeEqual(a[3:5], tensor_shape.TensorShape([2, None, 7]))\n      self.tensorShapeEqual(a[3:5, :, 4], tensor_shape.TensorShape([2, None]))\n      self.tensorShapeEqual(a[3:5, 3:4, 4], tensor_shape.TensorShape([2, None]))\n      self.tensorShapeEqual(a[3:5, :, 5:10],\n                            tensor_shape.TensorShape([2, None, 2]))\n      self.tensorShapeEqual(a[3:5, :, 50:3],\n                            tensor_shape.TensorShape([2, None, 0]))\n      self.tensorShapeEqual(a[3:5, :, array_ops.newaxis, 50:3,],\n                            tensor_shape.TensorShape([2, None, 1, 0]))\n      self.tensorShapeEqual(a[1:5:2, :, array_ops.newaxis, 50:3,],\n                            tensor_shape.TensorShape([2, None, 1, 0]))\n      self.tensorShapeEqual(a[:5:3, :, array_ops.newaxis, 50:3,],\n                            tensor_shape.TensorShape([2, None, 1, 0]))\n      self.tensorShapeEqual(a[:2:3, :, array_ops.newaxis, 50:3,],\n                            tensor_shape.TensorShape([1, None, 1, 0]))\n      self.tensorShapeEqual(a[::-1, :, array_ops.newaxis, ::-2],\n                            tensor_shape.TensorShape([5, None, 1, 4]))\n\n  @test_util.run_deprecated_v1\n  def testTensorValuedIndexShape(self):\n    with self.session(use_gpu=True):\n      defined_shape_tensor = array_ops.placeholder(\n          dtypes.float32, shape=(5, 3, 7))\n      index_value = array_ops.placeholder(dtypes.int32, shape=())\n      a = StridedSliceShapeChecker(defined_shape_tensor)\n      self.tensorShapeEqual(a[index_value], tensor_shape.TensorShape([3, 7]))\n      self.tensorShapeEqual(a[index_value, ::-1],\n                            tensor_shape.TensorShape([3, 7]))\n      self.tensorShapeEqual(a[index_value, ::-2],\n                            tensor_shape.TensorShape([2, 7]))\n      other_scalar = array_ops.placeholder(dtypes.int32, shape=())\n      self.tensorShapeEqual(a[index_value, other_scalar:2],\n                            tensor_shape.TensorShape([None, 7]))\n\n\nclass GradSliceChecker(object):\n  \"\"\"Tests that we can compute a gradient for var^2.\"\"\"\n\n  def __init__(self, test, sess, var, varnp):\n    self.test = test\n    self.sess = sess\n    self.val = var * var\n    self.var = var\n    self.varnp = varnp\n\n  def __getitem__(self, spec):\n    slice_var = self.var[spec]\n    slice_val = self.val[spec]\n\n    # compute analytic 2nd derivative\n    analytic_grad2 = 2 * slice_val\n\n    dy = variables.Variable(\n        array_ops.ones_like(slice_var, dtype=dtypes.float32))\n    assign = dy.assign(slice_var)\n    slice_val_grad, = gradients_impl.gradients(slice_val, self.var, grad_ys=dy)\n    slice_val_grad2, = gradients_impl.gradients(\n        slice_val_grad, dy, grad_ys=self.var)\n    self.sess.run(assign)\n    slice_val_grad_evaled, slice_val_grad2_evaled = (\n        self.sess.run([slice_val_grad, slice_val_grad2]))\n    analytic_grad2_evaled = analytic_grad2.eval()\n    self.test.assertAllEqual(slice_val_grad2_evaled, analytic_grad2_evaled)\n\n    # compute analytic gradient for slice\n    np_val_grad = (2 * self.varnp * self.varnp)\n    np_sliceval_grad = np.zeros(self.var.get_shape())\n    if isinstance(spec, ops.Tensor):\n      spec = self.sess.run([spec])\n    np_sliceval_grad[spec] = np_val_grad[spec]\n    # verify gradient\n    self.test.assertAllEqual(slice_val_grad_evaled, np_sliceval_grad)\n\n\nclass StridedSliceGradTest(test_util.TensorFlowTestCase):\n  \"\"\"Test that strided slice's custom gradient produces correct gradients.\"\"\"\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testGradient(self):\n    with self.session(use_gpu=True) as sess:\n      var = variables.Variable(\n          array_ops.reshape(\n              math_ops.range(1, 97, 1, dtype=dtypes.float32), shape=(6, 4, 4)))\n      init = variables.global_variables_initializer()\n      sess.run(init)\n\n      raw = np.array(range(1, 97, 1)).reshape((6, 4, 4))\n      grad = GradSliceChecker(self, sess, var, raw)\n      _ = grad[2:6:2, 1:3, 1:3]\n      _ = grad[3:0:-2, 1:3, 1:3]\n      _ = grad[3:0:-2, array_ops.newaxis, 1:3, 2, array_ops.newaxis]\n      _ = grad[3:0:-2, 1:3, 2]\n      _ = grad[:, -1, :]\n      _ = grad[:, -2, :]\n      with self.assertRaisesRegex(ValueError, \"out of bounds\"):\n        _ = grad[:, -200, :]\n      with self.assertRaisesRegex(ValueError, \"out of bounds\"):\n        _ = grad[:, 200, :]\n\n      # Test numpy array type mask\n      _ = grad[raw > 51]\n      # Test tensor type mask\n      _ = grad[ops.convert_to_tensor(raw) <= 76]\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testGradientZero(self):\n    with self.session(use_gpu=True) as sess:\n      var = variables.Variable(8.)\n      init = variables.global_variables_initializer()\n      sess.run(init)\n      grad = GradSliceChecker(self, sess, var, np.array(8))\n      _ = grad[tuple()]\n\n  @test_util.run_deprecated_v1\n  def testInt64Indices(self):\n    with self.session(use_gpu=True) as sess:\n      a = math_ops.range(3, dtype=dtypes.float32)\n      index = constant_op.constant(1, dtype=dtypes.int64)\n      b = 2. * a[index]\n      grad, = gradients_impl.gradients(b, a)\n      self.assertAllEqual(self.evaluate(grad), [0., 2., 0.])\n\n\nclass StridedSliceGradTypeTest(test_util.TensorFlowTestCase):\n  \"\"\"Test varied index types and host located memory.\"\"\"\n\n  @test_util.run_deprecated_v1\n  def testHostVsDevice(self):\n    with self.session(use_gpu=True) as sess:\n      var2 = variables.Variable(\n          array_ops.reshape(\n              math_ops.cast(math_ops.range(1, 5, 1), dtypes.float32),\n              shape=(4, 1, 1)))\n      varshape = variables.Variable([6, 4, 4], dtype=dtypes.int32)\n      self.evaluate(variables.global_variables_initializer())\n      begin = constant_op.constant([0, 0, 0])\n      end = constant_op.constant([4, 1, 1])\n      strides = constant_op.constant([1, 1, 1])\n      foo = array_ops.strided_slice_grad(varshape, begin, end, strides, var2)\n      sess.run(foo)\n\n  @test_util.run_deprecated_v1\n  def testInt64Shape(self):\n    with self.session(use_gpu=True) as sess:\n      original_dy = array_ops.reshape(\n          math_ops.cast(math_ops.range(1, 5, 1), dtypes.float32),\n          shape=(4, 1, 1))\n      original_shape = constant_op.constant([6, 4, 4], dtype=dtypes.int64)\n      self.evaluate(variables.global_variables_initializer())\n      begin = constant_op.constant([0, 0, 0], dtype=dtypes.int64)\n      end = constant_op.constant([4, 1, 1], dtype=dtypes.int64)\n      strides = constant_op.constant([1, 1, 1], dtype=dtypes.int64)\n      dx = array_ops.strided_slice_grad(original_shape, begin, end, strides,\n                                        original_dy)\n      sess.run(dx)\n\n  @test_util.run_deprecated_v1\n  def testMixedIndexTypes(self):\n    with self.session(use_gpu=True) as sess:\n      original_dy = array_ops.reshape(\n          math_ops.cast(math_ops.range(1, 5, 1), dtypes.float32),\n          shape=(4, 1, 1))\n      original_shape = constant_op.constant([6, 4, 4], dtype=dtypes.int64)\n      self.evaluate(variables.global_variables_initializer())\n      begin = constant_op.constant([0, 0, 0], dtype=dtypes.int32)\n      end = constant_op.constant([4, 1, 1], dtype=dtypes.int64)\n      strides = constant_op.constant([1, 1, 1], dtype=dtypes.int64)\n      with self.assertRaisesRegex(\n          TypeError, \"Input 'begin' of 'StridedSliceGrad' Op has type int32\"\n          \" that does not match type int64 of argument 'shape'\"):\n        dx = array_ops.strided_slice_grad(original_shape, begin, end, strides,\n                                          original_dy)\n        sess.run(dx)\n\n\nclass BenchmarkSlice(object):\n\n  def __init__(self, tensor):\n    self.tensor = tensor\n\n  def __getitem__(self, x):\n    return self.tensor[x]\n\n\nclass StridedSliceBenchmark(test_lib.Benchmark):\n  \"\"\"Benchmark new strided slice operation on non-trivial case.\"\"\"\n\n  def run_and_time(self, slice_op):\n    self.evaluate(variables.global_variables_initializer())\n    for _ in range(10):\n      _ = self.evaluate(slice_op)\n    iters = 1000\n    t0 = time.time()\n    for _ in range(iters):\n      self.evaluate(slice_op)\n    t1 = time.time()\n    self.report_benchmark(iters=iters, wall_time=(t1 - t0) / 1000.0)\n\n  def make_variable(self):\n    n = 256\n    shape = (n, n, n)\n    items = n**3\n    var = variables.Variable(\n        array_ops.reshape(math_ops.linspace(1., float(items), items), shape),\n        dtype=dtypes.float32)\n    return var\n\n  def benchmark_strided_slice_skip(self):\n    with session.Session():\n      var = self.make_variable()\n      helper = BenchmarkSlice(var)\n      slice_op = helper[::2, ::1, ::2]\n      self.run_and_time(slice_op)\n\n  def benchmark_strided_slice_easy(self):\n    with session.Session():\n      var = self.make_variable()\n      helper = BenchmarkSlice(var)\n      slice_op = helper[3::1, 3::1, 3::1]\n      self.run_and_time(slice_op)\n\n  def benchmark_slice_easy(self):\n    with session.Session():\n      var = self.make_variable()\n      slice_op = var[3::1, 3::1, 3::1]\n      self.run_and_time(slice_op)\n\n\nclass StridedSliceAssignChecker(object):\n\n  def __init__(self, test, x, tensor_type=dtypes.float32, use_resource=False):\n    self.tensor_type = tensor_type\n    self.test = test\n    self._use_resource = use_resource\n\n    self.x_np = np.array(x).astype(tensor_type.as_numpy_dtype)\n    # Give the value a non-zero imaginary component for complex types.\n    if tensor_type.is_complex:\n      self.x_np -= 1j * self.x_np\n    self.x = constant_op.constant(self.x_np, dtype=tensor_type)\n\n  def __setitem__(self, index, value):\n    value = np.array(value).astype(self.tensor_type.as_numpy_dtype)\n    # Give the value a non-zero imaginary component for complex types.\n    if self.tensor_type.is_complex:\n      value -= 1j * value\n\n    with self.test.test_session(use_gpu=True) as sess:\n      if self._use_resource:\n        var = resource_variable_ops.ResourceVariable(self.x)\n      else:\n        var = variables.Variable(self.x)\n      sess.run(variables.variables_initializer([var]))\n      val = sess.run(var[index].assign(value))\n      # val_copy is used to check that tf.compat.v1.assign works equivalently\n      # to the assign method above.\n      val_copy = sess.run(state_ops.assign(var[index], value))\n      valnp = np.copy(self.x_np)\n      valnp[index] = np.array(value)\n      self.test.assertAllEqual(val, valnp)\n      self.test.assertAllEqual(val_copy, valnp)\n\n\nclass SliceAssignTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testInvalidSlice(self):\n    with self.cached_session() as sess:\n      foo = constant_op.constant([1, 2, 3])\n      with self.assertRaisesRegex(\n          ValueError, \"Sliced assignment\"\n          \" is only supported for variables\"):\n        bar = foo[:2].assign(constant_op.constant([1, 2]))\n        sess.run(bar)\n\n  def doTestSliceAssign(self, use_resource):\n    for dtype in STRIDED_SLICE_TYPES:\n      with self.subTest(dtype=dtype):\n        checker = StridedSliceAssignChecker(\n            self, [[1, 2, 3], [4, 5, 6]],\n            use_resource=use_resource,\n            tensor_type=dtype)\n        # Check if equal\n        checker[:] = [[10, 20, 30], [40, 50, 60]]\n        # Check trivial (1,1) shape tensor\n        checker[1:2, 1:2] = [[66]]\n        # shrinks shape changes\n        checker[1:2, 1] = [66]\n        checker[1, 1:2] = [66]\n        checker[1, 1] = 66\n        # newaxis shape changes\n        checker[:, None, :] = [[[10, 20, 30]], [[40, 50, 50]]]\n        # shrink and newaxis\n        checker[None, None, 0, 0:1] = [[[99]]]\n        # Non unit strides\n        checker[::1, ::-2] = [[3, 33], [4, 44]]\n        # degenerate interval\n        checker[8:10, 0] = []\n        checker[8:10, 8:10] = [[]]\n    # Assign vector to scalar (rank-0) using newaxis\n    checker2 = StridedSliceAssignChecker(self, 222)\n    checker2[()] = 6  # no indices\n    checker2[...] = 6  # ellipsis\n    checker2[None] = [6]  # new axis\n\n  @test_util.run_deprecated_v1\n  @test_util.disable_xla(\"b/123559667\")\n  def testSliceAssign(self):\n    self.doTestSliceAssign(use_resource=False)\n\n  @test_util.run_deprecated_v1\n  @test_util.disable_xla(\"b/123559667\")\n  def testSliceAssignResource(self):\n    self.doTestSliceAssign(use_resource=True)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testUninitialized(self):\n    with self.assertRaisesRegex(\n        errors.FailedPreconditionError,\n        \"Attempting to use uninitialized value Variable\"):\n      with self.cached_session() as sess:\n        v = variables.VariableV1([1, 2])\n        sess.run(v[:].assign([1, 2]))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testTypeError(self):\n    init_val = constant_op.constant([1, 2], dtype=dtypes.int32)\n    too_small_val = constant_op.constant([3, 4], dtype=dtypes.int8)\n    too_large_val = constant_op.constant([3, 4], dtype=dtypes.int64)\n    v = variables.VariableV1(init_val)\n    with self.assertRaises(TypeError):\n      v[:].assign(too_small_val)\n    with self.assertRaises(TypeError):\n      v[:].assign(too_large_val)\n\n  @test_util.run_deprecated_v1\n  def testTypeErrorResource(self):\n    init_val = constant_op.constant([1, 2], dtype=dtypes.int32)\n    too_small_val = constant_op.constant([3, 4], dtype=dtypes.int8)\n    too_large_val = constant_op.constant([3, 4], dtype=dtypes.int64)\n    v = resource_variable_ops.ResourceVariable(init_val)\n    with self.cached_session() as sess:\n      self.evaluate(v.initializer)\n      with self.assertRaises(ValueError):\n        sess.run(v[:].assign(too_large_val))\n      with self.assertRaises(ValueError):\n        sess.run(v[:].assign(too_small_val))\n\n  @test_util.disable_xla(\"b/123559667\")\n  @test_util.run_in_graph_and_eager_modes\n  def testTensorStridedSliceUpdateWithInputForward(self):\n    \"\"\"Tests tensor_strided_slice_update with input-forwarding taking effect.\"\"\"\n    @def_function.function\n    def assign(x):\n      y = x + 1\n      return gen_array_ops.tensor_strided_slice_update(y, [0], [1], [1], [0])\n    self.assertAllEqual([0, 1], self.evaluate(assign(array_ops.zeros([2]))))\n\n  @test_util.disable_xla(\"b/123559667\")\n  @test_util.run_in_graph_and_eager_modes\n  def testTensorStridedSliceUpdateNoInputForward(self):\n    \"\"\"Tests tensor_strided_slice_update with no input-forwarding.\"\"\"\n    x = constant_op.constant([0.2, 0.3])\n    y = x + 1\n    # y's buffer won't be forwarded to z because y and z will be alive at the\n    # same time later.\n    z = gen_array_ops.tensor_strided_slice_update(y, [0], [1], [1], [0.4])\n    ans = y + z\n    self.assertAllClose([1.6, 2.6], self.evaluate(ans))\n\n  @test_util.disable_xla(\"b/123559667\")\n  def testTensorStridedSliceUpdateGradSimple(self):\n    original = constant_op.constant([0.2, 0.3])\n    updates = constant_op.constant([0.4])\n    with backprop.GradientTape() as tape:\n      tape.watch([original, updates])\n      updated = gen_array_ops.tensor_strided_slice_update(\n          original, [0], [1], [1], updates)\n    d1, d2 = tape.gradient(updated, [original, updates],\n                           output_gradients=constant_op.constant([2.0, 3.0]))\n    self.assertAllClose([0.0, 3.0], d1)\n    self.assertAllClose([2.0], d2)\n\n  @parameterized.named_parameters(\n      (\"_%s\" % i, *args) for i, args in enumerate([  # pylint:disable=g-complex-comprehension\n          ([2, 5], [0, 1], [1, 0], [1, 2], [2], 0, 2, 0, 0, 1),\n          ([4], [5], [3], [1], [3], 1, 0, 0, 0, 0),\n          ([2, 2, 3, 2], [0, 0, 1], [1, 0, 2], [1, 0, 1], [2, 3], 0, 0, 2, 0, 5)\n      ]))\n  @test_util.disable_xla(\"b/123559667\")\n  def testTensorStridedSliceUpdateGrad(\n      self, shape, begin, end, strides, updates_shape, *args):\n    with self.cached_session():\n      def f(a, b):\n        return gen_array_ops.tensor_strided_slice_update(\n            a, begin, end, strides, b, *args)\n      theoretical, numerical = gradient_checker_v2.compute_gradient(\n          f, [array_ops.zeros(shape), array_ops.ones(updates_shape)], delta=1.0)\n      self.assertAllClose(theoretical, numerical)\n\n\nclass ShapeSizeRankTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def testDenseShape(self):\n    t_value = [[0, 42], [24, 0]]\n    self.assertAllEqual((2, 2), self.evaluate(array_ops.shape(t_value)))\n    self.assertEqual(4, self.evaluate(array_ops.size(t_value)))\n    self.assertEqual(2, self.evaluate(array_ops.rank(t_value)))\n\n    t = constant_op.constant(t_value)\n    self.assertAllEqual((2, 2), self.evaluate(array_ops.shape(t)))\n    self.assertEqual(4, self.evaluate(array_ops.size(t)))\n    self.assertEqual(2, self.evaluate(array_ops.rank(t)))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSparseShape(self):\n    sp_value = sparse_tensor.SparseTensorValue(\n        indices=((0, 1), (1, 0)), values=(42, 24), dense_shape=(2, 2))\n    self.assertAllEqual((2, 2), self.evaluate(array_ops.shape(sp_value)))\n    self.assertEqual(4, self.evaluate(array_ops.size(sp_value)))\n    self.assertEqual(2, self.evaluate(array_ops.rank(sp_value)))\n\n    sp = sparse_tensor.SparseTensor.from_value(sp_value)\n    self.assertAllEqual((2, 2), self.evaluate(array_ops.shape(sp)))\n    self.assertEqual(4, self.evaluate(array_ops.size(sp)))\n    self.assertEqual(2, self.evaluate(array_ops.rank(sp)))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSizeDtype(self):\n    tensor = [1]\n    self.assertEqual(dtypes.int32, self.evaluate(array_ops.size(tensor)).dtype)\n    self.assertEqual(\n        dtypes.int64,\n        self.evaluate(array_ops.size(tensor, out_type=dtypes.int64)).dtype)\n\n\nclass SequenceMaskTest(test_util.TensorFlowTestCase):\n\n  def testExceptions(self):\n    with self.cached_session():\n      with self.assertRaisesRegex(ValueError, \"maxlen must be scalar\"):\n        array_ops.sequence_mask([10, 20], [10, 20])\n\n  @test_util.run_deprecated_v1\n  def testOneDimensionalWithMaxlen(self):\n    with self.cached_session():\n      res = array_ops.sequence_mask(constant_op.constant([1, 3, 2]), 5)\n      self.assertAllEqual(res.get_shape(), [3, 5])\n      self.assertAllEqual(\n          res,\n          [[True, False, False, False, False], [True, True, True, False, False],\n           [True, True, False, False, False]])\n\n  @test_util.run_deprecated_v1\n  def testOneDimensionalDtypeWithoutMaxlen(self):\n    with self.cached_session():\n      # test dtype and default maxlen:\n      res = array_ops.sequence_mask(\n          constant_op.constant([0, 1, 4]), dtype=dtypes.float32)\n      self.assertAllEqual(res.get_shape().as_list(), [3, 4])\n      self.assertAllEqual(\n          res,\n          [[0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0]])\n\n  @test_util.run_deprecated_v1\n  def testOneDimensionalWithoutMaxlen(self):\n    with self.cached_session():\n      res = array_ops.sequence_mask(constant_op.constant([0, 1, 4]))\n      self.assertAllEqual(res.get_shape().as_list(), [3, 4])\n      self.assertAllEqual(\n          res, [[False, False, False, False], [True, False, False, False],\n                [True, True, True, True]])\n\n  @test_util.run_deprecated_v1\n  def testTwoDimensional(self):\n    with self.cached_session():\n      res = array_ops.sequence_mask(constant_op.constant([[1, 3, 2]]), 5)\n      self.assertAllEqual(res.get_shape(), [1, 3, 5])\n      self.assertAllEqual(res, [[[True, False, False, False, False],\n                                 [True, True, True, False, False],\n                                 [True, True, False, False, False]]])\n\n      # test dtype and default maxlen:\n      res = array_ops.sequence_mask(\n          constant_op.constant([[0, 1, 4], [1, 2, 3]]), dtype=dtypes.float32)\n      self.assertAllEqual(res.get_shape().as_list(), [2, 3, 4])\n      self.assertAllEqual(\n          res,\n          [[[0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0]],\n           [[1.0, 0.0, 0.0, 0.0], [1.0, 1.0, 0.0, 0.0], [1.0, 1.0, 1.0, 0.0]]])\n\n  @test_util.run_deprecated_v1\n  def testUnknownShape(self):\n    lengths = array_ops.placeholder(dtype=dtypes.int32)\n    res = array_ops.sequence_mask(lengths)\n    self.assertEqual(res.shape, None)\n\n  @test_util.run_deprecated_v1\n  def testDtypes(self):\n\n    def check_dtypes(lengths_dtype, maxlen_dtype):\n      res = array_ops.sequence_mask(\n          constant_op.constant([1, 3, 2], dtype=lengths_dtype),\n          constant_op.constant(5, dtype=maxlen_dtype))\n      self.assertAllEqual(res.get_shape(), [3, 5])\n      self.assertAllEqual(\n          res,\n          [[True, False, False, False, False], [True, True, True, False, False],\n           [True, True, False, False, False]])\n\n    with self.cached_session():\n      check_dtypes(dtypes.int32, dtypes.int32)\n      check_dtypes(dtypes.int32, dtypes.int64)\n      check_dtypes(dtypes.int64, dtypes.int32)\n      check_dtypes(dtypes.int64, dtypes.int64)\n\n  def testOutputDtype(self):\n\n    def check_output_dtype(output_dtype):\n      res = self.evaluate(\n          array_ops.sequence_mask(\n              constant_op.constant([1, 3, 2], dtype=dtypes.int32),\n              constant_op.constant(5, dtype=dtypes.int32),\n              dtype=output_dtype))\n      self.assertAllEqual(\n          res,\n          self.evaluate(\n              math_ops.cast([[True, False, False, False, False],\n                             [True, True, True, False, False],\n                             [True, True, False, False, False]], output_dtype)))\n\n    check_output_dtype(dtypes.bool)\n    check_output_dtype(\"bool\")\n    check_output_dtype(np.bool)\n    check_output_dtype(dtypes.int32)\n    check_output_dtype(\"int32\")\n    check_output_dtype(np.int32)\n    check_output_dtype(dtypes.float32)\n    check_output_dtype(\"float32\")\n    check_output_dtype(np.float32)\n    check_output_dtype(dtypes.int64)\n    check_output_dtype(\"float64\")\n    check_output_dtype(np.float64)\n\n\nclass ConcatSliceResourceTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  @test_util.run_deprecated_v1\n  def testConcatSlice(self):\n    r1 = test_ops.stub_resource_handle_op(container=\"a\", shared_name=\"b\")\n    r2 = test_ops.stub_resource_handle_op(container=\"a\", shared_name=\"c\")\n    c = array_ops.stack([r1, r2])\n    s = array_ops.strided_slice(c, [1], [2])\n    self.evaluate(test_ops.resource_create_op(s))\n    with self.assertRaises(errors.AlreadyExistsError):\n      self.evaluate(test_ops.resource_create_op(r2))\n\n\nclass IdentityTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_gpu_only\n  def testEagerIdentity(self):\n    with context.eager_mode():\n\n      def _test(x, y, device):\n        self.assertAllEqual(x.numpy(), y.numpy())\n        self.assertTrue(device in y.device.lower())\n\n      with test_util.force_gpu():\n        a = constant_op.constant([[2], [3]], dtype=dtypes.float32)\n      with test_util.force_gpu():\n        b = array_ops.identity(a)\n        _test(a, b, \"gpu\")\n      with test_util.force_cpu():\n        c = array_ops.identity(b)\n        _test(b, c, \"cpu\")\n      with test_util.force_cpu():\n        d = array_ops.identity(c)\n        _test(c, d, \"cpu\")\n      with test_util.force_gpu():\n        e = array_ops.identity(d)\n        _test(d, e, \"gpu\")\n\n\nclass PadTest(test_util.TensorFlowTestCase):\n\n  def testEager(self):\n    with context.eager_mode():\n      t = constant_op.constant([[1, 2, 3], [4, 5, 6]])\n      paddings = constant_op.constant([[\n          1,\n          1,\n      ], [2, 2]])\n      padded = array_ops.pad(t, paddings, \"CONSTANT\")\n      self.assertAllEqual(padded.numpy(),\n                          [[0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 2, 3, 0, 0],\n                           [0, 0, 4, 5, 6, 0, 0], [0, 0, 0, 0, 0, 0, 0]])\n\n\nclass InvertPermutationTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_deprecated_v1\n  def testInvertPermutation(self):\n    for dtype in [dtypes.int32, dtypes.int64]:\n      with self.subTest(dtype=dtype):\n        with self.cached_session(use_gpu=True):\n          x = constant_op.constant([3, 4, 0, 2, 1], dtype=dtype)\n          y = array_ops.invert_permutation(x)\n          self.assertAllEqual(y.get_shape(), [5])\n          self.assertAllEqual(y, [2, 4, 3, 0, 1])\n\n\nclass UnravelIndexTest(test_util.TensorFlowTestCase):\n\n  # TODO(b/73086570): Reenable test.\n  @unittest.skip(\"Test does not pass internally.\")\n  def testUnravelIndex(self):\n    with self.cached_session():\n      for dtype in [dtypes.int32, dtypes.int64]:\n        with self.subTest(dtype=dtype):\n          indices_1 = constant_op.constant(1621, dtype=dtype)\n          dims_1 = constant_op.constant([6, 7, 8, 9], dtype=dtype)\n          out_1 = array_ops.unravel_index(indices_1, dims_1)\n          self.assertAllEqual(out_1, [3, 1, 4, 1])\n\n          indices_2 = constant_op.constant([1621], dtype=dtype)\n          dims_2 = constant_op.constant([6, 7, 8, 9], dtype=dtype)\n          out_2 = array_ops.unravel_index(indices_2, dims_2)\n          self.assertAllEqual(out_2, [[3], [1], [4], [1]])\n\n          indices_3 = constant_op.constant([22, 41, 37], dtype=dtype)\n          dims_3 = constant_op.constant([7, 6], dtype=dtype)\n          out_3 = array_ops.unravel_index(indices_3, dims_3)\n          self.assertAllEqual(out_3, [[3, 6, 6], [4, 5, 1]])\n\n  # Test case for GitHub issue 40204.\n  def testUnravelIndexZeroDim(self):\n    with self.cached_session():\n      for dtype in [dtypes.int32, dtypes.int64]:\n        with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                    \"index is out of bound as with dims\"):\n          indices = constant_op.constant([2, 5, 7], dtype=dtype)\n          dims = constant_op.constant([3, 0], dtype=dtype)\n          self.evaluate(array_ops.unravel_index(indices=indices, dims=dims))\n\n\nclass GuaranteeConstOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_deprecated_v1\n  def testSimple(self):\n    with self.cached_session():\n      a = array_ops.constant(10)\n      guarantee_a = array_ops.guarantee_const(a)\n      self.assertEqual(10, self.evaluate(guarantee_a))\n\n  @test_util.run_deprecated_v1\n  def testVariables(self):\n    with self.cached_session() as sess:\n      for use_resource in [False, True]:\n        with self.subTest(use_resource=use_resource):\n          a = variable_scope.get_variable(\n              \"var_{}\".format(use_resource), [],\n              initializer=init_ops.constant_initializer(10.0),\n              use_resource=use_resource)\n          guarantee_a = array_ops.guarantee_const(a)\n          self.evaluate(variables.global_variables_initializer())\n          self.assertEqual(10.0, self.evaluate(guarantee_a))\n\n  @test_util.run_deprecated_v1\n  def testResourceRejection(self):\n    with self.cached_session() as sess:\n      a = variable_scope.get_variable(\n          \"resource_var\", [],\n          initializer=init_ops.constant_initializer(10.0),\n          use_resource=True)\n      guarantee_a = array_ops.guarantee_const(a.handle)\n      self.evaluate(variables.global_variables_initializer())\n      with self.assertRaisesWithPredicateMatch(errors.InvalidArgumentError,\n                                               \"cannot be a resource variable\"):\n        self.evaluate(guarantee_a)\n\n\nclass SnapshotOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_deprecated_v1\n  def testInvertPermutation(self):\n    for dtype in [dtypes.int32, dtypes.int64, dtypes.float32, dtypes.float64]:\n      with self.subTest(dtype=dtype):\n        with self.cached_session(use_gpu=True):\n          x = constant_op.constant([0, 1, 2, 3], dtype=dtype)\n          y = gen_array_ops.snapshot(x)\n          self.assertAllEqual(y, [0, 1, 2, 3])\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass QuantizeAndDequantizeTest(test_util.TensorFlowTestCase):\n\n  # Generates a tensor of the specified `shape` using values from `values`\n  # scaled by (slice_idx + 1) along `axis` dimension.\n  def _scale_per_slice(self, shape, axis, values):\n    # Note: repeats the values if the shape is larger than values.\n    out = np.take(values, np.remainder(np.arange(np.prod(shape)),\n                                       len(values))).reshape(shape)\n    if axis is not None:\n      scale_shape = [1] * len(shape)\n      scale_shape[axis] = shape[axis]\n      out *= np.arange(1, shape[axis] + 1).reshape(scale_shape)\n    return out\n\n  def testAxis(self):\n    shape = np.array([2, 3, 4, 5])\n    values = np.array([-1, -0.5, 0, 0.3, 0.8, 0.555, 0.5], dtype=np.float32)\n    quant_values = np.array(\n        [-1, -0.5, 0, 38.0 / 128, 102.0 / 128, 71.0 / 128, 0.5],\n        dtype=np.float32)\n    for axis in [None, 0, 1, 2, 3]:\n      with self.subTest(axis=axis):\n        inputs = constant_op.constant(\n            self._scale_per_slice(shape, axis, values))\n        expected = self._scale_per_slice(shape, axis, quant_values)\n        unused_minmax_value = 0 if axis is None else [0] * shape[axis]\n        fake_quantized = self.evaluate(\n            array_ops.quantize_and_dequantize_v2(\n                inputs,\n                unused_minmax_value,\n                unused_minmax_value,\n                range_given=False,\n                round_mode=\"HALF_UP\",\n                axis=axis))\n        self.assertAllEqual(fake_quantized, expected)\n        if axis is not None:\n          fake_quantized = self.evaluate(\n              array_ops.quantize_and_dequantize_v2(\n                  inputs,\n                  unused_minmax_value,\n                  unused_minmax_value,\n                  range_given=False,\n                  axis=(axis - 4)))\n          self.assertAllClose(fake_quantized, expected)\n\n  def testBadAxis(self):\n    input_tensor = [2.5, 2.5]\n    input_min = [0, 0]\n    input_max = [1, 1]\n    error_message_pattern = \"Shape must be at least rank 11 but is rank 1\"\n    # TODO(b/171260356): Eager mode and graph mode throw different error types\n    error = errors.InvalidArgumentError if context.executing_eagerly(\n    ) else ValueError\n    with self.assertRaisesRegex(error, error_message_pattern):\n      self.evaluate(\n          array_ops.quantize_and_dequantize_v2(\n              input=input_tensor,\n              input_min=input_min,\n              input_max=input_max,\n              axis=10))\n\n  def testQuantizeDequantizeGrad(self):\n    shape = (2, 2)\n    max_threshold = 0\n    min_threshold = -10\n    input_value = np.random.rand(2, 2) * 40.0 - 20.0\n    input_tensor = constant_op.constant(input_value, shape=shape,\n                                        name=\"input_tensor\")\n    with self.cached_session():\n      def f(a):\n        return array_ops.quantize_and_dequantize_v2(\n            a,\n            input_min=min_threshold,\n            input_max=max_threshold,\n            range_given=True)\n      output_grad = gradient_checker_v2.compute_gradient(f, [input_tensor])\n      self.assertAllClose(output_grad[0], np.zeros([1, 4, 4]))\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass SortedSearchTest(test_util.TensorFlowTestCase):\n\n  def testUpperBoundFloatHandCoded(self):\n    cdf = np.array([0, .2, .5, .6, .8, 1.], dtype=np.float32)\n    arr = np.array([.04, .99, .53, .58, .31, .01, .79, .8, .21],\n                   dtype=np.float32)\n    result = np.searchsorted(cdf, arr, side=\"right\")\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n    self.assertAllEqual(result, tf_result)\n\n  def testUpperBoundFloatRandomNd(self):\n    dim_size = 7\n    for d in range(1, 5):\n      shape = [dim_size] * d\n      cdf = np.cumsum(\n          np.random.uniform(size=shape).astype(np.float32), axis=(d - 1))\n      arr = np.random.uniform(size=shape).astype(np.float32) * dim_size\n\n      tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n\n      cdf = cdf.reshape([-1, dim_size])\n      arr = arr.reshape([-1, dim_size])\n      result = np.zeros(arr.shape, dtype=np.int32)\n      for i in range(dim_size**(d - 1)):\n        result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"right\")\n\n      result = result.reshape(shape)\n\n      self.assertAllEqual(result, tf_result)\n\n  def testUpperBoundFloatUneven(self):\n    batch_size = 7\n    size_search_array = 1000\n    size_values = 47\n    cdf = np.cumsum(\n        np.random.uniform(size=[batch_size, size_search_array]).astype(\n            np.float32),\n        axis=1)\n    arr = np.random.uniform(size=[batch_size, size_values]).astype(\n        np.float32) * size_search_array\n\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n\n    result = np.zeros(arr.shape, dtype=np.int32)\n    for i in range(batch_size):\n      result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"right\")\n\n    self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundFloatHandCoded(self):\n    cdf = np.array([0, .2, .5, .6, .8, 1.], dtype=np.float32)\n    arr = np.array([.04, .99, .53, .58, .31, .01, .79, .8, .21],\n                   dtype=np.float32)\n    result = np.searchsorted(cdf, arr, side=\"left\")\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n    self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundFloatRandomNd(self):\n    dim_size = 7\n    for d in range(1, 5):\n      shape = [dim_size] * d\n      cdf = np.cumsum(\n          np.random.uniform(size=shape).astype(np.float32), axis=(d - 1))\n      arr = np.random.uniform(size=shape).astype(np.float32) * dim_size\n\n      tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n\n      cdf = cdf.reshape([-1, dim_size])\n      arr = arr.reshape([-1, dim_size])\n      result = np.zeros(arr.shape, dtype=np.int32)\n      for i in range(dim_size**(d - 1)):\n        result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"left\")\n\n      result = result.reshape(shape)\n\n      self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundFloatUneven(self):\n    batch_size = 7\n    size_search_array = 1000\n    size_values = 47\n    cdf = np.cumsum(\n        np.random.uniform(size=[batch_size, size_search_array]).astype(\n            np.float32),\n        axis=1)\n    arr = np.random.uniform(size=[batch_size, size_values]).astype(\n        np.float32) * size_search_array\n\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n\n    result = np.zeros(arr.shape, dtype=np.int32)\n    for i in range(batch_size):\n      result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"left\")\n\n    self.assertAllEqual(result, tf_result)\n\n  def testUpperBoundIntHandCoded(self):\n    cdf = np.array([0, 20, 50, 60, 80, 100], dtype=np.int64)\n    arr = np.array([4, 99, 53, 58, 31, 1, 79, 8, 21], dtype=np.int64)\n    result = np.searchsorted(cdf, arr, side=\"right\")\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n    self.assertAllEqual(result, tf_result)\n\n  def testUpperBoundIntRandomNd(self):\n    dim_size = 7\n    for d in range(1, 5):\n      shape = [dim_size] * d\n      cdf = np.cumsum(\n          np.random.randint(low=0, high=10, size=shape).astype(np.int64),\n          axis=(d - 1))\n      arr = np.random.randint(\n          low=0, high=10 * dim_size, size=shape).astype(np.int64)\n\n      tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n\n      cdf = cdf.reshape([-1, dim_size])\n      arr = arr.reshape([-1, dim_size])\n      result = np.zeros(arr.shape, dtype=np.int32)\n      for i in range(dim_size**(d - 1)):\n        result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"right\")\n\n      result = result.reshape(shape)\n\n      self.assertAllEqual(result, tf_result)\n\n  def testUpperBoundIntUneven(self):\n    batch_size = 7\n    size_search_array = 1000\n    size_values = 47\n    cdf = np.cumsum(\n        np.random.randint(low=0, high=10,\n                          size=[batch_size,\n                                size_search_array]).astype(np.int64),\n        axis=1)\n    arr = np.random.randint(\n        low=0, high=10 * size_search_array, size=[batch_size,\n                                                  size_values]).astype(np.int64)\n\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n\n    result = np.zeros(arr.shape, dtype=np.int32)\n    for i in range(batch_size):\n      result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"right\")\n\n    self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundIntHandCoded(self):\n    cdf = np.array([0, 20, 50, 60, 80, 100], dtype=np.int64)\n    arr = np.array([4, 99, 53, 58, 31, 1, 79, 8, 21], dtype=np.int64)\n    result = np.searchsorted(cdf, arr, side=\"left\")\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n    self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundIntRandomNd(self):\n    dim_size = 7\n    for d in range(1, 5):\n      shape = [dim_size] * d\n      cdf = np.cumsum(\n          np.random.randint(low=0, high=10, size=shape).astype(np.int64),\n          axis=(d - 1))\n      arr = np.random.randint(\n          low=0, high=10 * dim_size, size=shape).astype(np.int64)\n\n      tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n\n      cdf = cdf.reshape([-1, dim_size])\n      arr = arr.reshape([-1, dim_size])\n      result = np.zeros(arr.shape, dtype=np.int32)\n      for i in range(dim_size**(d - 1)):\n        result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"left\")\n\n      result = result.reshape(shape)\n\n      self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundIntUneven(self):\n    batch_size = 7\n    size_search_array = 1000\n    size_values = 47\n    cdf = np.cumsum(\n        np.random.randint(low=0, high=10,\n                          size=[batch_size,\n                                size_search_array]).astype(np.int64),\n        axis=1)\n    arr = np.random.randint(\n        low=0, high=10 * size_search_array, size=[batch_size,\n                                                  size_values]).astype(np.int64)\n\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n\n    result = np.zeros(arr.shape, dtype=np.int32)\n    for i in range(batch_size):\n      result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"left\")\n\n    self.assertAllEqual(result, tf_result)\n\n  def testZeroSequenceSize(self):\n    dtype = dtypes.int32\n    for side in (\"left\", \"right\"):\n      with self.subTest(side=side):\n        self.assertAllEqual(\n            array_ops.searchsorted(\n                array_ops.ones([2, 0]),\n                array_ops.ones([2, 3]),\n                side=side,\n                out_type=dtype), array_ops.zeros([2, 3], dtype))\n\n  def testZeroValueSize(self):\n    dtype = dtypes.int32\n    for side in (\"left\", \"right\"):\n      with self.subTest(side=side):\n        self.assertAllEqual(\n            array_ops.searchsorted(\n                array_ops.ones([2, 3]),\n                array_ops.ones([2, 0]),\n                side=side,\n                out_type=dtype), array_ops.zeros([2, 0], dtype))\n\n\nclass BatchGatherNdTest(test_util.TensorFlowTestCase):\n\n  def testShapesMatch(self):\n    \"\"\"Tests for various different shape combinations.\"\"\"\n    shapes = []\n    # params_shape, indices_shape, batch_dims\n    shapes.append(((2, 2, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 2), (2, 3), 0),)\n    shapes.append(((2, 2, 2), (3,), 0),)\n    shapes.append(((2, 2, 2), (1,), 0),)\n    shapes.append(((2, 2, 3, 2), (2, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 3, 1), 1),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 2),)\n\n    for params_shape, indices_shape, batch_dims in shapes:\n      with self.subTest(\n          params_shape=params_shape,\n          indices_shape=indices_shape,\n          batch_dims=batch_dims):\n        params = constant_op.constant(1.0, shape=(params_shape))\n        indices = constant_op.constant(\n            1, shape=(indices_shape), dtype=dtypes.int32)\n        out = array_ops.batch_gather_nd(\n            params=params, indices=indices, batch_dims=batch_dims)\n        ndims_params = len(params_shape) - batch_dims\n        ndims_rows = ndims_params - indices_shape[-1]\n        expected_out_shape = indices_shape[:-1]\n        if ndims_rows > 0:\n          expected_out_shape += params_shape[-ndims_rows:]\n        self.assertSequenceEqual(out.shape, expected_out_shape)\n\n  def testReducesToGatherNDWhenBatchDimIsZero(self):\n    \"\"\"Confirms setting batch_dims to zero reduces to tf.gather_nd.\"\"\"\n    params = constant_op.constant(np.random.uniform(0.0, 1.0, size=(7, 8, 9)))\n    indices_shapes = []\n    indices_shapes.append((1,))\n    indices_shapes.append((3, 1))\n    indices_shapes.append((3, 3, 1))\n    indices_shapes.append((2,))\n    indices_shapes.append((3, 2))\n    indices_shapes.append((3, 3, 2))\n    indices_shapes.append((3,))\n    indices_shapes.append((3, 3))\n    indices_shapes.append((3, 3, 3))\n\n    for indices_shape in indices_shapes:\n      with self.subTest(indices_shape=indices_shape):\n        indices = np.random.randint(0, 7, size=indices_shape)\n        gather_nd_result = gen_array_ops.gather_nd(params, indices)\n        batch_gather_nd_result = array_ops.batch_gather_nd(\n            params=params, indices=indices, batch_dims=0)\n        self.assertAllEqual(gather_nd_result, batch_gather_nd_result)\n\n  def testSameResultAsMapFn(self):\n    \"\"\"Compares results with gather_nd called on every element with map_fn.\"\"\"\n    shapes = []\n    # params_shape, indices_shape, batch_dims\n    shapes.append(((2, 2, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 3, 1), 1),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 2),)\n\n    for params_shape, indices_shape, batch_dims in shapes:\n      with self.subTest(\n          params_shape=params_shape,\n          indices_shape=indices_shape,\n          batch_dims=batch_dims):\n        params = constant_op.constant(\n            np.random.uniform(0.0, 1.0, size=(params_shape)))\n        indices = np.random.randint(0, 2, size=indices_shape)\n        batch_gather_nd_result = array_ops.batch_gather_nd(\n            params=params, indices=indices, batch_dims=batch_dims)\n\n        if batch_dims > 1:\n          params = array_ops.reshape(\n              params, shape=[-1] + list(params_shape[batch_dims:]))\n          indices = array_ops.reshape(\n              indices, shape=[-1] + list(indices_shape[batch_dims:]))\n\n        map_fn_gather_nd_result = map_fn.map_fn(\n            fn=self._map_fn_body, elems=(params, indices), dtype=dtypes.float64)\n\n        if batch_dims > 1:\n          out_shape = map_fn_gather_nd_result.shape.as_list()\n          out_shape = list(params_shape[:batch_dims]) + out_shape[1:]\n          map_fn_gather_nd_result = array_ops.reshape(\n              map_fn_gather_nd_result, shape=out_shape)\n\n        self.assertAllEqual(map_fn_gather_nd_result, batch_gather_nd_result)\n\n  def _map_fn_body(self, elems):\n    return gen_array_ops.gather_nd(elems[0], elems[1])\n\n  def testBatchDimsAsTensor(self):\n    \"\"\"Tests Tensor batch_dims as input works as intended.\"\"\"\n    shapes = []\n    # params_shape, indices_shape, batch_dims\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 0),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 1),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 2),)\n\n    for params_shape, indices_shape, batch_dims in shapes:\n      with self.subTest(\n          params_shape=params_shape,\n          indices_shape=indices_shape,\n          batch_dims=batch_dims):\n        params = constant_op.constant(\n            np.random.uniform(0.0, 1.0, size=(params_shape)))\n        indices = np.random.randint(0, 2, size=indices_shape)\n        batch_gather_nd_result = array_ops.gather_nd(\n            params=params, indices=indices, batch_dims=batch_dims)\n        batch_dims_tensor = constant_op.constant([batch_dims])\n        batch_gather_nd_tensor_batch_dims_result = array_ops.gather_nd(\n            params=params, indices=indices, batch_dims=batch_dims_tensor)\n\n        self.assertAllEqual(batch_gather_nd_tensor_batch_dims_result,\n                            batch_gather_nd_result)\n\n  def testInvalidBatchDimsRaisesException(self):\n    \"\"\"Tests whether invalid batch_dims raise expected exceptions.\"\"\"\n    params = constant_op.constant(\n        np.random.uniform(0.0, 1.0, size=(3, 2, 2, 3, 4)))\n    indices = np.random.randint(0, 2, size=(3, 2, 3))\n\n    with self.assertRaises(TypeError):\n      array_ops.batch_gather_nd(\n          params=params,\n          indices=indices,\n          batch_dims=constant_op.constant((0, 1)))\n\n    with self.assertRaises(ValueError):\n      array_ops.batch_gather_nd(params=params, indices=indices, batch_dims=-1)\n\n    with self.assertRaises(ValueError):\n      array_ops.batch_gather_nd(params=params, indices=indices, batch_dims=4)\n\n  @test_util.run_deprecated_v1\n  def testNoneBatchDimensions(self):\n    \"\"\"Tests gather_nd works with None dimensions.\"\"\"\n    shapes = []\n    # params_shape, indices_shape, batch_dims\n    shapes.append(((2, 2, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 3, 1), 1),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 2),)\n\n    for params_shape, indices_shape, batch_dims in shapes:\n      params_ph_shape = list(params_shape)\n      indices_ph_shape = list(indices_shape)\n      for i in range(batch_dims):\n        params_ph_shape[i] = None\n        indices_ph_shape[i] = None\n\n      params = array_ops.placeholder(dtypes.float32, shape=params_ph_shape)\n      indices = array_ops.placeholder(dtypes.int32, shape=indices_ph_shape)\n      out = array_ops.batch_gather_nd(\n          params=params, indices=indices, batch_dims=batch_dims)\n\n      with self.cached_session() as sess:\n        params_val = np.ones(dtype=np.float32, shape=params_shape)\n        indices_val = np.ones(dtype=np.int32, shape=indices_shape)\n        res = sess.run(\n            out, feed_dict={\n                params: params_val,\n                indices: indices_val\n            })\n      row_ndims = len(params_shape) - batch_dims - indices_shape[-1]\n      expected_out_shape = indices_shape[:-1]\n      if row_ndims > 0:\n        expected_out_shape += params_shape[-row_ndims:]\n\n      self.assertSequenceEqual(res.shape, expected_out_shape)\n\n  @test_util.run_deprecated_v1\n  def testUnknownIndices(self):\n    \"\"\"Tests whether indices with unknown rank works correctly.\"\"\"\n    params = constant_op.constant(((0, 1, 2),))\n    indices = array_ops.placeholder(dtypes.int32)\n    gather_nd_t = array_ops.gather_nd(params, indices, batch_dims=1)\n    shape = gather_nd_t.get_shape()\n    self.assertEqual(None, shape.ndims)\n    self.assertEqual(None, tensor_shape.dimension_value(shape[0]))\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass RepeatTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n\n  @parameterized.parameters(\n      (3, 4, None),\n      ([[1, 2], [3, 4]], 2, None),\n      ([[1, 2], [3, 4]], [1, 2], 0),\n      ([[1, 2], [3, 4]], [1, 2], 1),\n      ([[1, 2], [3, 4]], 3, 1),\n      ([[1, 2], [3, 4]], [1, 2, 3, 4], None),\n      (np.ones([0, 4]), 0, 1),\n      (np.ones([1, 2]), [2], None),\n  )\n  def testRepeat(self, array, repeats, axis):\n    array = np.array(array)\n\n    @def_function.function(\n        input_signature=[tensor_spec.TensorSpec(None, dtypes.int32)] * 2)\n    def repeat_fn(array, repeats):\n      return array_ops.repeat(array, repeats, axis)\n\n    v_tf = array_ops.repeat(constant_op.constant(array), repeats, axis)\n    v_tf_fn = repeat_fn(\n        constant_op.constant(array, dtype=dtypes.int32), repeats)\n    v_np = np.repeat(array, repeats, axis)\n    self.assertAllEqual(v_tf, v_np)\n    self.assertAllEqual(v_tf_fn, v_np)\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass TileVariantTest(test_util.TensorFlowTestCase):\n\n  def test_tile_tensor_list(self):\n    t = constant_op.constant(np.random.uniform(size=[2, 3, 4]))\n    handle = list_ops.tensor_list_from_tensor(t, element_shape=None)\n    with ops.device(\"CPU:0\"):\n      tiled_handles = array_ops.tile(array_ops.reshape(handle, [1]), [2])\n    tiled_tensor_0 = list_ops.tensor_list_stack(tiled_handles[0], t.dtype, 2,\n                                                [3, 4])\n    tiled_tensor_1 = list_ops.tensor_list_stack(tiled_handles[1], t.dtype, 2,\n                                                [3, 4])\n    self.assertAllEqual(t, tiled_tensor_0)\n    self.assertAllEqual(t, tiled_tensor_1)\n    # Now mutate some of the lists and make sure the changes are not reflected\n    # in the tiled handles.\n    with ops.control_dependencies([\n        list_ops.tensor_list_scatter([t[0] + 1], [0], input_handle=handle),\n        list_ops.tensor_list_set_item(tiled_handles[0], 0, t[0] + 2)]):\n      tiled_tensor_0 = list_ops.tensor_list_stack(tiled_handles[0], t.dtype, 2,\n                                                  [3, 4])\n      tiled_tensor_1 = list_ops.tensor_list_stack(tiled_handles[1], t.dtype, 2,\n                                                  [3, 4])\n    self.assertAllEqual(t, tiled_tensor_0)\n    self.assertAllEqual(t, tiled_tensor_1)\n\n\nif __name__ == \"__main__\":\n  test_lib.main()"