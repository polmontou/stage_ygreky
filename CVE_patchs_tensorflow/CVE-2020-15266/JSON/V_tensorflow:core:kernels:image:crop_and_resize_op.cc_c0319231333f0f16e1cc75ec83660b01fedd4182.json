"/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/image_ops.cc\n\n#define EIGEN_USE_THREADS\n\n#include \"tensorflow/core/kernels/image/crop_and_resize_op.h\"\n\n#include <functional>\n#include <string>\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/bounds_check.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_reference.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/lib/core/status.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/platform/types.h\"\n#include \"tensorflow/core/util/work_sharder.h\"\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#include \"tensorflow/core/common_runtime/gpu/gpu_event_mgr.h\"\n#include \"tensorflow/core/platform/stream_executor.h\"\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#if GOOGLE_CUDA\n#include \"tensorflow/stream_executor/cuda/cuda_activation.h\"\nusing stream_executor::cuda::ScopedActivateExecutorContext;\n#elif TENSORFLOW_USE_ROCM\n#include \"tensorflow/core/platform/rocm.h\"\nusing stream_executor::rocm::ScopedActivateExecutorContext;\n#endif\n\nnamespace tensorflow {\nnamespace {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\ntypedef Eigen::GpuDevice GPUDevice;\nusing Callback = std::function<void()>;\n\nstatic inline Status ParseAndCheckBoxSizes(const Tensor& boxes,\n                                           const Tensor& box_index,\n                                           int* num_boxes) {\n  if (boxes.NumElements() == 0 && box_index.NumElements() == 0) {\n    *num_boxes = 0;\n    return Status::OK();\n  }\n  // The shape of 'boxes' is [num_boxes, 4].\n  if (boxes.dims() != 2) {\n    return errors::InvalidArgument(\"boxes must be 2-D\",\n                                   boxes.shape().DebugString());\n  }\n  *num_boxes = boxes.dim_size(0);\n  if (boxes.dim_size(1) != 4) {\n    return errors::InvalidArgument(\"boxes must have 4 columns\");\n  }\n  // The shape of 'box_index' is [num_boxes].\n  if (box_index.dims() != 1) {\n    return errors::InvalidArgument(\"box_index must be 1-D\",\n                                   box_index.shape().DebugString());\n  }\n  if (box_index.dim_size(0) != *num_boxes) {\n    return errors::InvalidArgument(\"box_index has incompatible shape\");\n  }\n  return Status::OK();\n}\n\n// Conditionally calls the compute callback if all values in box_index are in\n// [0, batch_size) then calls done.\ntemplate <typename Device>\ninline void RunIfBoxIndexIsValid(\n    OpKernelContext* context, typename TTypes<int32, 1>::ConstTensor box_index,\n    int batch_size, const Callback& compute, const Callback& done);\n\n// Specialization of CheckValidBoxIndex for a CPUDevice.\ntemplate <>\ninline void RunIfBoxIndexIsValid<CPUDevice>(\n    OpKernelContext* context, typename TTypes<int32, 1>::ConstTensor box_index,\n    int batch_size, const Callback& compute, const Callback& done) {\n  const int num_boxes = box_index.dimension(0);\n  for (int b = 0; b < num_boxes; ++b) {\n    OP_REQUIRES_ASYNC(\n        context, FastBoundsCheck(box_index(b), batch_size),\n        errors::OutOfRange(\"box_index has values outside [0, batch_size)\"),\n        done);\n  }\n  if (compute) {\n    compute();\n  }\n  if (done) {\n    done();\n  }\n}\n\n}  // namespace\n\ntemplate <typename Device, typename T>\nclass CropAndResizeOp : public AsyncOpKernel {\n public:\n  explicit CropAndResizeOp(OpKernelConstruction* context)\n      : AsyncOpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"method\", &method_));\n    OP_REQUIRES(context, method_ == \"bilinear\" || method_ == \"nearest\",\n                errors::InvalidArgument(\n                    \"method must be 'bilinear' or 'nearest'\", method_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"extrapolation_value\",\n                                             &extrapolation_value_));\n  }\n\n  void ComputeAsync(OpKernelContext* context, DoneCallback done) override {\n    // The shape of 'image' is [batch_size, image_height, image_width,\n    // channels].\n    const Tensor& image = context->input(0);\n    // The shape of 'boxes' is [num_boxes, 4].\n    const Tensor& boxes = context->input(1);\n    // The shape of 'box_index' is [num_boxes].\n    const Tensor& box_index = context->input(2);\n    // The shape of 'crop_size' is [2].\n    const Tensor& crop_size = context->input(3);\n\n    // Validate inputs dimensions.\n    OP_REQUIRES_ASYNC(context, image.dims() == 4,\n                      errors::InvalidArgument(\"input image must be 4-D\",\n                                              image.shape().DebugString()),\n                      done);\n    const int batch_size = image.dim_size(0);\n    const int image_height = image.dim_size(1);\n    const int image_width = image.dim_size(2);\n    const int depth = image.dim_size(3);\n    OP_REQUIRES_ASYNC(\n        context, image_height > 0 && image_width > 0,\n        errors::InvalidArgument(\"image dimensions must be positive\"), done);\n    int num_boxes = 0;\n    OP_REQUIRES_OK_ASYNC(\n        context, ParseAndCheckBoxSizes(boxes, box_index, &num_boxes), done);\n\n    OP_REQUIRES_ASYNC(context, crop_size.dims() == 1,\n                      errors::InvalidArgument(\"crop_size must be 1-D\",\n                                              crop_size.shape().DebugString()),\n                      done);\n    OP_REQUIRES_ASYNC(\n        context, crop_size.dim_size(0) == 2,\n        errors::InvalidArgument(\"crop_size must have two elements\",\n                                crop_size.shape().DebugString()),\n        done);\n\n    // Copy and validate crop sizes.\n    auto crop_size_vec = crop_size.vec<int32>();\n    const int crop_height = internal::SubtleMustCopy(crop_size_vec(0));\n    const int crop_width = internal::SubtleMustCopy(crop_size_vec(1));\n    OP_REQUIRES_ASYNC(\n        context, crop_height > 0 && crop_width > 0,\n        errors::InvalidArgument(\"crop dimensions must be positive\"), done);\n\n    // Allocate output tensor.\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK_ASYNC(\n        context,\n        context->allocate_output(\n            0, TensorShape({num_boxes, crop_height, crop_width, depth}),\n            &output),\n        done);\n\n    auto compute_callback = [this, context, output]() {\n      const Tensor& image = context->input(0);\n      const Tensor& boxes = context->input(1);\n      const Tensor& box_index = context->input(2);\n      const bool status = functor::CropAndResize<Device, T>()(\n          context, image.tensor<T, 4>(), boxes.tensor<float, 2>(),\n          box_index.tensor<int32, 1>(), method_, extrapolation_value_,\n          output->tensor<float, 4>());\n\n      if (!status) {\n        context->SetStatus(\n            errors::Internal(\"Failed launch CropAndResizeKernel.\"));\n      }\n    };\n\n    RunIfBoxIndexIsValid<Device>(context, box_index.tensor<int32, 1>(),\n                                 batch_size, std::move(compute_callback),\n                                 std::move(done));\n  }\n\n private:\n  float extrapolation_value_;\n  string method_;\n};\n\n// Partial specialization of CropAndResize functor for a CPUDevice.\nnamespace functor {\ntemplate <typename T>\nstruct CropAndResize<CPUDevice, T> {\n  bool operator()(const OpKernelContext* context,\n                  typename TTypes<T, 4>::ConstTensor image,\n                  typename TTypes<float, 2>::ConstTensor boxes,\n                  typename TTypes<int32, 1>::ConstTensor box_index,\n                  const string& method_name, float extrapolation_value,\n                  typename TTypes<float, 4>::Tensor crops) {\n    const int batch_size = image.dimension(0);\n    const int image_height = image.dimension(1);\n    const int image_width = image.dimension(2);\n\n    const int num_boxes = crops.dimension(0);\n    const int crop_height = crops.dimension(1);\n    const int crop_width = crops.dimension(2);\n    const int depth = crops.dimension(3);\n\n    // Sharding across boxes.\n    auto CropAndResizePerBox = [&](int64 start_box, int64 limit_box) {\n      for (int b = start_box; b < limit_box; ++b) {\n        const float y1 = boxes(b, 0);\n        const float x1 = boxes(b, 1);\n        const float y2 = boxes(b, 2);\n        const float x2 = boxes(b, 3);\n\n        const int32 b_in = box_index(b);\n        if (!FastBoundsCheck(b_in, batch_size)) {\n          continue;\n        }\n\n        const float height_scale =\n            (crop_height > 1)\n                ? (y2 - y1) * (image_height - 1) / (crop_height - 1)\n                : 0;\n        const float width_scale =\n            (crop_width > 1) ? (x2 - x1) * (image_width - 1) / (crop_width - 1)\n                             : 0;\n\n        for (int y = 0; y < crop_height; ++y) {\n          const float in_y = (crop_height > 1)\n                                 ? y1 * (image_height - 1) + y * height_scale\n                                 : 0.5 * (y1 + y2) * (image_height - 1);\n          if (in_y < 0 || in_y > image_height - 1) {\n            for (int x = 0; x < crop_width; ++x) {\n              for (int d = 0; d < depth; ++d) {\n                crops(b, y, x, d) = extrapolation_value;\n              }\n            }\n            continue;\n          }\n          if (method_name == \"bilinear\") {\n            const int top_y_index = floorf(in_y);\n            const int bottom_y_index = ceilf(in_y);\n            const float y_lerp = in_y - top_y_index;\n\n            for (int x = 0; x < crop_width; ++x) {\n              const float in_x = (crop_width > 1)\n                                     ? x1 * (image_width - 1) + x * width_scale\n                                     : 0.5 * (x1 + x2) * (image_width - 1);\n              if (in_x < 0 || in_x > image_width - 1) {\n                for (int d = 0; d < depth; ++d) {\n                  crops(b, y, x, d) = extrapolation_value;\n                }\n                continue;\n              }\n              const int left_x_index = floorf(in_x);\n              const int right_x_index = ceilf(in_x);\n              const float x_lerp = in_x - left_x_index;\n\n              for (int d = 0; d < depth; ++d) {\n                const float top_left(static_cast<float>(\n                    image(b_in, top_y_index, left_x_index, d)));\n                const float top_right(static_cast<float>(\n                    image(b_in, top_y_index, right_x_index, d)));\n                const float bottom_left(static_cast<float>(\n                    image(b_in, bottom_y_index, left_x_index, d)));\n                const float bottom_right(static_cast<float>(\n                    image(b_in, bottom_y_index, right_x_index, d)));\n                const float top = top_left + (top_right - top_left) * x_lerp;\n                const float bottom =\n                    bottom_left + (bottom_right - bottom_left) * x_lerp;\n                crops(b, y, x, d) = top + (bottom - top) * y_lerp;\n              }\n            }\n          } else {  // method == \"nearest\"\n            for (int x = 0; x < crop_width; ++x) {\n              const float in_x = (crop_width > 1)\n                                     ? x1 * (image_width - 1) + x * width_scale\n                                     : 0.5 * (x1 + x2) * (image_width - 1);\n              if (in_x < 0 || in_x > image_width - 1) {\n                for (int d = 0; d < depth; ++d) {\n                  crops(b, y, x, d) = extrapolation_value;\n                }\n                continue;\n              }\n              const int closest_x_index = roundf(in_x);\n              const int closest_y_index = roundf(in_y);\n              for (int d = 0; d < depth; ++d) {\n                crops(b, y, x, d) = static_cast<float>(\n                    image(b_in, closest_y_index, closest_x_index, d));\n              }\n            }\n          }\n        }\n      }\n    };\n\n    // A rough estimation of the cost for each cropped box.\n    double cost_per_pixel =\n        depth * (Eigen::TensorOpCost::AddCost<float>() * 6 +\n                 Eigen::TensorOpCost::MulCost<float>() * 3 +\n                 Eigen::TensorOpCost::CastCost<T, float>() * 4) +\n        (Eigen::TensorOpCost::AddCost<float>() * 2 +\n         Eigen::TensorOpCost::AddCost<float>() * 3);\n    if (method_name == \"nearest\") {\n      cost_per_pixel = depth * Eigen::TensorOpCost::CastCost<T, float>() +\n                       Eigen::TensorOpCost::AddCost<float>() * 4 +\n                       Eigen::TensorOpCost::MulCost<float>() * 4;\n    }\n    const double cost_per_box = crop_height * crop_width * cost_per_pixel;\n\n    const DeviceBase::CpuWorkerThreads& worker_threads =\n        *(context->device()->tensorflow_cpu_worker_threads());\n    Shard(worker_threads.num_threads, worker_threads.workers, num_boxes,\n          cost_per_box, CropAndResizePerBox);\n\n    return true;\n  }\n};\n\n}  // namespace functor\n\ntemplate <typename Device, typename T>\nclass CropAndResizeGradImageOp : public AsyncOpKernel {\n public:\n  explicit CropAndResizeGradImageOp(OpKernelConstruction* context)\n      : AsyncOpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"method\", &method_));\n    OP_REQUIRES(context, method_ == \"bilinear\" || method_ == \"nearest\",\n                errors::InvalidArgument(\n                    \"method must be 'bilinear' or 'nearest'\", method_));\n  }\n\n  void ComputeAsync(OpKernelContext* context, DoneCallback done) override {\n    // The shape of 'grads' is [num_boxes, crop_height, crop_width, depth].\n    const Tensor& grads = context->input(0);\n    // The shape of 'boxes' is [num_boxes, 4].\n    const Tensor& boxes = context->input(1);\n    // The shape of 'box_index' is [num_boxes].\n    const Tensor& box_index = context->input(2);\n    // The shape of 'image_size' is [4].\n    const Tensor& image_size = context->input(3);\n\n    // Validate input shapes.\n    OP_REQUIRES_ASYNC(context, grads.dims() == 4,\n                      errors::InvalidArgument(\"grads image must be 4-D\",\n                                              grads.shape().DebugString()),\n                      done);\n    const int crop_height = grads.dim_size(1);\n    const int crop_width = grads.dim_size(2);\n    OP_REQUIRES_ASYNC(\n        context, crop_height > 0 && crop_width > 0,\n        errors::InvalidArgument(\"grads dimensions must be positive\"), done);\n    int num_boxes = 0;\n    OP_REQUIRES_OK_ASYNC(\n        context, ParseAndCheckBoxSizes(boxes, box_index, &num_boxes), done);\n    OP_REQUIRES_ASYNC(\n        context, grads.dim_size(0) == num_boxes,\n        errors::InvalidArgument(\"boxes and grads have incompatible shape\"),\n        done);\n\n    OP_REQUIRES_ASYNC(context, image_size.dims() == 1,\n                      errors::InvalidArgument(\"image_size must be 1-D\",\n                                              image_size.shape().DebugString()),\n                      done);\n    OP_REQUIRES_ASYNC(context, image_size.dim_size(0) == 4,\n                      errors::InvalidArgument(\"image_size must have 4 elements\",\n                                              image_size.shape().DebugString()),\n                      done);\n    auto image_size_vec = image_size.vec<int32>();\n    const int batch_size = internal::SubtleMustCopy(image_size_vec(0));\n    const int image_height = internal::SubtleMustCopy(image_size_vec(1));\n    const int image_width = internal::SubtleMustCopy(image_size_vec(2));\n    const int depth = internal::SubtleMustCopy(image_size_vec(3));\n    OP_REQUIRES_ASYNC(\n        context, image_height > 0 && image_width > 0,\n        errors::InvalidArgument(\"image dimensions must be positive\"), done);\n    OP_REQUIRES_ASYNC(\n        context, grads.dim_size(3) == depth,\n        errors::InvalidArgument(\"image_size and grads are incompatible\"), done);\n\n    // Allocate output tensor.\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK_ASYNC(\n        context,\n        context->allocate_output(\n            0, TensorShape({batch_size, image_height, image_width, depth}),\n            &output),\n        done);\n\n    auto compute_callback = [this, context, output]() {\n      const Tensor& grads = context->input(0);\n      const Tensor& boxes = context->input(1);\n      const Tensor& box_index = context->input(2);\n      const bool status = functor::CropAndResizeBackpropImage<Device, T>()(\n          context, grads.tensor<float, 4>(), boxes.tensor<float, 2>(),\n          box_index.tensor<int32, 1>(), output->tensor<T, 4>(), method_);\n\n      if (!status) {\n        context->SetStatus(errors::Internal(\n            \"Failed launch CropAndResizeBackpropImage kernel.\"));\n      }\n    };\n\n    RunIfBoxIndexIsValid<Device>(context, box_index.tensor<int32, 1>(),\n                                 batch_size, std::move(compute_callback),\n                                 std::move(done));\n  }\n\n private:\n  string method_;\n};\n\n// Partial specialization of CropAndResizeBackpropImage functor for a CPUDevice.\nnamespace functor {\ntemplate <typename T>\nstruct CropAndResizeBackpropImage<CPUDevice, T> {\n  bool operator()(const OpKernelContext* context,\n                  typename TTypes<float, 4>::ConstTensor grads,\n                  typename TTypes<float, 2>::ConstTensor boxes,\n                  typename TTypes<int32, 1>::ConstTensor box_index,\n                  typename TTypes<T, 4>::Tensor grads_image,\n                  const string& method_name) {\n    const int batch_size = grads_image.dimension(0);\n    const int image_height = grads_image.dimension(1);\n    const int image_width = grads_image.dimension(2);\n\n    const int num_boxes = grads.dimension(0);\n    const int crop_height = grads.dimension(1);\n    const int crop_width = grads.dimension(2);\n    const int depth = grads.dimension(3);\n\n    grads_image.setZero();\n\n    auto CropAndResizeBackImgPerBox = [&](int64 start_box, int64 limit_box) {\n      for (int b = start_box; b < limit_box; ++b) {\n        const float y1 = boxes(b, 0);\n        const float x1 = boxes(b, 1);\n        const float y2 = boxes(b, 2);\n        const float x2 = boxes(b, 3);\n\n        const int32 b_in = box_index(b);\n        if (!FastBoundsCheck(b_in, batch_size)) {\n          continue;\n        }\n\n        const float height_scale =\n            (crop_height > 1)\n                ? (y2 - y1) * (image_height - 1) / (crop_height - 1)\n                : 0;\n        const float width_scale =\n            (crop_width > 1) ? (x2 - x1) * (image_width - 1) / (crop_width - 1)\n                             : 0;\n\n        for (int y = 0; y < crop_height; ++y) {\n          const float in_y = (crop_height > 1)\n                                 ? y1 * (image_height - 1) + y * height_scale\n                                 : 0.5 * (y1 + y2) * (image_height - 1);\n          if (in_y < 0 || in_y > image_height - 1) {\n            continue;\n          }\n          const int top_y_index = floorf(in_y);\n          const int bottom_y_index = ceilf(in_y);\n          const float y_lerp = in_y - top_y_index;\n\n          for (int x = 0; x < crop_width; ++x) {\n            const float in_x = (crop_width > 1)\n                                   ? x1 * (image_width - 1) + x * width_scale\n                                   : 0.5 * (x1 + x2) * (image_width - 1);\n            if (in_x < 0 || in_x > image_width - 1) {\n              continue;\n            }\n\n            if (method_name == \"bilinear\") {\n              const int left_x_index = floorf(in_x);\n              const int right_x_index = ceilf(in_x);\n              const float x_lerp = in_x - left_x_index;\n\n              for (int d = 0; d < depth; ++d) {\n                const float dtop = (1 - y_lerp) * grads(b, y, x, d);\n                grads_image(b_in, top_y_index, left_x_index, d) +=\n                    static_cast<T>((1 - x_lerp) * dtop);\n                grads_image(b_in, top_y_index, right_x_index, d) +=\n                    static_cast<T>(x_lerp * dtop);\n                const float dbottom = y_lerp * grads(b, y, x, d);\n                grads_image(b_in, bottom_y_index, left_x_index, d) +=\n                    static_cast<T>((1 - x_lerp) * dbottom);\n                grads_image(b_in, bottom_y_index, right_x_index, d) +=\n                    static_cast<T>(x_lerp * dbottom);\n              }\n            } else {  // method_name == \"nearest\"\n              for (int d = 0; d < depth; ++d) {\n                int closest_x_index = roundf(in_x);\n                int closest_y_index = roundf(in_y);\n                grads_image(b_in, closest_y_index, closest_x_index, d) +=\n                    static_cast<T>(grads(b, y, x, d));\n              }\n            }\n          }\n        }\n      }\n    };\n\n    // A rough estimation of the cost for each cropped box.\n    // Including calculation cost in the depth loop and pixel loop.\n    const double cost_per_pixel =\n        (method_name == \"bilinear\"\n             ? depth * (Eigen::TensorOpCost::AddCost<float>() * 7 +\n                        Eigen::TensorOpCost::MulCost<float>() * 6 +\n                        Eigen::TensorOpCost::CastCost<T, float>() * 4) +\n                   Eigen::TensorOpCost::AddCost<float>() * 4\n             : depth * (Eigen::TensorOpCost::AddCost<float>() +\n                        Eigen::TensorOpCost::CastCost<T, float>()) +\n                   Eigen::TensorOpCost::AddCost<float>() * 3);\n\n    const double cost_per_box = crop_height * crop_width * cost_per_pixel;\n\n    const DeviceBase::CpuWorkerThreads& worker_threads =\n        *(context->device()->tensorflow_cpu_worker_threads());\n    Shard(worker_threads.num_threads, worker_threads.workers, num_boxes,\n          cost_per_box, CropAndResizeBackImgPerBox);\n\n    return true;\n  }\n};\n\n}  // namespace functor\n\ntemplate <typename Device, typename T>\nclass CropAndResizeGradBoxesOp : public AsyncOpKernel {\n public:\n  explicit CropAndResizeGradBoxesOp(OpKernelConstruction* context)\n      : AsyncOpKernel(context) {\n    string method;\n    OP_REQUIRES_OK(context, context->GetAttr(\"method\", &method));\n    OP_REQUIRES(context, method == \"bilinear\",\n                errors::InvalidArgument(\"method must be 'bilinear'\", method));\n  }\n\n  void ComputeAsync(OpKernelContext* context, DoneCallback done) override {\n    // The shape of 'grads' is [num_boxes, crop_height, crop_width, depth].\n    const Tensor& grads = context->input(0);\n    // The shape of 'boxes' is [num_boxes, 4].\n    const Tensor& boxes = context->input(2);\n    // The shape of 'box_index' is [num_boxes].\n    const Tensor& box_index = context->input(3);\n    // The shape of 'image' is [batch_size, image_height, image_width, depth].\n    const Tensor& image = context->input(1);\n\n    // Validate input shapes.\n    OP_REQUIRES_ASYNC(context, grads.dims() == 4,\n                      errors::InvalidArgument(\"grads image must be 4-D\",\n                                              grads.shape().DebugString()),\n                      done);\n    const int crop_height = grads.dim_size(1);\n    const int crop_width = grads.dim_size(2);\n    const int depth = grads.dim_size(3);\n    OP_REQUIRES_ASYNC(\n        context, crop_height > 0 && crop_width > 0,\n        errors::InvalidArgument(\"grads dimensions must be positive\"), done);\n\n    OP_REQUIRES_ASYNC(context, image.dims() == 4,\n                      errors::InvalidArgument(\"input image must be 4-D\",\n                                              image.shape().DebugString()),\n                      done);\n    const int batch_size = image.dim_size(0);\n    const int image_height = image.dim_size(1);\n    const int image_width = image.dim_size(2);\n    OP_REQUIRES_ASYNC(\n        context, image_height > 0 && image_width > 0,\n        errors::InvalidArgument(\"image dimensions must be positive\"), done);\n    OP_REQUIRES_ASYNC(context, image.dim_size(3) == depth,\n                      errors::InvalidArgument(\"image, grads depth differ\"),\n                      done);\n\n    int num_boxes = 0;\n    OP_REQUIRES_OK_ASYNC(\n        context, ParseAndCheckBoxSizes(boxes, box_index, &num_boxes), done);\n\n    OP_REQUIRES_ASYNC(\n        context, grads.dim_size(0) == num_boxes,\n        errors::InvalidArgument(\"boxes and grads have incompatible shape\"),\n        done);\n\n    // Allocate output tensor.\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK_ASYNC(\n        context,\n        context->allocate_output(0, TensorShape({num_boxes, 4}), &output),\n        done);\n\n    auto compute_callback = [context, output]() {\n      const Tensor& grads = context->input(0);\n      const Tensor& image = context->input(1);\n      const Tensor& boxes = context->input(2);\n      const Tensor& box_index = context->input(3);\n      const bool status = functor::CropAndResizeBackpropBoxes<Device, T>()(\n          context->eigen_device<Device>(), grads.tensor<float, 4>(),\n          image.tensor<T, 4>(), boxes.tensor<float, 2>(),\n          box_index.tensor<int32, 1>(), output->tensor<float, 2>());\n      if (!status) {\n        context->SetStatus(errors::Internal(\n            \"Failed launch CropAndResizeBackpropBoxes kernel.\"));\n      }\n    };\n\n    RunIfBoxIndexIsValid<Device>(context, box_index.tensor<int32, 1>(),\n                                 batch_size, std::move(compute_callback),\n                                 std::move(done));\n  }\n};\n\n// Partial specialization of CropAndResizeBackpropBoxes functor for a CPUDevice.\nnamespace functor {\ntemplate <typename T>\nstruct CropAndResizeBackpropBoxes<CPUDevice, T> {\n  bool operator()(const CPUDevice& d,\n                  typename TTypes<float, 4>::ConstTensor grads,\n                  typename TTypes<T, 4>::ConstTensor image,\n                  typename TTypes<float, 2>::ConstTensor boxes,\n                  typename TTypes<int32, 1>::ConstTensor box_index,\n                  typename TTypes<float, 2>::Tensor grads_boxes) {\n    const int batch_size = image.dimension(0);\n    const int image_height = image.dimension(1);\n    const int image_width = image.dimension(2);\n\n    const int num_boxes = grads.dimension(0);\n    const int crop_height = grads.dimension(1);\n    const int crop_width = grads.dimension(2);\n    const int depth = grads.dimension(3);\n\n    grads_boxes.setZero();\n\n    for (int b = 0; b < num_boxes; ++b) {\n      const float y1 = boxes(b, 0);\n      const float x1 = boxes(b, 1);\n      const float y2 = boxes(b, 2);\n      const float x2 = boxes(b, 3);\n\n      const int32 b_in = box_index(b);\n      if (!FastBoundsCheck(b_in, batch_size)) {\n        continue;\n      }\n\n      const float height_ratio =\n          (crop_height > 1)\n              ? static_cast<float>(image_height - 1) / (crop_height - 1)\n              : 0;\n      const float width_ratio =\n          (crop_width > 1)\n              ? static_cast<float>(image_width - 1) / (crop_width - 1)\n              : 0;\n\n      const float height_scale =\n          (crop_height > 1) ? (y2 - y1) * height_ratio : 0;\n      const float width_scale = (crop_width > 1) ? (x2 - x1) * width_ratio : 0;\n\n      for (int y = 0; y < crop_height; ++y) {\n        const float in_y = (crop_height > 1)\n                               ? y1 * (image_height - 1) + y * height_scale\n                               : 0.5 * (y1 + y2) * (image_height - 1);\n        if (in_y < 0 || in_y > image_height - 1) {\n          continue;\n        }\n        const int top_y_index = floorf(in_y);\n        const int bottom_y_index = ceilf(in_y);\n        const float y_lerp = in_y - top_y_index;\n\n        for (int x = 0; x < crop_width; ++x) {\n          const float in_x = (crop_width > 1)\n                                 ? x1 * (image_width - 1) + x * width_scale\n                                 : 0.5 * (x1 + x2) * (image_width - 1);\n          if (in_x < 0 || in_x > image_width - 1) {\n            continue;\n          }\n          const int left_x_index = floorf(in_x);\n          const int right_x_index = ceilf(in_x);\n          const float x_lerp = in_x - left_x_index;\n\n          for (int d = 0; d < depth; ++d) {\n            const float top_left(\n                static_cast<float>(image(b_in, top_y_index, left_x_index, d)));\n            const float top_right(\n                static_cast<float>(image(b_in, top_y_index, right_x_index, d)));\n            const float bottom_left(static_cast<float>(\n                image(b_in, bottom_y_index, left_x_index, d)));\n            const float bottom_right(static_cast<float>(\n                image(b_in, bottom_y_index, right_x_index, d)));\n            // Compute the image gradient.\n            float image_grad_y = (1 - x_lerp) * (bottom_left - top_left) +\n                                 x_lerp * (bottom_right - top_right);\n            float image_grad_x = (1 - y_lerp) * (top_right - top_left) +\n                                 y_lerp * (bottom_right - bottom_left);\n            // Modulate the image gradient with the incoming gradient.\n            const float top_grad = grads(b, y, x, d);\n            image_grad_y *= top_grad;\n            image_grad_x *= top_grad;\n            // dy1, dy2\n            if (crop_height > 1) {\n              grads_boxes(b, 0) +=\n                  image_grad_y * (image_height - 1 - y * height_ratio);\n              grads_boxes(b, 2) += image_grad_y * (y * height_ratio);\n            } else {\n              grads_boxes(b, 0) += image_grad_y * 0.5 * (image_height - 1);\n              grads_boxes(b, 2) += image_grad_y * 0.5 * (image_height - 1);\n            }\n            // dx1, dx2\n            if (crop_width > 1) {\n              grads_boxes(b, 1) +=\n                  image_grad_x * (image_width - 1 - x * width_ratio);\n              grads_boxes(b, 3) += image_grad_x * (x * width_ratio);\n            } else {\n              grads_boxes(b, 1) += image_grad_x * 0.5 * (image_width - 1);\n              grads_boxes(b, 3) += image_grad_x * 0.5 * (image_width - 1);\n            }\n          }\n        }\n      }\n    }\n    return true;\n  }\n};\n\n}  // namespace functor\n\n#define REGISTER_KERNEL(T)                                \\\n  REGISTER_KERNEL_BUILDER(Name(\"CropAndResize\")           \\\n                              .Device(DEVICE_CPU)         \\\n                              .TypeConstraint<T>(\"T\")     \\\n                              .HostMemory(\"crop_size\"),   \\\n                          CropAndResizeOp<CPUDevice, T>); \\\n                                                          \\\n  REGISTER_KERNEL_BUILDER(Name(\"CropAndResizeGradBoxes\")  \\\n                              .Device(DEVICE_CPU)         \\\n                              .TypeConstraint<T>(\"T\"),    \\\n                          CropAndResizeGradBoxesOp<CPUDevice, T>);\n\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNEL);\n\n#undef REGISTER_KERNEL\n\n#define REGISTER_KERNEL(T)                               \\\n  REGISTER_KERNEL_BUILDER(Name(\"CropAndResizeGradImage\") \\\n                              .Device(DEVICE_CPU)        \\\n                              .TypeConstraint<T>(\"T\")    \\\n                              .HostMemory(\"image_size\"), \\\n                          CropAndResizeGradImageOp<CPUDevice, T>);\n\nTF_CALL_half(REGISTER_KERNEL);\nTF_CALL_float(REGISTER_KERNEL);\nTF_CALL_double(REGISTER_KERNEL);\n\n#undef REGISTER_KERNEL\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n// Forward declaration of the CheckValidBoxIndexHelper specialization for GPU.\nnamespace functor {\ntemplate <>\nvoid CheckValidBoxIndexHelper<GPUDevice>::operator()(\n    const GPUDevice& d, typename TTypes<int32, 1>::ConstTensor box_index,\n    int batch_size, typename TTypes<bool, 0>::Tensor isvalid);\nextern template struct CheckValidBoxIndexHelper<GPUDevice>;\n}  // namespace functor\n\nnamespace {\n\n// Specialization of CheckValidBoxIndex for a GPUDevice.\ntemplate <>\ninline void RunIfBoxIndexIsValid<GPUDevice>(\n    OpKernelContext* context, typename TTypes<int32, 1>::ConstTensor box_index,\n    int batch_size, const Callback& compute, const Callback& done) {\n  const int num_boxes = box_index.dimension(0);\n  if (num_boxes == 0) {\n    compute();\n    done();\n    return;\n  }\n\n  Tensor isvalid_dev_tensor;\n  OP_REQUIRES_OK_ASYNC(\n      context,\n      context->allocate_temp(DataTypeToEnum<bool>::value, TensorShape({}),\n                             &isvalid_dev_tensor),\n      done);\n  typename TTypes<bool, 0>::Tensor isvalid_dev =\n      isvalid_dev_tensor.tensor<bool, 0>();\n\n  // Run the actual box check on the device.\n  functor::CheckValidBoxIndexHelper<GPUDevice>()(\n      context->eigen_device<GPUDevice>(), box_index, batch_size, isvalid_dev);\n\n  // Copy the result back to the host.\n  auto* stream = context->op_device_context()->stream();\n  OP_REQUIRES_ASYNC(context, stream,\n                    errors::Internal(\"No GPU stream available.\"), done);\n  Tensor isvalid_host_tensor;\n  // Use pinned host memory on the host to avoid unnecessary\n  // synchronization.\n  AllocatorAttributes alloc_attr;\n  alloc_attr.set_on_host(true);\n  alloc_attr.set_gpu_compatible(true);\n  OP_REQUIRES_OK_ASYNC(\n      context,\n      context->allocate_temp(DataTypeToEnum<bool>::value, TensorShape({}),\n                             &isvalid_host_tensor, alloc_attr),\n      done);\n  se::DeviceMemoryBase wrapped(isvalid_dev.data(), sizeof(bool));\n  const bool status =\n      stream\n          ->ThenMemcpy(\n              isvalid_host_tensor.scalar<bool>().data() /* destination */,\n              wrapped /* source */, sizeof(bool))\n          .ok();\n  OP_REQUIRES_ASYNC(\n      context, status,\n      errors::Internal(\"Failed to launch copy of isvalid from device to host.\"),\n      done);\n\n  // We capture both temporary tensors to prevent them from being deallocated\n  // when ComputeAsync returns and before the closure runs.\n  TensorReference isvalid_dev_ref(isvalid_dev_tensor);\n  auto wrapped_callback = [context, isvalid_host_tensor, isvalid_dev_ref,\n                           compute, done]() {\n    auto stream = context->op_device_context()->stream();\n    ScopedActivateExecutorContext scoped_activation{stream->parent()};\n    const bool isvalid = isvalid_host_tensor.scalar<bool>()();\n    isvalid_dev_ref.Unref();\n    OP_REQUIRES_ASYNC(\n        context, isvalid,\n        errors::OutOfRange(\"box_index has values outside [0, batch_size)\"),\n        done);\n    compute();\n    done();\n  };\n\n  context->device()->tensorflow_gpu_device_info()->event_mgr->ThenExecute(\n      stream, wrapped_callback);\n}\n\n}  // namespace\n\n#define REGISTER_KERNEL(T)                                         \\\n  REGISTER_KERNEL_BUILDER(Name(\"CropAndResize\")                    \\\n                              .Device(DEVICE_GPU)                  \\\n                              .TypeConstraint<T>(\"T\")              \\\n                              .HostMemory(\"crop_size\"),            \\\n                          CropAndResizeOp<GPUDevice, T>);          \\\n                                                                   \\\n  REGISTER_KERNEL_BUILDER(Name(\"CropAndResizeGradImage\")           \\\n                              .Device(DEVICE_GPU)                  \\\n                              .TypeConstraint<T>(\"T\")              \\\n                              .HostMemory(\"image_size\"),           \\\n                          CropAndResizeGradImageOp<GPUDevice, T>); \\\n                                                                   \\\n  REGISTER_KERNEL_BUILDER(Name(\"CropAndResizeGradBoxes\")           \\\n                              .Device(DEVICE_GPU)                  \\\n                              .TypeConstraint<T>(\"T\"),             \\\n                          CropAndResizeGradBoxesOp<GPUDevice, T>);\n\nTF_CALL_GPU_NUMBER_TYPES(REGISTER_KERNEL);\n\n#undef REGISTER_KERNEL\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n}  // namespace tensorflow"