"/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#define USE_EIGEN_TENSOR\n#define EIGEN_USE_THREADS\n\n#include \"tensorflow/core/framework/kernel_shape_util.h\"\n#include \"tensorflow/core/framework/numeric_op.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/framework/tensor_slice.h\"\n#include \"tensorflow/core/kernels/conv_2d.h\"\n#include \"tensorflow/core/kernels/conv_3d.h\"\n#include \"tensorflow/core/kernels/conv_ops_gpu.h\"\n#include \"tensorflow/core/kernels/ops_util.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/util/padding.h\"\n#include \"tensorflow/core/util/tensor_format.h\"\n#include \"tensorflow/core/util/use_cudnn.h\"\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#include \"tensorflow/core/platform/stream_executor.h\"\n#include \"tensorflow/core/protobuf/autotuning.pb.h\"\n#include \"tensorflow/core/util/proto/proto_utils.h\"\nusing stream_executor::dnn::DimIndex;\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#if GOOGLE_CUDA\n#include \"third_party/gpus/cudnn/cudnn.h\"\n#include \"tensorflow/stream_executor/gpu/asm_compiler.h\"\n#include \"tensorflow/stream_executor/gpu/redzone_allocator.h\"\n#include \"tensorflow/stream_executor/tf_allocator_adapter.h\"\n#endif  // GOOGLE_CUDA\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\ntypedef Eigen::GpuDevice GPUDevice;\n\ntemplate <typename Device, typename T>\nstruct LaunchConvOp;\n\ntemplate <typename T>\nstruct LaunchConvOp<CPUDevice, T> {\n  static void launch(OpKernelContext* context, bool cudnn_use_autotune,\n                     const Tensor& input, const Tensor& filter,\n                     const std::array<int64, 3>& dilations,\n                     const std::array<int64, 3>& strides, const Padding padding,\n                     TensorFormat data_format, Tensor* output) {\n    OP_REQUIRES(context, data_format == FORMAT_NHWC,\n                errors::InvalidArgument(\"CPU implementation of Conv3D \"\n                                        \"currently only supports the NHWC \"\n                                        \"tensor format.\"));\n    OP_REQUIRES(context,\n                dilations[0] == 1 && dilations[1] == 1 && dilations[2] == 1,\n                errors::InvalidArgument(\"CPU implementation of Conv3D \"\n                                        \"currently only supports dilated rates \"\n                                        \"of 1.\"));\n    functor::CuboidConvolution<CPUDevice, T>()(\n        context->eigen_device<CPUDevice>(), output->tensor<T, 5>(),\n        input.tensor<T, 5>(), filter.tensor<T, 5>(), strides[2], strides[1],\n        strides[0], BrainPadding2EigenPadding(padding));\n  }\n};\n\ntemplate <typename Device, typename T>\nclass Conv3DOp : public BinaryOp<T> {\n public:\n  explicit Conv3DOp(OpKernelConstruction* context) : BinaryOp<T>(context) {\n    string data_format;\n    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &data_format));\n    OP_REQUIRES(context, FormatFromString(data_format, &data_format_),\n                errors::InvalidArgument(\"Invalid data format\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &stride_));\n    OP_REQUIRES(context, stride_.size() == 5,\n                errors::InvalidArgument(\"Sliding window strides field must \"\n                                        \"specify 5 dimensions\"));\n    OP_REQUIRES(\n        context,\n        (GetTensorDim(stride_, data_format_, 'N') == 1 &&\n         GetTensorDim(stride_, data_format_, 'C') == 1),\n        errors::InvalidArgument(\"Current implementation does not yet support \"\n                                \"strides in the batch and depth dimensions.\"));\n    OP_REQUIRES(\n        context,\n        (GetTensorDim(stride_, data_format_, '0') > 0 &&\n         GetTensorDim(stride_, data_format_, '1') > 0 &&\n         GetTensorDim(stride_, data_format_, '2') > 0),\n        errors::InvalidArgument(\"Spatial strides should be larger than 0.\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"dilations\", &dilation_));\n    OP_REQUIRES(context, dilation_.size() == 5,\n                errors::InvalidArgument(\"Dilation rates field must \"\n                                        \"specify 5 dimensions\"));\n    OP_REQUIRES(context,\n                (GetTensorDim(dilation_, data_format_, 'N') == 1 &&\n                 GetTensorDim(dilation_, data_format_, 'C') == 1),\n                errors::InvalidArgument(\n                    \"Current implementation does not yet support \"\n                    \"dilation rates in the batch and depth dimensions.\"));\n    OP_REQUIRES(\n        context,\n        (GetTensorDim(dilation_, data_format_, '0') > 0 &&\n         GetTensorDim(dilation_, data_format_, '1') > 0 &&\n         GetTensorDim(dilation_, data_format_, '2') > 0),\n        errors::InvalidArgument(\"Dilated rates should be larger than 0.\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));\n    cudnn_use_autotune_ = CudnnUseAutotune();\n  }\n\n  void Compute(OpKernelContext* context) override {\n    // Input tensor is of the following dimensions:\n    // [ batch, in_z, in_y, in_x, in_channels ]\n    const Tensor& input = context->input(0);\n\n    // Input filter is of the following dimensions:\n    // [ filter_z, filter_y, filter_x, in_channels, out_channels]\n    const Tensor& filter = context->input(1);\n\n    // NOTE: The ordering of the spatial dimensions is arbitrary, but has to be\n    // kept consistent between input/filter/output.\n    OP_REQUIRES(context, input.dims() == 5,\n                errors::InvalidArgument(\"input must be 5-dimensional\"));\n    OP_REQUIRES(context, filter.dims() == 5,\n                errors::InvalidArgument(\"filter must be 5-dimensional\"));\n\n    const int64 in_depth = GetTensorDim(input, data_format_, 'C');\n    const int64 in_batch = GetTensorDim(input, data_format_, 'N');\n\n    const int64 filter_depth = filter.dim_size(3);\n    const int64 out_depth = filter.dim_size(4);\n\n    OP_REQUIRES(context, in_depth % filter_depth == 0,\n                errors::InvalidArgument(\n                    \"Input depth must be evenly divisible by filter depth: \",\n                    in_depth, \" vs \", filter_depth));\n\n    // Dimension order for these arrays is: z, y, x.\n    std::array<int64, 3> input_size = {\n        {GetTensorDim(input, data_format_, '0'),\n         GetTensorDim(input, data_format_, '1'),\n         GetTensorDim(input, data_format_, '2')}};\n    std::array<int64, 3> filter_size = {\n        {filter.dim_size(0), filter.dim_size(1), filter.dim_size(2)}};\n    std::array<int64, 3> dilations = {\n        {GetTensorDim(dilation_, data_format_, '0'),\n         GetTensorDim(dilation_, data_format_, '1'),\n         GetTensorDim(dilation_, data_format_, '2')}};\n    std::array<int64, 3> strides = {{GetTensorDim(stride_, data_format_, '0'),\n                                     GetTensorDim(stride_, data_format_, '1'),\n                                     GetTensorDim(stride_, data_format_, '2')}};\n    std::array<int64, 3> out, padding;\n\n    OP_REQUIRES_OK(\n        context, Get3dOutputSizeV2(input_size, filter_size, dilations, strides,\n                                   padding_, &out, &padding));\n    TensorShape out_shape = ShapeFromFormat(\n        data_format_, in_batch, {{out[0], out[1], out[2]}}, out_depth);\n    Tensor* output;\n    OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &output));\n\n    // Return early if nothing to do.\n    if (out_shape.num_elements() == 0) return;\n\n    LaunchConvOp<Device, T>::launch(context, cudnn_use_autotune_, input, filter,\n                                    dilations, strides, padding_, data_format_,\n                                    output);\n  }\n\n private:\n  std::vector<int32> dilation_;\n  std::vector<int32> stride_;\n  Padding padding_;\n  TensorFormat data_format_;\n  bool cudnn_use_autotune_;\n};\n\n#define REGISTER_CPU_KERNEL(T)                                  \\\n  REGISTER_KERNEL_BUILDER(                                      \\\n      Name(\"Conv3D\").Device(DEVICE_CPU).TypeConstraint<T>(\"T\"), \\\n      Conv3DOp<CPUDevice, T>);\nTF_CALL_half(REGISTER_CPU_KERNEL);\nTF_CALL_float(REGISTER_CPU_KERNEL);\nTF_CALL_double(REGISTER_CPU_KERNEL);\n#undef REGISTER_CPU_KERNEL\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n// A dummy type to group forward convolution autotune results together.\nstruct Conv3dAutoTuneGroup {\n  static string name() { return \"Conv3d\"; }\n};\n\ntypedef AutoTuneSingleton<Conv3dAutoTuneGroup, ConvParameters,\n                          se::dnn::AlgorithmConfig>\n    AutoTuneConv3d;\n\n// TODO(mjanusz): Share logic with 2d implementation as much as possible.\ntemplate <typename T>\nstruct LaunchConvOp<GPUDevice, T> {\n  static void launch(OpKernelContext* ctx, bool cudnn_use_autotune,\n                     const Tensor& input_param, const Tensor& filter,\n                     const std::array<int64, 3>& dilations,\n                     const std::array<int64, 3>& strides, const Padding padding,\n                     TensorFormat data_format, Tensor* output) {\n    auto* stream = ctx->op_device_context()->stream();\n    OP_REQUIRES(ctx, stream, errors::Internal(\"No GPU stream available.\"));\n\n    Tensor input = input_param;\n\n    const int64 in_batch = GetTensorDim(input, data_format, 'N');\n    int64 in_planes = GetTensorDim(input, data_format, '0');\n    int64 in_rows = GetTensorDim(input, data_format, '1');\n    int64 in_cols = GetTensorDim(input, data_format, '2');\n    const int64 in_depth = GetTensorDim(input, data_format, 'C');\n\n    const int64 filter_planes = filter.dim_size(0);\n    const int64 filter_rows = filter.dim_size(1);\n    const int64 filter_cols = filter.dim_size(2);\n    const int64 filter_depth = filter.dim_size(3);\n    const int64 out_depth = filter.dim_size(4);\n\n    int64 pad_planes = 0, pad_rows = 0, pad_cols = 0;\n    int64 out_planes = GetTensorDim(*output, data_format, '0');\n    int64 out_rows = GetTensorDim(*output, data_format, '1');\n    int64 out_cols = GetTensorDim(*output, data_format, '2');\n\n    if (padding == Padding::SAME) {\n      pad_planes = std::max<int64>(\n          0, (out_planes - 1) * strides[0] + filter_planes - in_planes);\n      pad_rows = std::max<int64>(\n          0, (out_rows - 1) * strides[1] + filter_rows - in_rows);\n      pad_cols = std::max<int64>(\n          0, (out_cols - 1) * strides[2] + filter_cols - in_cols);\n    }\n\n    bool is_grouped_convolution = filter_depth != in_depth;\n\n    // NOTE: This only works in NHWC.\n    if (!is_grouped_convolution && filter_planes == 1 && filter_rows == 1 &&\n        filter_cols == 1 && dilations[0] == 1 && dilations[1] == 1 &&\n        dilations[2] == 1 && strides[0] == 1 && strides[1] == 1 &&\n        strides[2] == 1 && data_format == FORMAT_NHWC) {\n      // 1x1 filter, so call cublas directly.\n      const uint64 m = in_batch * in_planes * in_rows * in_cols;\n      const uint64 k = in_depth;\n      const uint64 n = out_depth;\n\n      auto a_ptr = AsDeviceMemory(input.template flat<T>().data(),\n                                  input.template flat<T>().size());\n      auto b_ptr = AsDeviceMemory(filter.template flat<T>().data(),\n                                  filter.template flat<T>().size());\n      auto c_ptr = AsDeviceMemory(output->template flat<T>().data(),\n                                  output->template flat<T>().size());\n\n      auto no_transpose = se::blas::Transpose::kNoTranspose;\n      bool blas_launch_status =\n          stream\n              ->ThenBlasGemm(no_transpose, no_transpose, n, m, k, 1.0f, b_ptr,\n                             n, a_ptr, k, 0.0f, &c_ptr, n)\n              .ok();\n      if (!blas_launch_status) {\n        ctx->SetStatus(errors::Internal(\"Blas SGEMM launch failed : m=\", m,\n                                        \", n=\", n, \", k=\", k));\n      }\n      return;\n    } else if (!is_grouped_convolution && filter_planes == in_planes &&\n               filter_rows == in_rows && filter_cols == in_cols &&\n               padding == Padding::VALID && data_format == FORMAT_NHWC) {\n      // The input data and filter have the same planes/height/width, so call\n      // cublas directly.\n      const uint64 m = in_batch;\n      const uint64 k = in_planes * in_rows * in_cols * in_depth;\n      const uint64 n = out_depth;\n\n      auto a_ptr = AsDeviceMemory(input.template flat<T>().data(),\n                                  input.template flat<T>().size());\n      auto b_ptr = AsDeviceMemory(filter.template flat<T>().data(),\n                                  filter.template flat<T>().size());\n      auto c_ptr = AsDeviceMemory(output->template flat<T>().data(),\n                                  output->template flat<T>().size());\n\n      auto no_transpose = se::blas::Transpose::kNoTranspose;\n      bool blas_launch_status =\n          stream\n              ->ThenBlasGemm(no_transpose, no_transpose, n, m, k, 1.0f, b_ptr,\n                             n, a_ptr, k, 0.0f, &c_ptr, n)\n              .ok();\n      if (!blas_launch_status) {\n        ctx->SetStatus(errors::Internal(\"Blas SGEMM launch failed : m=\", m,\n                                        \", n=\", n, \", k=\", k));\n      }\n      return;\n    }\n\n    if (padding == Padding::SAME) {\n      const bool rows_odd = (pad_rows % 2 != 0);\n      const bool cols_odd = (pad_cols % 2 != 0);\n      const bool planes_odd = (pad_planes % 2 != 0);\n\n      // Necessary because cuDNN only supports symmetric padding.\n      // TODO(mjanusz): Consider making this optional? This would save some\n      // overhead and would work as long as an op trained this way is only\n      // used on GPU.\n      if (rows_odd || cols_odd || planes_odd) {\n        const int64 new_in_rows = in_rows + rows_odd;\n        const int64 new_in_cols = in_cols + cols_odd;\n        const int64 new_in_planes = in_planes + planes_odd;\n\n        Tensor transformed_input;\n        TensorShape transformed_shape = ShapeFromFormat(\n            data_format, in_batch, {{new_in_planes, new_in_rows, new_in_cols}},\n            in_depth);\n        OP_REQUIRES_OK(\n            ctx, ctx->allocate_temp(DataTypeToEnum<T>::value, transformed_shape,\n                                    &transformed_input));\n\n        functor::PadInput<GPUDevice, T, int, 5>()(\n            ctx->eigen_device<GPUDevice>(), To32Bit(input_param.tensor<T, 5>()),\n            {{0, 0, 0}}, {{planes_odd, rows_odd, cols_odd}},\n            To32Bit(transformed_input.tensor<T, 5>()), data_format, T{});\n        input = transformed_input;\n        in_rows = new_in_rows;\n        in_cols = new_in_cols;\n        in_planes = new_in_planes;\n      }\n    }\n\n#if GOOGLE_CUDA\n    const bool compute_in_nhwc =\n        CUDNN_VERSION >= 8000 && DataTypeToEnum<T>::value == DT_HALF;\n#else\n    // fast NHWC implementation is a CUDA only feature\n    const bool compute_in_nhwc = false;\n#endif\n    const TensorFormat compute_data_format =\n        (compute_in_nhwc && data_format == FORMAT_NHWC) ? FORMAT_NHWC\n                                                        : FORMAT_NCHW;\n\n    VLOG(3) << \"Compute Conv3D with cuDNN:\"\n            << \" data_format=\" << ToString(data_format)\n            << \" compute_data_format=\" << ToString(compute_data_format);\n\n    if (data_format == FORMAT_NHWC && compute_data_format == FORMAT_NCHW) {\n      VLOG(4) << \"Convert the input tensor from NDHWC to NCDHW.\";\n      const TensorShape nchw_shape = ShapeFromFormat(\n          FORMAT_NCHW, in_batch, {{in_planes, in_rows, in_cols}}, in_depth);\n      if (in_depth > 1) {\n        Tensor transformed_input;\n        OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::value,\n                                               nchw_shape, &transformed_input));\n        // input: [b, x, y, z, d]\n        // t_input: [b, d, x, y, z]\n        // NCDHW is the only format universally supported by cuDNN.\n        functor::NHWCToNCHW<GPUDevice, T, 5>()(\n            ctx->eigen_device<GPUDevice>(),\n            const_cast<const Tensor&>(input).tensor<T, 5>(),\n            transformed_input.tensor<T, 5>());\n        input = transformed_input;\n      } else {\n        CHECK(input.CopyFrom(input, nchw_shape));\n      }\n    } else {\n      CHECK(data_format == compute_data_format)  // Crash OK\n          << \"Illegal data and compute format pair:\"\n          << \" data_format=\" << ToString(data_format)\n          << \" compute_data_format=\" << ToString(compute_data_format);\n    }\n\n    constexpr auto kComputeInNHWC =\n        std::make_tuple(se::dnn::DataLayout::kBatchYXDepth,\n                        se::dnn::FilterLayout::kOutputYXInput);\n    constexpr auto kComputeInNCHW =\n        std::make_tuple(se::dnn::DataLayout::kBatchDepthYX,\n                        se::dnn::FilterLayout::kOutputInputYX);\n\n    se::dnn::DataLayout compute_data_layout;\n    se::dnn::FilterLayout filter_layout;\n\n    std::tie(compute_data_layout, filter_layout) =\n        compute_data_format == FORMAT_NHWC ? kComputeInNHWC : kComputeInNCHW;\n\n    CHECK(pad_rows >= 0 && pad_cols >= 0 && pad_planes >= 0)\n        << \"Negative paddings: (\" << pad_rows << \", \" << pad_cols << \", \"\n        << pad_planes << \")\";\n    se::dnn::BatchDescriptor input_desc(3);\n    input_desc.set_count(in_batch)\n        .set_feature_map_count(in_depth)\n        .set_spatial_dim(DimIndex::X, in_cols)\n        .set_spatial_dim(DimIndex::Y, in_rows)\n        .set_spatial_dim(DimIndex::Z, in_planes)\n        .set_layout(compute_data_layout);\n    se::dnn::BatchDescriptor output_desc(3);\n    output_desc.set_count(in_batch)\n        .set_spatial_dim(DimIndex::X, out_cols)\n        .set_spatial_dim(DimIndex::Y, out_rows)\n        .set_spatial_dim(DimIndex::Z, out_planes)\n        .set_feature_map_count(out_depth)\n        .set_layout(compute_data_layout);\n    se::dnn::FilterDescriptor filter_desc(3);\n    filter_desc.set_spatial_dim(DimIndex::X, filter_cols)\n        .set_spatial_dim(DimIndex::Y, filter_rows)\n        .set_spatial_dim(DimIndex::Z, filter_planes)\n        .set_input_feature_map_count(filter_depth)\n        .set_output_feature_map_count(out_depth)\n        .set_layout(filter_layout);\n    se::dnn::ConvolutionDescriptor conv_desc(3);\n    conv_desc.set_dilation_rate(DimIndex::X, dilations[2])\n        .set_dilation_rate(DimIndex::Y, dilations[1])\n        .set_dilation_rate(DimIndex::Z, dilations[0])\n        .set_filter_stride(DimIndex::X, strides[2])\n        .set_filter_stride(DimIndex::Y, strides[1])\n        .set_filter_stride(DimIndex::Z, strides[0])\n        .set_zero_padding(DimIndex::X, pad_cols / 2)\n        .set_zero_padding(DimIndex::Y, pad_rows / 2)\n        .set_zero_padding(DimIndex::Z, pad_planes / 2)\n        .set_group_count(in_depth / filter_depth);\n\n    Tensor transformed_filter;\n    auto dst_format =\n        compute_data_format == FORMAT_NCHW ? FORMAT_OIHW : FORMAT_OHWI;\n    VLOG(4) << \"Transform filter tensor from \" << ToString(FORMAT_HWIO)\n            << \" to \" << ToString(dst_format);\n    TensorShape dst_shape =\n        dst_format == FORMAT_OIHW\n            ? TensorShape({filter.dim_size(4), filter.dim_size(3),\n                           filter.dim_size(0), filter.dim_size(1),\n                           filter.dim_size(2)})\n            : TensorShape({filter.dim_size(4), filter.dim_size(0),\n                           filter.dim_size(1), filter.dim_size(2),\n                           filter.dim_size(3)});\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::value, dst_shape,\n                                           &transformed_filter));\n    // filter: [x, y, z, in, out]\n    // t_filter: [out, in, x, y, z] (NCDHW) or\n    // t_filter: [out, x, y, z, in] (NDHWC)\n    functor::TransformFilter<GPUDevice, T, int, 5>()(\n        ctx->eigen_device<GPUDevice>(), dst_format,\n        To32Bit(filter.tensor<T, 5>()),\n        To32Bit(transformed_filter.tensor<T, 5>()));\n\n    Tensor transformed_output;\n    if (data_format != compute_data_format) {\n      VLOG(4) << \"Allocate temporary memory for output in compute data format\";\n      OP_REQUIRES_OK(\n          ctx,\n          ctx->allocate_temp(\n              DataTypeToEnum<T>::value,\n              ShapeFromFormat(FORMAT_NCHW, in_batch,\n                              {{out_planes, out_rows, out_cols}}, out_depth),\n              &transformed_output));\n    } else {\n      transformed_output = *output;\n    }\n\n    auto input_ptr = AsDeviceMemory(input.template flat<T>().data(),\n                                    input.template flat<T>().size());\n    auto filter_ptr =\n        AsDeviceMemory(transformed_filter.template flat<T>().data(),\n                       transformed_filter.template flat<T>().size());\n    auto output_ptr =\n        AsDeviceMemory(transformed_output.template flat<T>().data(),\n                       transformed_output.template flat<T>().size());\n\n    static int64 ConvolveScratchSize = GetDnnWorkspaceLimit(\n        \"TF_CUDNN_WORKSPACE_LIMIT_IN_MB\", 1LL << 32);  // 4GB by default\n\n    int device_id = stream->parent()->device_ordinal();\n    DataType dtype = input.dtype();\n    ConvParameters conv_parameters = {\n        in_batch,\n        in_depth,\n        {{in_planes, in_rows, in_cols}},\n        compute_data_format,\n        out_depth,\n        {{filter_planes, filter_rows, filter_cols}},\n        {{dilations[0], dilations[1], dilations[2]}},\n        {{strides[0], strides[1], strides[2]}},\n        {{pad_planes, pad_rows, pad_cols}},\n        dtype,\n        device_id,\n        conv_desc.group_count()};\n\n    using se::dnn::AlgorithmConfig;\n    using se::dnn::AlgorithmDesc;\n    using se::dnn::ProfileResult;\n\n#if TENSORFLOW_USE_ROCM\n    // cudnn_use_autotune is applicable only the CUDA flow\n    // for ROCm/MIOpen, we need to call GetMIOpenConvolveAlgorithms explicitly\n    // if we do not have a cached algorithm_config for this conv_parameters\n    cudnn_use_autotune = true;\n#endif\n    AlgorithmConfig algorithm_config;\n\n    if (cudnn_use_autotune && !AutoTuneConv3d::GetInstance()->Find(\n                                  conv_parameters, &algorithm_config)) {\n      std::vector<std::unique_ptr<se::dnn::ConvolveExecutionPlan>> plans;\n#if GOOGLE_CUDA\n      std::vector<AlgorithmDesc> algorithms;\n      std::vector<AlgorithmConfig> configs;\n      if (CudnnUseFrontend()) {\n        OP_REQUIRES(ctx,\n                    stream->parent()->GetConvolveExecutionPlans(\n                        se::dnn::ConvolutionKind::FORWARD,\n                        se::dnn::ToDataType<T>::value, stream, input_desc,\n                        filter_desc, output_desc, conv_desc, &plans),\n                    errors::Unknown(\n                        \"Failed to get convolution execution plan. This is \"\n                        \"probably because cuDNN failed to initialize, so try \"\n                        \"looking to see if a warning log message was printed \"\n                        \"above.\"));\n        for (const auto& plan : plans) {\n          configs.push_back(AlgorithmConfig(\n              AlgorithmDesc{plan->getTag(), plan->get_raw_desc()},\n              plan->getWorkspaceSize()));\n        }\n      } else {\n        OP_REQUIRES(ctx,\n                    stream->parent()->GetConvolveAlgorithms(\n                        conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(\n                            stream->parent()),\n                        &algorithms),\n                    errors::Unknown(\n                        \"Failed to get convolution algorithm. This is probably \"\n                        \"because cuDNN failed to initialize, so try looking to \"\n                        \"see if a warning log message was printed above.\"));\n        for (const auto& algorithm : algorithms) {\n          configs.push_back(AlgorithmConfig(algorithm));\n        }\n      }\n      se::TfAllocatorAdapter tf_allocator_adapter(\n          ctx->device()->GetAllocator({}), stream);\n      se::RedzoneAllocator rz_allocator(stream, &tf_allocator_adapter,\n                                        se::GpuAsmOpts());\n      se::DeviceMemory<T> output_ptr_rz(\n          WrapRedzoneBestEffort(&rz_allocator, output_ptr));\n\n      std::vector<tensorflow::AutotuneResult> results;\n      for (auto& profile_config : configs) {\n        // TODO(zhengxq): profile each algorithm multiple times to better\n        // accuracy.\n        DnnScratchAllocator scratch_allocator(ConvolveScratchSize, ctx);\n        se::RedzoneAllocator rz_scratch_allocator(\n            stream, &tf_allocator_adapter, se::GpuAsmOpts(),\n            /*memory_limit=*/ConvolveScratchSize);\n        se::ScratchAllocator* allocator_used =\n            !RedzoneCheckDisabled()\n                ? static_cast<se::ScratchAllocator*>(&rz_scratch_allocator)\n                : static_cast<se::ScratchAllocator*>(&scratch_allocator);\n        ProfileResult profile_result;\n\n        Status cudnn_launch_status;\n        if (CudnnUseFrontend()) {\n          cudnn_launch_status = stream->ConvolveWithExecutionPlan(\n              input_desc, input_ptr, filter_desc, filter_ptr, conv_desc,\n              output_desc, &output_ptr_rz, allocator_used, profile_config,\n              &profile_result);\n        } else {\n          cudnn_launch_status = stream->ConvolveWithAlgorithm(\n              input_desc, input_ptr, filter_desc, filter_ptr, conv_desc,\n              output_desc, &output_ptr_rz, allocator_used, profile_config,\n              &profile_result);\n        }\n\n        if (cudnn_launch_status.ok() && profile_result.is_valid()) {\n          results.emplace_back();\n          auto& result = results.back();\n          if (CudnnUseFrontend()) {\n            result.mutable_cuda_conv_plan()->set_exec_plan_id(\n                profile_config.algorithm()->exec_plan_id());\n          } else {\n            result.mutable_conv()->set_algorithm(\n                profile_config.algorithm()->algo_id());\n            result.mutable_conv()->set_tensor_ops_enabled(\n                profile_config.algorithm()->tensor_ops_enabled());\n          }\n\n          result.set_scratch_bytes(\n              !RedzoneCheckDisabled()\n                  ? rz_scratch_allocator.TotalAllocatedBytesExcludingRedzones()\n                  : scratch_allocator.TotalByteSize());\n          *result.mutable_run_time() = proto_utils::ToDurationProto(\n              absl::Milliseconds(profile_result.elapsed_time_in_ms()));\n          CheckRedzones(rz_scratch_allocator, &result);\n          CheckRedzones(rz_allocator, &result);\n        } else if (CudnnUseFrontend()) {\n          // When CuDNN frontend APIs are used, we need to make sure the\n          // profiling results are one-to-one mapping of the \"plans\". So, we\n          // insert dummy results when the excution fails.\n          results.emplace_back();\n          auto& result = results.back();\n          result.mutable_failure()->set_kind(AutotuneResult::UNKNOWN);\n          result.mutable_failure()->set_msg(\n              absl::StrCat(\"Profiling failure on CUDNN engine: \",\n                           profile_config.algorithm()->exec_plan_id()));\n        }\n      }\n#elif TENSORFLOW_USE_ROCM\n      DnnScratchAllocator scratch_allocator(ConvolveScratchSize, ctx);\n\n      std::vector<ProfileResult> algorithms;\n      OP_REQUIRES(ctx,\n                  stream->parent()->GetMIOpenConvolveAlgorithms(\n                      se::dnn::ConvolutionKind::FORWARD,\n                      se::dnn::ToDataType<T>::value, stream, input_desc,\n                      input_ptr, filter_desc, filter_ptr, output_desc,\n                      output_ptr, conv_desc, &scratch_allocator, &algorithms),\n                  errors::Unknown(\n                      \"Failed to get convolution algorithm. This is probably \"\n                      \"because MIOpen failed to initialize, so try looking to \"\n                      \"see if a warning log message was printed above.\"));\n      std::vector<tensorflow::AutotuneResult> results;\n      if (algorithms.size() == 1) {\n        auto profile_result = algorithms[0];\n        results.emplace_back();\n        auto& result = results.back();\n        result.mutable_conv()->set_algorithm(\n            profile_result.algorithm().algo_id());\n        result.mutable_conv()->set_tensor_ops_enabled(\n            profile_result.algorithm().tensor_ops_enabled());\n\n        result.set_scratch_bytes(profile_result.scratch_size());\n        *result.mutable_run_time() = proto_utils::ToDurationProto(\n            absl::Milliseconds(profile_result.elapsed_time_in_ms()));\n      } else {\n        for (auto miopen_algorithm : algorithms) {\n          auto profile_algorithm = miopen_algorithm.algorithm();\n          ProfileResult profile_result;\n          auto miopen_launch_status = stream->ConvolveWithAlgorithm(\n              input_desc, input_ptr, filter_desc, filter_ptr, conv_desc,\n              output_desc, &output_ptr, &scratch_allocator,\n              AlgorithmConfig(profile_algorithm,\n                              miopen_algorithm.scratch_size()),\n              &profile_result);\n          if (miopen_launch_status.ok()) {\n            if (profile_result.is_valid()) {\n              results.emplace_back();\n              auto& result = results.back();\n              result.mutable_conv()->set_algorithm(profile_algorithm.algo_id());\n              result.mutable_conv()->set_tensor_ops_enabled(\n                  profile_algorithm.tensor_ops_enabled());\n              result.set_scratch_bytes(scratch_allocator.TotalByteSize());\n              *result.mutable_run_time() = proto_utils::ToDurationProto(\n                  absl::Milliseconds(profile_result.elapsed_time_in_ms()));\n            }\n          }\n        }\n      }\n#endif\n\n      LogConvAutotuneResults(se::dnn::ConvolutionKind::FORWARD,\n                             se::dnn::ToDataType<T>::value, input_ptr,\n                             filter_ptr, output_ptr, input_desc, filter_desc,\n                             output_desc, conv_desc, stream->parent(), results);\n      if (CudnnUseFrontend()) {\n        OP_REQUIRES_OK(\n            ctx, BestCudnnConvAlgorithm(results, &plans, &algorithm_config));\n      } else {\n        OP_REQUIRES_OK(\n            ctx, BestCudnnConvAlgorithm(results, nullptr, &algorithm_config));\n      }\n      AutoTuneConv3d::GetInstance()->Insert(conv_parameters, algorithm_config);\n    }\n\n    Status cudnn_launch_status;\n    DnnScratchAllocator scratch_allocator(ConvolveScratchSize, ctx);\n    if (CudnnUseFrontend()) {\n      if (algorithm_config.algorithm().has_value()) {\n        VLOG(4) << \"Conv3D Execution Plan: \"\n                << algorithm_config.algorithm()->exec_plan_id();\n      } else {\n        VLOG(4) << \"Convolution AutoTune has been turned off\";\n      }\n      cudnn_launch_status = stream->ConvolveWithExecutionPlan(\n          input_desc, input_ptr, filter_desc, filter_ptr, conv_desc,\n          output_desc, &output_ptr, &scratch_allocator, algorithm_config,\n          nullptr);\n    } else {\n      cudnn_launch_status = stream->ConvolveWithAlgorithm(\n          input_desc, input_ptr, filter_desc, filter_ptr, conv_desc,\n          output_desc, &output_ptr, &scratch_allocator, algorithm_config,\n          nullptr);\n    }\n\n    if (!cudnn_launch_status.ok()) {\n      ctx->SetStatus(cudnn_launch_status);\n    }\n\n    if (data_format == FORMAT_NHWC && compute_data_format == FORMAT_NCHW) {\n      VLOG(4) << \"Convert the output tensor back from NCDHW to NDHWC.\";\n      // t_output: [b, out, x, y, z]\n      // output: [b, x, y, z, out]\n      functor::NCHWToNHWC<GPUDevice, T, 5>()(\n          ctx->eigen_device<GPUDevice>(),\n          const_cast<const Tensor&>(transformed_output).tensor<T, 5>(),\n          output->tensor<T, 5>());\n    }\n  }\n};\n\n// Forward declarations of the functor specializations for GPU.\n// This ensures that the custom implementation is used instead of the default\n// Eigen one (which is used for CPU).\nnamespace functor {\n#define DECLARE_GPU_SPEC(T)                                           \\\n  template <>                                                         \\\n  void TransformFilter<GPUDevice, T, int, 5>::operator()(             \\\n      const GPUDevice& d, FilterTensorFormat dst_filter_format,       \\\n      typename TTypes<T, 5, int>::ConstTensor in,                     \\\n      typename TTypes<T, 5, int>::Tensor out);                        \\\n  template <>                                                         \\\n  void ReverseTransformFilter<GPUDevice, T, 5>::operator()(           \\\n      const GPUDevice& d, FilterTensorFormat src_filter_format,       \\\n      typename TTypes<T, 5>::ConstTensor in,                          \\\n      typename TTypes<T, 5>::Tensor out);                             \\\n  template <>                                                         \\\n  void PadInput<GPUDevice, T, int, 5>::operator()(                    \\\n      const GPUDevice& d, typename TTypes<T, 5, int>::ConstTensor in, \\\n      const std::array<int, 3>& padding_left,                         \\\n      const std::array<int, 3>& padding_right,                        \\\n      typename TTypes<T, 5, int>::Tensor out, TensorFormat format,    \\\n      const T& padding_value);                                        \\\n  template <>                                                         \\\n  void NHWCToNCHW<GPUDevice, T, 5>::operator()(                       \\\n      const GPUDevice& d, typename TTypes<T, 5>::ConstTensor in,      \\\n      typename TTypes<T, 5>::Tensor out);                             \\\n  template <>                                                         \\\n  void NCHWToNHWC<GPUDevice, T, 5>::operator()(                       \\\n      const GPUDevice& d, typename TTypes<T, 5>::ConstTensor in,      \\\n      typename TTypes<T, 5>::Tensor out);\n\nDECLARE_GPU_SPEC(Eigen::half);\nDECLARE_GPU_SPEC(float);\nDECLARE_GPU_SPEC(double);\n#undef DECLARE_GPU_SPEC\n\n}  // namespace functor\n\n// Registration of the GPU implementations.\nREGISTER_KERNEL_BUILDER(\n    Name(\"Conv3D\").Device(DEVICE_GPU).TypeConstraint<Eigen::half>(\"T\"),\n    Conv3DOp<GPUDevice, Eigen::half>);\nREGISTER_KERNEL_BUILDER(\n    Name(\"Conv3D\").Device(DEVICE_GPU).TypeConstraint<float>(\"T\"),\n    Conv3DOp<GPUDevice, float>);\nREGISTER_KERNEL_BUILDER(\n    Name(\"Conv3D\").Device(DEVICE_GPU).TypeConstraint<double>(\"T\"),\n    Conv3DOp<GPUDevice, double>);\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n}  // namespace tensorflow"