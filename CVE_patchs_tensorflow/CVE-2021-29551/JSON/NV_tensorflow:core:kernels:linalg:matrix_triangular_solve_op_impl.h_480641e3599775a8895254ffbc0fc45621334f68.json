"/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/linalg_ops.cc.\n//\n#ifndef TENSORFLOW_CORE_KERNELS_LINALG_MATRIX_TRIANGULAR_SOLVE_OP_IMPL_H_\n#define TENSORFLOW_CORE_KERNELS_LINALG_MATRIX_TRIANGULAR_SOLVE_OP_IMPL_H_\n\n#include \"third_party/eigen3/Eigen/Core\"\n#include \"tensorflow/core/framework/kernel_def_builder.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/kernels/fill_functor.h\"\n#include \"tensorflow/core/kernels/linalg/linalg_ops_common.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/platform/macros.h\"\n#include \"tensorflow/core/platform/types.h\"\n#include \"tensorflow/core/util/matmul_bcast.h\"\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/kernels/transpose_functor.h\"\n#include \"tensorflow/core/platform/stream_executor.h\"\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#if GOOGLE_CUDA\n#include \"tensorflow/core/util/cuda_solvers.h\"\n#elif TENSORFLOW_USE_ROCM\n#include \"tensorflow/core/util/rocm_solvers.h\"\n#endif\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\ntypedef Eigen::GpuDevice GPUDevice;\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\ntemplate <typename Scalar>\nse::DeviceMemory<Scalar> AsDeviceMemory(const Scalar* gpu_memory) {\n  se::DeviceMemoryBase wrapped(const_cast<Scalar*>(gpu_memory));\n  se::DeviceMemory<Scalar> typed(wrapped);\n  return typed;\n}\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n// Sequential batch matrix triangular solve kernel that calls Eigen's\n// matrix triangular solve.\ntemplate <typename Scalar>\nstruct SequentialMatrixTriangularSolveKernel {\n  using Matrix =\n      Eigen::Matrix<Scalar, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>;\n  using ConstMatrixMap = Eigen::Map<const Matrix>;\n  using MatrixMap = Eigen::Map<Matrix>;\n  using RealScalar = typename Eigen::NumTraits<Scalar>::Real;\n\n  static ConstMatrixMap ConstTensorSliceToEigenMatrix(const Tensor& t,\n                                                      int slice) {\n    return ConstMatrixMap(\n        t.flat<Scalar>().data() + slice * t.dim_size(1) * t.dim_size(2),\n        t.dim_size(1), t.dim_size(2));\n  }\n\n  static MatrixMap TensorSliceToEigenMatrix(Tensor* t, int slice) {\n    return MatrixMap(\n        t->flat<Scalar>().data() + slice * t->dim_size(1) * t->dim_size(2),\n        t->dim_size(1), t->dim_size(2));\n  }\n\n  static void Run(const Tensor& in_x, const Tensor& in_y, bool lower,\n                  bool adjoint, const MatMulBCast& bcast, Tensor* out,\n                  int start, int limit) {\n    const bool should_bcast = bcast.IsBroadcastingRequired();\n    const auto& x_batch_indices = bcast.x_batch_indices();\n    const auto& y_batch_indices = bcast.y_batch_indices();\n    for (int64 i = start; i < limit; ++i) {\n      const int64 x_batch_index = should_bcast ? x_batch_indices[i] : i;\n      const int64 y_batch_index = should_bcast ? y_batch_indices[i] : i;\n      auto matrix = ConstTensorSliceToEigenMatrix(in_x, x_batch_index);\n      auto rhs = ConstTensorSliceToEigenMatrix(in_y, y_batch_index);\n      auto output = TensorSliceToEigenMatrix(out, i);\n      if (lower) {\n        auto triangle = matrix.template triangularView<Eigen::Lower>();\n        if (adjoint) {\n          output.noalias() = triangle.adjoint().solve(rhs);\n        } else {\n          output.noalias() = triangle.solve(rhs);\n        }\n      } else {\n        auto triangle = matrix.template triangularView<Eigen::Upper>();\n        if (adjoint) {\n          output.noalias() = triangle.adjoint().solve(rhs);\n        } else {\n          output.noalias() = triangle.solve(rhs);\n        }\n      }\n    }\n  }\n};\n\ntemplate <typename Device, typename Scalar>\nstruct LaunchBatchMatrixTriangularSolve;\n\ntemplate <typename Scalar>\nstruct LaunchBatchMatrixTriangularSolve<CPUDevice, Scalar> {\n  static void Launch(OpKernelContext* context, const Tensor& in_x,\n                     const Tensor& in_y, bool adjoint, bool lower,\n                     const MatMulBCast& bcast, Tensor* out) {\n    // Number of matrix triangular solves i.e. size of the batch.\n    const int64 batch_size = bcast.output_batch_size();\n    const int64 cost_per_unit =\n        in_x.dim_size(1) * in_x.dim_size(1) * in_y.dim_size(2) / 2;\n    auto worker_threads = *(context->device()->tensorflow_cpu_worker_threads());\n\n    using Matrix =\n        Eigen::Matrix<Scalar, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>;\n    using ConstMatrixMap = Eigen::Map<const Matrix>;\n    using RealScalar = typename Eigen::NumTraits<Scalar>::Real;\n    // Check diagonal before doing any solves.\n    auto matrix = ConstMatrixMap(in_x.flat<Scalar>().data(), in_x.dim_size(1),\n                                 in_x.dim_size(2));\n    const RealScalar min_abs_pivot = matrix.diagonal().cwiseAbs().minCoeff();\n    OP_REQUIRES(context, min_abs_pivot > RealScalar(0),\n                errors::InvalidArgument(\"Input matrix is not invertible.\"));\n\n    Shard(worker_threads.num_threads, worker_threads.workers, batch_size,\n          cost_per_unit,\n          [&in_x, &in_y, adjoint, lower, &bcast, out](int start, int limit) {\n            SequentialMatrixTriangularSolveKernel<Scalar>::Run(\n                in_x, in_y, lower, adjoint, bcast, out, start, limit);\n          });\n  }\n};\n\ntemplate <typename Device, typename Scalar>\nclass BaseMatrixTriangularSolveOp : public OpKernel {\n public:\n  explicit BaseMatrixTriangularSolveOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"lower\", &lower_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"adjoint\", &adjoint_));\n  }\n\n  ~BaseMatrixTriangularSolveOp() override {}\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& in0 = ctx->input(0);\n    const Tensor& in1 = ctx->input(1);\n\n    ValidateInputTensors(ctx, in0, in1);\n    if (!ctx->status().ok()) {\n      return;\n    }\n\n    MatMulBCast bcast(in0.shape().dim_sizes(), in1.shape().dim_sizes());\n    OP_REQUIRES(\n        ctx, bcast.IsValid(),\n        errors::InvalidArgument(\n            \"In[0] and In[1] must have compatible batch dimensions: \",\n            in0.shape().DebugString(), \" vs. \", in1.shape().DebugString()));\n\n    TensorShape out_shape = bcast.output_batch_shape();\n    auto batch_size = bcast.output_batch_size();\n    auto d0 = in0.dim_size(in0.dims() - 2);\n    auto d1 = in0.dim_size(in0.dims() - 1);\n    Tensor in0_reshaped;\n    OP_REQUIRES(\n        ctx,\n        in0_reshaped.CopyFrom(in0, TensorShape({bcast.x_batch_size(), d0, d1})),\n        errors::Internal(\"Failed to reshape In[0] from \",\n                         in0.shape().DebugString()));\n    auto d2 = in1.dim_size(in1.dims() - 2);\n    auto d3 = in1.dim_size(in1.dims() - 1);\n    Tensor in1_reshaped;\n    OP_REQUIRES(\n        ctx,\n        in1_reshaped.CopyFrom(in1, TensorShape({bcast.y_batch_size(), d2, d3})),\n        errors::Internal(\"Failed to reshape In[1] from \",\n                         in1.shape().DebugString()));\n    if (adjoint_) std::swap(d0, d1);\n    OP_REQUIRES(ctx, d1 == d2,\n                errors::InvalidArgument(\n                    \"In[0] mismatch In[1] shape: \", d1, \" vs. \", d2, \": \",\n                    in0.shape().DebugString(), \" \", in1.shape().DebugString(),\n                    \" \", lower_, \" \", adjoint_));\n    out_shape.AddDim(d0);\n    out_shape.AddDim(d3);\n    Tensor* out = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, out_shape, &out));\n    if (out->NumElements() == 0) {\n      return;\n    }\n    Tensor out_reshaped;\n    OP_REQUIRES(ctx,\n                out_reshaped.CopyFrom(*out, TensorShape({batch_size, d0, d3})),\n                errors::Internal(\"Failed to reshape output from \",\n                                 out->shape().DebugString()));\n    LaunchBatchMatrixTriangularSolve<Device, Scalar>::Launch(\n        ctx, in0_reshaped, in1_reshaped, adjoint_, lower_, bcast,\n        &out_reshaped);\n  }\n\n private:\n  virtual void ValidateInputTensors(OpKernelContext* ctx, const Tensor& in0,\n                                    const Tensor& in1) = 0;\n  bool lower_;\n  bool adjoint_;\n};\n\ntemplate <class Device, class Scalar>\nclass MatrixTriangularSolveOp\n    : public BaseMatrixTriangularSolveOp<Device, Scalar> {\n public:\n  explicit MatrixTriangularSolveOp(OpKernelConstruction* context)\n      : BaseMatrixTriangularSolveOp<Device, Scalar>(context) {}\n\n  ~MatrixTriangularSolveOp() override {}\n\n private:\n  void ValidateInputTensors(OpKernelContext* ctx, const Tensor& in0,\n                            const Tensor& in1) override {\n    const auto in0_num_dims = in0.dims();\n    OP_REQUIRES(\n        ctx, in0_num_dims >= 2,\n        errors::InvalidArgument(\"In[0] ndims must be >= 2: \", in0_num_dims));\n\n    const auto in1_num_dims = in1.dims();\n    OP_REQUIRES(\n        ctx, in1_num_dims >= 2,\n        errors::InvalidArgument(\"In[1] ndims must be >= 2: \", in1_num_dims));\n\n    const auto in0_last_dim = in0.dim_size(in0_num_dims - 1);\n    const auto in0_prev_dim = in0.dim_size(in0_num_dims - 2);\n    OP_REQUIRES(ctx, in0_last_dim == in0_prev_dim,\n                errors::InvalidArgument(\n                    \"In[0] matrices in the last dimensions must be square (\",\n                    in0_last_dim, \" =/= \", in0_prev_dim, \")\"));\n  }\n};\n\n#define REGISTER_BATCH_MATRIX_TRIANGULAR_SOLVE_CPU(TYPE)             \\\n  REGISTER_KERNEL_BUILDER(Name(\"MatrixTriangularSolve\")              \\\n                              .Device(DEVICE_CPU)                    \\\n                              .TypeConstraint<TYPE>(\"T\"),            \\\n                          MatrixTriangularSolveOp<CPUDevice, TYPE>); \\\n  REGISTER_KERNEL_BUILDER(Name(\"BatchMatrixTriangularSolve\")         \\\n                              .Device(DEVICE_CPU)                    \\\n                              .TypeConstraint<TYPE>(\"T\"),            \\\n                          MatrixTriangularSolveOp<CPUDevice, TYPE>);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\ntemplate <typename Scalar>\nstruct LaunchBatchMatrixTriangularSolve<GPUDevice, Scalar> {\n  static void Launch(OpKernelContext* context, const Tensor& in_x,\n                     const Tensor& in_y, bool adjoint, bool lower,\n                     const MatMulBCast& bcast, Tensor* out) {\n    auto* stream = context->op_device_context()->stream();\n\n    const uint64 m = in_x.dim_size(1);\n    const uint64 n = out->dim_size(2);\n\n    //  Do a memcpy when we don't need to broadcast.\n    if (!bcast.IsBroadcastingRequired() || out->shape() == in_y.shape()) {\n      auto src_device_mem = AsDeviceMemory(in_y.template flat<Scalar>().data());\n      auto dst_device_mem = AsDeviceMemory(out->template flat<Scalar>().data());\n      OP_REQUIRES(\n          context,\n          stream\n              ->ThenMemcpyD2D(&dst_device_mem, src_device_mem,\n                              bcast.y_batch_size() * m * n * sizeof(Scalar))\n              .ok(),\n          errors::Internal(\"MatrixTriangularSolveOp: failed to copy rhs \"\n                           \"from device\"));\n    } else {\n      std::vector<Scalar*> out_ptrs;\n      std::vector<const Scalar*> b_tmp_ptrs;\n      auto* b_base_ptr = in_y.template flat<Scalar>().data();\n      const std::vector<int64>& b_batch_indices = bcast.y_batch_indices();\n      for (int64 i = 0; i < bcast.y_batch_size(); ++i) {\n        b_tmp_ptrs.push_back(b_base_ptr + i * m * n);\n      }\n      for (int64 i = 0; i < bcast.output_batch_size(); ++i) {\n        auto src_device_mem = AsDeviceMemory(b_tmp_ptrs[b_batch_indices[i]]);\n        auto dst_device_mem =\n            AsDeviceMemory(out->template flat<Scalar>().data() + i * m * n);\n        OP_REQUIRES(\n            context,\n            stream\n                ->ThenMemcpyD2D(&dst_device_mem, src_device_mem,\n                                m * n * sizeof(Scalar))\n                .ok(),\n            errors::Internal(\"MatrixTriangularSolveOp: failed to copy rhs \"\n                             \"from device\"));\n      }\n    }\n\n    if (out->NumElements() == 0) {\n      return;\n    }\n\n#if GOOGLE_CUDA\n\n    cublasSideMode_t side = CUBLAS_SIDE_RIGHT;\n    cublasFillMode_t uplo;\n    cublasOperation_t trans;\n    cublasDiagType_t diag = CUBLAS_DIAG_NON_UNIT;\n\n    // Cublas does\n    // output = matrix \\ rhs\n    // where matrix, rhs and output are assumed to be in column major.\n    // We want the output to be in row-major, so we can compute\n    // output' = rhs' / matrix' (' stands for transpose)\n    // Upper/lower needs to be swapped for this.\n\n    uplo = lower ? CUBLAS_FILL_MODE_UPPER : CUBLAS_FILL_MODE_LOWER;\n    trans = adjoint ? CUBLAS_OP_C : CUBLAS_OP_N;\n    auto solver = absl::make_unique<CudaSolver>(context);\n\n#elif TENSORFLOW_USE_ROCM\n    rocblas_side side = rocblas_side_right;\n    rocblas_fill uplo;\n    rocblas_operation trans;\n    rocblas_diagonal diag = rocblas_diagonal_non_unit;\n\n    // rocblas does\n    // output = matrix \\ rhs\n    // where matrix, rhs and output are assumed to be in column major.\n    // We want the output to be in row-major, so we can compute\n    // output' = rhs' / matrix' (' stands for transpose)\n    // Upper/lower needs to be swapped for this.\n\n    uplo = lower ? rocblas_fill_upper : rocblas_fill_lower;\n    trans = adjoint ? rocblas_operation_conjugate_transpose\n                    : rocblas_operation_none;\n    auto solver = absl::make_unique<ROCmSolver>(context);\n\n#endif\n\n    const uint64 leading_dim_matrix = m;\n    const uint64 leading_dim_output = n;\n    const uint64 colmajor_rows = n;\n    const uint64 colmajor_cols = m;\n\n    const int64 batch_size = bcast.output_batch_size();\n    std::vector<const Scalar*> a_ptrs;\n    std::vector<Scalar*> out_ptrs;\n    std::vector<const Scalar*> a_tmp_ptrs;\n    a_ptrs.reserve(batch_size);\n    out_ptrs.reserve(batch_size);\n    a_tmp_ptrs.reserve(bcast.x_batch_size());\n    auto* a_base_ptr = in_x.template flat<Scalar>().data();\n    auto* out_base_ptr = out->template flat<Scalar>().data();\n\n    if (!bcast.IsBroadcastingRequired()) {\n      for (int64 i = 0; i < batch_size; ++i) {\n        a_ptrs.push_back(a_base_ptr + i * m * m);\n        out_ptrs.push_back(out_base_ptr + i * m * n);\n      }\n    } else {\n      const std::vector<int64>& a_batch_indices = bcast.x_batch_indices();\n      for (int64 i = 0; i < bcast.x_batch_size(); ++i) {\n        a_tmp_ptrs.push_back(a_base_ptr + i * m * m);\n      }\n      for (int64 i = 0; i < batch_size; ++i) {\n        a_ptrs.push_back(a_tmp_ptrs[a_batch_indices[i]]);\n        out_ptrs.push_back(out_base_ptr + i * m * n);\n      }\n    }\n\n    typedef Scalar Coefficient;\n    const Scalar alpha = Scalar(1.0);\n\n#if GOOGLE_CUDA\n\n    // TODO(b/146763573): Consider using Trsv here when the right hand side is\n    // a vector. This will require an explicit transpose since Trsv assumes\n    // CUBLAS_SIDE_LEFT.\n    if (batch_size == 1) {\n      OP_REQUIRES_OK(\n          context,\n          solver->Trsm(side, uplo, trans, diag, colmajor_rows, colmajor_cols,\n                       &alpha, a_ptrs[0], leading_dim_matrix /*lda*/,\n                       out_ptrs[0], leading_dim_output /*ldb*/));\n    } else {\n      // Heuristic for choosing between batched interface vs. non-batched\n      // interface. This is inspired by matrix_solve_op and can probably be\n      // tuned.\n      // TODO(b/146763573): Tune this heuristic.\n      const int kMaxMatrixSizeToBatchSizeRatio = 128;\n      const bool use_batched_solver =\n          m <= kMaxMatrixSizeToBatchSizeRatio * batch_size;\n      if (use_batched_solver) {\n        OP_REQUIRES_OK(\n            context, solver->TrsmBatched(\n                         side, uplo, trans, diag, colmajor_rows, colmajor_cols,\n                         &alpha, &a_ptrs[0], leading_dim_matrix /*lda*/,\n                         &out_ptrs[0], leading_dim_output /*ldb*/, batch_size));\n      } else {\n        for (int batch = 0; batch < batch_size; ++batch) {\n          OP_REQUIRES_OK(\n              context, solver->Trsm(side, uplo, trans, diag, colmajor_rows,\n                                    colmajor_cols, &alpha, a_ptrs[batch],\n                                    leading_dim_matrix /*lda*/, out_ptrs[batch],\n                                    leading_dim_output /*ldb*/));\n        }\n      }\n    }\n#elif TENSORFLOW_USE_ROCM\n    for (int batch = 0; batch < batch_size; ++batch) {\n      OP_REQUIRES_OK(\n          context,\n          solver->Trsm(side, uplo, trans, diag, colmajor_rows, colmajor_cols,\n                       &alpha, a_ptrs[batch], leading_dim_matrix /*lda*/,\n                       out_ptrs[batch], leading_dim_output /*ldb*/));\n    }\n#endif\n  }\n};\n\n#define REGISTER_BATCH_MATRIX_TRIANGULAR_SOLVE_GPU(TYPE)             \\\n  REGISTER_KERNEL_BUILDER(Name(\"MatrixTriangularSolve\")              \\\n                              .Device(DEVICE_GPU)                    \\\n                              .TypeConstraint<TYPE>(\"T\"),            \\\n                          MatrixTriangularSolveOp<GPUDevice, TYPE>); \\\n  REGISTER_KERNEL_BUILDER(Name(\"BatchMatrixTriangularSolve\")         \\\n                              .Device(DEVICE_GPU)                    \\\n                              .TypeConstraint<TYPE>(\"T\"),            \\\n                          MatrixTriangularSolveOp<GPUDevice, TYPE>);\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n}  // namespace tensorflow\n\n#endif  // TENSORFLOW_CORE_KERNELS_LINALG_MATRIX_TRIANGULAR_SOLVE_OP_IMPL_H_"