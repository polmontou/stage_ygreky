"diff --git a/tensorflow/lite/core/subgraph.cc b/tensorflow/lite/core/subgraph.cc\nindex 0273018b3bf..123d8cfd034 100644\n--- a/tensorflow/lite/core/subgraph.cc\n+++ b/tensorflow/lite/core/subgraph.cc\n@@ -156,6 +156,42 @@ const char* GetTFLiteOpName(const TfLiteRegistration& op_reg) {\n   return tflite::EnumNamesBuiltinOperator()[op_reg.builtin_code];\n }\n \n+// An utility test to detect if the subgraph is abused:\n+// 1. Detects if recursion exists in the graph (recursion is not currently\n+//    supported.\n+// 2. Detects if the interpreter / subgraph is used in multiple subgraphs.\n+//    Note: It's clearly documented that the interpreter / subgraph are not\n+//    thread-safe. This serves as a check with possible false negatives\n+//    unless we switch to atomic boolean flags.\n+class SubgraphGuard {\n+ public:\n+  SubgraphGuard(TfLiteContext* context, bool* is_subgraph_in_use)\n+      : is_subgraph_in_use_(is_subgraph_in_use) {\n+    if (*is_subgraph_in_use_) {\n+      TF_LITE_KERNEL_LOG(\n+          context,\n+          \"Subgraph is already in use. Using an interpreter or a subgraph in \"\n+          \"multiple threads is not supported. Recursion in the graph is not \"\n+          \"supported.\");\n+      status_ = kTfLiteError;\n+    } else {\n+      *is_subgraph_in_use_ = true;\n+    }\n+  }\n+  ~SubgraphGuard() {\n+    // If tht original status was OK, recover the boolean flag.\n+    if (status_ == kTfLiteOk) {\n+      *is_subgraph_in_use_ = false;\n+    }\n+  }\n+\n+  TfLiteStatus status() const { return status_; }\n+\n+ private:\n+  TfLiteStatus status_ = kTfLiteOk;\n+  bool* is_subgraph_in_use_;\n+};\n+\n }  // namespace\n \n // A trivial implementation of GraphInfo around the Interpreter.\n@@ -655,6 +691,7 @@ TfLiteStatus Subgraph::BytesRequired(TfLiteType type, const int* dims,\n \n TfLiteStatus Subgraph::AllocateTensors() {\n   TFLITE_SCOPED_TAGGED_DEFAULT_PROFILE(profiler_.get(), \"AllocateTensors\");\n+\n   if (!consistent_) {\n     ReportError(\"AllocateTensors() called on inconsistent model.\");\n     return kTfLiteError;\n@@ -678,6 +715,12 @@ TfLiteStatus Subgraph::AllocateTensors() {\n     return kTfLiteOk;\n   }\n \n+  // Note `AllocateTensors` sometimes calls itself recursively above\n+  // for delegates. Therefore only the logic below need to be guarded\n+  // by `SubgraphGuard`.\n+  SubgraphGuard guard(&context_, &is_subgraph_in_use_);\n+  TF_LITE_ENSURE_OK(&context_, guard.status());\n+\n   next_execution_plan_index_to_prepare_ = 0;\n   next_execution_plan_index_to_plan_allocation_ = 0;\n   next_original_execution_plan_index_to_prepare_ = 0;\n@@ -1014,6 +1057,9 @@ TfLiteStatus Subgraph::PrepareOpsAndTensors() {\n }\n \n TfLiteStatus Subgraph::Invoke() {\n+  SubgraphGuard guard(&context_, &is_subgraph_in_use_);\n+  TF_LITE_ENSURE_OK(&context_, guard.status());\n+\n   if (!consistent_) {\n     ReportError(\"Invoke called on model that is not consistent.\");\n     return kTfLiteError;"