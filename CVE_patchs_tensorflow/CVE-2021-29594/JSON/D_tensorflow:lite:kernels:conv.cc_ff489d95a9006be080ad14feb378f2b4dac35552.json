"diff --git a/tensorflow/lite/kernels/conv.cc b/tensorflow/lite/kernels/conv.cc\nindex 0b594d1b153..6bc1adce8a0 100644\n--- a/tensorflow/lite/kernels/conv.cc\n+++ b/tensorflow/lite/kernels/conv.cc\n@@ -545,6 +545,7 @@ TfLiteStatus Prepare(KernelType kernel_type, TfLiteContext* context,\n     // Only one scale factor per batch is typically necessary. See optimized\n     // implementation for why we need to allocate for the height of the inputs\n     // flattened to 2D.\n+    TF_LITE_ENSURE(context, channels_in != 0);\n     const int height = NumElements(input) / channels_in;\n     int scaling_dims[1] = {height};\n     if (!TfLiteIntArrayEqualsArray(scaling_factors->dims, 1, scaling_dims)) {\n@@ -587,6 +588,7 @@ TfLiteStatus Prepare(KernelType kernel_type, TfLiteContext* context,\n       input_offsets->type = kTfLiteInt32;\n       input_offsets->allocation_type = kTfLiteArenaRw;\n       // See above comment for the need to allocate for height of inputs.\n+      TF_LITE_ENSURE(context, channels_in != 0);\n       const int height = NumElements(input) / channels_in;\n       const int input_offset_dims[1] = {height};\n       if (!TfLiteIntArrayEqualsArray(input_offsets->dims, 1,\n@@ -886,8 +888,9 @@ TfLiteStatus EvalHybridPerChannel(TfLiteContext* context, TfLiteNode* node,\n   CalculateActivationRange(params->activation, &output_activation_min,\n                            &output_activation_max);\n \n-  const int input_size = NumElements(input) / SizeOfDimension(input, 0);\n   const int batch_size = SizeOfDimension(input, 0);\n+  TF_LITE_ENSURE(context, batch_size != 0);\n+  const int input_size = NumElements(input) / batch_size;\n   TfLiteTensor* quantized_input_tensor;\n   TF_LITE_ENSURE_OK(context,\n                     GetTemporarySafe(context, node, data->input_quantized_index,\n@@ -989,8 +992,9 @@ TfLiteStatus EvalHybrid(TfLiteContext* context, TfLiteNode* node,\n   CalculateActivationRange(params->activation, &output_activation_min,\n                            &output_activation_max);\n \n-  const int input_size = NumElements(input) / SizeOfDimension(input, 0);\n   const int batch_size = SizeOfDimension(input, 0);\n+  TF_LITE_ENSURE(context, batch_size != 0);\n+  const int input_size = NumElements(input) / batch_size;\n \n   const float* input_ptr = GetTensorData<float>(input);\n   TfLiteTensor* quantized_input_tensor;"