"/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/linalg_ops.cc.\n\n#include \"third_party/eigen3/Eigen/Core\"\n#include \"tensorflow/core/framework/kernel_def_builder.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/kernels/fill_functor.h\"\n#include \"tensorflow/core/kernels/linalg/linalg_ops_common.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/platform/macros.h\"\n#include \"tensorflow/core/platform/types.h\"\n#include \"tensorflow/core/util/matmul_bcast.h\"\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n\ntemplate <typename Scalar>\nScalar eigen_conj(const Scalar& scalar) {\n  return Eigen::numext::conj<Scalar>(scalar);\n}\n\n// Sequential batch matrix triangular solve kernel that calls Eigen's\n// matrix triangular solve.\ntemplate <typename Scalar>\nstruct SequentialBandedTriangularSolveKernel {\n  using Matrix =\n      Eigen::Matrix<Scalar, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>;\n  using ConstMatrixMap = Eigen::Map<const Matrix>;\n  using MatrixMap = Eigen::Map<Matrix>;\n  using RealScalar = typename Eigen::NumTraits<Scalar>::Real;\n\n  static ConstMatrixMap ConstTensorSliceToEigenMatrix(const Tensor& t,\n                                                      int slice) {\n    return ConstMatrixMap(\n        t.flat<Scalar>().data() + slice * t.dim_size(1) * t.dim_size(2),\n        t.dim_size(1), t.dim_size(2));\n  }\n\n  static MatrixMap TensorSliceToEigenMatrix(Tensor* t, int slice) {\n    return MatrixMap(\n        t->flat<Scalar>().data() + slice * t->dim_size(1) * t->dim_size(2),\n        t->dim_size(1), t->dim_size(2));\n  }\n\n  static void Run(const Tensor& in_x, const Tensor& in_y, bool lower,\n                  bool adjoint, const MatMulBCast& bcast, Tensor* out,\n                  int start, int limit) {\n    const bool should_bcast = bcast.IsBroadcastingRequired();\n    const auto& x_batch_indices = bcast.x_batch_indices();\n    const auto& y_batch_indices = bcast.y_batch_indices();\n    int num_bands = in_x.dim_size(1);\n    int matrix_size = in_x.dim_size(2);\n\n    for (int64 i = start; i < limit; ++i) {\n      const int64 x_batch_index = should_bcast ? x_batch_indices[i] : i;\n      const int64 y_batch_index = should_bcast ? y_batch_indices[i] : i;\n      auto matrix = ConstTensorSliceToEigenMatrix(in_x, x_batch_index);\n      auto rhs = ConstTensorSliceToEigenMatrix(in_y, y_batch_index);\n      auto output = TensorSliceToEigenMatrix(out, i);\n      // Below, we use the standard algorithm for computing a triangular solve,\n      // except we band limit it.\n      // Given A x = b, where A is lower triangular,\n      // x_i = (b_i - sum a_ij * x_j) / a_ii, where the sum is from\n      // j = 0 to i - 1.\n      //\n      // Now, in a banded triangular matrix, when i exceeds the band size,\n      // then the sum goes from j = i - band_size to i - 1, since the other\n      // elements are zero.\n      //\n      // Finally, given the band storage format, we'll need to change the\n      // indexing.\n      if (lower) {\n        if (!adjoint) {\n          output.row(0) = rhs.row(0) / matrix(0, 0);\n          for (int i = 1; i < matrix_size; ++i) {\n            if (i < num_bands) {\n              output.row(i).noalias() =\n                  (rhs.row(i) - matrix.block(1, i, i, 1).reverse().transpose() *\n                                    output.topRows(i)) /\n                  matrix(0, i);\n            } else {\n              output.row(i).noalias() =\n                  (rhs.row(i) -\n                   matrix.block(1, i, num_bands - 1, 1).reverse().transpose() *\n                       output.middleRows(i - (num_bands - 1), num_bands - 1)) /\n                  matrix(0, i);\n            }\n          }\n        } else {\n          // In the adjoint case, here and below, we now have an upper (lower)\n          // triangular matrix, and thus need to work through with the other\n          // case. We can't simply conjugate `matrix` and use the upper (lower)\n          // algorithm because the band storage format for upper and lower\n          // triangular matrices are different (in the lower case, we pad\n          // entries on the left, and in the upper case we pad entries on the\n          // right.\n          output.row(matrix_size - 1) =\n              rhs.row(matrix_size - 1) / eigen_conj(matrix(0, matrix_size - 1));\n          for (int i = matrix_size - 1; i >= 0; --i) {\n            output.row(i).noalias() = rhs.row(i);\n            for (int j = i + 1; j < std::min(matrix_size, i + num_bands); ++j) {\n              output.row(i).noalias() -=\n                  eigen_conj(matrix(j - i, j)) * output.row(j);\n            }\n            output.row(i) /= eigen_conj(matrix(0, i));\n          }\n        }\n      } else {\n        if (!adjoint) {\n          output.row(matrix_size - 1) =\n              rhs.row(matrix_size - 1) / matrix(num_bands - 1, matrix_size - 1);\n          for (int i = 1; i < matrix_size; ++i) {\n            int k = matrix_size - 1 - i;\n            if (i < num_bands) {\n              output.row(k).noalias() =\n                  (rhs.row(k) - matrix.block(num_bands - 1 - i, k, i, 1)\n                                        .reverse()\n                                        .transpose() *\n                                    output.bottomRows(i)) /\n                  matrix(num_bands - 1, k);\n            } else {\n              output.row(k).noalias() =\n                  (rhs.row(k) -\n                   matrix.block(0, k, num_bands - 1, 1).reverse().transpose() *\n                       output.middleRows(k + 1, num_bands - 1)) /\n                  matrix(num_bands - 1, k);\n            }\n          }\n        } else {\n          output.row(0) = rhs.row(0) / eigen_conj(matrix(num_bands - 1, 0));\n          for (int i = 1; i < matrix_size; ++i) {\n            output.row(i).noalias() = rhs.row(i);\n            for (int j = std::max(0, i - (num_bands - 1)); j < i; ++j) {\n              output.row(i).noalias() -=\n                  eigen_conj(matrix(num_bands - 1 - (i - j), j)) *\n                  output.row(j);\n            }\n            output.row(i) /= eigen_conj(matrix(num_bands - 1, i));\n          }\n        }\n      }\n    }\n  }\n};\n\ntemplate <typename Scalar>\nstruct LaunchBatchBandedTriangularSolve;\n\ntemplate <typename Scalar>\nstruct LaunchBatchBandedTriangularSolve {\n  static void Launch(OpKernelContext* context, const Tensor& in_x,\n                     const Tensor& in_y, bool adjoint, bool lower,\n                     const MatMulBCast& bcast, Tensor* out) {\n    // Number of banded matrix triangular solves i.e. size of the batch.\n    const int64 batch_size = bcast.output_batch_size();\n    const int64 cost_per_unit =\n        in_x.dim_size(1) * in_x.dim_size(2) * in_y.dim_size(2);\n    auto worker_threads = *(context->device()->tensorflow_cpu_worker_threads());\n\n    using Matrix =\n        Eigen::Matrix<Scalar, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>;\n    using ConstMatrixMap = Eigen::Map<const Matrix>;\n    using RealScalar = typename Eigen::NumTraits<Scalar>::Real;\n    // Check diagonal before doing any solves. This is the first row in the\n    // lower case and else is the last row.\n    auto matrix = ConstMatrixMap(in_x.flat<Scalar>().data(), in_x.dim_size(1),\n                                 in_x.dim_size(2));\n    RealScalar min_abs_pivot;\n    if (lower) {\n      min_abs_pivot = matrix.row(0).cwiseAbs().minCoeff();\n    } else {\n      min_abs_pivot = matrix.row(in_x.dim_size(1) - 1).cwiseAbs().minCoeff();\n    }\n    OP_REQUIRES(context, min_abs_pivot > RealScalar(0),\n                errors::InvalidArgument(\"Input matrix is not invertible.\"));\n\n    Shard(worker_threads.num_threads, worker_threads.workers, batch_size,\n          cost_per_unit,\n          [&in_x, &in_y, adjoint, lower, &bcast, out](int64 start,\n                                                      int64 limit) {\n            SequentialBandedTriangularSolveKernel<Scalar>::Run(\n                in_x, in_y, lower, adjoint, bcast, out, start, limit);\n          });\n  }\n};\n\ntemplate <typename Scalar>\nclass BandedTriangularSolveOpCpu : public OpKernel {\n public:\n  explicit BandedTriangularSolveOpCpu(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"lower\", &lower_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"adjoint\", &adjoint_));\n  }\n\n  ~BandedTriangularSolveOpCpu() override {}\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& in0 = ctx->input(0);\n    const Tensor& in1 = ctx->input(1);\n\n    ValidateInputTensors(ctx, in0, in1);\n\n    MatMulBCast bcast(in0.shape().dim_sizes(), in1.shape().dim_sizes());\n    OP_REQUIRES(\n        ctx, bcast.IsValid(),\n        errors::InvalidArgument(\n            \"In[0] and In[1] must have compatible batch dimensions: \",\n            in0.shape().DebugString(), \" vs. \", in1.shape().DebugString()));\n\n    TensorShape out_shape = bcast.output_batch_shape();\n    auto batch_size = bcast.output_batch_size();\n    auto d0 = in0.dim_size(in0.dims() - 2);  // Band size.\n    auto d1 = in0.dim_size(in0.dims() - 1);\n    Tensor in0_reshaped;\n    OP_REQUIRES(\n        ctx,\n        in0_reshaped.CopyFrom(in0, TensorShape({bcast.x_batch_size(), d0, d1})),\n        errors::Internal(\"Failed to reshape In[0] from \",\n                         in0.shape().DebugString()));\n    auto d2 = in1.dim_size(in1.dims() - 2);\n    auto d3 = in1.dim_size(in1.dims() - 1);\n    Tensor in1_reshaped;\n    OP_REQUIRES(\n        ctx,\n        in1_reshaped.CopyFrom(in1, TensorShape({bcast.y_batch_size(), d2, d3})),\n        errors::Internal(\"Failed to reshape In[1] from \",\n                         in1.shape().DebugString()));\n    OP_REQUIRES(ctx, d1 == d2,\n                errors::InvalidArgument(\n                    \"In[0] mismatch In[1] shape: \", d1, \" vs. \", d2, \": \",\n                    in0.shape().DebugString(), \" \", in1.shape().DebugString(),\n                    \" \", lower_, \" \", adjoint_));\n    out_shape.AddDim(d1);\n    out_shape.AddDim(d3);\n    Tensor* out = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, out_shape, &out));\n    if (out->NumElements() == 0) {\n      return;\n    }\n    Tensor out_reshaped;\n    OP_REQUIRES(ctx,\n                out_reshaped.CopyFrom(*out, TensorShape({batch_size, d1, d3})),\n                errors::Internal(\"Failed to reshape output from \",\n                                 out->shape().DebugString()));\n    LaunchBatchBandedTriangularSolve<Scalar>::Launch(\n        ctx, in0_reshaped, in1_reshaped, adjoint_, lower_, bcast,\n        &out_reshaped);\n  }\n\n private:\n  void ValidateInputTensors(OpKernelContext* ctx, const Tensor& in0,\n                            const Tensor& in1) {\n    OP_REQUIRES(\n        ctx, in0.dims() >= 2,\n        errors::InvalidArgument(\"In[0] ndims must be >= 2: \", in0.dims()));\n\n    OP_REQUIRES(\n        ctx, in1.dims() >= 2,\n        errors::InvalidArgument(\"In[1] ndims must be >= 2: \", in1.dims()));\n\n    OP_REQUIRES(ctx, in0.NumElements() > 0,\n                errors::InvalidArgument(\"In[0] must not be an empty tensor: \",\n                                        in0.DebugString()));\n\n    OP_REQUIRES(ctx, in1.NumElements() > 0,\n                errors::InvalidArgument(\"In[1] must not be an empty tensor: \",\n                                        in1.DebugString()));\n  }\n  bool lower_;\n  bool adjoint_;\n};\n\n#define REGISTER_BANDED_TRIANGULAR_SOLVE_CPU(TYPE)        \\\n  REGISTER_KERNEL_BUILDER(Name(\"BandedTriangularSolve\")   \\\n                              .Device(DEVICE_CPU)         \\\n                              .TypeConstraint<TYPE>(\"T\"), \\\n                          BandedTriangularSolveOpCpu<TYPE>);\n\nREGISTER_BANDED_TRIANGULAR_SOLVE_CPU(float);\nREGISTER_BANDED_TRIANGULAR_SOLVE_CPU(double);\nREGISTER_BANDED_TRIANGULAR_SOLVE_CPU(complex64);\nREGISTER_BANDED_TRIANGULAR_SOLVE_CPU(complex128);\n\n}  // namespace tensorflow"