"/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include <algorithm>\n#include <cmath>\n\n#include \"tensorflow/core/framework/common_shape_fns.h\"\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/shape_inference.h\"\n\nnamespace tensorflow {\n\ntemplate <typename T>\nclass DecodePaddedRawOp : public OpKernel {\n public:\n  explicit DecodePaddedRawOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"out_type\", &out_type_));\n\n    const bool host_is_little_endian = port::kLittleEndian;\n    bool data_is_little_endian;\n    OP_REQUIRES_OK(context,\n                   context->GetAttr(\"little_endian\", &data_is_little_endian));\n    convert_data_endianness_ = host_is_little_endian != data_is_little_endian;\n  }\n\n  void Compute(OpKernelContext* context) override {\n    const auto& input = context->input(0);\n    auto flat_in = input.flat<tstring>();\n\n    int fixed_length;\n    const auto& length_input = context->input(1);\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(length_input.shape()),\n                errors::InvalidArgument(\"k must be scalar, got shape \",\n                                        length_input.shape().DebugString()));\n    fixed_length = length_input.scalar<int32>()();\n\n    OP_REQUIRES(\n        context, fixed_length % sizeof(T) == 0,\n        errors::InvalidArgument(\n            \"fixed_length (\", fixed_length,\n            \") must be a multiple of the size of out_type (\", sizeof(T), \")\"));\n\n    OP_REQUIRES(context, fixed_length > 0,\n                errors::InvalidArgument(\"fixed_length (\", fixed_length,\n                                        \") must be greater than zero.\"));\n\n    int width = fixed_length / sizeof(T);\n\n    TensorShape out_shape = input.shape();\n    out_shape.AddDim(width);\n    Tensor* output_tensor = nullptr;\n    OP_REQUIRES_OK(\n        context, context->allocate_output(\"output\", out_shape, &output_tensor));\n\n    if (flat_in.size() == 0) {  // Empty input\n      return;\n    }\n\n    auto out = output_tensor->flat_inner_dims<T>();\n    T* out_data = out.data();\n\n    // Forcibly clear memory - we're going to copy variable length strings in,\n    // and need to ensure that if we don't write to byte N when we copy, that\n    // we're not getting random data.\n    memset(out_data, 0, fixed_length * flat_in.size());\n\n    // If the data is already in the host's byte order, or if the width of the\n    // output type is a single byte (meaning the ordering doesn't matter), we\n    // can copy the memory directly.\n    if (!convert_data_endianness_ || sizeof(T) == 1) {\n      for (int64 i = 0; i < flat_in.size(); ++i) {\n        const T* in_data = reinterpret_cast<const T*>(flat_in(i).data());\n\n        if (flat_in(i).size() > fixed_length) {\n          memcpy(out_data, in_data, fixed_length);\n        } else {\n          memcpy(out_data, in_data, flat_in(i).size());\n        }\n        out_data += fixed_length;\n      }\n    } else {\n      // Otherwise, the data is not in the host's byte order, and rather than a\n      // direct copy, we need to reverse the byte ordering of each element.\n      for (int64 i = 0; i < flat_in.size(); ++i) {\n        const char* in_data_bytes =\n            reinterpret_cast<const char*>(flat_in(i).data());\n        char* out_data_bytes = reinterpret_cast<char*>(out_data);\n        const char* p_in = in_data_bytes;\n        char* p_out = out_data_bytes;\n        for (; p_in < in_data_bytes + fixed_length;\n             p_in += sizeof(T), p_out += sizeof(T)) {\n          std::reverse_copy(p_in, p_in + sizeof(T), p_out);\n        }\n        out_data += fixed_length;\n      }\n    }\n  }\n\n private:\n  // True if the endianness of the data and the endianness of the host are\n  // different, and the data needs conversion.\n  bool convert_data_endianness_;\n\n  // Data type of the output tensor.\n  DataType out_type_;\n};\n\n#define REGISTER(type)                                           \\\n  REGISTER_KERNEL_BUILDER(Name(\"DecodePaddedRaw\")                \\\n                              .Device(DEVICE_CPU)                \\\n                              .TypeConstraint<type>(\"out_type\"), \\\n                          DecodePaddedRawOp<type>)\n\nREGISTER(float);\nREGISTER(double);\nREGISTER(int32);\nREGISTER(uint16);\nREGISTER(uint8);\nREGISTER(int16);\nREGISTER(int8);\nREGISTER(int64);\n\n#undef REGISTER\n\n}  // namespace tensorflow"