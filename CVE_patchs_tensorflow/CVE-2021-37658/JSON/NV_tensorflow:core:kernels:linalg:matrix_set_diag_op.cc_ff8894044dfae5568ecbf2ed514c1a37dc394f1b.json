"/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/array_ops.cc.\n\n#define EIGEN_USE_THREADS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define EIGEN_USE_GPU\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#include \"tensorflow/core/kernels/linalg/matrix_set_diag_op.h\"\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/framework/tensor_types.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/kernels/linalg/matrix_diag_op.h\"\n#include \"tensorflow/core/lib/core/threadpool.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/platform/macros.h\"\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\ntypedef Eigen::GpuDevice GPUDevice;\n\ntemplate <typename Device, typename T>\nclass MatrixSetDiagOp : public OpKernel {\n public:\n  explicit MatrixSetDiagOp(OpKernelConstruction* context) : OpKernel(context) {\n    // MatrixSetDiagV3-specific.\n    if (context->HasAttr(\"align\")) {\n      functor::ReadAlignment(context, &left_align_superdiagonal_,\n                             &left_align_subdiagonal_);\n    }\n  }\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& input = context->input(0);\n    const Tensor& diag = context->input(1);\n\n    // MatrixSetDiag and MatrixSetDiagV2 both use this OpKernel. MatrixSetDiag\n    // only has two inputs, so we have to check the number of inputs before\n    // reading additional parameters in MatrixSetDiagV2.\n    int32_t lower_diag_index = 0;\n    int32_t upper_diag_index = 0;\n\n    // MatrixSetDiagV2-specific.\n    if (context->num_inputs() > kNumV1Inputs) {\n      auto& diag_index = context->input(2);\n      OP_REQUIRES(context,\n                  TensorShapeUtils::IsScalar(diag_index.shape()) ||\n                      TensorShapeUtils::IsVector(diag_index.shape()),\n                  errors::InvalidArgument(\n                      \"diag_index must be a scalar or vector, received shape: \",\n                      diag_index.shape().DebugString()));\n      OP_REQUIRES(\n          context, diag_index.NumElements() > 0,\n          errors::InvalidArgument(\"diag_index must have at least one element\"));\n      lower_diag_index = diag_index.flat<int32>()(0);\n      upper_diag_index = lower_diag_index;\n      if (TensorShapeUtils::IsVector(diag_index.shape())) {\n        auto diag_index_size = diag_index.dim_size(0);\n        OP_REQUIRES(\n            context, 0 < diag_index_size && diag_index_size <= 2,\n            errors::InvalidArgument(\n                \"diag_index must have only one or two elements, received \",\n                diag_index_size, \" elements.\"));\n        if (diag_index_size > 1) {\n          upper_diag_index = diag_index.flat<int32>()(1);\n        }\n      }\n    }\n\n    const TensorShape& input_shape = input.shape();\n    const TensorShape& diag_shape = diag.shape();\n    const int input_rank = input_shape.dims();\n\n    // Preliminary validation of sizes.\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrixOrHigher(input_shape),\n                errors::InvalidArgument(\n                    \"input must be at least 2-dim, received shape: \",\n                    input.shape().DebugString()));\n    OP_REQUIRES(context, TensorShapeUtils::IsVectorOrHigher(diag_shape),\n                errors::InvalidArgument(\n                    \"diagonal must be at least 1-dim, received shape: \",\n                    diag_shape.DebugString()));\n\n    // Make sure lower_diag_index and upper_diag_index is valid.\n    const Eigen::Index num_rows = input_shape.dim_size(input_rank - 2);\n    const Eigen::Index num_cols = input_shape.dim_size(input_rank - 1);\n    OP_REQUIRES(  // Checks lower_diag_index == 0 for when matrix shape = 0.\n        context,\n        (-num_rows < lower_diag_index && lower_diag_index < num_cols) ||\n            lower_diag_index == 0,\n        errors::InvalidArgument(\n            \"lower_diag_index is out of bound: \", lower_diag_index,\n            \" It must be between \", -num_rows, \" and \", num_cols));\n    OP_REQUIRES(context,\n                (-num_rows < upper_diag_index && upper_diag_index < num_cols) ||\n                    upper_diag_index == 0,\n                errors::InvalidArgument(\n                    \"upper_diag_index is out of bound: \", upper_diag_index,\n                    \" It must be between \", -num_rows, \" and \", num_cols));\n    OP_REQUIRES(\n        context, lower_diag_index <= upper_diag_index,\n        errors::InvalidArgument(\n            \"lower_diag_index must not be larger than upper_diag_index: \",\n            lower_diag_index, \" > \", upper_diag_index));\n\n    // Check if diag size is consistent with input.\n    const Eigen::Index num_diags = upper_diag_index - lower_diag_index + 1;\n    OP_REQUIRES(\n        context,\n        lower_diag_index == upper_diag_index ||\n            (diag_shape.dim_size(input_rank - 2) == num_diags),\n        errors::InvalidArgument(\"The number of diagonals provided in `diag` \"\n                                \"is not consistent with `lower_diag_index` and \"\n                                \"`upper_diag_index`\"));\n\n    TensorShape expected_diag_shape = input_shape;\n    expected_diag_shape.RemoveLastDims(2);\n    if (num_diags > 1) expected_diag_shape.AddDim(num_diags);\n    const int32_t max_diag_len =\n        std::min(num_rows + std::min(upper_diag_index, 0),\n                 num_cols - std::max(lower_diag_index, 0));\n    expected_diag_shape.AddDim(max_diag_len);\n    OP_REQUIRES(\n        context, expected_diag_shape == diag_shape,\n        errors::InvalidArgument(\n            \"Either first dimensions of diagonal don't match input.shape[:-2], \"\n            \"or diagonal.shape[:-1] is not equal to the longests diagonal in \"\n            \"range [lower_diag_index:upper_diag_index].\\nInput shape: \",\n            input_shape.DebugString(),\n            \"\\nDiagonal shape: \", diag_shape.DebugString(),\n            \"\\nExpected diagonal shape: \", expected_diag_shape.DebugString()));\n\n    if (input.NumElements() == 0) {\n      // This is a no-op.\n      context->set_output(0, input);\n      return;\n    }\n\n    auto input_reshaped = input.flat_inner_dims<T, 3>();\n    auto diag_reshaped = diag.flat<T>();\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {0}, 0, input_shape, &output));\n    auto output_reshaped = output->flat_inner_dims<T, 3>();\n    functor::MatrixSetDiag<Device, T>::Compute(\n        context, context->eigen_device<Device>(), input_reshaped, diag_reshaped,\n        output_reshaped, lower_diag_index, upper_diag_index, max_diag_len,\n        left_align_superdiagonal_, left_align_subdiagonal_);\n  }\n\n private:\n  bool left_align_superdiagonal_ = true;\n  bool left_align_subdiagonal_ = true;\n  static constexpr int kNumV1Inputs = 2;\n  TF_DISALLOW_COPY_AND_ASSIGN(MatrixSetDiagOp);\n};\n\n#define REGISTER_MATRIX_SET_DIAG(type)                                      \\\n  REGISTER_KERNEL_BUILDER(                                                  \\\n      Name(\"MatrixSetDiag\").Device(DEVICE_CPU).TypeConstraint<type>(\"T\"),   \\\n      MatrixSetDiagOp<CPUDevice, type>);                                    \\\n  REGISTER_KERNEL_BUILDER(                                                  \\\n      Name(\"MatrixSetDiagV2\").Device(DEVICE_CPU).TypeConstraint<type>(\"T\"), \\\n      MatrixSetDiagOp<CPUDevice, type>);                                    \\\n  REGISTER_KERNEL_BUILDER(                                                  \\\n      Name(\"MatrixSetDiagV3\").Device(DEVICE_CPU).TypeConstraint<type>(\"T\"), \\\n      MatrixSetDiagOp<CPUDevice, type>);\n\nTF_CALL_POD_TYPES(REGISTER_MATRIX_SET_DIAG);\n#undef REGISTER_MATRIX_SET_DIAG\n\n// Registration of the deprecated kernel.\n// Delete after 10mar2017.\n#define REGISTER_BATCH_MATRIX_SET_DIAG(type)                                   \\\n  REGISTER_KERNEL_BUILDER(                                                     \\\n      Name(\"BatchMatrixSetDiag\").Device(DEVICE_CPU).TypeConstraint<type>(\"T\"), \\\n      MatrixSetDiagOp<CPUDevice, type>);\nTF_CALL_POD_TYPES(REGISTER_BATCH_MATRIX_SET_DIAG);\n#undef REGISTER_BATCH_MATRIX_SET_DIAG\n\nnamespace functor {\n\n// Implementation of the functor specialization for CPU.\ntemplate <typename T>\nstruct MatrixSetDiag<CPUDevice, T> {\n  static void Compute(OpKernelContext* context, const CPUDevice& device,\n                      typename TTypes<T, 3>::ConstTensor& input,\n                      typename TTypes<T>::ConstTensor& diag,\n                      typename TTypes<T, 3>::Tensor& output,\n                      const Eigen::Index lower_diag_index,\n                      const Eigen::Index upper_diag_index,\n                      const Eigen::Index max_diag_len,\n                      const bool left_align_superdiagonal,\n                      const bool left_align_subdiagonal) {\n    if (input.data() != output.data()) {\n      output.device(device) = input;\n    }\n    const Eigen::Index num_diags = upper_diag_index - lower_diag_index + 1;\n    auto compute_shard = [&output, &diag, &upper_diag_index, &max_diag_len,\n                          &num_diags, &left_align_superdiagonal,\n                          &left_align_subdiagonal](Eigen::Index begin,\n                                                   Eigen::Index end) {\n      const Eigen::Index num_rows = output.dimension(1);\n      const Eigen::Index num_cols = output.dimension(2);\n      Eigen::Index diag_base_index = begin * num_diags * max_diag_len;\n      for (Eigen::Index batch = begin; batch < end; ++batch) {\n        for (Eigen::Index m = 0; m < num_diags; ++m) {\n          const Eigen::Index diag_index = upper_diag_index - m;\n          int diag_len, content_offset;\n          std::tie(diag_len, content_offset) = ComputeDiagLenAndContentOffset(\n              diag_index, max_diag_len, num_rows, num_cols,\n              left_align_superdiagonal, left_align_subdiagonal);\n\n          // Make two separate cases to save some index calculations.\n          if (diag_index >= 0) {\n            for (Eigen::Index n = 0; n < diag_len; ++n) {\n              output(batch, n, n + diag_index) =\n                  diag(diag_base_index + n + content_offset);\n            }\n          } else {\n            for (Eigen::Index n = 0; n < diag_len; ++n) {\n              output(batch, n - diag_index, n) =\n                  diag(diag_base_index + n + content_offset);\n            }\n          }\n          diag_base_index += max_diag_len;\n        }\n      }\n    };\n    auto thread_pool =\n        context->device()->tensorflow_cpu_worker_threads()->workers;\n    // TODO(penporn): Tune for the best constant in cost_per_batch.\n    const Eigen::Index cost_per_batch = 10 * num_diags * max_diag_len;\n    thread_pool->ParallelFor(output.dimension(0), cost_per_batch,\n                             std::move(compute_shard));\n  }\n};\n\n}  // namespace functor\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n// Forward declarations of the functor specializations for GPU.\nnamespace functor {\n#define DECLARE_GPU_SPEC(T)                                                    \\\n  template <>                                                                  \\\n  void MatrixSetDiag<GPUDevice, T>::Compute(                                   \\\n      OpKernelContext* context, const GPUDevice& device,                       \\\n      typename TTypes<T, 3>::ConstTensor& input,                               \\\n      typename TTypes<T>::ConstTensor& diag,                                   \\\n      typename TTypes<T, 3>::Tensor& output,                                   \\\n      const Eigen::Index lower_diag_index,                                     \\\n      const Eigen::Index upper_diag_index, const Eigen::Index max_diag_len,    \\\n      const bool left_align_superdiagonal, const bool left_align_subdiagonal); \\\n  extern template struct MatrixSetDiag<GPUDevice, T>;\n\nTF_CALL_GPU_ALL_TYPES(DECLARE_GPU_SPEC);\n\n}  // namespace functor\n\n// Registration of the GPU implementations.\n#define REGISTER_MATRIX_SET_DIAG_GPU(type)                                \\\n  REGISTER_KERNEL_BUILDER(                                                \\\n      Name(\"MatrixSetDiag\").Device(DEVICE_GPU).TypeConstraint<type>(\"T\"), \\\n      MatrixSetDiagOp<GPUDevice, type>);                                  \\\n  REGISTER_KERNEL_BUILDER(Name(\"MatrixSetDiagV2\")                         \\\n                              .Device(DEVICE_GPU)                         \\\n                              .TypeConstraint<type>(\"T\")                  \\\n                              .HostMemory(\"k\"),                           \\\n                          MatrixSetDiagOp<GPUDevice, type>);              \\\n  REGISTER_KERNEL_BUILDER(Name(\"MatrixSetDiagV3\")                         \\\n                              .Device(DEVICE_GPU)                         \\\n                              .TypeConstraint<type>(\"T\")                  \\\n                              .HostMemory(\"k\"),                           \\\n                          MatrixSetDiagOp<GPUDevice, type>);\n\nTF_CALL_GPU_ALL_TYPES(REGISTER_MATRIX_SET_DIAG_GPU);\n#undef REGISTER_MATRIX_SET_DIAG_GPU\n\n// Registration of the deprecated kernel.\n// Delete after 10mar2017.\n#define REGISTER_BATCH_MATRIX_SET_DIAG_GPU(type)                               \\\n  REGISTER_KERNEL_BUILDER(                                                     \\\n      Name(\"BatchMatrixSetDiag\").Device(DEVICE_GPU).TypeConstraint<type>(\"T\"), \\\n      MatrixSetDiagOp<GPUDevice, type>);\nTF_CALL_GPU_NUMBER_TYPES(REGISTER_BATCH_MATRIX_SET_DIAG_GPU);\n#undef REGISTER_BATCH_MATRIX_SET_DIAG_GPU\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n}  // namespace tensorflow"