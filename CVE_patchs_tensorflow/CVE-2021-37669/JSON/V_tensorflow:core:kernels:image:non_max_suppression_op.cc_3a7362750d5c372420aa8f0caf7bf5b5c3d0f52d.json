"/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/image_ops.cc\n\n#define EIGEN_USE_THREADS\n\n#include \"tensorflow/core/kernels/image/non_max_suppression_op.h\"\n\n#include <cmath>\n#include <functional>\n#include <queue>\n#include <vector>\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/bounds_check.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/lib/core/status.h\"\n#include \"tensorflow/core/platform/logging.h\"\n\nnamespace tensorflow {\nnamespace {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n\nstatic inline void CheckScoreSizes(OpKernelContext* context, int num_boxes,\n                                   const Tensor& scores) {\n  // The shape of 'scores' is [num_boxes]\n  OP_REQUIRES(context, scores.dims() == 1,\n              errors::InvalidArgument(\n                  \"scores must be 1-D\", scores.shape().DebugString(),\n                  \" (Shape must be rank 1 but is rank \", scores.dims(), \")\"));\n  OP_REQUIRES(\n      context, scores.dim_size(0) == num_boxes,\n      errors::InvalidArgument(\"scores has incompatible shape (Dimensions must \"\n                              \"be equal, but are \",\n                              num_boxes, \" and \", scores.dim_size(0), \")\"));\n}\n\nstatic inline void ParseAndCheckOverlapSizes(OpKernelContext* context,\n                                             const Tensor& overlaps,\n                                             int* num_boxes) {\n  // the shape of 'overlaps' is [num_boxes, num_boxes]\n  OP_REQUIRES(context, overlaps.dims() == 2,\n              errors::InvalidArgument(\"overlaps must be 2-D\",\n                                      overlaps.shape().DebugString()));\n\n  *num_boxes = overlaps.dim_size(0);\n  OP_REQUIRES(context, overlaps.dim_size(1) == *num_boxes,\n              errors::InvalidArgument(\"overlaps must be square\",\n                                      overlaps.shape().DebugString()));\n}\n\nstatic inline void ParseAndCheckBoxSizes(OpKernelContext* context,\n                                         const Tensor& boxes, int* num_boxes) {\n  // The shape of 'boxes' is [num_boxes, 4]\n  OP_REQUIRES(context, boxes.dims() == 2,\n              errors::InvalidArgument(\n                  \"boxes must be 2-D\", boxes.shape().DebugString(),\n                  \" (Shape must be rank 2 but is rank \", boxes.dims(), \")\"));\n  *num_boxes = boxes.dim_size(0);\n  OP_REQUIRES(context, boxes.dim_size(1) == 4,\n              errors::InvalidArgument(\"boxes must have 4 columns (Dimension \"\n                                      \"must be 4 but is \",\n                                      boxes.dim_size(1), \")\"));\n}\n\nstatic inline void CheckCombinedNMSScoreSizes(OpKernelContext* context,\n                                              int num_boxes,\n                                              const Tensor& scores) {\n  // The shape of 'scores' is [batch_size, num_boxes, num_classes]\n  OP_REQUIRES(context, scores.dims() == 3,\n              errors::InvalidArgument(\"scores must be 3-D\",\n                                      scores.shape().DebugString()));\n  OP_REQUIRES(context, scores.dim_size(1) == num_boxes,\n              errors::InvalidArgument(\"scores has incompatible shape\"));\n}\n\nstatic inline void ParseAndCheckCombinedNMSBoxSizes(OpKernelContext* context,\n                                                    const Tensor& boxes,\n                                                    int* num_boxes,\n                                                    const int num_classes) {\n  // The shape of 'boxes' is [batch_size, num_boxes, q, 4]\n  OP_REQUIRES(context, boxes.dims() == 4,\n              errors::InvalidArgument(\"boxes must be 4-D\",\n                                      boxes.shape().DebugString()));\n\n  bool box_check = boxes.dim_size(2) == 1 || boxes.dim_size(2) == num_classes;\n  OP_REQUIRES(context, box_check,\n              errors::InvalidArgument(\n                  \"third dimension of boxes must be either 1 or num classes\"));\n  *num_boxes = boxes.dim_size(1);\n  OP_REQUIRES(context, boxes.dim_size(3) == 4,\n              errors::InvalidArgument(\"boxes must have 4 columns\"));\n}\n// Return intersection-over-union overlap between boxes i and j\ntemplate <typename T>\nstatic inline float IOU(typename TTypes<T, 2>::ConstTensor boxes, int i,\n                        int j) {\n  const float ymin_i = Eigen::numext::mini<float>(boxes(i, 0), boxes(i, 2));\n  const float xmin_i = Eigen::numext::mini<float>(boxes(i, 1), boxes(i, 3));\n  const float ymax_i = Eigen::numext::maxi<float>(boxes(i, 0), boxes(i, 2));\n  const float xmax_i = Eigen::numext::maxi<float>(boxes(i, 1), boxes(i, 3));\n  const float ymin_j = Eigen::numext::mini<float>(boxes(j, 0), boxes(j, 2));\n  const float xmin_j = Eigen::numext::mini<float>(boxes(j, 1), boxes(j, 3));\n  const float ymax_j = Eigen::numext::maxi<float>(boxes(j, 0), boxes(j, 2));\n  const float xmax_j = Eigen::numext::maxi<float>(boxes(j, 1), boxes(j, 3));\n  const float area_i = (ymax_i - ymin_i) * (xmax_i - xmin_i);\n  const float area_j = (ymax_j - ymin_j) * (xmax_j - xmin_j);\n  if (area_i <= 0 || area_j <= 0) {\n    return 0.0;\n  }\n  const float intersection_ymin = Eigen::numext::maxi<float>(ymin_i, ymin_j);\n  const float intersection_xmin = Eigen::numext::maxi<float>(xmin_i, xmin_j);\n  const float intersection_ymax = Eigen::numext::mini<float>(ymax_i, ymax_j);\n  const float intersection_xmax = Eigen::numext::mini<float>(xmax_i, xmax_j);\n  const float intersection_area =\n      Eigen::numext::maxi<float>(intersection_ymax - intersection_ymin, 0.0) *\n      Eigen::numext::maxi<float>(intersection_xmax - intersection_xmin, 0.0);\n  return intersection_area / (area_i + area_j - intersection_area);\n}\n\ntemplate <typename T>\nstatic inline T Overlap(typename TTypes<T, 2>::ConstTensor overlaps, int i,\n                        int j) {\n  return overlaps(i, j);\n}\n\ntemplate <typename T>\nstatic inline std::function<float(int, int)> CreateIOUSimilarityFn(\n    const Tensor& boxes) {\n  typename TTypes<T, 2>::ConstTensor boxes_data = boxes.tensor<T, 2>();\n  return std::bind(&IOU<T>, boxes_data, std::placeholders::_1,\n                   std::placeholders::_2);\n}\n\ntemplate <typename T>\nstatic inline std::function<T(int, int)> CreateOverlapSimilarityFn(\n    const Tensor& overlaps) {\n  typename TTypes<T, 2>::ConstTensor overlaps_data =\n      overlaps.tensor<float, 2>();\n  return std::bind(&Overlap<T>, overlaps_data, std::placeholders::_1,\n                   std::placeholders::_2);\n}\n\ntemplate <typename T>\nvoid DoNonMaxSuppressionOp(OpKernelContext* context, const Tensor& scores,\n                           int num_boxes, const Tensor& max_output_size,\n                           const T similarity_threshold,\n                           const T score_threshold, const T soft_nms_sigma,\n                           const std::function<float(int, int)>& similarity_fn,\n                           bool return_scores_tensor = false,\n                           bool pad_to_max_output_size = false,\n                           int* ptr_num_valid_outputs = nullptr) {\n  const int output_size = max_output_size.scalar<int>()();\n\n  std::vector<T> scores_data(num_boxes);\n  std::copy_n(scores.flat<T>().data(), num_boxes, scores_data.begin());\n\n  // Data structure for a selection candidate in NMS.\n  struct Candidate {\n    int box_index;\n    T score;\n    int suppress_begin_index;\n  };\n\n  auto cmp = [](const Candidate bs_i, const Candidate bs_j) {\n    return ((bs_i.score == bs_j.score) && (bs_i.box_index > bs_j.box_index)) ||\n           bs_i.score < bs_j.score;\n  };\n  std::priority_queue<Candidate, std::deque<Candidate>, decltype(cmp)>\n      candidate_priority_queue(cmp);\n  for (int i = 0; i < scores_data.size(); ++i) {\n    if (scores_data[i] > score_threshold) {\n      candidate_priority_queue.emplace(Candidate({i, scores_data[i], 0}));\n    }\n  }\n\n  T scale = static_cast<T>(0.0);\n  bool is_soft_nms = soft_nms_sigma > static_cast<T>(0.0);\n  if (is_soft_nms) {\n    scale = static_cast<T>(-0.5) / soft_nms_sigma;\n  }\n\n  auto suppress_weight = [similarity_threshold, scale,\n                          is_soft_nms](const T sim) {\n    const T weight = Eigen::numext::exp<T>(scale * sim * sim);\n    return is_soft_nms || sim <= similarity_threshold ? weight\n                                                      : static_cast<T>(0.0);\n  };\n\n  std::vector<int> selected;\n  std::vector<T> selected_scores;\n  float similarity;\n  T original_score;\n  Candidate next_candidate;\n\n  while (selected.size() < output_size && !candidate_priority_queue.empty()) {\n    next_candidate = candidate_priority_queue.top();\n    original_score = next_candidate.score;\n    candidate_priority_queue.pop();\n\n    // Overlapping boxes are likely to have similar scores, therefore we\n    // iterate through the previously selected boxes backwards in order to\n    // see if `next_candidate` should be suppressed. We also enforce a property\n    // that a candidate can be suppressed by another candidate no more than\n    // once via `suppress_begin_index` which tracks which previously selected\n    // boxes have already been compared against next_candidate prior to a given\n    // iteration.  These previous selected boxes are then skipped over in the\n    // following loop.\n    bool should_hard_suppress = false;\n    for (int j = static_cast<int>(selected.size()) - 1;\n         j >= next_candidate.suppress_begin_index; --j) {\n      similarity = similarity_fn(next_candidate.box_index, selected[j]);\n\n      next_candidate.score *= suppress_weight(static_cast<T>(similarity));\n\n      // First decide whether to perform hard suppression\n      if (!is_soft_nms && static_cast<T>(similarity) > similarity_threshold) {\n        should_hard_suppress = true;\n        break;\n      }\n\n      // If next_candidate survives hard suppression, apply soft suppression\n      if (next_candidate.score <= score_threshold) break;\n    }\n    // If `next_candidate.score` has not dropped below `score_threshold`\n    // by this point, then we know that we went through all of the previous\n    // selections and can safely update `suppress_begin_index` to\n    // `selected.size()`. If on the other hand `next_candidate.score`\n    // *has* dropped below the score threshold, then since `suppress_weight`\n    // always returns values in [0, 1], further suppression by items that were\n    // not covered in the above for loop would not have caused the algorithm\n    // to select this item. We thus do the same update to\n    // `suppress_begin_index`, but really, this element will not be added back\n    // into the priority queue in the following.\n    next_candidate.suppress_begin_index = selected.size();\n\n    if (!should_hard_suppress) {\n      if (next_candidate.score == original_score) {\n        // Suppression has not occurred, so select next_candidate\n        selected.push_back(next_candidate.box_index);\n        selected_scores.push_back(next_candidate.score);\n        continue;\n      }\n      if (next_candidate.score > score_threshold) {\n        // Soft suppression has occurred and current score is still greater than\n        // score_threshold; add next_candidate back onto priority queue.\n        candidate_priority_queue.push(next_candidate);\n      }\n    }\n  }\n\n  int num_valid_outputs = selected.size();\n  if (pad_to_max_output_size) {\n    selected.resize(output_size, 0);\n    selected_scores.resize(output_size, static_cast<T>(0));\n  }\n  if (ptr_num_valid_outputs) {\n    *ptr_num_valid_outputs = num_valid_outputs;\n  }\n\n  // Allocate output tensors\n  Tensor* output_indices = nullptr;\n  TensorShape output_shape({static_cast<int>(selected.size())});\n  OP_REQUIRES_OK(context,\n                 context->allocate_output(0, output_shape, &output_indices));\n  TTypes<int, 1>::Tensor output_indices_data = output_indices->tensor<int, 1>();\n  std::copy_n(selected.begin(), selected.size(), output_indices_data.data());\n\n  if (return_scores_tensor) {\n    Tensor* output_scores = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(1, output_shape, &output_scores));\n    typename TTypes<T, 1>::Tensor output_scores_data =\n        output_scores->tensor<T, 1>();\n    std::copy_n(selected_scores.begin(), selected_scores.size(),\n                output_scores_data.data());\n  }\n}\n\nstruct ResultCandidate {\n  int box_index;\n  float score;\n  int class_idx;\n  float box_coord[4];\n};\n\nvoid DoNMSPerClass(int batch_idx, int class_idx, const float* boxes_data,\n                   const float* scores_data, int num_boxes, int q,\n                   int num_classes, const int size_per_class,\n                   const float score_threshold, const float iou_threshold,\n                   std::vector<ResultCandidate>& result_candidate_vec) {\n  std::vector<float> class_scores_data;\n  class_scores_data.reserve(num_boxes);\n  std::vector<float> class_boxes_data;\n  class_boxes_data.reserve(num_boxes * 4);\n\n  for (int box_idx = 0; box_idx < num_boxes; ++box_idx) {\n    class_scores_data.push_back(scores_data[box_idx * num_classes + class_idx]);\n    for (int cid = 0; cid < 4; ++cid) {\n      if (q > 1) {\n        class_boxes_data.push_back(\n            boxes_data[(box_idx * q + class_idx) * 4 + cid]);\n      } else {\n        class_boxes_data.push_back(boxes_data[box_idx * 4 + cid]);\n      }\n    }\n  }\n\n  // Do NMS, get the candidate indices of form vector<int>\n  // Data structure for selection candidate in NMS.\n  struct Candidate {\n    int box_index;\n    float score;\n  };\n  auto cmp = [](const Candidate bs_i, const Candidate bs_j) {\n    return bs_i.score < bs_j.score;\n  };\n  std::priority_queue<Candidate, std::vector<Candidate>, decltype(cmp)>\n      candidate_priority_queue(cmp);\n  for (int i = 0; i < num_boxes; ++i) {\n    if (class_scores_data[i] > score_threshold) {\n      candidate_priority_queue.emplace(Candidate({i, class_scores_data[i]}));\n    }\n  }\n\n  std::vector<int> selected;\n  std::vector<float> selected_boxes;\n  Candidate next_candidate;\n\n  // Move class_boxes_data to a tensor\n  Eigen::array<Eigen::DenseIndex, 2> boxesShape = {num_boxes, 4};\n  typename TTypes<float, 2>::ConstTensor boxes_data_t(class_boxes_data.data(),\n                                                      boxesShape);\n  float iou;\n  while (selected.size() < size_per_class &&\n         !candidate_priority_queue.empty()) {\n    next_candidate = candidate_priority_queue.top();\n    candidate_priority_queue.pop();\n    // Overlapping boxes are likely to have similar scores,\n    // therefore we iterate through the previously selected boxes backwards\n    // in order to see if `next_candidate` should be suppressed.\n    bool should_select = true;\n    for (int j = selected.size() - 1; j >= 0; --j) {\n      iou = IOU<float>(boxes_data_t, next_candidate.box_index, selected[j]);\n      if (iou > iou_threshold) {\n        should_select = false;\n        break;\n      }\n    }\n\n    if (should_select) {\n      // Add the selected box to the result candidate. Sorted by score\n      int id = next_candidate.box_index;\n      result_candidate_vec[selected.size() + size_per_class * class_idx] = {\n          next_candidate.box_index,\n          next_candidate.score,\n          class_idx,\n          {boxes_data_t(id, 0), boxes_data_t(id, 1), boxes_data_t(id, 2),\n           boxes_data_t(id, 3)}};\n      selected.push_back(next_candidate.box_index);\n    }\n  }\n}\n\nvoid SelectResultPerBatch(std::vector<float>& nmsed_boxes,\n                          std::vector<float>& nmsed_scores,\n                          std::vector<float>& nmsed_classes,\n                          std::vector<ResultCandidate>& result_candidate_vec,\n                          std::vector<int>& final_valid_detections,\n                          const int batch_idx, int total_size_per_batch,\n                          bool pad_per_class, int max_size_per_batch,\n                          bool clip_boxes, int per_batch_size) {\n  auto rc_cmp = [](const ResultCandidate rc_i, const ResultCandidate rc_j) {\n    return rc_i.score > rc_j.score;\n  };\n  std::sort(result_candidate_vec.begin(), result_candidate_vec.end(), rc_cmp);\n\n  int max_detections = 0;\n  int result_candidate_size =\n      std::count_if(result_candidate_vec.begin(), result_candidate_vec.end(),\n                    [](ResultCandidate rc) { return rc.box_index > -1; });\n  // If pad_per_class is false, we always pad to max_total_size\n  if (!pad_per_class) {\n    max_detections = std::min(result_candidate_size, total_size_per_batch);\n  } else {\n    max_detections = std::min(per_batch_size, result_candidate_size);\n  }\n\n  final_valid_detections[batch_idx] = max_detections;\n\n  int curr_total_size = max_detections;\n  int result_idx = 0;\n  // Pick the top max_detections values\n  while (curr_total_size > 0 && result_idx < result_candidate_vec.size()) {\n    ResultCandidate next_candidate = result_candidate_vec[result_idx++];\n    // Add to final output vectors\n    if (clip_boxes) {\n      const float box_min = 0.0;\n      const float box_max = 1.0;\n      nmsed_boxes.push_back(\n          std::max(std::min(next_candidate.box_coord[0], box_max), box_min));\n      nmsed_boxes.push_back(\n          std::max(std::min(next_candidate.box_coord[1], box_max), box_min));\n      nmsed_boxes.push_back(\n          std::max(std::min(next_candidate.box_coord[2], box_max), box_min));\n      nmsed_boxes.push_back(\n          std::max(std::min(next_candidate.box_coord[3], box_max), box_min));\n    } else {\n      nmsed_boxes.push_back(next_candidate.box_coord[0]);\n      nmsed_boxes.push_back(next_candidate.box_coord[1]);\n      nmsed_boxes.push_back(next_candidate.box_coord[2]);\n      nmsed_boxes.push_back(next_candidate.box_coord[3]);\n    }\n    nmsed_scores.push_back(next_candidate.score);\n    nmsed_classes.push_back(next_candidate.class_idx);\n    curr_total_size--;\n  }\n\n  nmsed_boxes.resize(per_batch_size * 4, 0);\n  nmsed_scores.resize(per_batch_size, 0);\n  nmsed_classes.resize(per_batch_size, 0);\n}\n\nvoid BatchedNonMaxSuppressionOp(\n    OpKernelContext* context, const Tensor& inp_boxes, const Tensor& inp_scores,\n    int num_boxes, const int max_size_per_class, const int total_size_per_batch,\n    const float score_threshold, const float iou_threshold,\n    bool pad_per_class = false, bool clip_boxes = true) {\n  const int num_batches = inp_boxes.dim_size(0);\n  int num_classes = inp_scores.dim_size(2);\n  int q = inp_boxes.dim_size(2);\n\n  const float* scores_data =\n      const_cast<float*>(inp_scores.flat<float>().data());\n  const float* boxes_data = const_cast<float*>(inp_boxes.flat<float>().data());\n\n  int boxes_per_batch = num_boxes * q * 4;\n  int scores_per_batch = num_boxes * num_classes;\n  const int size_per_class = std::min(max_size_per_class, num_boxes);\n  std::vector<std::vector<ResultCandidate>> result_candidate_vec(\n      num_batches,\n      std::vector<ResultCandidate>(size_per_class * num_classes,\n                                   {-1, -1.0, -1, {0.0, 0.0, 0.0, 0.0}}));\n\n  // [num_batches, per_batch_size * 4]\n  std::vector<std::vector<float>> nmsed_boxes(num_batches);\n  // [num_batches, per_batch_size]\n  std::vector<std::vector<float>> nmsed_scores(num_batches);\n  // [num_batches, per_batch_size]\n  std::vector<std::vector<float>> nmsed_classes(num_batches);\n  // [num_batches]\n  std::vector<int> final_valid_detections(num_batches);\n\n  auto shard_nms = [&](int begin, int end) {\n    for (int idx = begin; idx < end; ++idx) {\n      int batch_idx = idx / num_classes;\n      int class_idx = idx % num_classes;\n      DoNMSPerClass(batch_idx, class_idx,\n                    boxes_data + boxes_per_batch * batch_idx,\n                    scores_data + scores_per_batch * batch_idx, num_boxes, q,\n                    num_classes, size_per_class, score_threshold, iou_threshold,\n                    result_candidate_vec[batch_idx]);\n    }\n  };\n\n  int length = num_batches * num_classes;\n  // Input data boxes_data, scores_data\n  int input_bytes = num_boxes * 10 * sizeof(float);\n  int output_bytes = num_boxes * 10 * sizeof(float);\n  int compute_cycles = Eigen::TensorOpCost::AddCost<int>() * num_boxes * 14 +\n                       Eigen::TensorOpCost::MulCost<int>() * num_boxes * 9 +\n                       Eigen::TensorOpCost::MulCost<float>() * num_boxes * 9 +\n                       Eigen::TensorOpCost::AddCost<float>() * num_boxes * 8;\n  // The cost here is not the actual number of cycles, but rather a set of\n  // hand-tuned numbers that seem to work best.\n  const Eigen::TensorOpCost cost(input_bytes, output_bytes, compute_cycles);\n  const CPUDevice& d = context->eigen_device<CPUDevice>();\n  d.parallelFor(length, cost, shard_nms);\n\n  int per_batch_size = total_size_per_batch;\n  if (pad_per_class) {\n    per_batch_size =\n        std::min(total_size_per_batch, max_size_per_class * num_classes);\n  }\n\n  Tensor* valid_detections_t = nullptr;\n  TensorShape valid_detections_shape({num_batches});\n  OP_REQUIRES_OK(context, context->allocate_output(3, valid_detections_shape,\n                                                   &valid_detections_t));\n  auto valid_detections_flat = valid_detections_t->template flat<int>();\n\n  auto shard_result = [&](int begin, int end) {\n    for (int batch_idx = begin; batch_idx < end; ++batch_idx) {\n      SelectResultPerBatch(\n          nmsed_boxes[batch_idx], nmsed_scores[batch_idx],\n          nmsed_classes[batch_idx], result_candidate_vec[batch_idx],\n          final_valid_detections, batch_idx, total_size_per_batch,\n          pad_per_class, max_size_per_class * num_classes, clip_boxes,\n          per_batch_size);\n      valid_detections_flat(batch_idx) = final_valid_detections[batch_idx];\n    }\n  };\n  length = num_batches;\n  // Input data boxes_data, scores_data\n  input_bytes =\n      num_boxes * 10 * sizeof(float) + per_batch_size * 6 * sizeof(float);\n  output_bytes =\n      num_boxes * 5 * sizeof(float) + per_batch_size * 6 * sizeof(float);\n  compute_cycles = Eigen::TensorOpCost::AddCost<int>() * num_boxes * 5 +\n                   Eigen::TensorOpCost::AddCost<float>() * num_boxes * 5;\n  // The cost here is not the actual number of cycles, but rather a set of\n  // hand-tuned numbers that seem to work best.\n  const Eigen::TensorOpCost cost_result(input_bytes, output_bytes,\n                                        compute_cycles);\n  d.parallelFor(length, cost_result, shard_result);\n\n  Tensor* nmsed_boxes_t = nullptr;\n  TensorShape boxes_shape({num_batches, per_batch_size, 4});\n  OP_REQUIRES_OK(context,\n                 context->allocate_output(0, boxes_shape, &nmsed_boxes_t));\n  auto nmsed_boxes_flat = nmsed_boxes_t->template flat<float>();\n\n  Tensor* nmsed_scores_t = nullptr;\n  TensorShape scores_shape({num_batches, per_batch_size});\n  OP_REQUIRES_OK(context,\n                 context->allocate_output(1, scores_shape, &nmsed_scores_t));\n  auto nmsed_scores_flat = nmsed_scores_t->template flat<float>();\n\n  Tensor* nmsed_classes_t = nullptr;\n  OP_REQUIRES_OK(context,\n                 context->allocate_output(2, scores_shape, &nmsed_classes_t));\n  auto nmsed_classes_flat = nmsed_classes_t->template flat<float>();\n\n  auto shard_copy_result = [&](int begin, int end) {\n    for (int idx = begin; idx < end; ++idx) {\n      int batch_idx = idx / per_batch_size;\n      int j = idx % per_batch_size;\n      nmsed_scores_flat(idx) = nmsed_scores[batch_idx][j];\n      nmsed_classes_flat(idx) = nmsed_classes[batch_idx][j];\n      for (int k = 0; k < 4; ++k) {\n        nmsed_boxes_flat(idx * 4 + k) = nmsed_boxes[batch_idx][j * 4 + k];\n      }\n    }\n  };\n  length = num_batches * per_batch_size;\n  // Input data boxes_data, scores_data\n  input_bytes = 6 * sizeof(float);\n  output_bytes = 6 * sizeof(float);\n  compute_cycles = Eigen::TensorOpCost::AddCost<int>() * 2 +\n                   Eigen::TensorOpCost::MulCost<int>() * 2 +\n                   Eigen::TensorOpCost::DivCost<float>() * 2;\n  const Eigen::TensorOpCost cost_copy_result(input_bytes, output_bytes,\n                                             compute_cycles);\n  d.parallelFor(length, cost_copy_result, shard_copy_result);\n}\n\n}  // namespace\n\ntemplate <typename Device>\nclass NonMaxSuppressionOp : public OpKernel {\n public:\n  explicit NonMaxSuppressionOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"iou_threshold\", &iou_threshold_));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    // boxes: [num_boxes, 4]\n    const Tensor& boxes = context->input(0);\n    // scores: [num_boxes]\n    const Tensor& scores = context->input(1);\n    // max_output_size: scalar\n    const Tensor& max_output_size = context->input(2);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsScalar(max_output_size.shape()),\n        errors::InvalidArgument(\"max_output_size must be 0-D, got shape \",\n                                max_output_size.shape().DebugString()));\n\n    OP_REQUIRES(context, iou_threshold_ >= 0 && iou_threshold_ <= 1,\n                errors::InvalidArgument(\"iou_threshold must be in [0, 1]\"));\n    int num_boxes = 0;\n    ParseAndCheckBoxSizes(context, boxes, &num_boxes);\n    CheckScoreSizes(context, num_boxes, scores);\n    if (!context->status().ok()) {\n      return;\n    }\n    auto similarity_fn = CreateIOUSimilarityFn<float>(boxes);\n\n    const float score_threshold_val = std::numeric_limits<float>::lowest();\n    const float dummy_soft_nms_sigma = static_cast<float>(0.0);\n    DoNonMaxSuppressionOp<float>(context, scores, num_boxes, max_output_size,\n                                 iou_threshold_, score_threshold_val,\n                                 dummy_soft_nms_sigma, similarity_fn);\n  }\n\n private:\n  float iou_threshold_;\n};\n\ntemplate <typename Device, typename T>\nclass NonMaxSuppressionV2Op : public OpKernel {\n public:\n  explicit NonMaxSuppressionV2Op(OpKernelConstruction* context)\n      : OpKernel(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    // boxes: [num_boxes, 4]\n    const Tensor& boxes = context->input(0);\n    // scores: [num_boxes]\n    const Tensor& scores = context->input(1);\n    // max_output_size: scalar\n    const Tensor& max_output_size = context->input(2);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsScalar(max_output_size.shape()),\n        errors::InvalidArgument(\"max_output_size must be 0-D, got shape \",\n                                max_output_size.shape().DebugString()));\n    // iou_threshold: scalar\n    const Tensor& iou_threshold = context->input(3);\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(iou_threshold.shape()),\n                errors::InvalidArgument(\"iou_threshold must be 0-D, got shape \",\n                                        iou_threshold.shape().DebugString()));\n    const T iou_threshold_val = iou_threshold.scalar<T>()();\n\n    OP_REQUIRES(context,\n                iou_threshold_val >= static_cast<T>(0.0) &&\n                    iou_threshold_val <= static_cast<T>(1.0),\n                errors::InvalidArgument(\"iou_threshold must be in [0, 1]\"));\n    int num_boxes = 0;\n    ParseAndCheckBoxSizes(context, boxes, &num_boxes);\n    CheckScoreSizes(context, num_boxes, scores);\n    if (!context->status().ok()) {\n      return;\n    }\n    auto similarity_fn = CreateIOUSimilarityFn<T>(boxes);\n\n    const T score_threshold_val = std::numeric_limits<T>::lowest();\n    const T dummy_soft_nms_sigma = static_cast<T>(0.0);\n    DoNonMaxSuppressionOp<T>(context, scores, num_boxes, max_output_size,\n                             iou_threshold_val, score_threshold_val,\n                             dummy_soft_nms_sigma, similarity_fn);\n  }\n};\n\ntemplate <typename Device, typename T>\nclass NonMaxSuppressionV3Op : public OpKernel {\n public:\n  explicit NonMaxSuppressionV3Op(OpKernelConstruction* context)\n      : OpKernel(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    // boxes: [num_boxes, 4]\n    const Tensor& boxes = context->input(0);\n    // scores: [num_boxes]\n    const Tensor& scores = context->input(1);\n    // max_output_size: scalar\n    const Tensor& max_output_size = context->input(2);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsScalar(max_output_size.shape()),\n        errors::InvalidArgument(\"max_output_size must be 0-D, got shape \",\n                                max_output_size.shape().DebugString(),\n                                \" (Shape must be rank 0 but is \", \"rank \",\n                                max_output_size.dims(), \")\"));\n    // iou_threshold: scalar\n    const Tensor& iou_threshold = context->input(3);\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(iou_threshold.shape()),\n                errors::InvalidArgument(\"iou_threshold must be 0-D, got shape \",\n                                        iou_threshold.shape().DebugString(),\n                                        \" (Shape must be rank 0 but is rank \",\n                                        iou_threshold.dims(), \")\"));\n    const T iou_threshold_val = iou_threshold.scalar<T>()();\n    OP_REQUIRES(context,\n                iou_threshold_val >= static_cast<T>(0.0) &&\n                    iou_threshold_val <= static_cast<T>(1.0),\n                errors::InvalidArgument(\"iou_threshold must be in [0, 1]\"));\n    // score_threshold: scalar\n    const Tensor& score_threshold = context->input(4);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsScalar(score_threshold.shape()),\n        errors::InvalidArgument(\"score_threshold must be 0-D, got shape \",\n                                score_threshold.shape().DebugString()));\n    const T score_threshold_val = score_threshold.scalar<T>()();\n\n    int num_boxes = 0;\n    ParseAndCheckBoxSizes(context, boxes, &num_boxes);\n    CheckScoreSizes(context, num_boxes, scores);\n    if (!context->status().ok()) {\n      return;\n    }\n\n    auto similarity_fn = CreateIOUSimilarityFn<T>(boxes);\n\n    const T dummy_soft_nms_sigma = static_cast<T>(0.0);\n    DoNonMaxSuppressionOp<T>(context, scores, num_boxes, max_output_size,\n                             iou_threshold_val, score_threshold_val,\n                             dummy_soft_nms_sigma, similarity_fn);\n  }\n};\n\ntemplate <typename Device, typename T>\nclass NonMaxSuppressionV4Op : public OpKernel {\n public:\n  explicit NonMaxSuppressionV4Op(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"pad_to_max_output_size\",\n                                             &pad_to_max_output_size_));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    // boxes: [num_boxes, 4]\n    const Tensor& boxes = context->input(0);\n    // scores: [num_boxes]\n    const Tensor& scores = context->input(1);\n    // max_output_size: scalar\n    const Tensor& max_output_size = context->input(2);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsScalar(max_output_size.shape()),\n        errors::InvalidArgument(\"max_output_size must be 0-D, got shape \",\n                                max_output_size.shape().DebugString()));\n    // iou_threshold: scalar\n    const Tensor& iou_threshold = context->input(3);\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(iou_threshold.shape()),\n                errors::InvalidArgument(\"iou_threshold must be 0-D, got shape \",\n                                        iou_threshold.shape().DebugString()));\n    const T iou_threshold_val = iou_threshold.scalar<T>()();\n    OP_REQUIRES(context,\n                iou_threshold_val >= static_cast<T>(0.0) &&\n                    iou_threshold_val <= static_cast<T>(1.0),\n                errors::InvalidArgument(\"iou_threshold must be in [0, 1]\"));\n    // score_threshold: scalar\n    const Tensor& score_threshold = context->input(4);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsScalar(score_threshold.shape()),\n        errors::InvalidArgument(\"score_threshold must be 0-D, got shape \",\n                                score_threshold.shape().DebugString()));\n    const T score_threshold_val = score_threshold.scalar<T>()();\n\n    int num_boxes = 0;\n    ParseAndCheckBoxSizes(context, boxes, &num_boxes);\n    CheckScoreSizes(context, num_boxes, scores);\n    if (!context->status().ok()) {\n      return;\n    }\n\n    auto similarity_fn = CreateIOUSimilarityFn<T>(boxes);\n    int num_valid_outputs;\n\n    bool return_scores_tensor_ = false;\n    const T dummy_soft_nms_sigma = static_cast<T>(0.0);\n    DoNonMaxSuppressionOp<T>(\n        context, scores, num_boxes, max_output_size, iou_threshold_val,\n        score_threshold_val, dummy_soft_nms_sigma, similarity_fn,\n        return_scores_tensor_, pad_to_max_output_size_, &num_valid_outputs);\n\n    // Allocate scalar output tensor for number of indices computed.\n    Tensor* num_outputs_t = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(\n                                1, tensorflow::TensorShape{}, &num_outputs_t));\n    num_outputs_t->scalar<int32>().setConstant(num_valid_outputs);\n  }\n\n private:\n  bool pad_to_max_output_size_;\n};\n\ntemplate <typename Device, typename T>\nclass NonMaxSuppressionV5Op : public OpKernel {\n public:\n  explicit NonMaxSuppressionV5Op(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"pad_to_max_output_size\",\n                                             &pad_to_max_output_size_));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    // boxes: [num_boxes, 4]\n    const Tensor& boxes = context->input(0);\n    // scores: [num_boxes]\n    const Tensor& scores = context->input(1);\n    // max_output_size: scalar\n    const Tensor& max_output_size = context->input(2);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsScalar(max_output_size.shape()),\n        errors::InvalidArgument(\"max_output_size must be 0-D, got shape \",\n                                max_output_size.shape().DebugString()));\n    // iou_threshold: scalar\n    const Tensor& iou_threshold = context->input(3);\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(iou_threshold.shape()),\n                errors::InvalidArgument(\"iou_threshold must be 0-D, got shape \",\n                                        iou_threshold.shape().DebugString()));\n    const T iou_threshold_val = iou_threshold.scalar<T>()();\n    OP_REQUIRES(context,\n                iou_threshold_val >= static_cast<T>(0.0) &&\n                    iou_threshold_val <= static_cast<T>(1.0),\n                errors::InvalidArgument(\"iou_threshold must be in [0, 1]\"));\n    // score_threshold: scalar\n    const Tensor& score_threshold = context->input(4);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsScalar(score_threshold.shape()),\n        errors::InvalidArgument(\"score_threshold must be 0-D, got shape \",\n                                score_threshold.shape().DebugString()));\n    const T score_threshold_val = score_threshold.scalar<T>()();\n\n    // soft_nms_sigma: scalar\n    const Tensor& soft_nms_sigma = context->input(5);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsScalar(soft_nms_sigma.shape()),\n        errors::InvalidArgument(\"soft_nms_sigma must be 0-D, got shape \",\n                                soft_nms_sigma.shape().DebugString()));\n    const T soft_nms_sigma_val = soft_nms_sigma.scalar<T>()();\n    OP_REQUIRES(context, soft_nms_sigma_val >= static_cast<T>(0.0),\n                errors::InvalidArgument(\"soft_nms_sigma_val must be >= 0\"));\n\n    int num_boxes = 0;\n    ParseAndCheckBoxSizes(context, boxes, &num_boxes);\n    CheckScoreSizes(context, num_boxes, scores);\n    if (!context->status().ok()) {\n      return;\n    }\n\n    auto similarity_fn = CreateIOUSimilarityFn<T>(boxes);\n    int num_valid_outputs;\n\n    // For NonMaxSuppressionV5Op, we always return a second output holding\n    // corresponding scores, so `return_scores_tensor` should never be false.\n    const bool return_scores_tensor_ = true;\n    DoNonMaxSuppressionOp<T>(\n        context, scores, num_boxes, max_output_size, iou_threshold_val,\n        score_threshold_val, soft_nms_sigma_val, similarity_fn,\n        return_scores_tensor_, pad_to_max_output_size_, &num_valid_outputs);\n\n    // Allocate scalar output tensor for number of indices computed.\n    Tensor* num_outputs_t = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(\n                                2, tensorflow::TensorShape{}, &num_outputs_t));\n    num_outputs_t->scalar<int32>().setConstant(num_valid_outputs);\n  }\n\n private:\n  bool pad_to_max_output_size_;\n};\n\ntemplate <typename Device>\nclass NonMaxSuppressionWithOverlapsOp : public OpKernel {\n public:\n  explicit NonMaxSuppressionWithOverlapsOp(OpKernelConstruction* context)\n      : OpKernel(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    // overlaps: [num_boxes, num_boxes]\n    const Tensor& overlaps = context->input(0);\n    // scores: [num_boxes]\n    const Tensor& scores = context->input(1);\n    // max_output_size: scalar\n    const Tensor& max_output_size = context->input(2);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsScalar(max_output_size.shape()),\n        errors::InvalidArgument(\"max_output_size must be 0-D, got shape \",\n                                max_output_size.shape().DebugString()));\n    // overlap_threshold: scalar\n    const Tensor& overlap_threshold = context->input(3);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsScalar(overlap_threshold.shape()),\n        errors::InvalidArgument(\"overlap_threshold must be 0-D, got shape \",\n                                overlap_threshold.shape().DebugString()));\n    const float overlap_threshold_val = overlap_threshold.scalar<float>()();\n\n    // score_threshold: scalar\n    const Tensor& score_threshold = context->input(4);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsScalar(score_threshold.shape()),\n        errors::InvalidArgument(\"score_threshold must be 0-D, got shape \",\n                                score_threshold.shape().DebugString()));\n    const float score_threshold_val = score_threshold.scalar<float>()();\n\n    int num_boxes = 0;\n    ParseAndCheckOverlapSizes(context, overlaps, &num_boxes);\n    CheckScoreSizes(context, num_boxes, scores);\n    if (!context->status().ok()) {\n      return;\n    }\n    auto similarity_fn = CreateOverlapSimilarityFn<float>(overlaps);\n\n    const float dummy_soft_nms_sigma = static_cast<float>(0.0);\n    DoNonMaxSuppressionOp<float>(context, scores, num_boxes, max_output_size,\n                                 overlap_threshold_val, score_threshold_val,\n                                 dummy_soft_nms_sigma, similarity_fn);\n  }\n};\n\ntemplate <typename Device>\nclass CombinedNonMaxSuppressionOp : public OpKernel {\n public:\n  explicit CombinedNonMaxSuppressionOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"pad_per_class\", &pad_per_class_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"clip_boxes\", &clip_boxes_));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    // boxes: [batch_size, num_anchors, q, 4]\n    const Tensor& boxes = context->input(0);\n    // scores: [batch_size, num_anchors, num_classes]\n    const Tensor& scores = context->input(1);\n    OP_REQUIRES(\n        context, (boxes.dim_size(0) == scores.dim_size(0)),\n        errors::InvalidArgument(\"boxes and scores must have same batch size\"));\n\n    // max_output_size: scalar\n    const Tensor& max_output_size = context->input(2);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsScalar(max_output_size.shape()),\n        errors::InvalidArgument(\"max_size_per_class must be 0-D, got shape \",\n                                max_output_size.shape().DebugString()));\n    const int max_size_per_class = max_output_size.scalar<int>()();\n    OP_REQUIRES(context, max_size_per_class > 0,\n                errors::InvalidArgument(\"max_size_per_class must be positive\"));\n    // max_total_size: scalar\n    const Tensor& max_total_size = context->input(3);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsScalar(max_total_size.shape()),\n        errors::InvalidArgument(\"max_total_size must be 0-D, got shape \",\n                                max_total_size.shape().DebugString()));\n    const int max_total_size_per_batch = max_total_size.scalar<int>()();\n    OP_REQUIRES(context, max_total_size_per_batch > 0,\n                errors::InvalidArgument(\"max_total_size must be > 0\"));\n    // Throw warning when `max_total_size` is too large as it may cause OOM.\n    if (max_total_size_per_batch > pow(10, 6)) {\n      LOG(WARNING) << \"Detected a large value for `max_total_size`. This may \"\n                   << \"cause OOM error. (max_total_size: \"\n                   << max_total_size.scalar<int>()() << \")\";\n    }\n    // iou_threshold: scalar\n    const Tensor& iou_threshold = context->input(4);\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(iou_threshold.shape()),\n                errors::InvalidArgument(\"iou_threshold must be 0-D, got shape \",\n                                        iou_threshold.shape().DebugString()));\n    const float iou_threshold_val = iou_threshold.scalar<float>()();\n\n    // score_threshold: scalar\n    const Tensor& score_threshold = context->input(5);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsScalar(score_threshold.shape()),\n        errors::InvalidArgument(\"score_threshold must be 0-D, got shape \",\n                                score_threshold.shape().DebugString()));\n    const float score_threshold_val = score_threshold.scalar<float>()();\n\n    OP_REQUIRES(context, iou_threshold_val >= 0 && iou_threshold_val <= 1,\n                errors::InvalidArgument(\"iou_threshold must be in [0, 1]\"));\n    int num_boxes = 0;\n    const int num_classes = scores.dim_size(2);\n    ParseAndCheckCombinedNMSBoxSizes(context, boxes, &num_boxes, num_classes);\n    CheckCombinedNMSScoreSizes(context, num_boxes, scores);\n\n    if (!context->status().ok()) {\n      return;\n    }\n    BatchedNonMaxSuppressionOp(context, boxes, scores, num_boxes,\n                               max_size_per_class, max_total_size_per_batch,\n                               score_threshold_val, iou_threshold_val,\n                               pad_per_class_, clip_boxes_);\n  }\n\n private:\n  bool pad_per_class_;\n  bool clip_boxes_;\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"NonMaxSuppression\").Device(DEVICE_CPU),\n                        NonMaxSuppressionOp<CPUDevice>);\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"NonMaxSuppressionV2\").TypeConstraint<float>(\"T\").Device(DEVICE_CPU),\n    NonMaxSuppressionV2Op<CPUDevice, float>);\nREGISTER_KERNEL_BUILDER(Name(\"NonMaxSuppressionV2\")\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .Device(DEVICE_CPU),\n                        NonMaxSuppressionV2Op<CPUDevice, Eigen::half>);\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"NonMaxSuppressionV3\").TypeConstraint<float>(\"T\").Device(DEVICE_CPU),\n    NonMaxSuppressionV3Op<CPUDevice, float>);\nREGISTER_KERNEL_BUILDER(Name(\"NonMaxSuppressionV3\")\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .Device(DEVICE_CPU),\n                        NonMaxSuppressionV3Op<CPUDevice, Eigen::half>);\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"NonMaxSuppressionV4\").TypeConstraint<float>(\"T\").Device(DEVICE_CPU),\n    NonMaxSuppressionV4Op<CPUDevice, float>);\nREGISTER_KERNEL_BUILDER(Name(\"NonMaxSuppressionV4\")\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .Device(DEVICE_CPU),\n                        NonMaxSuppressionV4Op<CPUDevice, Eigen::half>);\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"NonMaxSuppressionV5\").TypeConstraint<float>(\"T\").Device(DEVICE_CPU),\n    NonMaxSuppressionV5Op<CPUDevice, float>);\nREGISTER_KERNEL_BUILDER(Name(\"NonMaxSuppressionV5\")\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .Device(DEVICE_CPU),\n                        NonMaxSuppressionV5Op<CPUDevice, Eigen::half>);\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"NonMaxSuppressionWithOverlaps\").Device(DEVICE_CPU),\n    NonMaxSuppressionWithOverlapsOp<CPUDevice>);\n\nREGISTER_KERNEL_BUILDER(Name(\"CombinedNonMaxSuppression\").Device(DEVICE_CPU),\n                        CombinedNonMaxSuppressionOp<CPUDevice>);\n\n}  // namespace tensorflow"