"# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#,============================================================================\n\"\"\"Tests for layer graphs construction & handling.\"\"\"\n\nimport warnings\n\nimport numpy as np\n\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.keras import backend\nfrom tensorflow.python.keras import combinations\nfrom tensorflow.python.keras import initializers\nfrom tensorflow.python.keras import keras_parameterized\nfrom tensorflow.python.keras import layers\nfrom tensorflow.python.keras import losses\nfrom tensorflow.python.keras import models\nfrom tensorflow.python.keras import testing_utils\nfrom tensorflow.python.keras.engine import base_layer\nfrom tensorflow.python.keras.engine import functional\nfrom tensorflow.python.keras.engine import input_layer as input_layer_lib\nfrom tensorflow.python.keras.engine import sequential\nfrom tensorflow.python.keras.engine import training as training_lib\nfrom tensorflow.python.keras.utils import layer_utils\nfrom tensorflow.python.keras.utils import tf_utils\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import string_ops\nfrom tensorflow.python.ops.ragged import ragged_factory_ops\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.training.tracking.util import Checkpoint\n\n\nclass NetworkConstructionTest(keras_parameterized.TestCase):\n\n  def test_default_model_name(self):\n    inputs = input_layer_lib.Input(shape=(1,))\n    outputs = layers.Dense(1, activation='relu')(inputs)\n    model = training_lib.Model(inputs=inputs, outputs=outputs)\n    self.assertEqual(model.name, 'model')\n\n    model_2 = training_lib.Model(inputs=inputs, outputs=outputs)\n    self.assertEqual(model_2.name, 'model_1')\n\n    model_3 = training_lib.Model(inputs=inputs, outputs=outputs)\n    self.assertEqual(model_3.name, 'model_2')\n\n  def test_get_updates(self):\n\n    class MyLayer(layers.Layer):\n\n      def build(self, input_shape):\n        self.a = self.add_variable('a',\n                                   (1, 1),\n                                   'float32',\n                                   trainable=False)\n        self.b = self.add_variable('b',\n                                   (1, 1),\n                                   'float32',\n                                   trainable=False)\n        self.add_update(state_ops.assign_add(self.a, [[1.]],\n                                             name='unconditional_update'))\n        self.built = True\n\n      def call(self, inputs):\n        self.add_update(state_ops.assign_add(self.b, inputs,\n                                             name='conditional_update'),\n                        inputs=True)\n        return inputs + 1\n\n    with ops.Graph().as_default():\n      x1 = input_layer_lib.Input(shape=(1,))\n      layer = MyLayer()\n      _ = layer(x1)\n\n      self.assertEqual(len(layer.updates), 2)\n\n      x2 = input_layer_lib.Input(shape=(1,))\n      y2 = layer(x2)\n\n      self.assertEqual(len(layer.updates), 3)\n\n      network = functional.Functional(x2, y2)\n      self.assertEqual(len(network.updates), 3)\n\n      x3 = input_layer_lib.Input(shape=(1,))\n      _ = layer(x3)\n      self.assertEqual(len(network.updates), 4)\n\n      x4 = input_layer_lib.Input(shape=(1,))\n      _ = network(x4)\n      self.assertEqual(len(network.updates), 5)\n\n      network.add_update(state_ops.assign_add(layer.a, [[1]]))\n      self.assertEqual(len(network.updates), 6)\n\n      network.add_update(state_ops.assign_add(layer.b, x4), inputs=True)\n      self.assertEqual(len(network.updates), 7)\n\n  @combinations.generate(combinations.combine(mode=['graph']))\n  def test_get_updates_bn(self):\n    x1 = input_layer_lib.Input(shape=(1,))\n    layer = layers.BatchNormalization()\n    _ = layer(x1)\n\n    self.assertEqual(len(layer.updates), 2)\n\n  def test_get_layer(self):\n    # create a simple network\n    x = input_layer_lib.Input(shape=(32,))\n    dense_a = layers.Dense(4, name='dense_a')\n    dense_b = layers.Dense(2, name='dense_b')\n    y = dense_b(dense_a(x))\n    network = functional.Functional(x, y, name='dense_network')\n\n    # test various get_layer by index\n    self.assertEqual(network.get_layer(index=1), dense_a)\n\n    # test invalid get_layer by index\n    with self.assertRaisesRegex(\n        ValueError, 'Was asked to retrieve layer at index ' + str(3) +\n        ' but model only has ' + str(len(network.layers)) + ' layers.'):\n      network.get_layer(index=3)\n\n    # test that only one between name and index is requested\n    with self.assertRaisesRegex(ValueError,\n                                'Provide only a layer name or a layer index'):\n      network.get_layer(index=1, name='dense_b')\n\n    # test that a name or an index must be provided\n    with self.assertRaisesRegex(ValueError,\n                                'Provide either a layer name or layer index.'):\n      network.get_layer()\n\n    # test various get_layer by name\n    self.assertEqual(network.get_layer(name='dense_a'), dense_a)\n\n    # test invalid get_layer by name\n    with self.assertRaisesRegex(ValueError, 'No such layer: dense_c.'):\n      network.get_layer(name='dense_c')\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testTopologicalAttributes(self):\n    # test layer attributes / methods related to cross-layer connectivity.\n    a = input_layer_lib.Input(shape=(32,), name='input_a')\n    b = input_layer_lib.Input(shape=(32,), name='input_b')\n\n    # test input, output, input_shape, output_shape\n    test_layer = layers.Dense(16, name='test_layer')\n    a_test = test_layer(a)\n    self.assertIs(test_layer.input, a)\n    self.assertIs(test_layer.output, a_test)\n    self.assertEqual(test_layer.input_shape, (None, 32))\n    self.assertEqual(test_layer.output_shape, (None, 16))\n\n    # test `get_*_at` methods\n    dense = layers.Dense(16, name='dense_1')\n    a_2 = dense(a)\n    b_2 = dense(b)\n\n    self.assertIs(dense.get_input_at(0), a)\n    self.assertIs(dense.get_input_at(1), b)\n    self.assertIs(dense.get_output_at(0), a_2)\n    self.assertIs(dense.get_output_at(1), b_2)\n    self.assertEqual(dense.get_input_shape_at(0), (None, 32))\n    self.assertEqual(dense.get_input_shape_at(1), (None, 32))\n    self.assertEqual(dense.get_output_shape_at(0), (None, 16))\n    self.assertEqual(dense.get_output_shape_at(1), (None, 16))\n\n    # Test invalid value for attribute retrieval.\n    with self.assertRaises(ValueError):\n      dense.get_input_at(2)\n    with self.assertRaises(AttributeError):\n      new_dense = layers.Dense(16)\n      _ = new_dense.input\n    with self.assertRaises(AttributeError):\n      new_dense = layers.Dense(16)\n      _ = new_dense.output\n    with self.assertRaises(AttributeError):\n      new_dense = layers.Dense(16)\n      _ = new_dense.output_shape\n    with self.assertRaises(AttributeError):\n      new_dense = layers.Dense(16)\n      _ = new_dense.input_shape\n    with self.assertRaises(AttributeError):\n      new_dense = layers.Dense(16)\n      a = input_layer_lib.Input(shape=(3, 32))\n      a = input_layer_lib.Input(shape=(5, 32))\n      a_2 = dense(a)\n      b_2 = dense(b)\n      _ = new_dense.input_shape\n    with self.assertRaises(AttributeError):\n      new_dense = layers.Dense(16)\n      a = input_layer_lib.Input(shape=(3, 32))\n      a = input_layer_lib.Input(shape=(5, 32))\n      a_2 = dense(a)\n      b_2 = dense(b)\n      _ = new_dense.output_shape\n\n  def _assertAllIs(self, a, b):\n    self.assertTrue(all(x is y for x, y in zip(a, b)))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testTopologicalAttributesMultiOutputLayer(self):\n\n    class PowersLayer(layers.Layer):\n\n      def call(self, inputs):\n        return [inputs**2, inputs**3]\n\n    x = input_layer_lib.Input(shape=(32,))\n    test_layer = PowersLayer()\n    p1, p2 = test_layer(x)  # pylint: disable=not-callable\n\n    self.assertIs(test_layer.input, x)\n    self._assertAllIs(test_layer.output, [p1, p2])\n    self.assertEqual(test_layer.input_shape, (None, 32))\n    self.assertEqual(test_layer.output_shape, [(None, 32), (None, 32)])\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testTopologicalAttributesMultiInputLayer(self):\n\n    class AddLayer(layers.Layer):\n\n      def call(self, inputs):\n        assert len(inputs) == 2\n        return inputs[0] + inputs[1]\n\n    a = input_layer_lib.Input(shape=(32,))\n    b = input_layer_lib.Input(shape=(32,))\n    test_layer = AddLayer()\n    y = test_layer([a, b])  # pylint: disable=not-callable\n\n    self._assertAllIs(test_layer.input, [a, b])\n    self.assertIs(test_layer.output, y)\n    self.assertEqual(test_layer.input_shape, [(None, 32), (None, 32)])\n    self.assertEqual(test_layer.output_shape, (None, 32))\n\n  def testBasicNetwork(self):\n    with ops.Graph().as_default():\n      # minimum viable network\n      x = input_layer_lib.Input(shape=(32,))\n      dense = layers.Dense(2)\n      y = dense(x)\n      network = functional.Functional(x, y, name='dense_network')\n\n      # test basic attributes\n      self.assertEqual(network.name, 'dense_network')\n      self.assertEqual(len(network.layers), 2)  # InputLayer + Dense\n      self.assertEqual(network.layers[1], dense)\n      self._assertAllIs(network.weights, dense.weights)\n      self._assertAllIs(network.trainable_weights, dense.trainable_weights)\n      self._assertAllIs(network.non_trainable_weights,\n                        dense.non_trainable_weights)\n\n      # test callability on Input\n      x_2 = input_layer_lib.Input(shape=(32,))\n      y_2 = network(x_2)\n      self.assertEqual(y_2.shape.as_list(), [None, 2])\n\n      # test callability on regular tensor\n      x_2 = array_ops.placeholder(dtype='float32', shape=(None, 32))\n      y_2 = network(x_2)\n      self.assertEqual(y_2.shape.as_list(), [None, 2])\n\n      # test network `trainable` attribute\n      network.trainable = False\n      self._assertAllIs(network.weights, dense.weights)\n      self.assertEqual(network.trainable_weights, [])\n      self._assertAllIs(network.non_trainable_weights,\n                        dense.trainable_weights + dense.non_trainable_weights)\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_trainable_weights(self):\n    a = layers.Input(shape=(2,))\n    b = layers.Dense(1)(a)\n    model = training_lib.Model(a, b)\n\n    weights = model.weights\n    self._assertAllIs(model.trainable_weights, weights)\n    self.assertListEqual(model.non_trainable_weights, [])\n\n    model.trainable = False\n    self.assertListEqual(model.trainable_weights, [])\n    self._assertAllIs(model.non_trainable_weights, weights)\n\n    model.trainable = True\n    self._assertAllIs(model.trainable_weights, weights)\n    self.assertListEqual(model.non_trainable_weights, [])\n\n    model.layers[1].trainable = False\n    self.assertListEqual(model.trainable_weights, [])\n    self._assertAllIs(model.non_trainable_weights, weights)\n\n    # sequential model\n    model = sequential.Sequential()\n    model.add(layers.Dense(1, input_dim=2))\n    weights = model.weights\n\n    self._assertAllIs(model.trainable_weights, weights)\n    self.assertListEqual(model.non_trainable_weights, [])\n\n    model.trainable = False\n    self.assertListEqual(model.trainable_weights, [])\n    self._assertAllIs(model.non_trainable_weights, weights)\n\n    model.trainable = True\n    self._assertAllIs(model.trainable_weights, weights)\n    self.assertListEqual(model.non_trainable_weights, [])\n\n    model.layers[0].trainable = False\n    self.assertListEqual(model.trainable_weights, [])\n    self._assertAllIs(model.non_trainable_weights, weights)\n\n  def test_layer_call_arguments(self):\n    with ops.Graph().as_default():\n      # Test the ability to pass and serialize arguments to `call`.\n      inp = layers.Input(shape=(2,))\n      x = layers.Dense(3)(inp)\n      x = layers.Dropout(0.5)(x, training=True)\n      model = training_lib.Model(inp, x)\n      # Would be `dropout/cond/Merge` by default\n      self.assertIn('dropout', model.output.op.name)\n\n      # Test that argument is kept when applying the model\n      inp2 = layers.Input(shape=(2,))\n      out2 = model(inp2)\n      self.assertIn('dropout', out2.op.name)\n\n      # Test that argument is kept after loading a model\n      config = model.get_config()\n      model = training_lib.Model.from_config(config)\n      self.assertIn('dropout', model.output.op.name)\n\n  def test_node_construction(self):\n    # test basics\n    a = layers.Input(shape=(32,), name='input_a')\n    b = layers.Input(shape=(32,), name='input_b')\n\n    with self.assertRaises(ValueError):\n      _ = layers.Input(shape=(32,), batch_shape=(10, 32))\n    with self.assertRaises(ValueError):\n      _ = layers.Input(shape=(32,), unknown_kwarg=None)\n\n    self.assertListEqual(a.shape.as_list(), [None, 32])\n    a_layer, a_node_index, a_tensor_index = a._keras_history\n    b_layer, _, _ = b._keras_history\n    self.assertEqual(len(a_layer._inbound_nodes), 1)\n    self.assertEqual(a_tensor_index, 0)\n    node = a_layer._inbound_nodes[a_node_index]\n    self.assertEqual(node.outbound_layer, a_layer)\n\n    self.assertListEqual(node.inbound_layers, [])\n    self.assertListEqual(node.input_tensors, [a])\n    self.assertListEqual(node.input_shapes, [(None, 32)])\n    self.assertListEqual(node.output_tensors, [a])\n    self.assertListEqual(node.output_shapes, [(None, 32)])\n\n    dense = layers.Dense(16, name='dense_1')\n    a_2 = dense(a)\n    b_2 = dense(b)\n\n    self.assertEqual(len(dense._inbound_nodes), 2)\n    self.assertEqual(len(dense._outbound_nodes), 0)\n    self.assertEqual(dense._inbound_nodes[0].inbound_layers, a_layer)\n    self.assertEqual(dense._inbound_nodes[0].outbound_layer, dense)\n    self.assertEqual(dense._inbound_nodes[1].inbound_layers, b_layer)\n    self.assertEqual(dense._inbound_nodes[1].outbound_layer, dense)\n    self.assertIs(dense._inbound_nodes[0].input_tensors, a)\n    self.assertIs(dense._inbound_nodes[1].input_tensors, b)\n\n    # test layer properties\n    test_layer = layers.Dense(16, name='test_layer')\n    a_test = test_layer(a)\n    self.assertListEqual(test_layer.kernel.shape.as_list(), [32, 16])\n    self.assertIs(test_layer.input, a)\n    self.assertIs(test_layer.output, a_test)\n    self.assertEqual(test_layer.input_shape, (None, 32))\n    self.assertEqual(test_layer.output_shape, (None, 16))\n\n    self.assertIs(dense.get_input_at(0), a)\n    self.assertIs(dense.get_input_at(1), b)\n    self.assertIs(dense.get_output_at(0), a_2)\n    self.assertIs(dense.get_output_at(1), b_2)\n    self.assertEqual(dense.get_input_shape_at(0), (None, 32))\n    self.assertEqual(dense.get_input_shape_at(1), (None, 32))\n    self.assertEqual(dense.get_output_shape_at(0), (None, 16))\n    self.assertEqual(dense.get_output_shape_at(1), (None, 16))\n    self.assertEqual(dense.get_input_mask_at(0), None)\n    self.assertEqual(dense.get_input_mask_at(1), None)\n    self.assertEqual(dense.get_output_mask_at(0), None)\n    self.assertEqual(dense.get_output_mask_at(1), None)\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_multi_input_layer(self):\n    with self.cached_session():\n      # test multi-input layer\n      a = layers.Input(shape=(32,), name='input_a')\n      b = layers.Input(shape=(32,), name='input_b')\n\n      dense = layers.Dense(16, name='dense_1')\n      a_2 = dense(a)\n      b_2 = dense(b)\n\n      merged = layers.concatenate([a_2, b_2], name='merge')\n      self.assertListEqual(merged.shape.as_list(), [None, 16 * 2])\n      merge_layer, merge_node_index, merge_tensor_index = merged._keras_history\n\n      self.assertEqual(merge_node_index, 0)\n      self.assertEqual(merge_tensor_index, 0)\n\n      self.assertEqual(len(merge_layer._inbound_nodes), 1)\n      self.assertEqual(len(merge_layer._outbound_nodes), 0)\n\n      self.assertEqual(len(merge_layer._inbound_nodes[0].input_tensors), 2)\n      self.assertEqual(len(merge_layer._inbound_nodes[0].inbound_layers), 2)\n\n      c = layers.Dense(64, name='dense_2')(merged)\n      d = layers.Dense(5, name='dense_3')(c)\n\n      model = training_lib.Model(inputs=[a, b], outputs=[c, d], name='model')\n      self.assertEqual(len(model.layers), 6)\n      output_shapes = model.compute_output_shape([(None, 32), (None, 32)])\n      self.assertListEqual(output_shapes[0].as_list(), [None, 64])\n      self.assertListEqual(output_shapes[1].as_list(), [None, 5])\n      self.assertListEqual(\n          model.compute_mask([a, b], [None, None]), [None, None])\n\n      # we don't check names of first 2 layers (inputs) because\n      # ordering of same-level layers is not fixed\n      self.assertListEqual([l.name for l in model.layers][2:],\n                           ['dense_1', 'merge', 'dense_2', 'dense_3'])\n      self.assertListEqual([l.name for l in model._input_layers],\n                           ['input_a', 'input_b'])\n      self.assertListEqual([l.name for l in model._output_layers],\n                           ['dense_2', 'dense_3'])\n\n      # actually run model\n      fn = backend.function(model.inputs, model.outputs)\n      input_a_np = np.random.random((10, 32))\n      input_b_np = np.random.random((10, 32))\n      fn_outputs = fn([input_a_np, input_b_np])\n      self.assertListEqual([x.shape for x in fn_outputs], [(10, 64), (10, 5)])\n\n      # test get_source_inputs\n      self._assertAllIs(layer_utils.get_source_inputs(c), [a, b])\n\n      # serialization / deserialization\n      json_config = model.to_json()\n      recreated_model = models.model_from_json(json_config)\n      recreated_model.compile('rmsprop', 'mse')\n\n      self.assertListEqual([l.name for l in recreated_model.layers][2:],\n                           ['dense_1', 'merge', 'dense_2', 'dense_3'])\n      self.assertListEqual([l.name for l in recreated_model._input_layers],\n                           ['input_a', 'input_b'])\n      self.assertListEqual([l.name for l in recreated_model._output_layers],\n                           ['dense_2', 'dense_3'])\n\n      fn = backend.function(recreated_model.inputs, recreated_model.outputs)\n      input_a_np = np.random.random((10, 32))\n      input_b_np = np.random.random((10, 32))\n      fn_outputs = fn([input_a_np, input_b_np])\n      self.assertListEqual([x.shape for x in fn_outputs], [(10, 64), (10, 5)])\n\n  def test_multi_output_layer_output_names(self):\n    inp = layers.Input(name='inp', shape=(None,), dtype=dtypes.float32)\n\n    class _MultiOutput(layers.Layer):\n\n      def call(self, x):\n        return x + 1., x + 2.\n\n    out = _MultiOutput(name='out')(inp)\n    model = training_lib.Model(inp, out)\n    self.assertEqual(['out', 'out_1'], model.output_names)\n    self.assertAllClose([2., 3.], model(1.))\n\n  def test_recursion(self):\n    with ops.Graph().as_default(), self.cached_session():\n      a = layers.Input(shape=(32,), name='input_a')\n      b = layers.Input(shape=(32,), name='input_b')\n\n      dense = layers.Dense(16, name='dense_1')\n      a_2 = dense(a)\n      b_2 = dense(b)\n      merged = layers.concatenate([a_2, b_2], name='merge')\n      c = layers.Dense(64, name='dense_2')(merged)\n      d = layers.Dense(5, name='dense_3')(c)\n\n      model = training_lib.Model(inputs=[a, b], outputs=[c, d], name='model')\n\n      e = layers.Input(shape=(32,), name='input_e')\n      f = layers.Input(shape=(32,), name='input_f')\n      self.assertEqual(len(model.inputs), 2)\n      g, h = model([e, f])\n      self.assertEqual(len(model.inputs), 2)\n      self.assertEqual(g.name, 'model/dense_2/BiasAdd:0')\n\n      self.assertListEqual(g.shape.as_list(), c.shape.as_list())\n      self.assertListEqual(h.shape.as_list(), d.shape.as_list())\n\n      # test separate manipulation of different layer outputs\n      i = layers.Dense(7, name='dense_4')(h)\n\n      final_model = training_lib.Model(\n          inputs=[e, f], outputs=[i, g], name='final')\n      self.assertEqual(len(final_model.inputs), 2)\n      self.assertEqual(len(final_model.outputs), 2)\n      self.assertEqual(len(final_model.layers), 4)\n\n      # we don't check names of first 2 layers (inputs) because\n      # ordering of same-level layers is not fixed\n      self.assertListEqual([layer.name for layer in final_model.layers][2:],\n                           ['model', 'dense_4'])\n      self.assertListEqual(\n          model.compute_mask([e, f], [None, None]), [None, None])\n      self.assertListEqual(\n          final_model.compute_output_shape([(10, 32), (10, 32)]), [(10, 7),\n                                                                   (10, 64)])\n\n      # run recursive model\n      fn = backend.function(final_model.inputs, final_model.outputs)\n      input_a_np = np.random.random((10, 32))\n      input_b_np = np.random.random((10, 32))\n      fn_outputs = fn([input_a_np, input_b_np])\n      self.assertListEqual([x.shape for x in fn_outputs], [(10, 7), (10, 64)])\n\n      # test serialization\n      model_config = final_model.get_config()\n      recreated_model = models.Model.from_config(model_config)\n\n      fn = backend.function(recreated_model.inputs, recreated_model.outputs)\n      input_a_np = np.random.random((10, 32))\n      input_b_np = np.random.random((10, 32))\n      fn_outputs = fn([input_a_np, input_b_np])\n      self.assertListEqual([x.shape for x in fn_outputs], [(10, 7), (10, 64)])\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_multi_input_multi_output_recursion(self):\n    with self.cached_session():\n      # test multi-input multi-output\n      a = layers.Input(shape=(32,), name='input_a')\n      b = layers.Input(shape=(32,), name='input_b')\n\n      dense = layers.Dense(16, name='dense_1')\n      a_2 = dense(a)\n      b_2 = dense(b)\n      merged = layers.concatenate([a_2, b_2], name='merge')\n      c = layers.Dense(64, name='dense_2')(merged)\n      d = layers.Dense(5, name='dense_3')(c)\n\n      model = training_lib.Model(inputs=[a, b], outputs=[c, d], name='model')\n\n      j = layers.Input(shape=(32,), name='input_j')\n      k = layers.Input(shape=(32,), name='input_k')\n      _, n = model([j, k])\n\n      o = layers.Input(shape=(32,), name='input_o')\n      p = layers.Input(shape=(32,), name='input_p')\n      q, _ = model([o, p])\n\n      self.assertListEqual(n.shape.as_list(), [None, 5])\n      self.assertListEqual(q.shape.as_list(), [None, 64])\n      s = layers.concatenate([n, q], name='merge_nq')\n      self.assertListEqual(s.shape.as_list(), [None, 64 + 5])\n\n      # test with single output as 1-elem list\n      multi_io_model = training_lib.Model([j, k, o, p], [s])\n\n      fn = backend.function(multi_io_model.inputs, multi_io_model.outputs)\n      fn_outputs = fn([\n          np.random.random((10, 32)), np.random.random((10, 32)),\n          np.random.random((10, 32)), np.random.random((10, 32))\n      ])\n      self.assertListEqual([x.shape for x in fn_outputs], [(10, 69)])\n\n      # test with single output as tensor\n      multi_io_model = training_lib.Model([j, k, o, p], s)\n\n      fn = backend.function(multi_io_model.inputs, multi_io_model.outputs)\n      fn_outputs = fn([\n          np.random.random((10, 32)), np.random.random((10, 32)),\n          np.random.random((10, 32)), np.random.random((10, 32))\n      ])\n      # note that the output of the function will still be a 1-elem list\n      self.assertListEqual([x.shape for x in fn_outputs], [(10, 69)])\n\n      # test serialization\n      model_config = multi_io_model.get_config()\n      recreated_model = models.Model.from_config(model_config)\n\n      fn = backend.function(recreated_model.inputs, recreated_model.outputs)\n      fn_outputs = fn([\n          np.random.random((10, 32)), np.random.random((10, 32)),\n          np.random.random((10, 32)), np.random.random((10, 32))\n      ])\n      # note that the output of the function will still be a 1-elem list\n      self.assertListEqual([x.shape for x in fn_outputs], [(10, 69)])\n\n      config = model.get_config()\n      models.Model.from_config(config)\n\n      model.summary()\n      json_str = model.to_json()\n      models.model_from_json(json_str)\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_invalid_graphs(self):\n    a = layers.Input(shape=(32,), name='input_a')\n    b = layers.Input(shape=(32,), name='input_b')\n\n    dense = layers.Dense(16, name='dense_1')\n    a_2 = dense(a)\n    b_2 = dense(b)\n    merged = layers.concatenate([a_2, b_2], name='merge')\n    c = layers.Dense(64, name='dense_2')(merged)\n    d = layers.Dense(5, name='dense_3')(c)\n\n    model = training_lib.Model(inputs=[a, b], outputs=[c, d], name='model')\n\n    # input is not an Input tensor\n    j = layers.Input(shape=(32,), name='input_j')\n    j = layers.Dense(32)(j)\n    k = layers.Input(shape=(32,), name='input_k')\n    m, n = model([j, k])\n\n    with self.assertRaises(Exception):\n      training_lib.Model([j, k], [m, n])\n\n    # disconnected graph\n    j = layers.Input(shape=(32,), name='input_j')\n    k = layers.Input(shape=(32,), name='input_k')\n    m, n = model([j, k])\n    with self.assertRaises(Exception):\n      training_lib.Model([j], [m, n])\n\n    # redundant outputs\n    j = layers.Input(shape=(32,), name='input_j')\n    k = layers.Input(shape=(32,), name='input_k')\n    m, n = model([j, k])\n\n    training_lib.Model([j, k], [m, n, n])\n\n    # redundant inputs\n    j = layers.Input(shape=(32,), name='input_j')\n    k = layers.Input(shape=(32,), name='input_k')\n    m, n = model([j, k])\n    with self.assertRaises(Exception):\n      training_lib.Model([j, k, j], [m, n])\n\n    # i have not idea what I'm doing: garbage as inputs/outputs\n    j = layers.Input(shape=(32,), name='input_j')\n    k = layers.Input(shape=(32,), name='input_k')\n    m, n = model([j, k])\n    with self.assertRaises(Exception):\n      training_lib.Model([j, k], [m, n, 0])\n\n  def test_raw_tf_compatibility(self):\n    with ops.Graph().as_default():\n      # test calling layers/models on TF tensors\n      a = layers.Input(shape=(32,), name='input_a')\n      b = layers.Input(shape=(32,), name='input_b')\n\n      dense = layers.Dense(16, name='dense_1')\n      a_2 = dense(a)\n      b_2 = dense(b)\n      merged = layers.concatenate([a_2, b_2], name='merge')\n      c = layers.Dense(64, name='dense_2')(merged)\n      d = layers.Dense(5, name='dense_3')(c)\n\n      model = training_lib.Model(inputs=[a, b], outputs=[c, d], name='model')\n\n      j = layers.Input(shape=(32,), name='input_j')\n      k = layers.Input(shape=(32,), name='input_k')\n      self.assertEqual(len(model.inputs), 2)\n      m, n = model([j, k])\n      self.assertEqual(len(model.inputs), 2)\n      tf_model = training_lib.Model([j, k], [m, n])\n\n      j_tf = array_ops.placeholder(dtype=dtypes.float32, shape=(None, 32))\n      k_tf = array_ops.placeholder(dtype=dtypes.float32, shape=(None, 32))\n      m_tf, n_tf = tf_model([j_tf, k_tf])\n      self.assertListEqual(m_tf.shape.as_list(), [None, 64])\n      self.assertListEqual(n_tf.shape.as_list(), [None, 5])\n\n      # test merge\n      layers.concatenate([j_tf, k_tf], axis=1)\n      layers.add([j_tf, k_tf])\n\n      # test tensor input\n      x = array_ops.placeholder(shape=(None, 2), dtype=dtypes.float32)\n      layers.InputLayer(input_tensor=x)\n\n      x = layers.Input(tensor=x)\n      layers.Dense(2)(x)\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_basic_masking(self):\n    a = layers.Input(shape=(10, 32), name='input_a')\n    b = layers.Masking()(a)\n    model = training_lib.Model(a, b)\n    self.assertEqual(model.output_mask.shape.as_list(), [None, 10])\n\n  def testMaskingSingleInput(self):\n\n    class MaskedLayer(layers.Layer):\n\n      def call(self, inputs, mask=None):\n        if mask is not None:\n          return inputs * mask\n        return inputs\n\n      def compute_mask(self, inputs, mask=None):\n        return array_ops.ones_like(inputs)\n\n    if context.executing_eagerly():\n      a = constant_op.constant([2] * 32)\n      mask = constant_op.constant([0, 1] * 16)\n      a._keras_mask = mask\n      b = MaskedLayer().apply(a)\n      self.assertTrue(hasattr(b, '_keras_mask'))\n      self.assertAllEqual(\n          self.evaluate(array_ops.ones_like(mask)),\n          self.evaluate(getattr(b, '_keras_mask')))\n      self.assertAllEqual(self.evaluate(a * mask), self.evaluate(b))\n    else:\n      x = input_layer_lib.Input(shape=(32,))\n      y = MaskedLayer()(x)  # pylint: disable=not-callable\n      network = functional.Functional(x, y)\n\n      # test callability on Input\n      x_2 = input_layer_lib.Input(shape=(32,))\n      y_2 = network(x_2)\n      self.assertEqual(y_2.shape.as_list(), [None, 32])\n\n      # test callability on regular tensor\n      x_2 = array_ops.placeholder(dtype='float32', shape=(None, 32))\n      y_2 = network(x_2)\n      self.assertEqual(y_2.shape.as_list(), [None, 32])\n\n  def test_activity_regularization_with_model_composition(self):\n\n    def reg(x):\n      return math_ops.reduce_sum(x)\n\n    net_a_input = input_layer_lib.Input((2,))\n    net_a = net_a_input\n    net_a = layers.Dense(\n        2, kernel_initializer='ones', use_bias=False, activity_regularizer=reg)(\n            net_a)\n    model_a = training_lib.Model([net_a_input], [net_a])\n\n    net_b_input = input_layer_lib.Input((2,))\n    net_b = model_a(net_b_input)\n    model_b = training_lib.Model([net_b_input], [net_b])\n\n    model_b.compile(optimizer='sgd', loss=None)\n    x = np.ones((1, 2))\n    loss = model_b.evaluate(x)\n    self.assertEqual(loss, 4.)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_layer_sharing_at_heterogenous_depth(self):\n    x_val = np.random.random((10, 5))\n\n    x = input_layer_lib.Input(shape=(5,))\n    a = layers.Dense(5, name='A')\n    b = layers.Dense(5, name='B')\n    output = a(b(a(b(x))))\n    m = training_lib.Model(x, output)\n    m.run_eagerly = testing_utils.should_run_eagerly()\n\n    output_val = m.predict(x_val)\n\n    config = m.get_config()\n    weights = m.get_weights()\n\n    m2 = models.Model.from_config(config)\n    m2.set_weights(weights)\n\n    output_val_2 = m2.predict(x_val)\n    self.assertAllClose(output_val, output_val_2, atol=1e-6)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_layer_sharing_at_heterogenous_depth_with_concat(self):\n    input_shape = (16, 9, 3)\n    input_layer = input_layer_lib.Input(shape=input_shape)\n\n    a = layers.Dense(3, name='dense_A')\n    b = layers.Dense(3, name='dense_B')\n    c = layers.Dense(3, name='dense_C')\n\n    x1 = b(a(input_layer))\n    x2 = a(c(input_layer))\n    output = layers.concatenate([x1, x2])\n\n    m = training_lib.Model(inputs=input_layer, outputs=output)\n    m.run_eagerly = testing_utils.should_run_eagerly()\n\n    x_val = np.random.random((10, 16, 9, 3))\n    output_val = m.predict(x_val)\n\n    config = m.get_config()\n    weights = m.get_weights()\n\n    m2 = models.Model.from_config(config)\n    m2.set_weights(weights)\n\n    output_val_2 = m2.predict(x_val)\n    self.assertAllClose(output_val, output_val_2, atol=1e-6)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_explicit_training_argument(self):\n    a = layers.Input(shape=(2,))\n    b = layers.Dropout(0.5)(a)\n    base_model = training_lib.Model(a, b)\n\n    a = layers.Input(shape=(2,))\n    b = base_model(a, training=False)\n    model = training_lib.Model(a, b)\n\n    x = np.ones((100, 2))\n    y = np.ones((100, 2))\n    model.compile(\n        optimizer='sgd',\n        loss='mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    loss = model.train_on_batch(x, y)\n    self.assertEqual(loss, 0)  # In inference mode, output is equal to input.\n\n    a = layers.Input(shape=(2,))\n    b = base_model(a, training=True)\n    model = training_lib.Model(a, b)\n    preds = model.predict(x)\n    self.assertEqual(np.min(preds), 0.)  # At least one unit was dropped.\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_mask_derived_from_keras_layer(self):\n    inputs = input_layer_lib.Input((5, 10))\n    mask = input_layer_lib.Input((5,))\n    outputs = layers.RNN(layers.LSTMCell(100))(inputs, mask=mask)\n    model = training_lib.Model([inputs, mask], outputs)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[np.ones((10, 5, 10)), np.zeros((10, 5))],\n        y=np.zeros((10, 100)),\n        batch_size=2)\n    # All data is masked, returned values are 0's.\n    self.assertEqual(history.history['loss'][0], 0.0)\n    history = model.fit(\n        x=[np.ones((10, 5, 10)), np.ones((10, 5))],\n        y=np.zeros((10, 100)),\n        batch_size=2)\n    # Data is not masked, returned values are random.\n    self.assertGreater(history.history['loss'][0], 0.0)\n\n    model = training_lib.Model.from_config(model.get_config())\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[np.ones((10, 5, 10)), np.zeros((10, 5))],\n        y=np.zeros((10, 100)),\n        batch_size=2)\n    # All data is masked, returned values are 0's.\n    self.assertEqual(history.history['loss'][0], 0.0)\n    history = model.fit(\n        x=[np.ones((10, 5, 10)), np.ones((10, 5))],\n        y=np.zeros((10, 100)),\n        batch_size=2)\n    # Data is not masked, returned values are random.\n    self.assertGreater(history.history['loss'][0], 0.0)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_call_arg_derived_from_keras_layer(self):\n\n    class MyAdd(layers.Layer):\n\n      def call(self, x1, x2):\n        return x1 + x2\n\n    input1 = input_layer_lib.Input(10)\n    input2 = input_layer_lib.Input(10)\n    outputs = MyAdd()(input1, input2)\n    model = training_lib.Model([input1, input2], outputs)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],\n        y=10 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    # Check serialization.\n    model = training_lib.Model.from_config(\n        model.get_config(), custom_objects={'MyAdd': MyAdd})\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],\n        y=10 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n  @combinations.generate(combinations.keras_mode_combinations(mode='eager'),)\n  def test_only_some_in_first_arg_derived_from_keras_layer_keras_tensors(self):\n    # This functionality is unsupported in v1 graphs\n\n    class MyAddAll(layers.Layer):\n\n      def call(self, inputs):\n        x = inputs[0]\n        for inp in inputs[1:]:\n          if inp is not None:\n            x = x + inp\n        return x\n\n    input1 = input_layer_lib.Input(10)\n    input2 = input_layer_lib.Input(10)\n    layer = MyAddAll()\n    outputs = layer([0.0, input1, None, input2, None])\n    model = training_lib.Model([input1, input2], outputs)\n    self.assertIn(layer, model.layers)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],\n        y=10 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    # Check serialization.\n    model = training_lib.Model.from_config(\n        model.get_config(), custom_objects={'MyAddAll': MyAddAll})\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],\n        y=10 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n  @combinations.generate(\n      combinations.times(\n          combinations.keras_mode_combinations(),\n          combinations.combine(share_already_used_layer=[True, False])))\n  def test_call_kwarg_derived_from_keras_layer(self, share_already_used_layer):\n\n    class MaybeAdd(layers.Layer):\n\n      def call(self, x1, x2=None):\n        if x2 is not None:\n          return x1 + x2\n        return x1\n\n    class IdentityLayer(layers.Layer):\n\n      def call(self, x):\n        return x\n\n    input1 = input_layer_lib.Input(10)\n    input2 = input_layer_lib.Input(10)\n    identity_layer = IdentityLayer()\n\n    if share_already_used_layer:\n      # We have had model serialization/deserialization break in the past:\n      # when a layer was previously used to construct other functional models\n      # and had a non-empty list of inbound nodes before being used to define\n      # the model being serialized/deserialized.\n      # (The serialization/deserialization was not correctly adjusting\n      # the node_index serialization/deserialization).\n      # So, we explicitly test this case.\n      training_lib.Model([input1], identity_layer(input1))\n\n    outputs = MaybeAdd()(input1, x2=identity_layer(input2))\n    model = training_lib.Model([input1, input2], outputs)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],\n        y=10 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    model = training_lib.Model.from_config(\n        model.get_config(),\n        custom_objects={\n            'MaybeAdd': MaybeAdd,\n            'IdentityLayer': IdentityLayer\n        })\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],\n        y=10 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_call_kwarg_dtype_serialization(self):\n\n    class Double(layers.Layer):\n\n      def call(self, x1, dtype=None):\n        return math_ops.cast(x1 + x1, dtype=dtype)\n\n    input1 = input_layer_lib.Input(10)\n    outputs = Double()(input1, dtype=dtypes.float16)\n    model = training_lib.Model([input1], outputs)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10))],\n        y=6 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that input was correctly doubled.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    # Check the output dtype\n    self.assertEqual(model(array_ops.ones((3, 10))).dtype, dtypes.float16)\n\n    model = training_lib.Model.from_config(\n        model.get_config(), custom_objects={'Double': Double})\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10))],\n        y=6 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that input was correctly doubled.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    # Check the output dtype\n    self.assertEqual(model(array_ops.ones((3, 10))).dtype, dtypes.float16)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_call_kwarg_nonserializable(self):\n\n    class Double(layers.Layer):\n\n      def call(self, x1, kwarg=None):\n        return x1 + x1\n\n    class NonSerializable(object):\n\n      def __init__(self, foo=None):\n        self.foo = foo\n\n    input1 = input_layer_lib.Input(10)\n    outputs = Double()(input1, kwarg=NonSerializable())\n    model = training_lib.Model([input1], outputs)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10))],\n        y=6 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that input was correctly doubled.\n    self.assertEqual(history.history['loss'][0], 0.0)\n    with self.assertRaisesRegex(\n        TypeError, 'Layer double was passed non-JSON-serializable arguments.'):\n      model.get_config()\n\n  @combinations.generate(\n      combinations.times(\n          combinations.keras_mode_combinations(),\n          combinations.combine(share_already_used_layer=[True, False])))\n  def test_call_kwarg_derived_from_keras_layer_and_first_arg_is_constant(\n      self, share_already_used_layer):\n\n    class IdentityLayer(layers.Layer):\n\n      def call(self, x):\n        return x\n\n    class MaybeAdd(layers.Layer):\n\n      def call(self, x1, x2=None):\n        if x2 is not None:\n          return x1 + x2\n        return x1\n\n    input2 = input_layer_lib.Input(10)\n    identity_layer = IdentityLayer()\n    if share_already_used_layer:\n      # We have had model serialization/deserialization break in the past:\n      # when a layer was previously used to construct other functional models\n      # and had a non-empty list of inbound nodes before being used to define\n      # the model being serialized/deserialized.\n      # (The serialization/deserialization was not correctly adjusting\n      # the node_index serialization/deserialization).\n      # So, we explicitly test this case.\n      training_lib.Model([input2], identity_layer(input2))\n\n    outputs = MaybeAdd()(3., x2=identity_layer(input2))\n    model = training_lib.Model([input2], outputs)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=7 * np.ones((10, 10)),\n        y=10 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    model = training_lib.Model.from_config(\n        model.get_config(),\n        custom_objects={\n            'MaybeAdd': MaybeAdd,\n            'IdentityLayer': IdentityLayer\n        })\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=7 * np.ones((10, 10)),\n        y=10 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_composite_call_kwarg_derived_from_keras_layer(self):\n\n    # Create a test layer that accepts composite tensor inputs.\n    class MaybeAdd(layers.Layer):\n\n      def call(self, x1, x2=None):\n        # We need to convert this to a tensor for loss calculations -\n        # losses don't play nicely with ragged tensors yet.\n        if x2 is not None:\n          return (x1 + x2).to_tensor(default_value=0)\n        return x1.to_tensor(default_value=0)\n\n    input1 = input_layer_lib.Input((None,), ragged=True)\n    input2 = input_layer_lib.Input((None,), ragged=True)\n    outputs = MaybeAdd()(input1, x2=input2)\n    model = training_lib.Model([input1, input2], outputs)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    input_data = [\n        ragged_factory_ops.constant([[3.0, 3.0], [3.0, 3.0], [3.0]]),\n        ragged_factory_ops.constant([[7.0, 7.0], [7.0, 7.0], [7.0]])\n    ]\n    expected_data = np.array([[10.0, 10.0], [10.0, 10.0], [10.0, 0.0]])\n\n    history = model.fit(x=input_data, y=expected_data)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    model = training_lib.Model.from_config(\n        model.get_config(), custom_objects={'MaybeAdd': MaybeAdd})\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(x=input_data, y=expected_data)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n  @combinations.generate(combinations.keras_mode_combinations(mode='eager'))\n  def test_call_some_not_all_nested_in_first_arg_derived_from_keras_layer(self):\n    # This functionality is unsupported in v1 graphs\n\n    class AddAll(layers.Layer):\n\n      def call(self, x1_x2, x3):\n        x1, x2 = x1_x2\n        out = x1 + x2\n        if x3 is not None:\n          for t in x3.values():\n            out += t\n        return out\n\n    input1 = input_layer_lib.Input(10)\n    input2 = input_layer_lib.Input(10)\n    input3 = input_layer_lib.Input(10)\n\n    layer = AddAll()\n    outputs = layer(\n        [input1, 4 * array_ops.ones((1, 10))],\n        x3={\n            'a': input2,\n            'b': input3,\n            'c': 5 * array_ops.ones((1, 10))\n        })\n    model = training_lib.Model([input1, input2, input3], outputs)\n    self.assertIn(layer, model.layers)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[np.ones((10, 10)), 2 * np.ones((10, 10)), 3 * np.ones((10, 10))],\n        y=15 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that all inputs were correctly added.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    model = training_lib.Model.from_config(\n        model.get_config(), custom_objects={'AddAll': AddAll})\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[np.ones((10, 10)), 2 * np.ones((10, 10)), 3 * np.ones((10, 10))],\n        y=15 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that all inputs were correctly added.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_call_nested_arg_derived_from_keras_layer(self):\n\n    class AddAll(layers.Layer):\n\n      def call(self, x1, x2, x3=None):\n        out = x1 + x2\n        if x3 is not None:\n          for t in x3.values():\n            out += t\n        return out\n\n    input1 = input_layer_lib.Input(10)\n    input2 = input_layer_lib.Input(10)\n    input3 = input_layer_lib.Input(10)\n    outputs = AddAll()(\n        input1,\n        4 * array_ops.ones((1, 10)),\n        x3={\n            'a': input2,\n            'b': input3,\n            'c': 5 * array_ops.ones((1, 10))\n        })\n    model = training_lib.Model([input1, input2, input3], outputs)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[np.ones((10, 10)), 2 * np.ones((10, 10)), 3 * np.ones((10, 10))],\n        y=15 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that all inputs were correctly added.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    model = training_lib.Model.from_config(\n        model.get_config(), custom_objects={'AddAll': AddAll})\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[np.ones((10, 10)), 2 * np.ones((10, 10)), 3 * np.ones((10, 10))],\n        y=15 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that all inputs were correctly added.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_multi_output_model_with_none_masking(self):\n    def func(x):\n      return [x * 0.2, x * 0.3]\n\n    def output_shape(input_shape):\n      return [input_shape, input_shape]\n\n    i = layers.Input(shape=(3, 2, 1))\n    o = layers.Lambda(function=func, output_shape=output_shape)(i)\n\n    self.assertEqual(backend.int_shape(o[0]), (None, 3, 2, 1))\n    self.assertEqual(backend.int_shape(o[1]), (None, 3, 2, 1))\n\n    o = layers.add(o)\n    model = training_lib.Model(i, o)\n    model.run_eagerly = testing_utils.should_run_eagerly()\n\n    i2 = layers.Input(shape=(3, 2, 1))\n    o2 = model(i2)\n    model2 = training_lib.Model(i2, o2)\n    model2.run_eagerly = testing_utils.should_run_eagerly()\n\n    x = np.random.random((4, 3, 2, 1))\n    out = model2.predict(x)\n    assert out.shape == (4, 3, 2, 1)\n    self.assertAllClose(out, x * 0.2 + x * 0.3, atol=1e-4)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_constant_initializer_with_numpy(self):\n    initializer = initializers.Constant(np.ones((3, 2)))\n    model = sequential.Sequential()\n    model.add(layers.Dense(2, input_shape=(3,), kernel_initializer=initializer))\n    model.add(layers.Dense(3))\n    model.compile(\n        loss='mse',\n        optimizer='sgd',\n        metrics=['acc'],\n        run_eagerly=testing_utils.should_run_eagerly())\n\n    json_str = model.to_json()\n    models.model_from_json(json_str)\n\n  def test_subclassed_error_if_init_not_called(self):\n\n    class MyNetwork(training_lib.Model):\n\n      def __init__(self):\n        self._foo = [layers.Dense(10), layers.Dense(10)]\n\n    with self.assertRaisesRegex(RuntimeError, 'forgot to call'):\n      MyNetwork()\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_int_input_shape(self):\n    inputs = input_layer_lib.Input(10)\n    self.assertEqual([None, 10], inputs.shape.as_list())\n\n    inputs_with_batch = input_layer_lib.Input(batch_size=20, shape=5)\n    self.assertEqual([20, 5], inputs_with_batch.shape.as_list())\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_model_initialization(self):\n    # Functional model\n    inputs = input_layer_lib.Input(shape=(32,))\n    outputs = layers.Dense(4)(inputs)\n\n    with self.assertRaisesRegex(TypeError,\n                                'Keyword argument not understood'):\n      model = training_lib.Model(\n          inputs, outputs, name='m', trainable=False, dtype='int64')\n    with self.assertRaisesRegex(TypeError,\n                                'Keyword argument not understood'):\n      model = training_lib.Model(\n          inputs, outputs, name='m', trainable=False, dynamic=False)\n\n    model = training_lib.Model(inputs, outputs, name='m', trainable=False)\n    self.assertEqual('m', model.name)\n    self.assertFalse(model.trainable)\n    self.assertFalse(model.dynamic)\n\n    class SubclassModel(training_lib.Model):\n      pass\n    # Subclassed model\n    model = SubclassModel(\n        name='subclassed', trainable=True, dtype='int64', dynamic=True)\n    self.assertEqual('subclassed', model.name)\n    self.assertTrue(model.dynamic)\n    self.assertTrue(model.trainable)\n    w = model.add_weight('w', [], initializer=initializers.Constant(1))\n    self.assertEqual(dtypes.int64, w.dtype)\n\n  def test_disconnected_inputs(self):\n    input_tensor1 = input_layer_lib.Input(shape=[200], name='a')\n    input_tensor2 = input_layer_lib.Input(shape=[10], name='b')\n    output_tensor1 = layers.Dense(units=10)(input_tensor1)\n\n    net = functional.Functional(\n        inputs=[input_tensor1, input_tensor2], outputs=[output_tensor1])\n    net2 = functional.Functional.from_config(net.get_config())\n    self.assertLen(net2.inputs, 2)\n    self.assertEqual('a', net2.layers[0].name)\n    self.assertEqual('b', net2.layers[1].name)\n\n  @combinations.generate(combinations.keras_model_type_combinations())\n  def test_dependency_tracking(self):\n    model = testing_utils.get_small_mlp(1, 4, input_dim=3)\n    model.trackable = Checkpoint()\n    self.assertIn('trackable', model._unconditional_dependency_names)\n    self.assertEqual(model.trackable, model._lookup_dependency('trackable'))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_model_construction_in_tf_function(self):\n\n    d = {'model': None}\n\n    @def_function.function\n    def fn(x):\n      if d['model'] is None:\n        # Check that Functional can be built in a `tf.function`.\n        inputs = input_layer_lib.Input(10)\n        outputs = layers.Dense(1)(inputs)\n        model = functional.Functional(inputs, outputs)\n        d['model'] = model\n      else:\n        model = d['model']\n\n      return model(x)\n\n    x = array_ops.ones((10, 10))\n    y = fn(x)\n    self.assertEqual(y.shape.as_list(), [10, 1])\n\n\nclass DeferredModeTest(keras_parameterized.TestCase):\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testSimpleNetworkBuilding(self):\n    inputs = input_layer_lib.Input(shape=(32,))\n    if context.executing_eagerly():\n      self.assertEqual(inputs.dtype.name, 'float32')\n      self.assertEqual(inputs.shape.as_list(), [None, 32])\n\n    x = layers.Dense(2)(inputs)\n    if context.executing_eagerly():\n      self.assertEqual(x.dtype.name, 'float32')\n      self.assertEqual(x.shape.as_list(), [None, 2])\n\n    outputs = layers.Dense(4)(x)\n    network = functional.Functional(inputs, outputs)\n    self.assertIsInstance(network, functional.Functional)\n\n    if context.executing_eagerly():\n      # It should be possible to call such a network on EagerTensors.\n      inputs = constant_op.constant(\n          np.random.random((10, 32)).astype('float32'))\n      outputs = network(inputs)\n      self.assertEqual(outputs.shape.as_list(), [10, 4])\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testMultiIONetworkBuilding(self):\n    input_a = input_layer_lib.Input(shape=(32,))\n    input_b = input_layer_lib.Input(shape=(16,))\n    a = layers.Dense(16)(input_a)\n\n    class AddLayer(layers.Layer):\n\n      def call(self, inputs):\n        return inputs[0] + inputs[1]\n\n    c = AddLayer()([a, input_b])  # pylint: disable=not-callable\n    c = layers.Dense(2)(c)\n\n    network = functional.Functional([input_a, input_b], [a, c])\n    if context.executing_eagerly():\n      a_val = constant_op.constant(\n          np.random.random((10, 32)).astype('float32'))\n      b_val = constant_op.constant(\n          np.random.random((10, 16)).astype('float32'))\n      outputs = network([a_val, b_val])\n      self.assertEqual(len(outputs), 2)\n      self.assertEqual(outputs[0].shape.as_list(), [10, 16])\n      self.assertEqual(outputs[1].shape.as_list(), [10, 2])\n\n\nclass DefaultShapeInferenceBehaviorTest(keras_parameterized.TestCase):\n\n  def _testShapeInference(self, model, input_shape, expected_output_shape):\n    input_value = np.random.random(input_shape)\n    output_value = model.predict(input_value)\n    self.assertEqual(output_value.shape, expected_output_shape)\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testSingleInputCase(self):\n\n    class LayerWithOneInput(layers.Layer):\n\n      def build(self, input_shape):\n        self.w = array_ops.ones(shape=(3, 4))\n\n      def call(self, inputs):\n        return backend.dot(inputs, self.w)\n\n    inputs = input_layer_lib.Input(shape=(3,))\n    layer = LayerWithOneInput()\n\n    if context.executing_eagerly():\n      self.assertEqual(\n          layer.compute_output_shape((None, 3)).as_list(), [None, 4])\n      # As a side-effect, compute_output_shape builds the layer.\n      self.assertTrue(layer.built)\n      # We can still query the layer's compute_output_shape with compatible\n      # input shapes.\n      self.assertEqual(\n          layer.compute_output_shape((6, 3)).as_list(), [6, 4])\n\n    outputs = layer(inputs)\n    model = training_lib.Model(inputs, outputs)\n    self._testShapeInference(model, (2, 3), (2, 4))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testMultiInputOutputCase(self):\n\n    class MultiInputOutputLayer(layers.Layer):\n\n      def build(self, input_shape):\n        self.w = array_ops.ones(shape=(3, 4))\n\n      def call(self, inputs):\n        a = backend.dot(inputs[0], self.w)\n        b = a + inputs[1]\n        return [a, b]\n\n    input_a = input_layer_lib.Input(shape=(3,))\n    input_b = input_layer_lib.Input(shape=(4,))\n    output_a, output_b = MultiInputOutputLayer()([input_a, input_b])\n    model = training_lib.Model([input_a, input_b], [output_a, output_b])\n    output_a_val, output_b_val = model.predict(\n        [np.random.random((2, 3)), np.random.random((2, 4))])\n    self.assertEqual(output_a_val.shape, (2, 4))\n    self.assertEqual(output_b_val.shape, (2, 4))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testTrainingArgument(self):\n\n    class LayerWithTrainingArg(layers.Layer):\n\n      def build(self, input_shape):\n        self.w = array_ops.ones(shape=(3, 4))\n\n      def call(self, inputs, training):\n        return backend.dot(inputs, self.w)\n\n    inputs = input_layer_lib.Input(shape=(3,))\n    outputs = LayerWithTrainingArg()(inputs, training=False)\n    model = training_lib.Model(inputs, outputs)\n    self._testShapeInference(model, (2, 3), (2, 4))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testNoneInShape(self):\n\n    class Model(training_lib.Model):\n\n      def __init__(self):\n        super(Model, self).__init__()\n        self.conv1 = layers.Conv2D(8, 3)\n        self.pool = layers.GlobalAveragePooling2D()\n        self.fc = layers.Dense(3)\n\n      def call(self, x):\n        x = self.conv1(x)\n        x = self.pool(x)\n        x = self.fc(x)\n        return x\n\n    model = Model()\n    model.build(tensor_shape.TensorShape((None, None, None, 1)))\n    self.assertTrue(model.built, 'Model should be built')\n    self.assertTrue(model.weights,\n                    'Model should have its weights created as it '\n                    'has been built')\n    sample_input = array_ops.ones((1, 10, 10, 1))\n    output = model(sample_input)\n    self.assertEqual(output.shape, (1, 3))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testNoneInShapeWithCompoundModel(self):\n\n    class BasicBlock(training_lib.Model):\n\n      def __init__(self):\n        super(BasicBlock, self).__init__()\n        self.conv1 = layers.Conv2D(8, 3)\n        self.pool = layers.GlobalAveragePooling2D()\n        self.dense = layers.Dense(3)\n\n      def call(self, x):\n        x = self.conv1(x)\n        x = self.pool(x)\n        x = self.dense(x)\n        return x\n\n    class CompoundModel(training_lib.Model):\n\n      def __init__(self):\n        super(CompoundModel, self).__init__()\n        self.block = BasicBlock()\n\n      def call(self, x):\n        x = self.block(x)  # pylint: disable=not-callable\n        return x\n\n    model = CompoundModel()\n    model.build(tensor_shape.TensorShape((None, None, None, 1)))\n    self.assertTrue(model.built, 'Model should be built')\n    self.assertTrue(model.weights,\n                    'Model should have its weights created as it '\n                    'has been built')\n    sample_input = array_ops.ones((1, 10, 10, 1))\n    output = model(sample_input)  # pylint: disable=not-callable\n    self.assertEqual(output.shape, (1, 3))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testNoneInShapeWithFunctionalAPI(self):\n\n    class BasicBlock(training_lib.Model):\n      # Inheriting from layers.Layer since we are calling this layer\n      # inside a model created using functional API.\n\n      def __init__(self):\n        super(BasicBlock, self).__init__()\n        self.conv1 = layers.Conv2D(8, 3)\n\n      def call(self, x):\n        x = self.conv1(x)\n        return x\n\n    input_layer = layers.Input(shape=(None, None, 1))\n    x = BasicBlock()(input_layer)\n    x = layers.GlobalAveragePooling2D()(x)\n    output_layer = layers.Dense(3)(x)\n\n    model = training_lib.Model(inputs=input_layer, outputs=output_layer)\n\n    model.build(tensor_shape.TensorShape((None, None, None, 1)))\n    self.assertTrue(model.built, 'Model should be built')\n    self.assertTrue(model.weights,\n                    'Model should have its weights created as it '\n                    'has been built')\n    sample_input = array_ops.ones((1, 10, 10, 1))\n    output = model(sample_input)\n    self.assertEqual(output.shape, (1, 3))\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_sequential_as_downstream_of_masking_layer(self):\n    inputs = layers.Input(shape=(3, 4))\n    x = layers.Masking(mask_value=0., input_shape=(3, 4))(inputs)\n\n    s = sequential.Sequential()\n    s.add(layers.Dense(5, input_shape=(4,)))\n\n    x = layers.wrappers.TimeDistributed(s)(x)\n    model = training_lib.Model(inputs=inputs, outputs=x)\n    model.compile(\n        optimizer='rmsprop',\n        loss='mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n\n    model_input = np.random.randint(\n        low=1, high=5, size=(10, 3, 4)).astype('float32')\n    for i in range(4):\n      model_input[i, i:, :] = 0.\n    model.fit(model_input,\n              np.random.random((10, 3, 5)), epochs=1, batch_size=6)\n\n    if not context.executing_eagerly():\n      # Note: this doesn't work in eager due to DeferredTensor/ops compatibility\n      # issue.\n      mask_outputs = [model.layers[1].compute_mask(model.layers[1].input)]\n      mask_outputs += [model.layers[2].compute_mask(\n          model.layers[2].input, mask_outputs[-1])]\n      func = backend.function([model.input], mask_outputs)\n      mask_outputs_val = func([model_input])\n      self.assertAllClose(mask_outputs_val[0], np.any(model_input, axis=-1))\n      self.assertAllClose(mask_outputs_val[1], np.any(model_input, axis=-1))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_external_keras_serialization_compat_input_layers(self):\n    inputs = input_layer_lib.Input(shape=(10,))\n    outputs = layers.Dense(1)(inputs)\n    model = training_lib.Model(inputs, outputs)\n    config = model.get_config()\n    # Checks that single inputs and outputs are still saved as 1-element lists.\n    # Saving as 1-element lists or not is equivalent in TF Keras, but only the\n    # 1-element list format is supported in TF.js and keras-team/Keras.\n    self.assertLen(config['input_layers'], 1)\n    self.assertLen(config['output_layers'], 1)\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_external_keras_serialization_compat_inbound_nodes(self):\n    # Check single Tensor input.\n    inputs = input_layer_lib.Input(shape=(10,), name='in')\n    outputs = layers.Dense(1)(inputs)\n    model = training_lib.Model(inputs, outputs)\n    config = model.get_config()\n    self.assertEqual(config['layers'][1]['inbound_nodes'], [[['in', 0, 0, {}]]])\n\n    # Check multiple Tensor input.\n    inputs1 = input_layer_lib.Input(shape=(10,), name='in1')\n    inputs2 = input_layer_lib.Input(shape=(10,), name='in2')\n    outputs = layers.Add()([inputs1, inputs2])\n    model = training_lib.Model([inputs1, inputs2], outputs)\n    config = model.get_config()\n    self.assertEqual(config['layers'][2]['inbound_nodes'],\n                     [[['in1', 0, 0, {}], ['in2', 0, 0, {}]]])\n\n  @combinations.generate(combinations.combine(mode=['eager']))\n  def test_dict_inputs_tensors(self):\n    # Note that this test is running with v2 eager only, since the v1\n    # will behave differently wrt to dict input for training.\n    inputs = {\n        'sentence2': input_layer_lib.Input(\n            shape=(), name='a', dtype=dtypes.string),\n        'sentence1': input_layer_lib.Input(\n            shape=(), name='b', dtype=dtypes.string),\n    }\n    strlen = layers.Lambda(string_ops.string_length_v2)\n    diff = layers.Subtract()(\n        [strlen(inputs['sentence1']), strlen(inputs['sentence2'])])\n    diff = math_ops.cast(diff, dtypes.float32)\n    model = training_lib.Model(inputs, diff)\n\n    extra_keys = {\n        'sentence1': constant_op.constant(['brown fox', 'lazy dog']),\n        'sentence2': constant_op.constant(['owl', 'cheeky cat']),\n        'label': constant_op.constant([0, 1]),\n    }\n\n    with warnings.catch_warnings(record=True) as w:\n      warnings.simplefilter('always')\n      model(extra_keys)\n      self.assertIn('ignored by the model', str(w[-1].message))\n\n    model.compile('sgd', 'mse')\n    with warnings.catch_warnings(record=True) as w:\n      warnings.simplefilter('always')\n      model.fit(extra_keys, y=constant_op.constant([0, 1]), steps_per_epoch=1)\n      self.assertIn('ignored by the model', str(w[-1].message))\n\n    with warnings.catch_warnings(record=True) as w:\n      warnings.simplefilter('always')\n      model.evaluate(extra_keys, constant_op.constant([0, 1]))\n      self.assertIn('ignored by the model', str(w[-1].message))\n\n    # Make sure the model inputs are sorted with the dict keys.\n    self.assertEqual(model.inputs[0]._keras_history.layer.name, 'b')\n    self.assertEqual(model.inputs[1]._keras_history.layer.name, 'a')\n\n\nclass GraphUtilsTest(test.TestCase):\n\n  def testGetReachableFromInputs(self):\n\n    with ops.Graph().as_default(), self.cached_session():\n      pl_1 = array_ops.placeholder(shape=None, dtype='float32')\n      pl_2 = array_ops.placeholder(shape=None, dtype='float32')\n      pl_3 = array_ops.placeholder(shape=None, dtype='float32')\n      x_1 = pl_1 + pl_2\n      x_2 = pl_2 * 2\n      x_3 = pl_3 + 1\n      x_4 = x_1 + x_2\n      x_5 = x_3 * pl_1\n\n      self.assertEqual(\n          tf_utils.get_reachable_from_inputs([pl_1]),\n          {pl_1, x_1, x_4, x_5, x_1.op, x_4.op, x_5.op})\n      self.assertEqual(\n          tf_utils.get_reachable_from_inputs([pl_1, pl_2]),\n          {pl_1, pl_2, x_1, x_2, x_4, x_5, x_1.op, x_2.op, x_4.op, x_5.op})\n      self.assertEqual(\n          tf_utils.get_reachable_from_inputs([pl_3]),\n          {pl_3, x_3, x_5, x_3.op, x_5.op})\n      self.assertEqual(\n          tf_utils.get_reachable_from_inputs([x_3]), {x_3, x_5, x_5.op})\n\n\nclass NestedNetworkTest(keras_parameterized.TestCase):\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_nested_inputs_network(self):\n    inputs = {\n        'x1': input_layer_lib.Input(shape=(1,)),\n        'x2': input_layer_lib.Input(shape=(1,))\n    }\n    outputs = layers.Add()([inputs['x1'], inputs['x2']])\n    network = functional.Functional(inputs, outputs)\n\n    network = functional.Functional.from_config(network.get_config())\n\n    result_tensor = network({\n        'x1': array_ops.ones((1, 1), 'float32'),\n        'x2': array_ops.ones((1, 1), 'float32')\n    })\n    result = self.evaluate(result_tensor)\n    self.assertAllEqual(result, [[2.]])\n\n    # TODO(b/122726584): Investigate why concrete batch is flaky in some builds.\n    output_shape = network.compute_output_shape({\n        'x1': (None, 1),\n        'x2': (None, 1)\n    })\n    self.assertListEqual(output_shape.as_list(), [None, 1])\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_nested_outputs_network(self):\n    inputs = input_layer_lib.Input(shape=(1,))\n    outputs = {\n        'x+x': layers.Add()([inputs, inputs]),\n        'x*x': layers.Multiply()([inputs, inputs])\n    }\n\n    network = functional.Functional(inputs, outputs)\n\n    network = functional.Functional.from_config(network.get_config())\n\n    result_tensor = network(array_ops.ones((1, 1), 'float32'))\n    result = self.evaluate(result_tensor)\n    self.assertAllEqual(result['x+x'], [[2.]])\n    self.assertAllEqual(result['x*x'], [[1.]])\n\n    output_shape = network.compute_output_shape((None, 1))\n    self.assertListEqual(output_shape['x+x'].as_list(), [None, 1])\n    self.assertListEqual(output_shape['x*x'].as_list(), [None, 1])\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_nested_network_inside_network(self):\n    inner_inputs = {\n        'x1': input_layer_lib.Input(shape=(1,)),\n        'x2': input_layer_lib.Input(shape=(1,))\n    }\n    inner_outputs = {\n        'x1+x2': layers.Add()([inner_inputs['x1'], inner_inputs['x2']]),\n        'x1*x2': layers.Multiply()([inner_inputs['x1'], inner_inputs['x2']])\n    }\n    inner_network = functional.Functional(\n        inner_inputs, inner_outputs)\n\n    inputs = [\n        input_layer_lib.Input(shape=(1,)),\n        input_layer_lib.Input(shape=(1,))\n    ]\n    middle = inner_network({'x1': inputs[0], 'x2': inputs[1]})\n    outputs = layers.Add()([middle['x1+x2'], middle['x1*x2']])\n    network = functional.Functional(inputs, outputs)\n\n    network = functional.Functional.from_config(network.get_config())\n\n    # Computes: `(x1+x2) + (x1*x2)`\n    result_tensor = network(\n        [array_ops.ones((1, 1), 'float32'),\n         array_ops.ones((1, 1), 'float32')])\n    result = self.evaluate(result_tensor)\n    self.assertAllEqual(result, [[3.]])\n\n    output_shape = network.compute_output_shape([(None, 1), (None, 1)])\n    self.assertListEqual(output_shape.as_list(), [None, 1])\n\n  @combinations.generate(combinations.combine(mode=['graph']))\n  def test_updates_with_direct_call(self):\n    inputs = input_layer_lib.Input(shape=(10,))\n    x = layers.BatchNormalization()(inputs)\n    x = layers.Dense(10)(x)\n    model = training_lib.Model(inputs, x)\n\n    ph = backend.placeholder(shape=(10, 10))\n    model(ph)\n\n    self.assertLen(model.updates, 4)\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_dict_mapping_input(self):\n\n    class ReturnFirst(layers.Layer):\n\n      def call(self, inputs):\n        b, _ = inputs\n        return b\n\n    # Checks that inputs are put in same order as the\n    # Model was constructed with.\n    b = input_layer_lib.Input(shape=(10,), name='b')\n    a = input_layer_lib.Input(shape=(10,), name='a')\n    outputs = ReturnFirst()([b, a])\n\n    b_val = array_ops.ones((10, 10))\n    a_val = array_ops.zeros((10, 10))\n\n    model = training_lib.Model([b, a], outputs)\n    res = model({'a': a_val, 'b': b_val})\n    self.assertAllClose(self.evaluate(res), self.evaluate(b_val))\n\n    reversed_model = training_lib.Model([a, b], outputs)\n    res = reversed_model({'a': a_val, 'b': b_val})\n    self.assertAllClose(self.evaluate(res), self.evaluate(b_val))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_dict_mapping_single_input(self):\n    b = input_layer_lib.Input(shape=(1,), name='b')\n    outputs = b * 2\n    model = training_lib.Model(b, outputs)\n\n    b_val = array_ops.ones((1, 1))\n    extra_val = array_ops.ones((1, 10))\n\n    inputs = {'a': extra_val, 'b': b_val}\n    res = model(inputs)\n\n    # Check that 'b' was used and 'a' was ignored.\n    self.assertEqual(res.shape.as_list(), [1, 1])\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_nested_dict_mapping(self):\n    a = input_layer_lib.Input(shape=(1,), dtype='int32', name='a')\n    b = input_layer_lib.Input(shape=(1,), dtype='int32', name='b')\n    c = input_layer_lib.Input(shape=(1,), dtype='int32', name='c')\n    d = input_layer_lib.Input(shape=(1,), dtype='int32', name='d')\n    inputs = {'a': (a, b), 'c': (c, d)}\n    outputs = 1000 * a + 100 * b + 10 * c + d\n    model = training_lib.Model(inputs, outputs)\n\n    a_val = array_ops.ones((1, 1), dtype='int32')\n    b_val = 2 * array_ops.ones((1, 1), dtype='int32')\n    c_val = 3 * array_ops.ones((1, 1), dtype='int32')\n    d_val = 4 * array_ops.ones((1, 1), dtype='int32')\n\n    inputs_val = {'a': (a_val, b_val), 'c': (c_val, d_val)}\n    res = model(inputs_val)\n\n    # Check that inputs were flattened in the correct order.\n    self.assertFalse(model._enable_dict_to_input_mapping)\n    self.assertEqual(self.evaluate(res), [1234])\n\n\n@combinations.generate(combinations.keras_mode_combinations())\nclass AddLossTest(keras_parameterized.TestCase):\n\n  def test_add_loss_outside_call_only_loss(self):\n    inputs = input_layer_lib.Input((10,))\n    mid = layers.Dense(10)(inputs)\n    outputs = layers.Dense(1)(mid)\n    model = training_lib.Model(inputs, outputs)\n    model.add_loss(math_ops.reduce_mean(outputs))\n    self.assertLen(model.losses, 1)\n\n    initial_weights = model.get_weights()\n\n    x = np.ones((10, 10))\n    model.compile(\n        'sgd',\n        run_eagerly=testing_utils.should_run_eagerly())\n    model.fit(x, batch_size=2, epochs=1)\n\n    model2 = model.from_config(model.get_config())\n    model2.compile(\n        'sgd',\n        run_eagerly=testing_utils.should_run_eagerly())\n    model2.set_weights(initial_weights)\n    model2.fit(x, batch_size=2, epochs=1)\n\n    # The TFOpLayer and the AddLoss layer are serialized.\n    self.assertLen(model2.layers, 5)\n    self.assertAllClose(model.get_weights(), model2.get_weights())\n\n  def test_add_loss_outside_call_multiple_losses(self):\n    inputs = input_layer_lib.Input((10,))\n    x1 = layers.Dense(10)(inputs)\n    x2 = layers.Dense(10)(x1)\n    outputs = layers.Dense(1)(x2)\n    model = training_lib.Model(inputs, outputs)\n    model.add_loss(math_ops.reduce_sum(x1 * x2))\n    model.add_loss(math_ops.reduce_mean(outputs))\n    self.assertLen(model.losses, 2)\n\n    initial_weights = model.get_weights()\n\n    x, y = np.ones((10, 10)), np.ones((10, 1))\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    model.fit(x, y, batch_size=2, epochs=1)\n\n    model2 = model.from_config(model.get_config())\n    model2.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    model2.set_weights(initial_weights)\n    model2.fit(x, y, batch_size=2, epochs=1)\n\n    self.assertAllClose(model.get_weights(), model2.get_weights())\n\n  def test_add_loss_crossentropy_backtracking(self):\n    inputs = input_layer_lib.Input((2,))\n    labels = input_layer_lib.Input((1,))\n    outputs = layers.Dense(1, activation='sigmoid')(inputs)\n    model = functional.Functional([inputs, labels], outputs)\n    model.add_loss(losses.binary_crossentropy(labels, outputs))\n    model.compile('adam')\n    x = np.random.random((2, 2))\n    y = np.random.random((2, 1))\n    model.fit([x, y])\n\n    inputs = input_layer_lib.Input((2,))\n    labels = input_layer_lib.Input((2,))\n    outputs = layers.Dense(2, activation='softmax')(inputs)\n    model = functional.Functional([inputs, labels], outputs)\n    model.add_loss(losses.categorical_crossentropy(labels, outputs))\n    model.compile('adam')\n    x = np.random.random((2, 2))\n    y = np.random.random((2, 2))\n    model.fit([x, y])\n\n    inputs = input_layer_lib.Input((2,))\n    labels = input_layer_lib.Input((1,), dtype='int32')\n    outputs = layers.Dense(2, activation='softmax')(inputs)\n    model = functional.Functional([inputs, labels], outputs)\n    model.add_loss(losses.sparse_categorical_crossentropy(labels, outputs))\n    model.compile('adam')\n    x = np.random.random((2, 2))\n    y = np.random.randint(0, 2, size=(2, 1))\n    model.fit([x, y])\n\n\n@combinations.generate(combinations.keras_mode_combinations())\nclass WeightAccessTest(keras_parameterized.TestCase):\n\n  def test_functional_model(self):\n    inputs = input_layer_lib.Input((10,))\n    x1 = layers.Dense(10)(inputs)\n    x2 = layers.Dense(10)(x1)\n    outputs = layers.Dense(1)(x2)\n    model = training_lib.Model(inputs, outputs)\n\n    self.assertEqual(len(model.weights), 6)\n\n  def test_sequential_model_with_input_shape(self):\n    x1 = layers.Dense(10, input_shape=(10,))\n    x2 = layers.Dense(10)\n    x3 = layers.Dense(1)\n    model = sequential.Sequential([x1, x2, x3])\n\n    self.assertEqual(len(model.weights), 6)\n\n  def test_sequential_model_without_input_shape(self):\n    x1 = layers.Dense(10)\n    x2 = layers.Dense(10)\n    x3 = layers.Dense(1)\n    model = sequential.Sequential([x1, x2, x3])\n\n    with self.assertRaisesRegex(\n        ValueError, 'Weights for model .* have not yet been created'):\n      _ = model.weights\n\n  def test_subclass_model_with_build_method(self):\n\n    class SubclassModel(models.Model):\n\n      def build(self, input_shape):\n        self.w = self.add_weight(shape=input_shape[-1], initializer='ones')\n\n      def call(self, inputs):\n        return inputs * self.w\n\n    model = SubclassModel()\n\n    with self.assertRaisesRegex(\n        ValueError, 'Weights for model .* have not yet been created'):\n      _ = model.weights\n\n    model(input_layer_lib.Input((10,)))\n    self.assertEqual(len(model.weights), 1)\n\n  def test_subclass_model_without_build_method(self):\n\n    class SubclassModel(models.Model):\n\n      def __init__(self):\n        super(SubclassModel, self).__init__()\n        self.w = self.add_weight(shape=(), initializer='ones')\n\n      def call(self, inputs):\n        return inputs * self.w\n\n    model = SubclassModel()\n    self.assertEqual(len(model.weights), 1)\n\n\n@combinations.generate(combinations.combine(mode=['graph', 'eager']))\nclass DTypeTest(keras_parameterized.TestCase):\n\n  @testing_utils.enable_v2_dtype_behavior\n  def test_graph_network_dtype(self):\n    inputs = input_layer_lib.Input((10,))\n    outputs = layers.Dense(10)(inputs)\n    network = functional.Functional(inputs, outputs)\n    self.assertEqual(network.dtype, 'float32')\n\n  @testing_utils.enable_v2_dtype_behavior\n  def test_subclassed_network_dtype(self):\n\n    class IdentityNetwork(training_lib.Model):\n\n      def call(self, inputs):\n        return inputs\n\n    network = IdentityNetwork()\n    self.assertEqual(network.dtype, 'float32')\n    self.assertEqual(network(array_ops.constant(1, 'float64')).dtype, 'float32')\n\n    network = IdentityNetwork(dtype='float16')\n    self.assertEqual(network.dtype, 'float16')\n    self.assertEqual(network(array_ops.constant(1, 'float64')).dtype, 'float16')\n\n    network = IdentityNetwork(autocast=False)\n    self.assertEqual(network.dtype, 'float32')\n    self.assertEqual(network(array_ops.constant(1, 'float64')).dtype, 'float64')\n\n\nclass AttrTrackingLayer(base_layer.Layer):\n  \"\"\"Count how many times `dynamic` and `stateful` are called.\n\n  These counts are used to test that the attribute cache behaves as expected.\n  \"\"\"\n  def __init__(self, *args, **kwargs):\n    self.stateful_count = 0\n    self.dynamic_count = 0\n    super(AttrTrackingLayer, self).__init__(*args, **kwargs)\n\n  @base_layer.Layer.stateful.getter\n  def stateful(self):\n    self.stateful_count += 1\n    return super(AttrTrackingLayer, self).stateful\n\n  @property\n  def dynamic(self):\n    self.dynamic_count += 1\n    return super(AttrTrackingLayer, self).dynamic\n\n\n@combinations.generate(combinations.combine(mode=['graph', 'eager']))\nclass CacheCorrectnessTest(keras_parameterized.TestCase):\n\n  def layer_and_network_test(self):\n    # Top level layer\n    network = functional.Functional()\n\n    layer_0 = AttrTrackingLayer()\n\n    sub_network = functional.Functional()\n    layer_1 = AttrTrackingLayer(dynamic=True)\n    layer_2 = AttrTrackingLayer()\n    sub_network.sub_layers = [layer_1, layer_2]\n\n    network.sub_layer = layer_0\n\n    for _ in range(2):\n      self.assertEqual(network.dynamic, False)\n      self.assertEqual(network.stateful, False)\n\n      # The second pass should be a cache hit.\n      self.assertEqual(layer_0.dynamic_count, 1)\n      self.assertEqual(layer_0.stateful_count, 1)\n\n    # Mutations of the sub-layer should force recalculation of the network's\n    # stateful attribute. (mutations bubble up.)\n    layer_0.stateful = True\n    self.assertEqual(network.stateful, True)\n    self.assertEqual(layer_0.stateful_count, 2)\n\n    layer_0.stateful = False\n    self.assertEqual(network.stateful, False)\n    self.assertEqual(layer_0.stateful_count, 3)\n\n    # But changing stateful should not affect dynamic.\n    self.assertEqual(network.dynamic, False)\n    self.assertEqual(layer_0.dynamic_count, 1)\n\n    network.sub_network = sub_network\n\n    # Adding to the topology should invalidate the cache and reflect in the top\n    # level network.\n    self.assertEqual(network.dynamic, True)\n    self.assertEqual(layer_0.dynamic_count, 2)\n    self.assertEqual(layer_1.dynamic_count, 1)\n\n    # Still dynamic, but we need to recompute.\n    sub_network.sub_layers.pop()\n    self.assertEqual(network.dynamic, True)\n    self.assertEqual(layer_0.dynamic_count, 3)\n    self.assertEqual(layer_1.dynamic_count, 2)\n\n    # Now that we've removed the dynamic layer deep in the layer hierarchy, we\n    # need to make sure that that bubbles up through all the levels.\n    sub_network.sub_layers.pop()\n    self.assertEqual(network.dynamic, False)\n    self.assertEqual(layer_0.dynamic_count, 4)\n    self.assertEqual(layer_1.dynamic_count, 2)\n\n    # Now check with a tracked dict.\n    sub_network.sub_layers = {\n        \"layer_1\": layer_1,\n        \"layer_2\": layer_2,\n    }\n\n    self.assertEqual(network.dynamic, True)\n    self.assertEqual(layer_0.dynamic_count, 5)\n    self.assertEqual(layer_1.dynamic_count, 3)\n\n    # In-place assignment should still invalidate the cache.\n    sub_network.sub_layers[\"layer_1\"] = layer_1\n    self.assertEqual(network.dynamic, True)\n    self.assertEqual(layer_0.dynamic_count, 6)\n    self.assertEqual(layer_1.dynamic_count, 4)\n\n    sub_network.sub_layers[\"layer_1\"] = None\n    for _ in range(2):\n      self.assertEqual(network.dynamic, False)\n      self.assertEqual(layer_0.dynamic_count, 7)\n      self.assertEqual(layer_1.dynamic_count, 4)\n\n    layer_3 = AttrTrackingLayer()\n    layer_3.stateful = True\n\n    sub_network.sub_layers = None\n    self.assertEqual(network.dynamic, False)\n    self.assertEqual(network.stateful, False)\n\n    # Test duplicate layers.\n    sub_network.sub_layers = [layer_1, layer_1, layer_1, layer_3]\n    self.assertEqual(network.dynamic, True)\n    self.assertEqual(network.stateful, True)\n\n    for _ in range(3):\n      sub_network.sub_layers.pop()\n      self.assertEqual(network.dynamic, True)\n      self.assertEqual(network.stateful, False)\n\n    sub_network.sub_layers.pop()\n    self.assertEqual(network.dynamic, False)\n    self.assertEqual(network.stateful, False)\n\n  def test_compute_output_shape_cache(self):\n    # See https://github.com/tensorflow/tensorflow/issues/32029.\n    x = input_layer_lib.Input(shape=(None, 32))\n    dense = layers.Dense(2)\n    y = dense(x)\n    network = functional.Functional(x, y, name='dense_network')\n\n    for i in range(999, 1024):\n      self.assertEqual(network.compute_output_shape((1, i, 32)), (1, i, 2))\n\n  def test_2d_inputs_squeezed_to_1d(self):\n    input_1d = input_layer_lib.Input(shape=())\n    outputs = input_1d * 2.\n    net = functional.Functional(input_1d, outputs)\n\n    x = np.ones((10, 1))\n    y = net(x)\n    self.assertEqual(y.shape.rank, 1)\n\n  def test_1d_inputs_expanded_to_2d(self):\n    input_1d = input_layer_lib.Input(shape=(1,))\n    outputs = input_1d * 2.\n    net = functional.Functional(input_1d, outputs)\n\n    x = np.ones((10,))\n    y = net(x)\n    self.assertEqual(y.shape.rank, 2)\n\n  def test_training_passed_during_construction(self):\n\n    def _call(inputs, training):\n      if training is None:\n        return inputs * -1.0\n      elif training:\n        return inputs\n      else:\n        return inputs * 0.0\n\n    class MyLayer(base_layer.Layer):\n\n      def call(self, inputs, training=True):\n        return _call(inputs, training)\n\n    my_layer = MyLayer()\n    x = np.ones((1, 10))\n\n    # Hard-coded `true` value passed during construction is respected.\n    inputs = input_layer_lib.Input(10)\n    outputs = my_layer(inputs, training=True)\n    network = functional.Functional(inputs, outputs)\n    self.assertAllEqual(network(x, training=True), _call(x, True))\n    self.assertAllEqual(network(x, training=False), _call(x, True))\n    self.assertAllEqual(network(x), _call(x, True))\n\n    # Hard-coded `false` value passed during construction is respected.\n    inputs = input_layer_lib.Input(10)\n    outputs = my_layer(inputs, training=False)\n    network = functional.Functional(inputs, outputs)\n    self.assertAllEqual(network(x, training=True), _call(x, False))\n    self.assertAllEqual(network(x, training=False), _call(x, False))\n    self.assertAllEqual(network(x), _call(x, False))\n\n    if context.executing_eagerly():\n      # In v2, construction still works when no `training` is specified\n      # When no value passed during construction, it uses the local default.\n      inputs = input_layer_lib.Input(10)\n      outputs = my_layer(inputs)\n      network = functional.Functional(inputs, outputs)\n      self.assertAllEqual(network(x, training=True), _call(x, True))\n      self.assertAllEqual(network(x, training=False), _call(x, False))\n      self.assertAllEqual(network(x), _call(x, True))  # Use local default\n\n    # `None` value passed positionally during construction is ignored at runtime\n    inputs = input_layer_lib.Input(10)\n    outputs = my_layer(inputs, None)\n    network = functional.Functional(inputs, outputs)\n    self.assertAllEqual(network(x, training=True), _call(x, True))\n    self.assertAllEqual(network(x, training=False), _call(x, False))\n    if context.executing_eagerly():\n      self.assertAllEqual(network(x), _call(x, True))  # Use local default\n    else:\n      # in v1 training would have defaulted to using the `None` inside the layer\n      # if training is not passed at runtime\n      self.assertAllEqual(network(x), _call(x, None))\n\n    # `None` value passed as kwarg during construction is ignored at runtime.\n    inputs = input_layer_lib.Input(10)\n    outputs = my_layer(inputs, training=None)\n    network = functional.Functional(inputs, outputs)\n    self.assertAllEqual(network(x, training=True), _call(x, True))\n    self.assertAllEqual(network(x, training=False), _call(x, False))\n    if context.executing_eagerly():\n      self.assertAllEqual(network(x), _call(x, True))  # Use local default\n    else:\n      # in v1 training would have defaulted to using the `None` inside the layer\n      # if training is not passed at runtime\n      self.assertAllEqual(network(x), _call(x, None))\n\n\nclass InputsOutputsErrorTest(keras_parameterized.TestCase):\n\n  @testing_utils.enable_v2_dtype_behavior\n  def test_input_error(self):\n    inputs = input_layer_lib.Input((10,))\n    outputs = layers.Dense(10)(inputs)\n    with self.assertRaisesRegex(\n        TypeError, \"('Keyword argument not understood:', 'input')\"):\n      models.Model(input=inputs, outputs=outputs)\n\n  @testing_utils.enable_v2_dtype_behavior\n  def test_output_error(self):\n    inputs = input_layer_lib.Input((10,))\n    outputs = layers.Dense(10)(inputs)\n    with self.assertRaisesRegex(\n        TypeError, \"('Keyword argument not understood:', 'output')\"):\n      models.Model(inputs=inputs, output=outputs)\n\n  def test_input_spec(self):\n    if not context.executing_eagerly():\n      return\n    inputs = input_layer_lib.Input((10,))\n    outputs = layers.Dense(10)(inputs)\n    model = models.Model(inputs, outputs)\n    with self.assertRaisesRegex(\n        ValueError, r'.*expected shape=.*'):\n      model(np.zeros((3, 11)))\n\n  def test_input_spec_list_of_inputs(self):\n    if not context.executing_eagerly():\n      return\n    input_1 = input_layer_lib.Input((10,), name='1')\n    input_2 = input_layer_lib.Input((5,), name='2')\n    x = layers.Concatenate()([input_1, input_2])\n    outputs = layers.Dense(10)(x)\n    model = models.Model([input_1, input_2], outputs)\n    with self.assertRaisesRegex(\n        ValueError, r'.*expects 2 input.*'):\n      model(np.zeros((3, 10)))\n    with self.assertRaisesRegex(\n        ValueError, r'.*expects 2 input.*'):\n      model([np.zeros((3, 10)), np.zeros((3, 5)), np.zeros((3, 10))])\n    with self.assertRaisesRegex(\n        ValueError, r'.*expected shape=.*'):\n      model([np.zeros((3, 10)), np.zeros((3, 6))])\n\n    # Test passing data via dict keyed by input name\n    with self.assertRaisesRegex(\n        ValueError, r'Missing data for input.*'):\n      model({'1': np.zeros((3, 10))})\n    with self.assertRaisesRegex(\n        ValueError, r'.*expected shape=.*'):\n      model({'1': np.zeros((3, 10)), '2': np.zeros((3, 6))})\n\n  def test_input_spec_dict(self):\n    if not context.executing_eagerly():\n      return\n    input_1 = input_layer_lib.Input((10,))\n    input_2 = input_layer_lib.Input((5,))\n    x = layers.Concatenate()([input_1, input_2])\n    outputs = layers.Dense(10)(x)\n    model = models.Model({'1': input_1, '2': input_2}, outputs)\n    with self.assertRaisesRegex(\n        ValueError, r'Missing data for input.*'):\n      model({'1': np.zeros((3, 10))})\n    with self.assertRaisesRegex(\n        ValueError, r'.*expected shape=.*'):\n      model({'1': np.zeros((3, 10)), '2': np.zeros((3, 6))})\n\n\nclass FunctionalSubclassModel(training_lib.Model):\n\n  def __init__(self, *args, **kwargs):\n    self.foo = {'foo': 'bar'}  # Make sure users can assign dict attributes\n    my_input = input_layer_lib.Input(shape=(16,))\n    dense = layers.Dense(32, activation='relu')\n    output = dense(my_input)\n    outputs = {'output': output}\n    super().__init__(inputs=[my_input], outputs=outputs, *args, **kwargs)\n\n\nclass MixinClass(object):\n\n  def __init__(self, foo, **kwargs):\n    self._foo = foo\n    super().__init__(**kwargs)\n\n  def get_foo(self):\n    return self._foo\n\n\nclass SubclassedModel(training_lib.Model):\n\n  def __init__(self, bar, **kwargs):\n    self._bar = bar\n    super().__init__(**kwargs)\n\n  def get_bar(self):\n    return self._bar\n\n\nclass MultipleInheritanceModelTest(keras_parameterized.TestCase):\n\n  def testFunctionalSubclass(self):\n    m = FunctionalSubclassModel()\n    # Some smoke test for the weights and output shape of the model\n    self.assertLen(m.weights, 2)\n    self.assertEqual(m.outputs[0].shape.as_list(), [None, 32])\n\n  def testFunctionalSubclassPreMixin(self):\n    class MixedFunctionalSubclassModel(MixinClass, FunctionalSubclassModel):\n      pass\n\n    m = MixedFunctionalSubclassModel(foo='123')\n    self.assertTrue(m._is_graph_network)\n    self.assertLen(m.weights, 2)\n    self.assertEqual(m.outputs[0].shape.as_list(), [None, 32])\n    self.assertEqual(m.get_foo(), '123')\n\n  def testFunctionalSubclassPostMixin(self):\n    # Make sure the the mixin class is also init correct when the order changed.\n\n    class MixedFunctionalSubclassModel(FunctionalSubclassModel, MixinClass):\n      pass\n\n    m = MixedFunctionalSubclassModel(foo='123')\n    self.assertTrue(m._is_graph_network)\n    self.assertLen(m.weights, 2)\n    self.assertEqual(m.outputs[0].shape.as_list(), [None, 32])\n    self.assertEqual(m.get_foo(), '123')\n\n  def testSubclassModelPreMixin(self):\n    class MixedSubclassModel(MixinClass, SubclassedModel):\n      pass\n\n    m = MixedSubclassModel(foo='123', bar='456')\n    self.assertFalse(m._is_graph_network)\n    self.assertEqual(m.get_foo(), '123')\n    self.assertEqual(m.get_bar(), '456')\n\n\nif __name__ == '__main__':\n  test.main()"