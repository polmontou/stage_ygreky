"/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#include <utility>\n#include <vector>\n\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/variant.h\"\n#include \"tensorflow/core/framework/variant_encode_decode.h\"\n#include \"tensorflow/core/kernels/ragged_tensor_variant.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/lib/core/status.h\"\n\nnamespace tensorflow {\nnamespace {\n\nStatus RaggedComponentsFromVariant(\n    const Tensor& encoded_variant, int ragged_rank, DataType value_dtype,\n    DataType split_dtype, std::vector<RaggedTensorVariant>* decoded_ragged) {\n  const auto& flat_variants = encoded_variant.flat<Variant>();\n  decoded_ragged->reserve(flat_variants.size());\n\n  for (int i = 0; i < flat_variants.size(); i++) {\n    const auto& flat_variant = flat_variants(i);\n    const RaggedTensorVariant* decoded =\n        flat_variant.get<RaggedTensorVariant>();\n    if (decoded == nullptr) {\n      return errors::InvalidArgument(\n          \"Input Variant element at index \", i,\n          \" doesn't hold a RaggedTensorVariant: \", flat_variant.DebugString());\n    }\n    decoded_ragged->push_back(*decoded);\n    decoded = &decoded_ragged->back();\n    // Check ragged rank & types\n    if (decoded->ragged_rank() != ragged_rank) {\n      return errors::InvalidArgument(\n          \"Encoded input RaggedTensorVariant has ragged_rank=\",\n          decoded->ragged_rank(), \".  Expected ragged_rank=\", ragged_rank, \".\");\n    }\n    if (decoded->values().dtype() != value_dtype) {\n      return errors::InvalidArgument(\n          \"Expected values Tensor dtype: \", DataTypeString(value_dtype),\n          \", found: \", DataTypeString(decoded->values().dtype()));\n    }\n    if (decoded->values().dims() < 1) {\n      return errors::InvalidArgument(\n          \"Ragged values must have rank >= 1; encoded scalar element at index \",\n          i, \" has values Tensor: \", decoded->values().DebugString());\n    }\n    for (const auto& splits : decoded->nested_splits()) {\n      if (splits.dtype() != split_dtype) {\n        return errors::InvalidArgument(\n            \"Expected row_splits Tensor dtype: \", DataTypeString(split_dtype),\n            \", found: \", DataTypeString(splits.dtype()));\n      }\n      if (splits.dims() != 1) {\n        return errors::InvalidArgument(\n            \"Ragged splits must have rank 1; encoded scalar element at index \",\n            i, \" has splits Tensor \", splits.DebugString());\n      }\n    }\n  }\n  return Status::OK();\n}\n\ntemplate <typename VALUE_TYPE, typename SPLIT_TYPE>\nStatus NestedStackRaggedTensors(\n    const std::vector<RaggedTensorVariant>& ragged_components,\n    const std::vector<int>& nested_dim_sizes, const int input_ragged_rank,\n    const int output_ragged_rank, RaggedTensorVariant* output_ragged) {\n  output_ragged->mutable_nested_splits()->reserve(output_ragged_rank);\n  const int dims = nested_dim_sizes.size();\n\n  // Populate first `dims - 1` splits.\n  for (int i = 0; i < dims - 1; i++) {\n    int dims_splits_size = nested_dim_sizes[i] + 1;\n    output_ragged->append_splits(Tensor(DataTypeToEnum<SPLIT_TYPE>::value,\n                                        TensorShape({dims_splits_size})));\n    auto splits_vec = output_ragged->mutable_splits(i)->vec<SPLIT_TYPE>();\n    int split_diff = nested_dim_sizes[i + 1];\n    for (int j = 0; j < dims_splits_size; j++) {\n      splits_vec(j) = j * split_diff;\n    }\n  }\n\n  // Populate `dims`-th split.\n  int splits_size = ragged_components.size() + 1;\n  output_ragged->append_splits(\n      Tensor(DataTypeToEnum<SPLIT_TYPE>::value, TensorShape({splits_size})));\n  auto dims_splits_vec =\n      output_ragged->mutable_splits(dims - 1)->vec<SPLIT_TYPE>();\n  dims_splits_vec(0) = 0;\n  for (int i = 0; i < ragged_components.size(); i++) {\n    int split_val = ragged_components[i].values().shape().dim_size(0);\n    if (input_ragged_rank != 0 && ragged_components[i].ragged_rank() > 0) {\n      split_val = ragged_components[i].splits(0).NumElements() - 1;\n    }\n    dims_splits_vec(i + 1) = dims_splits_vec(i) + split_val;\n  }\n\n  // Populate last `input_ragged_rank` splits.\n  for (int i = 0; i < input_ragged_rank; i++) {\n    int split_index = dims + i;\n    int split_size = 1;\n    for (int j = 0; j < ragged_components.size(); j++) {\n      if (!ragged_components[j].nested_splits().empty()) {\n        split_size += ragged_components[j].splits(i).NumElements() - 1;\n      }\n    }\n    output_ragged->append_splits(\n        Tensor(DataTypeToEnum<SPLIT_TYPE>::value, TensorShape({split_size})));\n    auto splits_vec =\n        output_ragged->mutable_splits(split_index)->vec<SPLIT_TYPE>();\n    splits_vec(0) = 0;\n    SPLIT_TYPE last_split_value = 0;\n    int index = 1;\n    for (int j = 0; j < ragged_components.size(); j++) {\n      if (ragged_components[j].nested_splits().empty()) {\n        // Corner case: empty row. e.g [ [[x], [x]], [] ]\n        continue;\n      }\n      auto component_splits_vec =\n          ragged_components[j].splits(i).vec<SPLIT_TYPE>();\n      for (int k = 1; k < component_splits_vec.size(); k++, index++) {\n        splits_vec(index) = component_splits_vec(k) + last_split_value;\n      }\n      last_split_value = splits_vec(index - 1);\n    }\n  }\n\n  // If the variant tensor input is empty, then we have no way to determine\n  // the correct shape for the dense_values.  (It must have rank>=1, and its\n  // outer dimension must be 0, but we don't know its shape beyond that.)\n  // For now, we just use a shape of `[0]` in this case.\n  // TODO(edloper): Update this op with an attribute containing information\n  // about dense_values shape.  If it's `None`, then we'll probably still have\n  // to use shape=[0] here, but if we have more info, then we can use it.\n  // E.g., in map_fn, we may have shape info from the RaggedTensorSpec.\n  TensorShape component_values_shape;\n  if (ragged_components.empty()) {\n    component_values_shape = TensorShape({0});\n  } else {\n    component_values_shape = ragged_components[0].values().shape();\n  }\n\n  // Populate values.\n  int values_size = component_values_shape.dim_size(0);\n  for (int i = 1; i < ragged_components.size(); i++) {\n    if (ragged_components[i].values().dims() != component_values_shape.dims()) {\n      return errors::InvalidArgument(\n          \"Rank of values must match for all \"\n          \"components; values shape at index 0: \",\n          component_values_shape.DebugString(), \", values shape at index \", i,\n          \": \", ragged_components[i].values().shape().DebugString());\n    }\n    values_size += ragged_components[i].values().shape().dim_size(0);\n  }\n  component_values_shape.set_dim(0, values_size);\n  output_ragged->set_values(\n      Tensor(DataTypeToEnum<VALUE_TYPE>::value, component_values_shape));\n  auto output_values_flat =\n      output_ragged->mutable_values()->flat_outer_dims<VALUE_TYPE, 2>();\n  int values_index = 0;\n  for (int i = 0; i < ragged_components.size(); i++) {\n    auto component_values_flat =\n        ragged_components[i].values().flat_outer_dims<VALUE_TYPE, 2>();\n    int num_inner_elements = ragged_components[i].values().NumElements();\n    if (ragged_components[i].values().dim_size(0) > 0) {\n      num_inner_elements /= ragged_components[i].values().dim_size(0);\n    }\n    for (int j = 0; j < ragged_components[i].values().dim_size(0);\n         j++, values_index++) {\n      for (int k = 0; k < num_inner_elements; k++) {\n        output_values_flat(values_index, k) = component_values_flat(j, k);\n      }\n    }\n  }\n  return Status::OK();\n}\n}  // namespace\n\ntemplate <typename VALUE_TYPE, typename SPLIT_TYPE>\nclass RaggedTensorFromVariantOp : public OpKernel {\n public:\n  explicit RaggedTensorFromVariantOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"input_ragged_rank\",\n                                             &input_ragged_rank_attr_));\n    OP_REQUIRES_OK(\n        context, context->GetAttr(\"output_ragged_rank\", &output_ragged_rank_));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    // Read input Tensor.\n    const Tensor& encoded_variant = context->input(0);\n    auto input_ragged_rank_ = input_ragged_rank_attr_;\n\n    if (input_ragged_rank_ == -1) {  // Infer input_ragged_rank_.\n      input_ragged_rank_ = output_ragged_rank_ - encoded_variant.dims();\n      OP_REQUIRES(context, input_ragged_rank_ >= 0,\n                  errors::InvalidArgument(\n                      \"Inferred input_ragged_rank (output_ragged_rank - \"\n                      \"encoded_variant.dims()) must be >= 0, found \"\n                      \"output_ragged_rank: \",\n                      output_ragged_rank_,\n                      \", encoded_variant.dims(): \", encoded_variant.dims(),\n                      \", inferred input_ragged_rank: \", input_ragged_rank_));\n    }\n    OP_REQUIRES(\n        context,\n        output_ragged_rank_ == encoded_variant.dims() + input_ragged_rank_,\n        errors::InvalidArgument(\n            \"output_ragged_rank must be equal to input_ragged_rank + \"\n            \"encoded_ragged.dims(); output_ragged_rank: \",\n            output_ragged_rank_, \", input_ragged_rank: \", input_ragged_rank_,\n            \", encoded_variant.dims(): \", encoded_variant.dims(), \".\"));\n\n    // Decode all variants.\n    const auto value_dtype = DataTypeToEnum<VALUE_TYPE>::v();\n    const auto split_dtype = DataTypeToEnum<SPLIT_TYPE>::v();\n    std::vector<RaggedTensorVariant> decoded_components;\n    OP_REQUIRES_OK(context, RaggedComponentsFromVariant(\n                                encoded_variant, input_ragged_rank_,\n                                value_dtype, split_dtype, &decoded_components));\n\n    // Corner case: input is a scalar.\n    if (encoded_variant.dims() == 0) {\n      ReturnRaggedTensor(context, decoded_components[0]);\n      return;\n    }\n\n    // Nested-Stack Ragged components into a batched RaggedTensor.\n    std::vector<int> encoded_dim_sizes(encoded_variant.dims(), 0);\n    for (int i = 0; i < encoded_variant.dims(); i++) {\n      encoded_dim_sizes[i] = encoded_variant.dim_size(i);\n    }\n    RaggedTensorVariant output_ragged;\n    OP_REQUIRES_OK(\n        context, NestedStackRaggedTensors<VALUE_TYPE, SPLIT_TYPE>(\n                     decoded_components, encoded_dim_sizes, input_ragged_rank_,\n                     output_ragged_rank_, &output_ragged));\n\n    // Set output.\n    ReturnRaggedTensor(context, output_ragged);\n  }\n\n private:\n  int input_ragged_rank_attr_;\n  int output_ragged_rank_;\n\n  void ReturnRaggedTensor(OpKernelContext* context,\n                          const RaggedTensorVariant& ragged_tensor) {\n    int ragged_rank = ragged_tensor.ragged_rank();\n    OpOutputList splits_out;\n    OP_REQUIRES_OK(context,\n                   context->output_list(\"output_nested_splits\", &splits_out));\n    for (int i = 0; i < ragged_rank; i++) {\n      splits_out.set(i, ragged_tensor.splits(i));\n    }\n    context->set_output(ragged_rank, ragged_tensor.values());\n  }\n};\n\n#define REGISTER_KERNELS_WITH_SPLIT_TYPE(value_type, split_type)      \\\n  REGISTER_KERNEL_BUILDER(Name(\"RaggedTensorFromVariant\")             \\\n                              .Device(DEVICE_CPU)                     \\\n                              .TypeConstraint<value_type>(\"Tvalues\")  \\\n                              .TypeConstraint<split_type>(\"Tsplits\"), \\\n                          RaggedTensorFromVariantOp<value_type, split_type>);\n#define REGISTER_KERNELS(value_type)                  \\\n  REGISTER_KERNELS_WITH_SPLIT_TYPE(value_type, int32) \\\n  REGISTER_KERNELS_WITH_SPLIT_TYPE(value_type, int64)\nTF_CALL_POD_TYPES(REGISTER_KERNELS);\nTF_CALL_tstring(REGISTER_KERNELS);\nTF_CALL_QUANTIZED_TYPES(REGISTER_KERNELS);\nTF_CALL_quint16(REGISTER_KERNELS);\nTF_CALL_qint16(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n#undef REGISTER_KERNELS_WITH_SPLIT_TYPE\n}  // namespace tensorflow"