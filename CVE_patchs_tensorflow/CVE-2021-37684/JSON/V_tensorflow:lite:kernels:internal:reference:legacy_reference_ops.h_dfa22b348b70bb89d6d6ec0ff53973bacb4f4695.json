"/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_LEGACY_REFERENCE_OPS_H_\n#define TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_LEGACY_REFERENCE_OPS_H_\n\n#include <stdint.h>\n#include <sys/types.h>\n\n#include \"public/gemmlowp.h\"\n#include \"tensorflow/lite/kernels/internal/common.h\"\n#include \"tensorflow/lite/kernels/internal/legacy_types.h\"\n#include \"tensorflow/lite/kernels/internal/reference/conv.h\"\n#include \"tensorflow/lite/kernels/internal/reference/depthwiseconv_float.h\"\n#include \"tensorflow/lite/kernels/internal/reference/depthwiseconv_uint8.h\"\n#include \"tensorflow/lite/kernels/internal/reference/reference_ops.h\"\n#include \"tensorflow/lite/kernels/internal/reference/tanh.h\"\n#include \"tensorflow/lite/kernels/internal/types.h\"\n\nnamespace tflite {\n\nnamespace reference_ops {\n\nstatic constexpr int kDepthwiseReverseShift = -1;\n\ninline void ShapeFromDims(const tflite::Dims<4>& dims, RuntimeShape* shape) {\n  shape->BuildFrom(\n      {dims.sizes[3], dims.sizes[2], dims.sizes[1], dims.sizes[0]});\n}\n\ninline void DepthwiseConv(const float* input_data, const Dims<4>& input_dims,\n                          const float* filter_data, const Dims<4>& filter_dims,\n                          const float* bias_data, const Dims<4>& bias_dims,\n                          int stride_width, int stride_height,\n                          int dilation_width_factor, int dilation_height_factor,\n                          int pad_width, int pad_height, int depth_multiplier,\n                          float output_activation_min,\n                          float output_activation_max, float* output_data,\n                          const Dims<4>& output_dims) {\n  tflite::DepthwiseParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n  op_params.dilation_width_factor = dilation_width_factor;\n  op_params.dilation_height_factor = dilation_height_factor;\n  op_params.depth_multiplier = depth_multiplier;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n\n  DepthwiseConv(op_params, DimsToShape(input_dims), input_data,\n                DimsToShape(filter_dims), filter_data, DimsToShape(bias_dims),\n                bias_data, DimsToShape(output_dims), output_data);\n}\n\ninline void DepthwiseConv(const float* input_data, const Dims<4>& input_dims,\n                          const float* filter_data, const Dims<4>& filter_dims,\n                          const float* bias_data, const Dims<4>& bias_dims,\n                          int stride_width, int stride_height, int pad_width,\n                          int pad_height, int depth_multiplier,\n                          float output_activation_min,\n                          float output_activation_max, float* output_data,\n                          const Dims<4>& output_dims) {\n  DepthwiseConv(input_data, input_dims, filter_data, filter_dims, bias_data,\n                bias_dims, stride_width, stride_height, 1, 1, pad_width,\n                pad_height, depth_multiplier, output_activation_min,\n                output_activation_max, output_data, output_dims);\n}\n\n// Legacy, for compatibility with old checked-in code.\ntemplate <FusedActivationFunctionType Ac>\nvoid DepthwiseConv(const float* input_data, const Dims<4>& input_dims,\n                   const float* filter_data, const Dims<4>& filter_dims,\n                   const float* bias_data, const Dims<4>& bias_dims,\n                   int stride_width, int stride_height, int pad_width,\n                   int pad_height, int depth_multiplier, float* output_data,\n                   const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  DepthwiseConv(input_data, input_dims, filter_data, filter_dims, bias_data,\n                bias_dims, stride_width, stride_height, pad_width, pad_height,\n                depth_multiplier, output_activation_min, output_activation_max,\n                output_data, output_dims);\n}\n\n// Legacy, for compatibility with old checked-in code.\ntemplate <FusedActivationFunctionType Ac>\nvoid DepthwiseConv(const float* input_data, const Dims<4>& input_dims,\n                   const float* filter_data, const Dims<4>& filter_dims,\n                   const float* bias_data, const Dims<4>& bias_dims, int stride,\n                   int pad_width, int pad_height, int depth_multiplier,\n                   float* output_data, const Dims<4>& output_dims) {\n  DepthwiseConv<Ac>(input_data, input_dims, filter_data, filter_dims, bias_data,\n                    bias_dims, stride, stride, pad_width, pad_height,\n                    depth_multiplier, output_data, output_dims);\n}\n\ninline void DepthwiseConv(const uint8* input_data, const Dims<4>& input_dims,\n                          int32 input_offset, const uint8* filter_data,\n                          const Dims<4>& filter_dims, int32 filter_offset,\n                          const int32* bias_data, const Dims<4>& bias_dims,\n                          int stride_width, int stride_height,\n                          int dilation_width_factor, int dilation_height_factor,\n                          int pad_width, int pad_height, int depth_multiplier,\n                          int32 output_offset, int32 output_multiplier,\n                          int output_shift, int32 output_activation_min,\n                          int32 output_activation_max, uint8* output_data,\n                          const Dims<4>& output_dims) {\n  tflite::DepthwiseParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n  op_params.dilation_width_factor = dilation_width_factor;\n  op_params.dilation_height_factor = dilation_height_factor;\n  op_params.depth_multiplier = depth_multiplier;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  op_params.input_offset = input_offset;\n  op_params.weights_offset = filter_offset;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.output_shift = kDepthwiseReverseShift * output_shift;\n\n  DepthwiseConv(op_params, DimsToShape(input_dims), input_data,\n                DimsToShape(filter_dims), filter_data, DimsToShape(bias_dims),\n                bias_data, DimsToShape(output_dims), output_data);\n}\n\ninline void DepthwiseConv(const uint8* input_data, const Dims<4>& input_dims,\n                          int32 input_offset, const uint8* filter_data,\n                          const Dims<4>& filter_dims, int32 filter_offset,\n                          const int32* bias_data, const Dims<4>& bias_dims,\n                          int stride_width, int stride_height, int pad_width,\n                          int pad_height, int depth_multiplier,\n                          int32 output_offset, int32 output_multiplier,\n                          int output_shift, int32 output_activation_min,\n                          int32 output_activation_max, uint8* output_data,\n                          const Dims<4>& output_dims) {\n  DepthwiseConv(input_data, input_dims, input_offset, filter_data, filter_dims,\n                filter_offset, bias_data, bias_dims, stride_width,\n                stride_height, 1, 1, pad_width, pad_height, depth_multiplier,\n                output_offset, output_multiplier, output_shift,\n                output_activation_min, output_activation_max, output_data,\n                output_dims);\n}\n\n// Legacy, for compatibility with old checked-in code.\ntemplate <FusedActivationFunctionType Ac>\nvoid DepthwiseConv(const uint8* input_data, const Dims<4>& input_dims,\n                   int32 input_offset, const uint8* filter_data,\n                   const Dims<4>& filter_dims, int32 filter_offset,\n                   const int32* bias_data, const Dims<4>& bias_dims,\n                   int stride_width, int stride_height, int pad_width,\n                   int pad_height, int depth_multiplier, int32 output_offset,\n                   int32 output_multiplier, int output_shift,\n                   int32 output_activation_min, int32 output_activation_max,\n                   uint8* output_data, const Dims<4>& output_dims) {\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n  DepthwiseConv(input_data, input_dims, input_offset, filter_data, filter_dims,\n                filter_offset, bias_data, bias_dims, stride_width,\n                stride_height, pad_width, pad_height, depth_multiplier,\n                output_offset, output_multiplier, output_shift,\n                output_activation_min, output_activation_max, output_data,\n                output_dims);\n}\n\n// Legacy, for compatibility with old checked-in code.\ntemplate <FusedActivationFunctionType Ac>\nvoid DepthwiseConv(const uint8* input_data, const Dims<4>& input_dims,\n                   int32 input_offset, const uint8* filter_data,\n                   const Dims<4>& filter_dims, int32 filter_offset,\n                   const int32* bias_data, const Dims<4>& bias_dims, int stride,\n                   int pad_width, int pad_height, int depth_multiplier,\n                   int32 output_offset, int32 output_multiplier,\n                   int output_shift, int32 output_activation_min,\n                   int32 output_activation_max, uint8* output_data,\n                   const Dims<4>& output_dims) {\n  DepthwiseConv<Ac>(input_data, input_dims, input_offset, filter_data,\n                    filter_dims, filter_offset, bias_data, bias_dims, stride,\n                    stride, pad_width, pad_height, depth_multiplier,\n                    output_offset, output_multiplier, output_shift,\n                    output_activation_min, output_activation_max, output_data,\n                    output_dims);\n}\n\ninline void Conv(const float* input_data, const Dims<4>& input_dims,\n                 const float* filter_data, const Dims<4>& filter_dims,\n                 const float* bias_data, const Dims<4>& bias_dims,\n                 int stride_width, int stride_height, int dilation_width_factor,\n                 int dilation_height_factor, int pad_width, int pad_height,\n                 float output_activation_min, float output_activation_max,\n                 float* output_data, const Dims<4>& output_dims,\n                 float* im2col_data, const Dims<4>& im2col_dims) {\n  tflite::ConvParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n  op_params.dilation_width_factor = dilation_width_factor;\n  op_params.dilation_height_factor = dilation_height_factor;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n\n  Conv(op_params, DimsToShape(input_dims), input_data, DimsToShape(filter_dims),\n       filter_data, DimsToShape(bias_dims), bias_data, DimsToShape(output_dims),\n       output_data, DimsToShape(im2col_dims), im2col_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\nvoid Conv(const float* input_data, const Dims<4>& input_dims,\n          const float* filter_data, const Dims<4>& filter_dims,\n          const float* bias_data, const Dims<4>& bias_dims, int stride_width,\n          int stride_height, int dilation_width_factor,\n          int dilation_height_factor, int pad_width, int pad_height,\n          float* output_data, const Dims<4>& output_dims, float* im2col_data,\n          const Dims<4>& im2col_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  Conv(input_data, input_dims, filter_data, filter_dims, bias_data, bias_dims,\n       stride_width, stride_height, dilation_width_factor,\n       dilation_height_factor, pad_width, pad_height, output_activation_min,\n       output_activation_max, output_data, output_dims, im2col_data,\n       im2col_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid Conv(const float* input_data, const Dims<4>& input_dims,\n          const float* filter_data, const Dims<4>& filter_dims,\n          const float* bias_data, const Dims<4>& bias_dims, int stride_width,\n          int stride_height, int pad_width, int pad_height, float* output_data,\n          const Dims<4>& output_dims, float* im2col_data,\n          const Dims<4>& im2col_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  Conv(input_data, input_dims, filter_data, filter_dims, bias_data, bias_dims,\n       stride_width, stride_height, 1, 1, pad_width, pad_height,\n       output_activation_min, output_activation_max, output_data, output_dims,\n       im2col_data, im2col_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid Conv(const float* input_data, const Dims<4>& input_dims,\n          const float* filter_data, const Dims<4>& filter_dims,\n          const float* bias_data, const Dims<4>& bias_dims, int stride,\n          int pad_width, int pad_height, float* output_data,\n          const Dims<4>& output_dims, float* im2col_data,\n          const Dims<4>& im2col_dims) {\n  Conv<Ac>(input_data, input_dims, filter_data, filter_dims, bias_data,\n           bias_dims, stride, stride, 1, 1, pad_width, pad_height, output_data,\n           output_dims, im2col_data, im2col_dims);\n}\n\ninline void Conv(const uint8* input_data, const Dims<4>& input_dims,\n                 int32 input_offset, const uint8* filter_data,\n                 const Dims<4>& filter_dims, int32 filter_offset,\n                 const int32* bias_data, const Dims<4>& bias_dims,\n                 int stride_width, int stride_height, int dilation_width_factor,\n                 int dilation_height_factor, int pad_width, int pad_height,\n                 int32 output_offset, int32 output_multiplier, int output_shift,\n                 int32 output_activation_min, int32 output_activation_max,\n                 uint8* output_data, const Dims<4>& output_dims,\n                 uint8* im2col_data, const Dims<4>& im2col_dims,\n                 gemmlowp::GemmContext* gemmlowp_context) {\n  tflite::ConvParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n  op_params.dilation_width_factor = dilation_width_factor;\n  op_params.dilation_height_factor = dilation_height_factor;\n  op_params.input_offset = input_offset;\n  op_params.weights_offset = filter_offset;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n\n  Conv(op_params, DimsToShape(input_dims), input_data, DimsToShape(filter_dims),\n       filter_data, DimsToShape(bias_dims), bias_data, DimsToShape(output_dims),\n       output_data, DimsToShape(im2col_dims), im2col_data, gemmlowp_context);\n}\n\ninline void Conv(const uint8* input_data, const Dims<4>& input_dims,\n                 int32 input_offset, const uint8* filter_data,\n                 const Dims<4>& filter_dims, int32 filter_offset,\n                 const int32* bias_data, const Dims<4>& bias_dims,\n                 int stride_width, int stride_height, int pad_width,\n                 int pad_height, int32 output_offset, int32 output_multiplier,\n                 int output_shift, int32 output_activation_min,\n                 int32 output_activation_max, uint8* output_data,\n                 const Dims<4>& output_dims, uint8* im2col_data,\n                 const Dims<4>& im2col_dims,\n                 gemmlowp::GemmContext* gemmlowp_context) {\n  Conv(input_data, input_dims, input_offset, filter_data, filter_dims,\n       filter_offset, bias_data, bias_dims, stride_width, stride_height, 1, 1,\n       pad_width, pad_height, output_offset, output_multiplier, output_shift,\n       output_activation_min, output_activation_max, output_data, output_dims,\n       im2col_data, im2col_dims, gemmlowp_context);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\ninline void Conv(const uint8* input_data, const Dims<4>& input_dims,\n                 int32 input_offset, const uint8* filter_data,\n                 const Dims<4>& filter_dims, int32 filter_offset,\n                 const int32* bias_data, const Dims<4>& bias_dims,\n                 int stride_width, int stride_height, int pad_width,\n                 int pad_height, int32 output_offset, int32 output_multiplier,\n                 int output_shift, int32 output_activation_min,\n                 int32 output_activation_max, uint8* output_data,\n                 const Dims<4>& output_dims, uint8* im2col_data,\n                 const Dims<4>& im2col_dims,\n                 gemmlowp::GemmContext* gemmlowp_context) {\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n  Conv(input_data, input_dims, input_offset, filter_data, filter_dims,\n       filter_offset, bias_data, bias_dims, stride_width, stride_height,\n       pad_width, pad_height, output_offset, output_multiplier, output_shift,\n       output_activation_min, output_activation_max, output_data, output_dims,\n       im2col_data, im2col_dims, gemmlowp_context);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid Conv(const uint8* input_data, const Dims<4>& input_dims,\n          int32 input_offset, const uint8* filter_data,\n          const Dims<4>& filter_dims, int32 filter_offset,\n          const int32* bias_data, const Dims<4>& bias_dims, int stride,\n          int pad_width, int pad_height, int32 output_offset,\n          int32 output_multiplier, int output_shift,\n          int32 output_activation_min, int32 output_activation_max,\n          uint8* output_data, const Dims<4>& output_dims, uint8* im2col_data,\n          const Dims<4>& im2col_dims, gemmlowp::GemmContext* gemmlowp_context) {\n  Conv<Ac>(input_data, input_dims, input_offset, filter_data, filter_dims,\n           filter_offset, bias_data, bias_dims, stride, stride, pad_width,\n           pad_height, output_offset, output_multiplier, output_shift,\n           output_activation_min, output_activation_max, output_data,\n           output_dims, im2col_data, im2col_dims, gemmlowp_context);\n}\n\ninline void TransposeConv(const float* input_data, const Dims<4>& input_dims,\n                          const float* filter_data, const Dims<4>& filter_dims,\n                          int stride_width, int stride_height, int pad_width,\n                          int pad_height, float* output_data,\n                          const Dims<4>& output_dims, float* im2col_data,\n                          const Dims<4>& im2col_dims) {\n  tflite::ConvParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n\n  TransposeConv(op_params, DimsToShape(input_dims), input_data,\n                DimsToShape(filter_dims), filter_data,\n                /*bias_shape*/ RuntimeShape(), /*bias*/ nullptr,\n                DimsToShape(output_dims), output_data, DimsToShape(im2col_dims),\n                im2col_data);\n}\n\ninline void TransposeConv(\n    const ConvParams& params, const RuntimeShape& input_shape,\n    const float* input_data, const RuntimeShape& filter_shape,\n    const float* filter_data, const RuntimeShape& output_shape,\n    float* output_data, const RuntimeShape& im2col_shape, float* im2col_data) {\n  TransposeConv(params, input_shape, input_data, filter_shape, filter_data,\n                /*bias_shape*/ RuntimeShape(), /*bias*/ nullptr, output_shape,\n                output_data, im2col_shape, im2col_data);\n}\n\ninline void FullyConnected(const float* input_data, const Dims<4>& input_dims,\n                           const float* weights_data,\n                           const Dims<4>& weights_dims, const float* bias_data,\n                           const Dims<4>& bias_dims,\n                           float output_activation_min,\n                           float output_activation_max, float* output_data,\n                           const Dims<4>& output_dims) {\n  tflite::FullyConnectedParams op_params;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n\n  FullyConnected(op_params, DimsToShape(input_dims), input_data,\n                 DimsToShape(weights_dims), weights_data,\n                 DimsToShape(bias_dims), bias_data, DimsToShape(output_dims),\n                 output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid FullyConnected(const float* input_data, const Dims<4>& input_dims,\n                    const float* weights_data, const Dims<4>& weights_dims,\n                    const float* bias_data, const Dims<4>& bias_dims,\n                    float* output_data, const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  FullyConnected(input_data, input_dims, weights_data, weights_dims, bias_data,\n                 bias_dims, output_activation_min, output_activation_max,\n                 output_data, output_dims);\n}\n\ninline void FullyConnected(\n    const FullyConnectedParams& params, const RuntimeShape& input_shape,\n    const uint8* input_data, const RuntimeShape& filter_shape,\n    const uint8* filter_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape,\n    uint8* output_data, gemmlowp::GemmContext*) {\n  FullyConnected(params, input_shape, input_data, filter_shape, filter_data,\n                 bias_shape, bias_data, output_shape, output_data);\n}\n\ninline void FullyConnected(\n    const FullyConnectedParams& params, const RuntimeShape& input_shape,\n    const uint8* input_data, const RuntimeShape& filter_shape,\n    const uint8* filter_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape,\n    int16* output_data, gemmlowp::GemmContext*) {\n  FullyConnected(params, input_shape, input_data, filter_shape, filter_data,\n                 bias_shape, bias_data, output_shape, output_data);\n}\n\ninline void FullyConnected(const uint8* input_data, const Dims<4>& input_dims,\n                           int32 input_offset, const uint8* filter_data,\n                           const Dims<4>& filter_dims, int32 filter_offset,\n                           const int32* bias_data, const Dims<4>& bias_dims,\n                           int32 output_offset, int32 output_multiplier,\n                           int output_shift, int32 output_activation_min,\n                           int32 output_activation_max, uint8* output_data,\n                           const Dims<4>& output_dims,\n                           gemmlowp::GemmContext* gemmlowp_context) {\n  tflite::FullyConnectedParams op_params;\n  op_params.input_offset = input_offset;\n  op_params.weights_offset = filter_offset;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n\n  FullyConnected(op_params, DimsToShape(input_dims), input_data,\n                 DimsToShape(filter_dims), filter_data, DimsToShape(bias_dims),\n                 bias_data, DimsToShape(output_dims), output_data,\n                 gemmlowp_context);\n}\n\ninline void FullyConnected(const uint8* input_data, const Dims<4>& input_dims,\n                           int32 input_offset, const uint8* filter_data,\n                           const Dims<4>& filter_dims, int32 filter_offset,\n                           const int32* bias_data, const Dims<4>& bias_dims,\n                           int32 output_offset, int32 output_multiplier,\n                           int output_shift, int32 output_activation_min,\n                           int32 output_activation_max, int16* output_data,\n                           const Dims<4>& output_dims,\n                           gemmlowp::GemmContext* gemmlowp_context) {\n  tflite::FullyConnectedParams op_params;\n  op_params.input_offset = input_offset;\n  op_params.weights_offset = filter_offset;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n\n  FullyConnected(op_params, DimsToShape(input_dims), input_data,\n                 DimsToShape(filter_dims), filter_data, DimsToShape(bias_dims),\n                 bias_data, DimsToShape(output_dims), output_data,\n                 gemmlowp_context);\n}\n\ninline void ShuffledFullyConnected(\n    const FullyConnectedParams& params, const RuntimeShape& input_shape,\n    const uint8* input_data, const RuntimeShape& weights_shape,\n    const uint8* shuffled_weights_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape,\n    int16* output_data, uint8* shuffled_input_workspace_data,\n    gemmlowp::GemmContext*) {\n  ShuffledFullyConnected(params, input_shape, input_data, weights_shape,\n                         shuffled_weights_data, bias_shape, bias_data,\n                         output_shape, output_data,\n                         shuffled_input_workspace_data);\n}\n\ninline void ShuffledFullyConnected(\n    const uint8* input_data, const Dims<4>& input_dims,\n    const uint8* shuffled_weights_data, const Dims<4>& weights_dims,\n    const int32* bias_data, const Dims<4>& bias_dims, int32 output_multiplier,\n    int output_shift, int32 output_activation_min, int32 output_activation_max,\n    int16* output_data, const Dims<4>& output_dims,\n    uint8* shuffled_input_workspace_data,\n    gemmlowp::GemmContext* gemmlowp_context) {\n  tflite::FullyConnectedParams op_params;\n  op_params.output_multiplier = output_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n\n  ShuffledFullyConnected(op_params, DimsToShape(input_dims), input_data,\n                         DimsToShape(weights_dims), shuffled_weights_data,\n                         DimsToShape(bias_dims), bias_data,\n                         DimsToShape(output_dims), output_data,\n                         shuffled_input_workspace_data, gemmlowp_context);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid FullyConnected(const uint8* input_data, const Dims<4>& input_dims,\n                    int32 input_offset, const uint8* filter_data,\n                    const Dims<4>& filter_dims, int32 filter_offset,\n                    const int32* bias_data, const Dims<4>& bias_dims,\n                    int32 output_offset, int32 output_multiplier,\n                    int output_shift, int32 output_activation_min,\n                    int32 output_activation_max, uint8* output_data,\n                    const Dims<4>& output_dims,\n                    gemmlowp::GemmContext* gemmlowp_context) {\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n  FullyConnected(input_data, input_dims, input_offset, filter_data, filter_dims,\n                 filter_offset, bias_data, bias_dims, output_offset,\n                 output_multiplier, output_shift, output_activation_min,\n                 output_activation_max, output_data, output_dims,\n                 gemmlowp_context);\n}\n\ninline void LstmCell(const float* input_data, const Dims<4>& input_dims,\n                     const float* prev_activ_data,\n                     const Dims<4>& prev_activ_dims, const float* weights_data,\n                     const Dims<4>& weights_dims, const float* bias_data,\n                     const Dims<4>& bias_dims, const float* prev_state_data,\n                     const Dims<4>& prev_state_dims, float* output_state_data,\n                     const Dims<4>& output_state_dims, float* output_activ_data,\n                     const Dims<4>& output_activ_dims, float* concat_temp_data,\n                     const Dims<4>& concat_temp_dims, float* activ_temp_data,\n                     const Dims<4>& activ_temp_dims) {\n  tflite::LstmCellParams op_params;\n  // Float LSTM cell does not need parameters to be set: leave untouched.\n\n  LstmCell(op_params, DimsToShape(input_dims), input_data,\n           DimsToShape(prev_activ_dims), prev_activ_data,\n           DimsToShape(weights_dims), weights_data, DimsToShape(bias_dims),\n           bias_data, DimsToShape(prev_state_dims), prev_state_data,\n           DimsToShape(output_state_dims), output_state_data,\n           DimsToShape(output_activ_dims), output_activ_data,\n           DimsToShape(concat_temp_dims), concat_temp_data,\n           DimsToShape(activ_temp_dims), activ_temp_data);\n}\n\ntemplate <int StateIntegerBits>\nvoid LstmCell(const uint8* input_data_uint8, const Dims<4>& input_dims,\n              const uint8* prev_activ_data_uint8,\n              const Dims<4>& prev_activ_dims, const uint8* weights_data_uint8,\n              const Dims<4>& weights_dims, const int32* bias_data_int32,\n              const Dims<4>& bias_dims, const int16* prev_state_data_int16,\n              const Dims<4>& prev_state_dims, int16* output_state_data_int16,\n              const Dims<4>& output_state_dims, uint8* output_activ_data_uint8,\n              const Dims<4>& output_activ_dims, uint8* concat_temp_data_uint8,\n              const Dims<4>& concat_temp_dims, int16* activ_temp_data_int16,\n              const Dims<4>& activ_temp_dims, int32 weights_zero_point,\n              int32 accum_multiplier, int accum_shift,\n              gemmlowp::GemmContext* gemmlowp_context) {\n  tflite::LstmCellParams op_params;\n  op_params.weights_zero_point = weights_zero_point;\n  op_params.accum_multiplier = accum_multiplier;\n  op_params.accum_shift = accum_shift;\n\n  LstmCell<StateIntegerBits>(\n      op_params, DimsToShape(input_dims), input_data_uint8,\n      DimsToShape(prev_activ_dims), prev_activ_data_uint8,\n      DimsToShape(weights_dims), weights_data_uint8, DimsToShape(bias_dims),\n      bias_data_int32, DimsToShape(prev_state_dims), prev_state_data_int16,\n      DimsToShape(output_state_dims), output_state_data_int16,\n      DimsToShape(output_activ_dims), output_activ_data_uint8,\n      DimsToShape(concat_temp_dims), concat_temp_data_uint8,\n      DimsToShape(activ_temp_dims), activ_temp_data_int16, gemmlowp_context);\n}\n\ntemplate <typename T>\nvoid BroadcastDiv(const T* input1_data, const Dims<4>& input1_dims,\n                  const T* input2_data, const Dims<4>& input2_dims,\n                  T output_activation_min, T output_activation_max,\n                  T* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  SetActivationParams(output_activation_min, output_activation_max, &op_params);\n\n  BroadcastDivSlow(op_params, DimsToShape(input1_dims), input1_data,\n                   DimsToShape(input2_dims), input2_data,\n                   DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\ninline void Div(const T* input1_data, const Dims<4>& input1_dims,\n                const T* input2_data, const Dims<4>& input2_dims,\n                T output_activation_min, T output_activation_max,\n                T* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  SetActivationParams(output_activation_min, output_activation_max, &op_params);\n\n  Div(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac, typename Scalar>\ninline void Concatenation(int concat_dim, const Scalar* const* input_data,\n                          const Dims<4>* const* input_dims, int inputs_count,\n                          Scalar* output_data, const Dims<4>& output_dims) {\n  // For now we don't have a model with a Concatenation with fused activation.\n  TFLITE_DCHECK_EQ(Ac, FusedActivationFunctionType::kNone);\n\n  std::vector<RuntimeShape> input_shapes(inputs_count);\n  std::vector<const RuntimeShape*> input_shapes_indirect(inputs_count);\n  for (int i = 0; i < inputs_count; ++i) {\n    ShapeFromDims(*input_dims[i], &input_shapes[i]);\n    input_shapes_indirect[i] = &input_shapes[i];\n  }\n  tflite::ConcatenationParams op_params;\n  op_params.axis = 3 - concat_dim;\n  op_params.inputs_count = inputs_count;\n\n  Concatenation(op_params, input_shapes_indirect.data(), input_data,\n                DimsToShape(output_dims), output_data);\n}\n\ninline void Concatenation(int concat_dim, const uint8* const* input_data,\n                          const Dims<4>* const* input_dims,\n                          const int32* input_zeropoint,\n                          const float* input_scale, int inputs_count,\n                          uint8* output_data, const Dims<4>& output_dims,\n                          const int32 output_zeropoint,\n                          const float output_scale) {\n  std::vector<RuntimeShape> input_shapes(inputs_count);\n  std::vector<const RuntimeShape*> input_shapes_indirect(inputs_count);\n  for (int i = 0; i < inputs_count; ++i) {\n    ShapeFromDims(*input_dims[i], &input_shapes[i]);\n    input_shapes_indirect[i] = &input_shapes[i];\n  }\n  tflite::ConcatenationParams op_params;\n  op_params.axis = 3 - concat_dim;\n  op_params.input_zeropoint = input_zeropoint;\n  op_params.input_scale = input_scale;\n  op_params.inputs_count = inputs_count;\n  op_params.output_zeropoint = output_zeropoint;\n  op_params.output_scale = output_scale;\n\n  ConcatenationWithScaling(op_params, input_shapes_indirect.data(), input_data,\n                           DimsToShape(output_dims), output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac, typename Scalar>\nvoid DepthConcatenation(const Scalar* const* input_data,\n                        const Dims<4>* const* input_dims, int inputs_count,\n                        Scalar* output_data, const Dims<4>& output_dims) {\n  // For now we don't have a model with a Concatenation with fused activation.\n  TFLITE_DCHECK_EQ(Ac, FusedActivationFunctionType::kNone);\n\n  std::vector<RuntimeShape> input_shapes(inputs_count);\n  std::vector<const RuntimeShape*> input_shapes_indirect(inputs_count);\n  for (int i = 0; i < inputs_count; ++i) {\n    ShapeFromDims(*input_dims[i], &input_shapes[i]);\n    input_shapes_indirect[i] = &input_shapes[i];\n  }\n  tflite::ConcatenationParams op_params;\n  op_params.inputs_count = inputs_count;\n\n  DepthConcatenation(op_params, input_shapes_indirect.data(), input_data,\n                     DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename Scalar>\nvoid TensorFlowSplit(const Scalar* input_data, const Dims<4>& input_dims,\n                     int axis, int outputs_count, Scalar* const* output_data,\n                     const Dims<4>* const* output_dims) {\n  std::vector<RuntimeShape> output_shapes(outputs_count);\n  std::vector<const RuntimeShape*> output_shapes_indirect(outputs_count);\n  for (int i = 0; i < outputs_count; ++i) {\n    ShapeFromDims(*output_dims[i], &output_shapes[i]);\n    output_shapes_indirect[i] = &output_shapes[i];\n  }\n  tflite::SplitParams op_params;\n  op_params.axis = 3 - axis;\n  op_params.num_split = outputs_count;\n\n  Split(op_params, DimsToShape(input_dims), input_data,\n        output_shapes_indirect.data(), output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac, typename Scalar>\nvoid TensorFlowSplit(const Scalar* input_data, const Dims<4>& input_dims,\n                     int outputs_count, Scalar* const* output_data,\n                     const Dims<4>* const* output_dims) {\n  TFLITE_DCHECK_GE(outputs_count, 1);\n  for (int i = 0; i < outputs_count; i++) {\n    /* batches = */ MatchingArraySize(*output_dims[i], 3, input_dims, 3);\n    /* height = */ MatchingArraySize(*output_dims[i], 2, input_dims, 2);\n    /* width = */ MatchingArraySize(*output_dims[i], 1, input_dims, 1);\n  }\n  // For now we don't have a model with a Split with fused activation.\n  TFLITE_DCHECK_EQ(Ac, FusedActivationFunctionType::kNone);\n\n  TensorFlowSplit(input_data, input_dims, /*axis=*/0, outputs_count,\n                  output_data, output_dims);\n}\n\ninline void Softmax(const float* input_data, const RuntimeShape& input_shape,\n                    float beta, float* output_data,\n                    const RuntimeShape& output_shape) {\n  SoftmaxParams params;\n  params.beta = beta;\n  Softmax(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Softmax(const uint8* input_data, const RuntimeShape& input_shape,\n                    int32 input_beta_multiplier, int32 input_beta_left_shift,\n                    int diff_min, uint8* output_data,\n                    const RuntimeShape& output_shape) {\n  SoftmaxParams params;\n  params.input_multiplier = input_beta_multiplier;\n  params.input_left_shift = input_beta_left_shift;\n  params.diff_min = diff_min;\n  Softmax(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void LogSoftmax(const float* input_data, const RuntimeShape& input_shape,\n                       float* output_data, const RuntimeShape& output_shape) {\n  SoftmaxParams params;\n  // No params currently used for float LogSoftmax.\n  LogSoftmax(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void LogSoftmax(const uint8* input_data, const RuntimeShape& input_shape,\n                       int32 input_multiplier, int32 input_left_shift,\n                       int32 reverse_scaling_divisor,\n                       int32 reverse_scaling_right_shift, int diff_min,\n                       uint8* output_data, const RuntimeShape& output_shape) {\n  SoftmaxParams params;\n  params.input_multiplier = input_multiplier;\n  params.input_left_shift = input_left_shift;\n  params.reverse_scaling_divisor = reverse_scaling_divisor;\n  params.reverse_scaling_right_shift = reverse_scaling_right_shift;\n  params.diff_min = diff_min;\n  LogSoftmax(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Logistic(const LogisticParams& params,\n                     const RuntimeShape& input_shape, const uint8* input_data,\n                     const RuntimeShape& output_shape, uint8* output_data) {\n  const int32 input_zero_point = params.input_zero_point;\n  const int32 input_range_radius = params.input_range_radius;\n  const int32 input_multiplier = params.input_multiplier;\n  const int input_left_shift = params.input_left_shift;\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n\n  for (int i = 0; i < flat_size; i++) {\n    const uint8 input_val_u8 = input_data[i];\n    const int32 input_val_centered =\n        static_cast<int32>(input_val_u8) - input_zero_point;\n    uint8 output_val;\n    if (input_val_centered <= -input_range_radius) {\n      output_val = 0;\n    } else if (input_val_centered >= input_range_radius) {\n      output_val = 255;\n    } else {\n      const int32 input_val_rescaled =\n          MultiplyByQuantizedMultiplierGreaterThanOne(\n              input_val_centered, input_multiplier, input_left_shift);\n      using FixedPoint4 = gemmlowp::FixedPoint<int32, 4>;\n      using FixedPoint0 = gemmlowp::FixedPoint<int32, 0>;\n      const FixedPoint4 input_val_f4 = FixedPoint4::FromRaw(input_val_rescaled);\n      const FixedPoint0 output_val_f0 = gemmlowp::logistic(input_val_f4);\n      // Convert from Q0.31 to Q23.8.\n      using gemmlowp::RoundingDivideByPOT;\n      int32 output_val_s32 = RoundingDivideByPOT(output_val_f0.raw(), 23);\n      if (output_val_s32 == 256) {\n        output_val_s32 = 255;\n      }\n      // Reinterpret as U0.8.\n      TFLITE_DCHECK_GE(output_val_s32, 0);\n      TFLITE_DCHECK_LE(output_val_s32, 255);\n      output_val = static_cast<uint8>(output_val_s32);\n    }\n    output_data[i] = output_val;\n  }\n}\n\ninline void Logistic(const uint8* input_data, const RuntimeShape& input_shape,\n                     int32 input_zero_point, int32 input_range_radius,\n                     int32 input_multiplier, int input_left_shift,\n                     uint8* output_data, const RuntimeShape& output_shape) {\n  LogisticParams params;\n  params.input_zero_point = input_zero_point;\n  params.input_range_radius = input_range_radius;\n  params.input_multiplier = input_multiplier;\n  params.input_left_shift = input_left_shift;\n  Logistic(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Logistic(const RuntimeShape& input_shape, const int16* input_data,\n                     const RuntimeShape& output_shape, int16* output_data) {\n  LogisticParams params;\n  // No params currently needed by int16 Logistic.\n  Logistic(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Tanh(const uint8* input_data, const RuntimeShape& input_shape,\n                 int32 input_zero_point, int32 input_range_radius,\n                 int32 input_multiplier, int input_left_shift,\n                 uint8* output_data, const RuntimeShape& output_shape) {\n  TanhParams params;\n  params.input_zero_point = input_zero_point;\n  params.input_range_radius = input_range_radius;\n  params.input_multiplier = input_multiplier;\n  params.input_left_shift = input_left_shift;\n  Tanh(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Tanh(const int16* input_data, const RuntimeShape& input_shape,\n                 int input_left_shift, int16* output_data,\n                 const RuntimeShape& output_shape) {\n  TanhParams params;\n  params.input_left_shift = input_left_shift;\n  Tanh(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Dequantize(const uint8* input_data, const Dims<4>& input_dims,\n                       int32 zero_point, double scale, float* output_data,\n                       const Dims<4>& output_dims) {\n  tflite::DequantizationParams op_params;\n  op_params.zero_point = zero_point;\n  op_params.scale = scale;\n\n  Dequantize(op_params, DimsToShape(input_dims), input_data,\n             DimsToShape(output_dims), output_data);\n}\n\ninline void FakeQuant(const float* input_data, const Dims<4>& input_dims,\n                      float rmin, float rmax, int num_bits, float* output_data,\n                      const Dims<4>& output_dims) {\n  tflite::FakeQuantParams op_params;\n  op_params.num_bits = num_bits;\n  op_params.minmax.min = rmin;\n  op_params.minmax.max = rmax;\n\n  FakeQuant(op_params, DimsToShape(input_dims), input_data,\n            DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\ninline void Gather(const T* input_data, const Dims<4>& input_dims,\n                   int input_rank, const int32* coords_data,\n                   const Dims<4>& coords_dims, T* output_data,\n                   const Dims<4>& output_dims) {\n  tflite::GatherParams op_params;\n  op_params.axis = 4 - input_rank;\n  op_params.batch_dims = 0;\n\n  Gather(op_params, DimsToShape(input_dims), input_data,\n         DimsToShape(coords_dims), coords_data, DimsToShape(output_dims),\n         output_data);\n}\n\ninline uint32 LegacyReverseBits32(uint32 n) {\n  n = ((n >> 1) & 0x55555555) | ((n & 0x55555555) << 1);\n  n = ((n >> 2) & 0x33333333) | ((n & 0x33333333) << 2);\n  n = ((n >> 4) & 0x0F0F0F0F) | ((n & 0x0F0F0F0F) << 4);\n  return (((n & 0xFF) << 24) | ((n & 0xFF00) << 8) | ((n & 0xFF0000) >> 8) |\n          ((n & 0xFF000000) >> 24));\n}\n\ninline void StridedSliceReverseIndices(tflite::StridedSliceParams* p) {\n  TFLITE_CHECK_EQ(p->start_indices_count, p->stop_indices_count);\n  TFLITE_CHECK_EQ(p->stop_indices_count, p->strides_count);\n\n  std::reverse(p->start_indices, p->start_indices + p->start_indices_count);\n  std::reverse(p->stop_indices, p->stop_indices + p->stop_indices_count);\n  std::reverse(p->strides, p->strides + p->strides_count);\n\n  p->begin_mask = LegacyReverseBits32(static_cast<uint32>(p->begin_mask)) >>\n                  (32 - p->start_indices_count);\n  p->ellipsis_mask =\n      LegacyReverseBits32(static_cast<uint32>(p->ellipsis_mask)) >>\n      (32 - p->start_indices_count);\n  p->end_mask = LegacyReverseBits32(static_cast<uint32>(p->end_mask)) >>\n                (32 - p->start_indices_count);\n  p->new_axis_mask =\n      LegacyReverseBits32(static_cast<uint32>(p->new_axis_mask)) >>\n      (32 - p->start_indices_count);\n  p->shrink_axis_mask =\n      LegacyReverseBits32(static_cast<uint32>(p->shrink_axis_mask)) >>\n      (32 - p->start_indices_count);\n}\n\ntemplate <typename T>\ninline void StridedSlice(const T* input_data, const Dims<4>& input_dims,\n                         int begin_mask, int end_mask, int shrink_axis_mask,\n                         const std::vector<int>& start_indices,\n                         const std::vector<int>& stop_indices,\n                         const std::vector<int>& strides, T* output_data,\n                         const Dims<4>& output_dims) {\n  TFLITE_DCHECK_EQ(start_indices.size(), 4);\n  auto op_params = strided_slice::BuildStridedSliceParams(\n      begin_mask, end_mask, shrink_axis_mask, start_indices, stop_indices,\n      strides);\n  StridedSliceReverseIndices(&op_params);\n\n  StridedSlice(op_params, DimsToShape(input_dims), input_data,\n               DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\ninline void Mean(const T* input_data, const Dims<4>& input_dims,\n                 const std::vector<int>& reduction_indices, T* output_data,\n                 const Dims<4>& output_dims) {\n  tflite::MeanParams op_params;\n  op_params.axis_count = reduction_indices.size();\n  for (int i = 0; i < op_params.axis_count; ++i) {\n    op_params.axis[i] = reduction_indices[op_params.axis_count - 1 - i];\n  }\n\n  Mean(op_params, DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n       output_data);\n}\n\ntemplate <typename T>\nvoid Transpose(const T* input, const Dims<4>& input_dims, T* output,\n               const Dims<4>& output_dims, const int* permuted_axes) {\n  TransposeParams params;\n  params.perm_count = 4;\n  for (int i = 0; i < 4; ++i) {\n    params.perm[i] = 3 - permuted_axes[3 - i];\n  }\n  Transpose(params, DimsToShape(input_dims), input, DimsToShape(output_dims),\n            output);\n}\n\ntemplate <typename T, ComparisonFn<T> F>\ninline void Comparison(const T* input1_data, const Dims<4>& input1_dims,\n                       const T* input2_data, const Dims<4>& input2_dims,\n                       bool* output_data, const Dims<4>& output_dims) {\n  ComparisonParams op_params;\n  // No parameters needed.\n  ComparisonImpl<T, F>(op_params, DimsToShape(input1_dims), input1_data,\n                       DimsToShape(input2_dims), input2_data,\n                       DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T, ComparisonFn<int32> F>\ninline void Comparison(int left_shift, const T* input1_data,\n                       const Dims<4>& input1_dims, int32 input1_offset,\n                       int32 input1_multiplier, int input1_shift,\n                       const T* input2_data, const Dims<4>& input2_dims,\n                       int32 input2_offset, int32 input2_multiplier,\n                       int input2_shift, bool* output_data,\n                       const Dims<4>& output_dims) {\n  tflite::ComparisonParams op_params;\n  op_params.left_shift = left_shift;\n  op_params.input1_offset = input1_offset;\n  op_params.input1_multiplier = input1_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.input1_shift = kReverseShift * input1_shift;\n  op_params.input2_offset = input2_offset;\n  op_params.input2_multiplier = input2_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.input2_shift = kReverseShift * input2_shift;\n\n  ComparisonWithScaling<T, F>(op_params, DimsToShape(input1_dims), input1_data,\n                              DimsToShape(input2_dims), input2_data,\n                              DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T, ComparisonFn<T> F>\ninline void BroadcastComparison(const T* input1_data,\n                                const Dims<4>& input1_dims,\n                                const T* input2_data,\n                                const Dims<4>& input2_dims, bool* output_data,\n                                const Dims<4>& output_dims) {\n  ComparisonParams op_params;\n  // No parameters needed.\n  BroadcastComparison4DSlowImpl<T, F>(op_params, DimsToShape(input1_dims),\n                                      input1_data, DimsToShape(input2_dims),\n                                      input2_data, DimsToShape(output_dims),\n                                      output_data);\n}\n\ntemplate <typename T, ComparisonFn<int32> F>\ninline void BroadcastComparison(int left_shift, const T* input1_data,\n                                const Dims<4>& input1_dims, int32 input1_offset,\n                                int32 input1_multiplier, int input1_shift,\n                                const T* input2_data,\n                                const Dims<4>& input2_dims, int32 input2_offset,\n                                int32 input2_multiplier, int input2_shift,\n                                bool* output_data, const Dims<4>& output_dims) {\n  ComparisonParams op_params;\n\n  op_params.left_shift = left_shift;\n  op_params.input1_offset = input1_offset;\n  op_params.input1_multiplier = input1_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.input1_shift = kReverseShift * input1_shift;\n  op_params.input2_offset = input2_offset;\n  op_params.input2_multiplier = input2_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.input2_shift = kReverseShift * input2_shift;\n\n  BroadcastComparison4DSlowWithScaling<T, F>(\n      op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\n#define TFLITE_LEGACY_COMPARISON_OP(name)                                     \\\n  template <typename T>                                                       \\\n  inline void name(const T* input1_data, const Dims<4>& input1_dims,          \\\n                   const T* input2_data, const Dims<4>& input2_dims,          \\\n                   bool* output_data, const Dims<4>& output_dims) {           \\\n    ruy::profiler::ScopeLabel label(#name);                                   \\\n    Comparison<T, name##Fn>(input1_data, input1_dims, input2_data,            \\\n                            input2_dims, output_data, output_dims);           \\\n  }                                                                           \\\n  template <typename T>                                                       \\\n  inline void name(                                                           \\\n      int left_shift, const T* input1_data, const Dims<4>& input1_dims,       \\\n      int32 input1_offset, int32 input1_multiplier, int input1_shift,         \\\n      const T* input2_data, const Dims<4>& input2_dims, int32 input2_offset,  \\\n      int32 input2_multiplier, int input2_shift, bool* output_data,           \\\n      const Dims<4>& output_dims) {                                           \\\n    ruy::profiler::ScopeLabel label(#name \"/8bit\");                           \\\n    Comparison<T, name##Fn>(left_shift, input1_data, input1_dims,             \\\n                            input1_offset, input1_multiplier, input1_shift,   \\\n                            input2_data, input2_dims, input2_offset,          \\\n                            input2_multiplier, input2_shift, output_data,     \\\n                            output_dims);                                     \\\n  }                                                                           \\\n  template <typename T>                                                       \\\n  inline void Broadcast##name(                                                \\\n      const T* input1_data, const Dims<4>& input1_dims, const T* input2_data, \\\n      const Dims<4>& input2_dims, bool* output_data,                          \\\n      const Dims<4>& output_dims) {                                           \\\n    ruy::profiler::ScopeLabel label(\"Broadcast\" #name);                       \\\n    BroadcastComparison<T, name##Fn>(input1_data, input1_dims, input2_data,   \\\n                                     input2_dims, output_data, output_dims);  \\\n  }                                                                           \\\n  template <typename T>                                                       \\\n  inline void Broadcast##name(                                                \\\n      int left_shift, const T* input1_data, const Dims<4>& input1_dims,       \\\n      int32 input1_offset, int32 input1_multiplier, int input1_shift,         \\\n      const T* input2_data, const Dims<4>& input2_dims, int32 input2_offset,  \\\n      int32 input2_multiplier, int input2_shift, bool* output_data,           \\\n      const Dims<4>& output_dims) {                                           \\\n    ruy::profiler::ScopeLabel label(\"Broadcast\" #name \"/8bit\");               \\\n    BroadcastComparison<T, name##Fn>(left_shift, input1_data, input1_dims,    \\\n                                     input1_offset, input1_multiplier,        \\\n                                     input1_shift, input2_data, input2_dims,  \\\n                                     input2_offset, input2_multiplier,        \\\n                                     input2_shift, output_data, output_dims); \\\n  }\nTFLITE_LEGACY_COMPARISON_OP(Equal);\nTFLITE_LEGACY_COMPARISON_OP(NotEqual);\nTFLITE_LEGACY_COMPARISON_OP(Greater);\nTFLITE_LEGACY_COMPARISON_OP(GreaterEqual);\nTFLITE_LEGACY_COMPARISON_OP(Less);\nTFLITE_LEGACY_COMPARISON_OP(LessEqual);\n#undef TFLITE_LEGACY_COMPARISON_OP\n\ntemplate <typename D, typename T>\ninline void Select(const D* input_condition_data,\n                   const Dims<4>& input_condition_dims, const T* input_x_data,\n                   const Dims<4>& input_x_dims, const T* input_y_data,\n                   const Dims<4>& input_y_dims, T* output_data,\n                   const Dims<4>& output_dims) {\n  Select(DimsToShape(input_condition_dims), input_condition_data,\n         DimsToShape(input_x_dims), input_x_data, DimsToShape(input_y_dims),\n         input_y_data, DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename D, typename T>\ninline void RankOneSelect(const D* input_condition_data,\n                          const Dims<4>& input_condition_dims,\n                          const T* input_x_data, const Dims<4>& input_x_dims,\n                          const T* input_y_data, const Dims<4>& input_y_dims,\n                          T* output_data, const Dims<4>& output_dims) {\n  RankOneSelect(DimsToShape(input_condition_dims), input_condition_data,\n                DimsToShape(input_x_dims), input_x_data,\n                DimsToShape(input_y_dims), input_y_data,\n                DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T, typename TI>\ninline void SparseToDense(const std::vector<std::vector<TI>>& indices,\n                          const T* values, T default_value, T* output_data,\n                          const Dims<4>& output_dims, bool value_is_scalar) {\n  SparseToDense(indices, values, default_value, value_is_scalar,\n                DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename Scalar>\nvoid Pack(int dim, const Scalar* const* input_data,\n          const Dims<4>* const* input_dims, int inputs_count,\n          Scalar* output_data, const Dims<4>& output_dims) {\n  std::vector<RuntimeShape> input_shapes(inputs_count);\n  std::vector<const RuntimeShape*> input_shapes_indirect(inputs_count);\n  for (int i = 0; i < inputs_count; ++i) {\n    ShapeFromDims(*input_dims[i], &input_shapes[i]);\n    input_shapes_indirect[i] = &input_shapes[i];\n  }\n  tflite::PackParams op_params;\n  op_params.axis = 3 - dim;\n  op_params.inputs_count = inputs_count;\n\n  Pack(op_params, input_shapes_indirect.data(), input_data,\n       DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename Scalar>\nvoid Unpack(int axis, const Scalar* input_data, const Dims<4>& input_dims,\n            int dimensions, int outputs_count, Scalar* const* output_datas,\n            const Dims<4>& output_dims) {\n  tflite::UnpackParams op_params;\n  op_params.axis = 3 - axis;\n  op_params.num_split = outputs_count;\n\n  Unpack(op_params, DimsToShape(input_dims), input_data,\n         DimsToShape(output_dims), output_datas);\n}\n\ntemplate <typename Scalar>\nvoid Pack(int dim, const Scalar* const* input_data,\n          const Dims<4>* const* input_dims, const int32* input_zeropoint,\n          const float* input_scale, int inputs_count, Scalar* output_data,\n          const Dims<4>& output_dims, const int32 output_zeropoint,\n          const float output_scale) {\n  std::vector<RuntimeShape> input_shapes(inputs_count);\n  std::vector<const RuntimeShape*> input_shapes_indirect(inputs_count);\n  for (int i = 0; i < inputs_count; ++i) {\n    ShapeFromDims(*input_dims[i], &input_shapes[i]);\n    input_shapes_indirect[i] = &input_shapes[i];\n  }\n  tflite::PackParams op_params;\n  op_params.axis = 3 - dim;\n  op_params.input_zeropoint = input_zeropoint;\n  op_params.input_scale = input_scale;\n  op_params.inputs_count = inputs_count;\n  op_params.output_zeropoint = output_zeropoint;\n  op_params.output_scale = output_scale;\n\n  PackWithScaling(op_params, input_shapes_indirect.data(), input_data,\n                  DimsToShape(output_dims), output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\nvoid L2Normalization(const float* input_data, const RuntimeShape& input_shape,\n                     float* output_data, const RuntimeShape& output_shape) {\n  static_assert(Ac == FusedActivationFunctionType::kNone, \"\");\n  tflite::L2NormalizationParams op_params;\n  // No params need to be set for float.\n\n  L2Normalization(op_params, input_shape, input_data, output_shape,\n                  output_data);\n}\n\ninline void L2Normalization(const uint8* input_data,\n                            const RuntimeShape& input_shape,\n                            int32 input_zero_point, uint8* output_data,\n                            const RuntimeShape& output_shape) {\n  tflite::L2NormalizationParams op_params;\n  op_params.input_zero_point = input_zero_point;\n\n  L2Normalization(op_params, input_shape, input_data, output_shape,\n                  output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\nvoid L2Normalization(const float* input_data, const Dims<4>& input_dims,\n                     float* output_data, const Dims<4>& output_dims) {\n  L2Normalization<Ac>(input_data, DimsToShape(input_dims), output_data,\n                      DimsToShape(output_dims));\n}\n\ninline void L2Normalization(const uint8* input_data, const Dims<4>& input_dims,\n                            int32 input_zero_point, uint8* output_data,\n                            const Dims<4>& output_dims) {\n  L2Normalization(input_data, DimsToShape(input_dims), input_zero_point,\n                  output_data, DimsToShape(output_dims));\n}\n\ninline void Relu(const float* input_data, const Dims<4>& input_dims,\n                 float* output_data, const Dims<4>& output_dims) {\n  Relu(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n       output_data);\n}\n\ninline void Relu1(const float* input_data, const Dims<4>& input_dims,\n                  float* output_data, const Dims<4>& output_dims) {\n  Relu1(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n        output_data);\n}\n\ninline void Relu6(const float* input_data, const Dims<4>& input_dims,\n                  float* output_data, const Dims<4>& output_dims) {\n  Relu6(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n        output_data);\n}\n\ninline void ReluX(uint8 min_value, uint8 max_value, const uint8* input_data,\n                  const RuntimeShape& input_shape, uint8* output_data,\n                  const RuntimeShape& output_shape) {\n  tflite::ActivationParams params;\n  params.quantized_activation_max = max_value;\n  params.quantized_activation_min = min_value;\n  ReluX(params, input_shape, input_data, output_shape, output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\ninline void Add(int left_shift, const uint8* input1_data,\n                const Dims<4>& input1_dims, int32 input1_offset,\n                int32 input1_multiplier, int input1_shift,\n                const uint8* input2_data, const Dims<4>& input2_dims,\n                int32 input2_offset, int32 input2_multiplier, int input2_shift,\n                int32 output_offset, int32 output_multiplier, int output_shift,\n                int32 output_activation_min, int32 output_activation_max,\n                uint8* output_data, const Dims<4>& output_dims) {\n  constexpr int kReverseShift = -1;\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n\n  tflite::ArithmeticParams op_params;\n  op_params.left_shift = left_shift;\n  op_params.input1_offset = input1_offset;\n  op_params.input1_multiplier = input1_multiplier;\n  op_params.input1_shift = kReverseShift * input1_shift;\n  op_params.input2_offset = input2_offset;\n  op_params.input2_multiplier = input2_multiplier;\n  op_params.input2_shift = kReverseShift * input2_shift;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  Add(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\nvoid Add(const int32* input1_data, const Dims<4>& input1_dims,\n         const int32* input2_data, const Dims<4>& input2_dims,\n         int32* output_data, const Dims<4>& output_dims) {\n  ruy::profiler::ScopeLabel label(\"Add/int32\");\n  TFLITE_DCHECK(Ac == FusedActivationFunctionType::kNone);\n\n  tflite::ArithmeticParams op_params;\n  op_params.quantized_activation_min = std::numeric_limits<int32>::min();\n  op_params.quantized_activation_max = std::numeric_limits<int32>::max();\n  Add(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\ninline void BroadcastAdd(int left_shift, const uint8* input1_data,\n                         const Dims<4>& input1_dims, int32 input1_offset,\n                         int32 input1_multiplier, int input1_shift,\n                         const uint8* input2_data, const Dims<4>& input2_dims,\n                         int32 input2_offset, int32 input2_multiplier,\n                         int input2_shift, int32 output_offset,\n                         int32 output_multiplier, int output_shift,\n                         int32 output_activation_min,\n                         int32 output_activation_max, uint8* output_data,\n                         const Dims<4>& output_dims) {\n  constexpr int kReverseShift = -1;\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n\n  tflite::ArithmeticParams op_params;\n  op_params.left_shift = left_shift;\n  op_params.input1_offset = input1_offset;\n  op_params.input1_multiplier = input1_multiplier;\n  op_params.input1_shift = kReverseShift * input1_shift;\n  op_params.input2_offset = input2_offset;\n  op_params.input2_multiplier = input2_multiplier;\n  op_params.input2_shift = kReverseShift * input2_shift;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  BroadcastAdd4DSlow(op_params, DimsToShape(input1_dims), input1_data,\n                     DimsToShape(input2_dims), input2_data,\n                     DimsToShape(output_dims), output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\nvoid Add(const float* input1_data, const Dims<4>& input1_dims,\n         const float* input2_data, const Dims<4>& input2_dims,\n         float* output_data, const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n\n  tflite::ArithmeticParams op_params;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n  Add(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <typename T>\nvoid BroadcastAdd(const T* input1_data, const Dims<4>& input1_dims,\n                  const T* input2_data, const Dims<4>& input2_dims,\n                  T output_activation_min, T output_activation_max,\n                  T* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n  BroadcastAdd4DSlow(op_params, DimsToShape(input1_dims), input1_data,\n                     DimsToShape(input2_dims), input2_data,\n                     DimsToShape(output_dims), output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\ninline void BroadcastAddFivefold(\n    int y0, int y1, int y2, int y3, int y4, int left_shift,\n    const uint8* input1_data, const Dims<4>& input1_dims, int32 input1_offset,\n    int32 input1_multiplier, int input1_shift, const uint8* input2_data,\n    const Dims<4>& input2_dims, int32 input2_offset, int32 input2_multiplier,\n    int input2_shift, int32 output_offset, int32 output_multiplier,\n    int output_shift, int32 output_activation_min, int32 output_activation_max,\n    uint8* output_data, const Dims<4>& output_dims) {\n  constexpr int kReverseShift = -1;\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n  tflite::ArithmeticParams op_params;\n  op_params.broadcast_category =\n      tflite::BroadcastableOpCategory::kFirstInputBroadcastsFast;\n  op_params.left_shift = left_shift;\n  op_params.input1_offset = input1_offset;\n  op_params.input1_multiplier = input1_multiplier;\n  op_params.input1_shift = kReverseShift * input1_shift;\n  op_params.input2_offset = input2_offset;\n  op_params.input2_multiplier = input2_multiplier;\n  op_params.input2_shift = kReverseShift * input2_shift;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  op_params.broadcast_shape[4] = y0;\n  op_params.broadcast_shape[3] = y1;\n  op_params.broadcast_shape[2] = y2;\n  op_params.broadcast_shape[1] = y3;\n  op_params.broadcast_shape[0] = y4;\n  BroadcastAddFivefold(op_params, DimsToShape(input1_dims), input1_data,\n                       DimsToShape(input2_dims), input2_data,\n                       DimsToShape(output_dims), output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac, typename T>\nvoid BroadcastAdd(const T* input1_data, const Dims<4>& input1_dims,\n                  const T* input2_data, const Dims<4>& input2_dims,\n                  T* output_data, const Dims<4>& output_dims) {\n  T output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n\n  BroadcastAdd(input1_data, input1_dims, input2_data, input2_dims,\n               output_activation_min, output_activation_max, output_data,\n               output_dims);\n}\n\ntemplate <FusedActivationFunctionType Ac>\ninline void Add(const int16* input1_data, const Dims<4>& input1_dims,\n                int input1_shift, const int16* input2_data,\n                const Dims<4>& input2_dims, int input2_shift,\n                int16 output_activation_min, int16 output_activation_max,\n                int16* output_data, const Dims<4>& output_dims) {\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, -32768);\n    TFLITE_DCHECK_EQ(output_activation_max, 32767);\n  }\n\n  tflite::ArithmeticParams op_params;\n  op_params.input1_shift = kReverseShift * input1_shift;\n  op_params.input2_shift = kReverseShift * input2_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  Add(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ninline void Sub(const float* input1_data, const Dims<4>& input1_dims,\n                const float* input2_data, const Dims<4>& input2_dims,\n                float* output_data, const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(FusedActivationFunctionType::kNone,\n                      &output_activation_min, &output_activation_max);\n  tflite::ArithmeticParams op_params;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n  Sub(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <typename T>\nvoid Sub(const T* input1_data, const Dims<4>& input1_dims, const T* input2_data,\n         const Dims<4>& input2_dims, T* output_data,\n         const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  op_params.quantized_activation_min = std::numeric_limits<T>::min();\n  op_params.quantized_activation_max = std::numeric_limits<T>::max();\n  Sub(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ninline void AveragePool(const float* input_data, const Dims<4>& input_dims,\n                        int stride_width, int stride_height, int pad_width,\n                        int pad_height, int kwidth, int kheight,\n                        float output_activation_min,\n                        float output_activation_max, float* output_data,\n                        const Dims<4>& output_dims) {\n  tflite::PoolParams params;\n  params.stride_height = stride_height;\n  params.stride_width = stride_width;\n  params.filter_height = kheight;\n  params.filter_width = kwidth;\n  params.padding_values.height = pad_height;\n  params.padding_values.width = pad_width;\n  params.float_activation_min = output_activation_min;\n  params.float_activation_max = output_activation_max;\n  AveragePool(params, DimsToShape(input_dims), input_data,\n              DimsToShape(output_dims), output_data);\n}\n\n// Transitional version that will be moved shortly to legacy_reference_ops, as\n// part of RuntimeShape revisions.\ninline void BroadcastMul4DSlow(const uint8* input1_data,\n                               const Dims<4>& input1_dims, int32 input1_offset,\n                               const uint8* input2_data,\n                               const Dims<4>& input2_dims, int32 input2_offset,\n                               int32 output_offset, int32 output_multiplier,\n                               int output_shift, int32 output_activation_min,\n                               int32 output_activation_max, uint8* output_data,\n                               const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  SetActivationParams(output_activation_min, output_activation_max, &op_params);\n  op_params.input1_offset = input1_offset;\n  op_params.input2_offset = input2_offset;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  op_params.output_shift = output_shift;\n\n  BroadcastMul4DSlow(op_params, DimsToShape(input1_dims), input1_data,\n                     DimsToShape(input2_dims), input2_data,\n                     DimsToShape(output_dims), output_data);\n}\n\ninline void BroadcastMul(const uint8* input1_data, const Dims<4>& input1_dims,\n                         int32 input1_offset, const uint8* input2_data,\n                         const Dims<4>& input2_dims, int32 input2_offset,\n                         int32 output_offset, int32 output_multiplier,\n                         int output_shift, int32 output_activation_min,\n                         int32 output_activation_max, uint8* output_data,\n                         const Dims<4>& output_dims) {\n  BroadcastMul4DSlow(\n      input1_data, input1_dims, input1_offset, input2_data, input2_dims,\n      input2_offset, output_offset, output_multiplier,\n      //\n      kReverseShift * output_shift,\n      //\n      output_activation_min, output_activation_max, output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\ninline void BroadcastMul(const uint8* input1_data, const Dims<4>& input1_dims,\n                         int32 input1_offset, const uint8* input2_data,\n                         const Dims<4>& input2_dims, int32 input2_offset,\n                         int32 output_offset, int32 output_multiplier,\n                         int output_shift, int32 output_activation_min,\n                         int32 output_activation_max, uint8* output_data,\n                         const Dims<4>& output_dims) {\n  BroadcastMul(input1_data, input1_dims, input1_offset, input2_data,\n               input2_dims, input2_offset, output_offset, output_multiplier,\n               output_shift, output_activation_min, output_activation_max,\n               output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid AveragePool(const float* input_data, const Dims<4>& input_dims,\n                 int stride_width, int stride_height, int pad_width,\n                 int pad_height, int kwidth, int kheight, float* output_data,\n                 const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n\n  AveragePool(input_data, input_dims, stride_width, stride_height, pad_width,\n              pad_height, kwidth, kheight, output_activation_min,\n              output_activation_max, output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid AveragePool(const float* input_data, const Dims<4>& input_dims, int stride,\n                 int pad_width, int pad_height, int filter_width,\n                 int filter_height, float* output_data,\n                 const Dims<4>& output_dims) {\n  AveragePool<Ac>(input_data, input_dims, stride, stride, pad_width, pad_height,\n                  filter_width, filter_height, output_data, output_dims);\n}\n\ninline void AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n                        int stride_width, int stride_height, int pad_width,\n                        int pad_height, int filter_width, int filter_height,\n                        int32 output_activation_min,\n                        int32 output_activation_max, uint8* output_data,\n                        const Dims<4>& output_dims) {\n  tflite::PoolParams params;\n  params.stride_height = stride_height;\n  params.stride_width = stride_width;\n  params.filter_height = filter_height;\n  params.filter_width = filter_width;\n  params.padding_values.height = pad_height;\n  params.padding_values.width = pad_width;\n  params.quantized_activation_min = output_activation_min;\n  params.quantized_activation_max = output_activation_max;\n  AveragePool(params, DimsToShape(input_dims), input_data,\n              DimsToShape(output_dims), output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n                 int stride_width, int stride_height, int pad_width,\n                 int pad_height, int filter_width, int filter_height,\n                 int32 output_activation_min, int32 output_activation_max,\n                 uint8* output_data, const Dims<4>& output_dims) {\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n  AveragePool(input_data, input_dims, stride_width, stride_height, pad_width,\n              pad_height, filter_width, filter_height, output_activation_min,\n              output_activation_max, output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid AveragePool(const uint8* input_data, const Dims<4>& input_dims, int stride,\n                 int pad_width, int pad_height, int filter_width,\n                 int filter_height, int32 output_activation_min,\n                 int32 output_activation_max, uint8* output_data,\n                 const Dims<4>& output_dims) {\n  AveragePool<Ac>(input_data, input_dims, stride, stride, pad_width, pad_height,\n                  filter_width, filter_height, output_activation_min,\n                  output_activation_max, output_data, output_dims);\n}\n\ninline void MaxPool(const float* input_data, const Dims<4>& input_dims,\n                    int stride_width, int stride_height, int pad_width,\n                    int pad_height, int kwidth, int kheight,\n                    float output_activation_min, float output_activation_max,\n                    float* output_data, const Dims<4>& output_dims) {\n  tflite::PoolParams params;\n  params.stride_height = stride_height;\n  params.stride_width = stride_width;\n  params.filter_height = kheight;\n  params.filter_width = kwidth;\n  params.padding_values.height = pad_height;\n  params.padding_values.width = pad_width;\n  params.float_activation_min = output_activation_min;\n  params.float_activation_max = output_activation_max;\n  MaxPool(params, DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n          output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid MaxPool(const float* input_data, const Dims<4>& input_dims,\n             int stride_width, int stride_height, int pad_width, int pad_height,\n             int kwidth, int kheight, float* output_data,\n             const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  MaxPool(input_data, input_dims, stride_width, stride_height, pad_width,\n          pad_height, kwidth, kheight, output_activation_min,\n          output_activation_max, output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid MaxPool(const float* input_data, const Dims<4>& input_dims, int stride,\n             int pad_width, int pad_height, int filter_width, int filter_height,\n             float* output_data, const Dims<4>& output_dims) {\n  MaxPool<Ac>(input_data, input_dims, stride, stride, pad_width, pad_height,\n              filter_width, filter_height, output_data, output_dims);\n}\n\ninline void MaxPool(const uint8* input_data, const Dims<4>& input_dims,\n                    int stride_width, int stride_height, int pad_width,\n                    int pad_height, int filter_width, int filter_height,\n                    int32 output_activation_min, int32 output_activation_max,\n                    uint8* output_data, const Dims<4>& output_dims) {\n  PoolParams params;\n  params.stride_height = stride_height;\n  params.stride_width = stride_width;\n  params.filter_height = filter_height;\n  params.filter_width = filter_width;\n  params.padding_values.height = pad_height;\n  params.padding_values.width = pad_width;\n  params.quantized_activation_min = output_activation_min;\n  params.quantized_activation_max = output_activation_max;\n  MaxPool(params, DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n          output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid MaxPool(const uint8* input_data, const Dims<4>& input_dims,\n             int stride_width, int stride_height, int pad_width, int pad_height,\n             int filter_width, int filter_height, int32 output_activation_min,\n             int32 output_activation_max, uint8* output_data,\n             const Dims<4>& output_dims) {\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n  MaxPool(input_data, input_dims, stride_width, stride_height, pad_width,\n          pad_height, filter_width, filter_height, output_activation_min,\n          output_activation_max, output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid MaxPool(const uint8* input_data, const Dims<4>& input_dims, int stride,\n             int pad_width, int pad_height, int filter_width, int filter_height,\n             int32 output_activation_min, int32 output_activation_max,\n             uint8* output_data, const Dims<4>& output_dims) {\n  MaxPool<Ac>(input_data, input_dims, stride, stride, pad_width, pad_height,\n              filter_width, filter_height, output_activation_min,\n              output_activation_max, output_data, output_dims);\n}\n\ninline void L2Pool(const float* input_data, const Dims<4>& input_dims,\n                   int stride_width, int stride_height, int pad_width,\n                   int pad_height, int filter_width, int filter_height,\n                   float output_activation_min, float output_activation_max,\n                   float* output_data, const Dims<4>& output_dims) {\n  PoolParams params;\n  params.stride_height = stride_height;\n  params.stride_width = stride_width;\n  params.filter_height = filter_height;\n  params.filter_width = filter_width;\n  params.padding_values.height = pad_height;\n  params.padding_values.width = pad_width;\n  params.float_activation_min = output_activation_min;\n  params.float_activation_max = output_activation_max;\n  L2Pool(params, DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n         output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid L2Pool(const float* input_data, const Dims<4>& input_dims,\n            int stride_width, int stride_height, int pad_width, int pad_height,\n            int filter_width, int filter_height, float* output_data,\n            const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  L2Pool(input_data, input_dims, stride_width, stride_height, pad_width,\n         pad_height, filter_width, filter_height, output_activation_min,\n         output_activation_max, output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid L2Pool(const float* input_data, const Dims<4>& input_dims, int stride,\n            int pad_width, int pad_height, int filter_width, int filter_height,\n            float* output_data, const Dims<4>& output_dims) {\n  L2Pool<Ac>(input_data, input_dims, stride, stride, pad_width, pad_height,\n             filter_width, filter_height, output_data, output_dims);\n}\n\ninline void Softmax(const float* input_data, const Dims<4>& input_dims,\n                    float beta, float* output_data,\n                    const Dims<4>& output_dims) {\n  Softmax(input_data, DimsToShape(input_dims), beta, output_data,\n          DimsToShape(output_dims));\n}\n\ninline void Softmax(const uint8* input_data, const Dims<4>& input_dims,\n                    int32 input_beta_multiplier, int32 input_beta_left_shift,\n                    int diff_min, uint8* output_data,\n                    const Dims<4>& output_dims) {\n  Softmax(input_data, DimsToShape(input_dims), input_beta_multiplier,\n          input_beta_left_shift, diff_min, output_data,\n          DimsToShape(output_dims));\n}\n\ninline void LogSoftmax(const float* input_data, const Dims<4>& input_dims,\n                       float* output_data, const Dims<4>& output_dims) {\n  LogSoftmax(input_data, DimsToShape(input_dims), output_data,\n             DimsToShape(output_dims));\n}\n\ninline void LogSoftmax(const uint8* input_data, const Dims<4>& input_dims,\n                       int32 input_multiplier, int32 input_left_shift,\n                       int32 reverse_scaling_divisor,\n                       int32 reverse_scaling_right_shift, int diff_min,\n                       uint8* output_data, const Dims<4>& output_dims) {\n  LogSoftmax(input_data, DimsToShape(input_dims), input_multiplier,\n             input_left_shift, reverse_scaling_divisor,\n             reverse_scaling_right_shift, diff_min, output_data,\n             DimsToShape(output_dims));\n}\n\ninline void Logistic(const float* input_data, const Dims<4>& input_dims,\n                     float* output_data, const Dims<4>& output_dims) {\n  Logistic(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n           output_data);\n}\n\ninline void Logistic(const uint8* input_data, const Dims<4>& input_dims,\n                     int32 input_zero_point, int32 input_range_radius,\n                     int32 input_multiplier, int input_left_shift,\n                     uint8* output_data, const Dims<4>& output_dims) {\n  Logistic(input_data, DimsToShape(input_dims), input_zero_point,\n           input_range_radius, input_multiplier, input_left_shift, output_data,\n           DimsToShape(output_dims));\n}\n\ninline void Logistic(const int16* input_data, const Dims<4>& input_dims,\n                     int16* output_data, const Dims<4>& output_dims) {\n  Logistic(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n           output_data);\n}\n\ninline void Tanh(const float* input_data, const Dims<4>& input_dims,\n                 float* output_data, const Dims<4>& output_dims) {\n  Tanh(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n       output_data);\n}\n\ninline void Tanh(const uint8* input_data, const Dims<4>& input_dims,\n                 int32 input_zero_point, int32 input_range_radius,\n                 int32 input_multiplier, int input_left_shift,\n                 uint8* output_data, const Dims<4>& output_dims) {\n  Tanh(input_data, DimsToShape(input_dims), input_zero_point,\n       input_range_radius, input_multiplier, input_left_shift, output_data,\n       DimsToShape(output_dims));\n}\n\ninline void Tanh(const int16* input_data, const Dims<4>& input_dims,\n                 int input_left_shift, int16* output_data,\n                 const Dims<4>& output_dims) {\n  Tanh(input_data, DimsToShape(input_dims), input_left_shift, output_data,\n       DimsToShape(output_dims));\n}\n\ntemplate <typename T>\ninline void DepthToSpace(const T* input_data, const Dims<4>& input_dims,\n                         int block_size, T* output_data,\n                         const Dims<4>& output_dims) {\n  tflite::DepthToSpaceParams op_params;\n  op_params.block_size = block_size;\n\n  DepthToSpace(op_params, DimsToShape(input_dims), input_data,\n               DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\ninline void SpaceToDepth(const T* input_data, const Dims<4>& input_dims,\n                         int block_size, T* output_data,\n                         const Dims<4>& output_dims) {\n  tflite::SpaceToDepthParams op_params;\n  op_params.block_size = block_size;\n\n  SpaceToDepth(op_params, DimsToShape(input_dims), input_data,\n               DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\ninline void Mul(const T* input1_data, const Dims<4>& input1_dims,\n                const T* input2_data, const Dims<4>& input2_dims,\n                T output_activation_min, T output_activation_max,\n                T* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  SetActivationParams(output_activation_min, output_activation_max, &op_params);\n\n  Mul(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid Mul(const float* input1_data, const Dims<4>& input1_dims,\n         const float* input2_data, const Dims<4>& input2_dims,\n         float* output_data, const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n\n  tflite::ArithmeticParams op_params;\n  SetActivationParams(output_activation_min, output_activation_max, &op_params);\n\n  Mul(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <typename T>\nvoid BroadcastMul(const T* input1_data, const Dims<4>& input1_dims,\n                  const T* input2_data, const Dims<4>& input2_dims,\n                  T output_activation_min, T output_activation_max,\n                  T* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  SetActivationParams(output_activation_min, output_activation_max, &op_params);\n\n  BroadcastMul4DSlow(op_params, DimsToShape(input1_dims), input1_data,\n                     DimsToShape(input2_dims), input2_data,\n                     DimsToShape(output_dims), output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac, typename T>\nvoid BroadcastMul(const T* input1_data, const Dims<4>& input1_dims,\n                  const T* input2_data, const Dims<4>& input2_dims,\n                  T* output_data, const Dims<4>& output_dims) {\n  T output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n\n  tflite::ArithmeticParams op_params;\n  SetActivationParams(output_activation_min, output_activation_max, &op_params);\n\n  BroadcastMul4DSlow(op_params, DimsToShape(input1_dims), input1_data,\n                     DimsToShape(input2_dims), input2_data,\n                     DimsToShape(output_dims), output_data);\n}\n\ninline void Mul(const int16* input1_data, const Dims<4>& input1_dims,\n                const int16* input2_data, const Dims<4>& input2_dims,\n                int16* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  // No params in this version.\n\n  Mul(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ninline void Mul(const int16* input1_data, const Dims<4>& input1_dims,\n                const int16* input2_data, const Dims<4>& input2_dims,\n                int32 output_offset, int32 output_activation_min,\n                int32 output_activation_max, uint8* output_data,\n                const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  op_params.output_offset = output_offset;\n\n  Mul(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ninline void LocalResponseNormalization(const float* input_data,\n                                       const Dims<4>& input_dims, int range,\n                                       float bias, float alpha, float beta,\n                                       float* output_data,\n                                       const Dims<4>& output_dims) {\n  tflite::LocalResponseNormalizationParams op_params;\n  op_params.range = range;\n  op_params.bias = bias;\n  op_params.alpha = alpha;\n  op_params.beta = beta;\n\n  LocalResponseNormalization(op_params, DimsToShape(input_dims), input_data,\n                             DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename SrcT, typename DstT>\nvoid Cast(const SrcT* input_data, const Dims<4>& input_dims, DstT* output_data,\n          const Dims<4>& output_dims) {\n  Cast(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n       output_data);\n}\n\ninline void Floor(const float* input_data, const Dims<4>& input_dims,\n                  float* output_data, const Dims<4>& output_dims) {\n  Floor(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n        output_data);\n}\n\ntemplate <typename T>\ninline void ResizeBilinear(const T* input_data, const Dims<4>& input_dims,\n                           const int32* output_size_data,\n                           const Dims<4>& output_size_dims, T* output_data,\n                           const Dims<4>& output_dims, bool align_corners) {\n  tflite::ResizeBilinearParams op_params;\n  op_params.align_corners = align_corners;\n  op_params.half_pixel_centers = false;\n  ResizeBilinear(op_params, DimsToShape(input_dims), input_data,\n                 DimsToShape(output_size_dims), output_size_data,\n                 DimsToShape(output_dims), output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ninline void ResizeBilinear(const float* input_data, const Dims<4>& input_dims,\n                           const int32* output_size_data,\n                           const Dims<4>& output_size_dims, float* output_data,\n                           const Dims<4>& output_dims) {\n  ResizeBilinear<float>(input_data, input_dims, output_size_data,\n                        output_size_dims, output_data, output_dims,\n                        /*align_corners=*/false);\n}\n\ninline void ResizeBilinear(const uint8* input_data, const Dims<4>& input_dims,\n                           const int32* output_size_data,\n                           const Dims<4>& output_size_dims, uint8* output_data,\n                           const Dims<4>& output_dims) {\n  ResizeBilinear<uint8>(input_data, input_dims, output_size_data,\n                        output_size_dims, output_data, output_dims,\n                        /*align_corners=*/false);\n}\n\ntemplate <typename T>\ninline void SpaceToBatchND(const T* input_data, const Dims<4>& input_dims,\n                           const int32* block_shape_data,\n                           const Dims<4>& block_shape_dims,\n                           const int32* paddings_data,\n                           const Dims<4>& paddings_dims, T* output_data,\n                           const Dims<4>& output_dims,\n                           const int32_t pad_value) {\n  tflite::SpaceToBatchParams op_params;\n  op_params.output_offset = pad_value;\n\n  SpaceToBatchND(op_params, DimsToShape(input_dims), input_data,\n                 DimsToShape(block_shape_dims), block_shape_data,\n                 DimsToShape(paddings_dims), paddings_data,\n                 DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\ninline void SpaceToBatchND(const T* input_data, const Dims<4>& input_dims,\n                           const int32* block_shape_data,\n                           const Dims<4>& block_shape_dims,\n                           const int32* paddings_data,\n                           const Dims<4>& paddings_dims, T* output_data,\n                           const Dims<4>& output_dims) {\n  tflite::SpaceToBatchParams op_params;\n  op_params.output_offset = 0;\n\n  SpaceToBatchND(op_params, DimsToShape(input_dims), input_data,\n                 DimsToShape(block_shape_dims), block_shape_data,\n                 DimsToShape(paddings_dims), paddings_data,\n                 DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\ninline void BatchToSpaceND(const T* input_data, const Dims<4>& input_dims,\n                           const int32* block_shape_data,\n                           const Dims<4>& block_shape_dims,\n                           const int32* crops_data, const Dims<4>& crops_dims,\n                           T* output_data, const Dims<4>& output_dims) {\n  BatchToSpaceND(DimsToShape(input_dims), input_data,\n                 DimsToShape(block_shape_dims), block_shape_data,\n                 DimsToShape(crops_dims), crops_data, DimsToShape(output_dims),\n                 output_data);\n}\n\n// Legacy signature, function covered both Pad and PadV2.\ntemplate <typename T>\ninline void PadV2(const T* input_data, const Dims<4>& input_dims,\n                  const std::vector<int>& left_paddings,\n                  const std::vector<int>& right_paddings, T* output_data,\n                  const Dims<4>& output_dims, const T pad_value) {\n  TFLITE_DCHECK_EQ(left_paddings.size(), 4);\n  TFLITE_DCHECK_EQ(right_paddings.size(), 4);\n  tflite::PadParams op_params;\n  op_params.left_padding_count = 4;\n  op_params.right_padding_count = 4;\n  for (int i = 0; i < 4; ++i) {\n    op_params.left_padding[i] = left_paddings[3 - i];\n    op_params.right_padding[i] = right_paddings[3 - i];\n  }\n  // SetFloatOrInt(pad_value, &op_params.pad_value);\n  const T pad_value_copy = pad_value;\n\n  Pad(op_params, DimsToShape(input_dims), input_data, &pad_value_copy,\n      DimsToShape(output_dims), output_data);\n}\n\n// Old Pad that calls legacy PadV2.\ntemplate <typename T>\ninline void Pad(const T* input_data, const Dims<4>& input_dims,\n                const std::vector<int>& left_paddings,\n                const std::vector<int>& right_paddings, T* output_data,\n                const Dims<4>& output_dims, const int32_t pad_value) {\n  const T converted_pad_value = static_cast<T>(pad_value);\n  PadV2<T>(input_data, input_dims, left_paddings, right_paddings, output_data,\n           output_dims, converted_pad_value);\n}\n\n// Old Pad that only padded with 0.\ntemplate <typename T>\ninline void Pad(const T* input_data, const Dims<4>& input_dims,\n                const std::vector<int>& left_paddings,\n                const std::vector<int>& right_paddings, T* output_data,\n                const Dims<4>& output_dims) {\n  const T pad_value = static_cast<T>(0);\n  PadV2<T>(input_data, input_dims, left_paddings, right_paddings, output_data,\n           output_dims, pad_value);\n}\n\ntemplate <typename T>\nvoid TensorFlowMinimum(const T* input1_data, const Dims<4>& input1_dims,\n                       const T* input2_data, T* output_data,\n                       const Dims<4>& output_dims) {\n  Minimum(DimsToShape(input1_dims), input1_data, input2_data,\n          DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\nvoid TensorFlowMaximum(const T* input1_data, const Dims<4>& input1_dims,\n                       const T* input2_data, T* output_data,\n                       const Dims<4>& output_dims) {\n  Maximum(DimsToShape(input1_dims), input1_data, input2_data,\n          DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T, typename Op>\nvoid TensorFlowMaximumMinimum(const T* input1_data, const Dims<4>& input1_dims,\n                              const T* input2_data, const Dims<4>& input2_dims,\n                              T* output_data, const Dims<4>& output_dims,\n                              Op op) {\n  MaximumMinimumBroadcastSlow(DimsToShape(input1_dims), input1_data,\n                              DimsToShape(input2_dims), input2_data,\n                              DimsToShape(output_dims), output_data, op);\n}\n\ntemplate <typename T1, typename T2, typename T3>\nvoid ArgMax(const T3* axis, const T1* input_data,\n            const tflite::Dims<4>& input_dims, T2* output_data,\n            const tflite::Dims<4>& output_dims) {\n  // Assumes the input always has 4 dimensions, and therefore,\n  // output always has three dimensions.\n  auto output_shape = RuntimeShape(\n      {output_dims.sizes[2], output_dims.sizes[1], output_dims.sizes[0]});\n  // Another way to interpret this is that output_dims.sizes[4] is always 1.\n  TFLITE_DCHECK_EQ(output_shape.FlatSize(),\n                   DimsToShape(output_dims).FlatSize());\n  // Legacy path only supported this.\n  TFLITE_DCHECK_EQ(axis[0], 3);\n  ArgMinMax(DimsToShape(input_dims), input_data, axis, output_shape,\n            output_data, std::greater<T1>());\n}\n\ntemplate <typename T1, typename T2, typename T3, typename Cmp>\nvoid ArgMinMax(const T3* axis, const T1* input_data, const Dims<4>& input_dims,\n               T2* output_data, const Dims<4>& output_dims, const Cmp& cmp) {\n  ArgMinMax(axis, DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n            output_data, cmp);\n}\n\ntemplate <typename T>\ninline void Pow(const T* input1_data, const Dims<4>& input1_dims,\n                const T* input2_data, const Dims<4>& input2_dims,\n                T* output_data, const Dims<4>& output_dims) {\n  Pow(DimsToShape(input1_dims), input1_data, DimsToShape(input2_dims),\n      input2_data, DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\ninline void BroadcastPow(const T* input1_data, const Dims<4>& input1_dims,\n                         const T* input2_data, const Dims<4>& input2_dims,\n                         T* output_data, const Dims<4>& output_dims) {\n  BroadcastPow4DSlow(DimsToShape(input1_dims), input1_data,\n                     DimsToShape(input2_dims), input2_data,\n                     DimsToShape(output_dims), output_data);\n}\n\n// R: Result type. T1: Input 1 type. T2: Input 2 type.\ntemplate <typename R, typename T1, typename T2>\ninline void BroadcastBinaryFunction(const T1* input1_data,\n                                    const Dims<4>& input1_dims,\n                                    const T2* input2_data,\n                                    const Dims<4>& input2_dims, R* output_data,\n                                    const Dims<4>& output_dims,\n                                    R (*func)(T1, T2)) {\n  BroadcastBinaryFunction(DimsToShape(input1_dims), input1_data,\n                          DimsToShape(input2_dims), input2_data,\n                          DimsToShape(output_dims), output_data, func);\n}\n\n// R: Result type. T1: Input 1 type. T2: Input 2 type.\ntemplate <typename R, typename T1, typename T2>\ninline void BinaryFunction(const T1* input1_data, const Dims<4>& input1_dims,\n                           const T2* input2_data, const Dims<4>& input2_dims,\n                           R* output_data, const Dims<4>& output_dims,\n                           R (*func)(T1, T2)) {\n  BinaryFunction(DimsToShape(input1_dims), input1_data,\n                 DimsToShape(input2_dims), input2_data,\n                 DimsToShape(output_dims), output_data, func);\n}\n\ntemplate <typename T>\ninline void Slice(const T* input_data, const Dims<4>& input_dims,\n                  const std::vector<int>& begin, const std::vector<int>& size,\n                  T* output_data, const Dims<4>& output_dims) {\n  tflite::SliceParams op_params;\n  op_params.begin_count = 4;\n  op_params.size_count = 4;\n  for (int i = 0; i < 4; ++i) {\n    op_params.begin[i] = begin[3 - i];\n    op_params.size[i] = size[3 - i];\n  }\n\n  Slice(op_params, DimsToShape(input_dims), input_data,\n        DimsToShape(output_dims), output_data);\n}\n\n}  // namespace reference_ops\n}  // namespace tflite\n#endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_LEGACY_REFERENCE_OPS_H_"