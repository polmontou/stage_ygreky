"/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_OPTIMIZED_LEGACY_OPTIMIZED_OPS_H_\n#define TENSORFLOW_LITE_KERNELS_INTERNAL_OPTIMIZED_LEGACY_OPTIMIZED_OPS_H_\n\n#include <stdint.h>\n#include <sys/types.h>\n\n#include \"public/gemmlowp.h\"\n#include \"tensorflow/lite/kernels/cpu_backend_context.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/cpu_check.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/integer_ops/depthwise_conv.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/integer_ops/fully_connected.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/optimized_ops.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/resize_bilinear.h\"\n#include \"tensorflow/lite/kernels/internal/reference/legacy_reference_ops.h\"\n#include \"tensorflow/lite/kernels/internal/types.h\"\n\nnamespace tflite {\nnamespace optimized_ops {\n\n// Unoptimized reference ops:\nusing reference_ops::Broadcast4DSlowGreater;\nusing reference_ops::Broadcast4DSlowGreaterEqual;\nusing reference_ops::Broadcast4DSlowGreaterEqualWithScaling;\nusing reference_ops::Broadcast4DSlowGreaterWithScaling;\nusing reference_ops::Broadcast4DSlowLess;\nusing reference_ops::Broadcast4DSlowLessEqual;\nusing reference_ops::Broadcast4DSlowLessEqualWithScaling;\nusing reference_ops::Broadcast4DSlowLessWithScaling;\nusing reference_ops::BroadcastAdd4DSlow;\nusing reference_ops::BroadcastGreater;\nusing reference_ops::BroadcastGreaterEqual;\nusing reference_ops::BroadcastLess;\nusing reference_ops::BroadcastLessEqual;\nusing reference_ops::BroadcastMul4DSlow;\nusing reference_ops::BroadcastSubSlow;\nusing reference_ops::Concatenation;\nusing reference_ops::ConcatenationWithScaling;\nusing reference_ops::DepthConcatenation;\nusing reference_ops::Div;\nusing reference_ops::FakeQuant;\nusing reference_ops::Gather;\nusing reference_ops::Greater;\nusing reference_ops::GreaterEqual;\nusing reference_ops::GreaterEqualWithScaling;\nusing reference_ops::GreaterWithScaling;\nusing reference_ops::Less;\nusing reference_ops::LessEqual;\nusing reference_ops::LessEqualWithScaling;\nusing reference_ops::LessWithScaling;\nusing reference_ops::Mean;\nusing reference_ops::RankOneSelect;\nusing reference_ops::Relu1;\nusing reference_ops::Relu6;\nusing reference_ops::ReluX;\nusing reference_ops::Select;\nusing reference_ops::SpaceToBatchND;\nusing reference_ops::Split;\nusing reference_ops::TensorFlowSplit;\n\nstatic constexpr int kDepthwiseReverseShift = -1;\n\ntemplate <typename Scalar, int N>\nVectorMap<Scalar> MapAsVector(Scalar* data, const Dims<N>& dims) {\n  const int size = FlatSize(dims);\n  return VectorMap<Scalar>(data, size, 1);\n}\n\ntemplate <typename Scalar, int N>\nMatrixMap<Scalar> MapAsMatrixWithFirstDimAsRows(Scalar* data,\n                                                const Dims<N>& dims) {\n  const int rows = dims.sizes[0];\n  int cols = 1;\n  for (int d = 1; d < N; d++) {\n    cols *= dims.sizes[d];\n  }\n  return MatrixMap<Scalar>(data, rows, cols);\n}\n\ntemplate <typename Scalar, int N>\nMatrixMap<Scalar> MapAsMatrixWithLastDimAsCols(Scalar* data,\n                                               const Dims<N>& dims) {\n  const int cols = dims.sizes[N - 1];\n  int rows = 1;\n  for (int d = 0; d < N - 1; d++) {\n    rows *= dims.sizes[d];\n  }\n  return MatrixMap<Scalar>(data, rows, cols);\n}\n\ntemplate <typename Scalar, int N>\nArrayMap<Scalar> MapAsArrayWithFirstDimAsRows(Scalar* data,\n                                              const Dims<N>& dims) {\n  const int rows = dims.sizes[0];\n  int cols = 1;\n  for (int d = 1; d < N; d++) {\n    cols *= dims.sizes[d];\n  }\n  return ArrayMap<Scalar>(data, rows, cols);\n}\n\n// TODO(b/62193649): this function is only needed as long\n// as we have the --variable_batch hack.\ntemplate <typename Scalar, int N>\nMatrixMap<Scalar> MapAsMatrixWithGivenNumberOfRows(Scalar* data,\n                                                   const Dims<N>& dims,\n                                                   int rows) {\n  const int flatsize = FlatSize(dims);\n  TFLITE_DCHECK((flatsize % rows) == 0);\n  const int cols = flatsize / rows;\n  return MatrixMap<Scalar>(data, rows, cols);\n}\n\ninline bool AreSameDims(const Dims<4>& dims1, const Dims<4>& dims2) {\n  for (int i = 0; i < 4; i++) {\n    if (dims1.sizes[i] != dims2.sizes[i]) {\n      return false;\n    }\n  }\n  return true;\n}\n\ninline void DepthwiseConv(const float* input_data, const Dims<4>& input_dims,\n                          const float* filter_data, const Dims<4>& filter_dims,\n                          const float* bias_data, const Dims<4>& bias_dims,\n                          int stride_width, int stride_height,\n                          int dilation_width_factor, int dilation_height_factor,\n                          int pad_width, int pad_height, int depth_multiplier,\n                          float output_activation_min,\n                          float output_activation_max, float* output_data,\n                          const Dims<4>& output_dims) {\n  tflite::DepthwiseParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n  op_params.dilation_width_factor = dilation_width_factor;\n  op_params.dilation_height_factor = dilation_height_factor;\n  op_params.depth_multiplier = depth_multiplier;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n\n  const RuntimeShape output_shape = DimsToShape(output_dims);\n  const int output_height = output_shape.Dims(1);\n\n  DepthwiseConvImpl(op_params, DimsToShape(input_dims), input_data,\n                    DimsToShape(filter_dims), filter_data,\n                    DimsToShape(bias_dims), bias_data, output_shape,\n                    output_data, CpuFlags(), /*thread_start=*/0,\n                    /*thread_end=*/output_height, /*thread_dim=*/1);\n}\n\ninline void DepthwiseConv(const float* input_data, const Dims<4>& input_dims,\n                          const float* filter_data, const Dims<4>& filter_dims,\n                          const float* bias_data, const Dims<4>& bias_dims,\n                          int stride_width, int stride_height, int pad_width,\n                          int pad_height, int depth_multiplier,\n                          float output_activation_min,\n                          float output_activation_max, float* output_data,\n                          const Dims<4>& output_dims) {\n  DepthwiseConv(input_data, input_dims, filter_data, filter_dims, bias_data,\n                bias_dims, stride_width, stride_height, 1, 1, pad_width,\n                pad_height, depth_multiplier, output_activation_min,\n                output_activation_max, output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid DepthwiseConv(const float* input_data, const Dims<4>& input_dims,\n                   const float* filter_data, const Dims<4>& filter_dims,\n                   const float* bias_data, const Dims<4>& bias_dims,\n                   int stride_width, int stride_height, int pad_width,\n                   int pad_height, int depth_multiplier, float* output_data,\n                   const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  DepthwiseConv(input_data, input_dims, filter_data, filter_dims, bias_data,\n                bias_dims, stride_width, stride_height, pad_width, pad_height,\n                depth_multiplier, output_activation_min, output_activation_max,\n                output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid DepthwiseConv(const float* input_data, const Dims<4>& input_dims,\n                   const float* filter_data, const Dims<4>& filter_dims,\n                   const float* bias_data, const Dims<4>& bias_dims, int stride,\n                   int pad_width, int pad_height, int depth_multiplier,\n                   float* output_data, const Dims<4>& output_dims) {\n  DepthwiseConv<Ac>(input_data, input_dims, filter_data, filter_dims, bias_data,\n                    bias_dims, stride, stride, pad_width, pad_height,\n                    depth_multiplier, output_data, output_dims);\n}\n\ntemplate <DepthwiseConvOutputRounding kOutputRounding>\ninline void LegacyDepthwiseConvWithRounding(\n    const DepthwiseParams& params, const RuntimeShape& input_shape,\n    const uint8* input_data, const RuntimeShape& filter_shape,\n    const uint8* filter_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape,\n    uint8* output_data, int thread_start, int thread_end, int thread_dim) {\n  ruy::profiler::ScopeLabel label(\"DepthwiseConv/8bit\");\n  const int depth_multiplier = params.depth_multiplier;\n  const int32 output_activation_min = params.quantized_activation_min;\n  const int32 output_activation_max = params.quantized_activation_max;\n  const int dilation_width_factor = params.dilation_width_factor;\n  const int dilation_height_factor = params.dilation_height_factor;\n  TFLITE_DCHECK_GE(dilation_width_factor, 1);\n  TFLITE_DCHECK_GE(dilation_height_factor, 1);\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);\n  const int input_depth = input_shape.Dims(3);\n  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);\n  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);\n\n// Enable for arm64 except for the Nvidia Linux 4 Tegra (L4T) running on\n// Jetson TX-2. This compiler does not support the offsetof() macro.\n#if defined(__aarch64__) && !defined(GOOGLE_L4T)\n  const int stride_width = params.stride_width;\n  const int stride_height = params.stride_height;\n  const int pad_width = params.padding_values.width;\n  const int pad_height = params.padding_values.height;\n  const int output_shift = params.output_shift;\n\n  // Call kernel optimized for depthwise convolutions using 3x3 filters if\n  // parameters are supported.\n  if (depthwise_conv::Fast3x3FilterKernelSupported(\n          input_shape, filter_shape, stride_width, stride_height,\n          dilation_width_factor, dilation_height_factor, pad_width, pad_height,\n          depth_multiplier, output_shape, output_shift)) {\n    ruy::profiler::ScopeLabel specialized_label(\"DepthwiseConv/8bit/3x3\");\n    depthwise_conv::DepthwiseConv3x3Filter<kOutputRounding>(\n        params, input_shape, input_data, filter_shape, filter_data, bias_shape,\n        bias_data, output_shape, output_data, thread_start, thread_end,\n        thread_dim);\n    return;\n  }\n#endif\n\n  ruy::profiler::ScopeLabel specialized_label(\"DepthwiseConv/8bit/General\");\n  depthwise_conv::DepthwiseConvGeneral(params, input_shape, input_data,\n                                       filter_shape, filter_data, bias_shape,\n                                       bias_data, output_shape, output_data,\n                                       thread_start, thread_end, thread_dim);\n}\n\ninline void LegacyDepthwiseConvImpl(\n    const DepthwiseParams& params, const RuntimeShape& input_shape,\n    const uint8* input_data, const RuntimeShape& filter_shape,\n    const uint8* filter_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape,\n    uint8* output_data, int thread_start, int thread_end, int thread_dim) {\n  return LegacyDepthwiseConvWithRounding<\n      DepthwiseConvOutputRounding::kAwayFromZero>(\n      params, input_shape, input_data, filter_shape, filter_data, bias_shape,\n      bias_data, output_shape, output_data, thread_start, thread_end,\n      thread_dim);\n}\n\ninline void DepthwiseConv(const uint8* input_data, const Dims<4>& input_dims,\n                          int32 input_offset, const uint8* filter_data,\n                          const Dims<4>& filter_dims, int32 filter_offset,\n                          const int32* bias_data, const Dims<4>& bias_dims,\n                          int stride_width, int stride_height,\n                          int dilation_width_factor, int dilation_height_factor,\n                          int pad_width, int pad_height, int depth_multiplier,\n                          int32 output_offset, int32 output_multiplier,\n                          int output_shift, int32 output_activation_min,\n                          int32 output_activation_max, uint8* output_data,\n                          const Dims<4>& output_dims) {\n  tflite::DepthwiseParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n  op_params.dilation_width_factor = dilation_width_factor;\n  op_params.dilation_height_factor = dilation_height_factor;\n  op_params.depth_multiplier = depth_multiplier;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  op_params.input_offset = input_offset;\n  op_params.weights_offset = filter_offset;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.output_shift = kDepthwiseReverseShift * output_shift;\n\n  const RuntimeShape output_shape = DimsToShape(output_dims);\n  const int output_height = output_shape.Dims(1);\n\n  LegacyDepthwiseConvImpl(\n      op_params, DimsToShape(input_dims), input_data, DimsToShape(filter_dims),\n      filter_data, DimsToShape(bias_dims), bias_data, DimsToShape(output_dims),\n      output_data, /*thread_start=*/0,\n      /*thread_end=*/output_height, /*thread_dim=*/1);\n}\n\ninline void DepthwiseConv(const uint8* input_data, const Dims<4>& input_dims,\n                          int32 input_offset, const uint8* filter_data,\n                          const Dims<4>& filter_dims, int32 filter_offset,\n                          const int32* bias_data, const Dims<4>& bias_dims,\n                          int stride_width, int stride_height, int pad_width,\n                          int pad_height, int depth_multiplier,\n                          int32 output_offset, int32 output_multiplier,\n                          int output_shift, int32 output_activation_min,\n                          int32 output_activation_max, uint8* output_data,\n                          const Dims<4>& output_dims) {\n  DepthwiseConv(input_data, input_dims, input_offset, filter_data, filter_dims,\n                filter_offset, bias_data, bias_dims, stride_width,\n                stride_height, 1, 1, pad_width, pad_height, depth_multiplier,\n                output_offset, output_multiplier, output_shift,\n                output_activation_min, output_activation_max, output_data,\n                output_dims);\n}\n\n// Legacy, for compatibility with old checked-in code.\ntemplate <FusedActivationFunctionType Ac>\nvoid DepthwiseConv(const uint8* input_data, const Dims<4>& input_dims,\n                   int32 input_offset, const uint8* filter_data,\n                   const Dims<4>& filter_dims, int32 filter_offset,\n                   const int32* bias_data, const Dims<4>& bias_dims,\n                   int stride_width, int stride_height, int pad_width,\n                   int pad_height, int depth_multiplier, int32 output_offset,\n                   int32 output_multiplier, int output_shift,\n                   int32 output_activation_min, int32 output_activation_max,\n                   uint8* output_data, const Dims<4>& output_dims) {\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n  DepthwiseConv(input_data, input_dims, input_offset, filter_data, filter_dims,\n                filter_offset, bias_data, bias_dims, stride_width,\n                stride_height, pad_width, pad_height, depth_multiplier,\n                output_offset, output_multiplier, output_shift,\n                output_activation_min, output_activation_max, output_data,\n                output_dims);\n}\n\n// Legacy, for compatibility with old checked-in code.\ntemplate <FusedActivationFunctionType Ac>\nvoid DepthwiseConv(const uint8* input_data, const Dims<4>& input_dims,\n                   int32 input_offset, const uint8* filter_data,\n                   const Dims<4>& filter_dims, int32 filter_offset,\n                   const int32* bias_data, const Dims<4>& bias_dims, int stride,\n                   int pad_width, int pad_height, int depth_multiplier,\n                   int32 output_offset, int32 output_multiplier,\n                   int output_shift, int32 output_activation_min,\n                   int32 output_activation_max, uint8* output_data,\n                   const Dims<4>& output_dims) {\n  DepthwiseConv<Ac>(input_data, input_dims, input_offset, filter_data,\n                    filter_dims, filter_offset, bias_data, bias_dims, stride,\n                    stride, pad_width, pad_height, depth_multiplier,\n                    output_offset, output_multiplier, output_shift,\n                    output_activation_min, output_activation_max, output_data,\n                    output_dims);\n}\n\ntemplate <typename T, typename TS>\nstruct LegacyDepthwiseConvWorkerTask : public gemmlowp::Task {\n  LegacyDepthwiseConvWorkerTask(\n      const DepthwiseParams& params, const RuntimeShape& input_shape,\n      const T* input_data, const RuntimeShape& filter_shape,\n      const T* filter_data, const RuntimeShape& bias_shape, const TS* bias_data,\n      const RuntimeShape& output_shape, T* output_data, int thread_start,\n      int thread_end, int thread_dim)\n      : params_(params),\n        input_shape_(input_shape),\n        input_data_(input_data),\n        filter_shape_(filter_shape),\n        filter_data_(filter_data),\n        bias_shape_(bias_shape),\n        bias_data_(bias_data),\n        output_shape_(output_shape),\n        output_data_(output_data),\n        thread_start_(thread_start),\n        thread_end_(thread_end),\n        thread_dim_(thread_dim) {}\n\n  void Run() override {\n    LegacyDepthwiseConvImpl(params_, input_shape_, input_data_, filter_shape_,\n                            filter_data_, bias_shape_, bias_data_,\n                            output_shape_, output_data_, thread_start_,\n                            thread_end_, thread_dim_);\n  }\n\n private:\n  const DepthwiseParams& params_;\n  const RuntimeShape& input_shape_;\n  const T* input_data_;\n  const RuntimeShape& filter_shape_;\n  const T* filter_data_;\n  const RuntimeShape& bias_shape_;\n  const TS* bias_data_;\n  const RuntimeShape& output_shape_;\n  T* output_data_;\n  int thread_start_;\n  int thread_end_;\n  int thread_dim_;\n};\n\ninline int HowManyConvThreads(const RuntimeShape& output_shape,\n                              const RuntimeShape& filter_shape,\n                              int thread_dim) {\n  constexpr int kMinMulPerThread = 8;\n  const int output_units = output_shape.Dims(thread_dim);\n  const int filter_height = filter_shape.Dims(1);\n  const int filter_width = filter_shape.Dims(2);\n  const int num_mul_per_unit =\n      FlatSizeSkipDim(output_shape, thread_dim) * filter_height * filter_width;\n  const int min_units_per_thread = kMinMulPerThread / num_mul_per_unit + 1;\n  int thread_count = output_units / min_units_per_thread;\n  return thread_count;\n}\n\ninline void DepthwiseConv(\n    const DepthwiseParams& params, const RuntimeShape& input_shape,\n    const uint8* input_data, const RuntimeShape& filter_shape,\n    const uint8* filter_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape,\n    uint8* output_data, gemmlowp::GemmContext* gemmlowp_context = nullptr) {\n  ruy::profiler::ScopeLabel label(\"DepthwiseConv\");\n\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n\n  const int output_batches = output_shape.Dims(0);\n  const int output_rows = output_shape.Dims(1);\n  int thread_count_batch = HowManyConvThreads(output_shape, filter_shape, 0);\n  int thread_count_row = HowManyConvThreads(output_shape, filter_shape, 1);\n  int thread_dim, thread_count, thread_dim_size;\n  if (thread_count_batch > thread_count_row) {\n    thread_dim = 0;\n    thread_dim_size = output_batches;\n    thread_count = thread_count_batch;\n  } else {\n    thread_dim = 1;\n    thread_dim_size = output_rows;\n    thread_count = thread_count_row;\n  }\n\n  const int max_threads =\n      gemmlowp_context ? gemmlowp_context->max_num_threads() : 1;\n  thread_count = std::max(1, std::min(thread_count, max_threads));\n\n  if (thread_count == 1) {\n    LegacyDepthwiseConvImpl(params, input_shape, input_data, filter_shape,\n                            filter_data, bias_shape, bias_data, output_shape,\n                            output_data, /*thread_start=*/0,\n                            /*thread_end=*/output_rows, /*thread_dim=*/1);\n  } else {\n    std::vector<gemmlowp::Task*> tasks(thread_count);\n    int thread_start = 0;\n    for (int i = 0; i < thread_count; ++i) {\n      int thread_end =\n          thread_start + (thread_dim_size - thread_start) / (thread_count - i);\n      tasks[i] = new LegacyDepthwiseConvWorkerTask<uint8, int32>(\n          params, input_shape, input_data, filter_shape, filter_data,\n          bias_shape, bias_data, output_shape, output_data, thread_start,\n          thread_end, thread_dim);\n      thread_start = thread_end;\n    }\n    gemmlowp_context->workers_pool()->LegacyExecuteAndDestroyTasks(tasks);\n  }\n}\n\ntemplate <typename T, typename TS>\nstruct LegacyPerChannelDepthwiseConvWorkerTask : public gemmlowp::Task {\n  LegacyPerChannelDepthwiseConvWorkerTask(\n      const DepthwiseParams& params, const int32* output_multiplier,\n      const int32* output_shift, const RuntimeShape& input_shape,\n      const T* input_data, const RuntimeShape& filter_shape,\n      const T* filter_data, const RuntimeShape& bias_shape, const TS* bias_data,\n      const RuntimeShape& output_shape, T* output_data, int thread_start,\n      int thread_end, int thread_dim)\n      : params_(params),\n        output_multiplier_(output_multiplier),\n        output_shift_(output_shift),\n        input_shape_(input_shape),\n        input_data_(input_data),\n        filter_shape_(filter_shape),\n        filter_data_(filter_data),\n        bias_shape_(bias_shape),\n        bias_data_(bias_data),\n        output_shape_(output_shape),\n        output_data_(output_data),\n        thread_start_(thread_start),\n        thread_end_(thread_end),\n        thread_dim_(thread_dim) {}\n\n  void Run() override {\n    CpuBackendContext backend_context;\n    optimized_integer_ops::DepthwiseConvImpl(\n        params_, output_multiplier_, output_shift_, input_shape_, input_data_,\n        filter_shape_, filter_data_, bias_shape_, bias_data_, output_shape_,\n        output_data_, thread_start_, thread_end_, thread_dim_, backend_context);\n  }\n\n private:\n  const DepthwiseParams& params_;\n  const int32* output_multiplier_;\n  const int32* output_shift_;\n  const RuntimeShape& input_shape_;\n  const T* input_data_;\n  const RuntimeShape& filter_shape_;\n  const T* filter_data_;\n  const RuntimeShape& bias_shape_;\n  const TS* bias_data_;\n  const RuntimeShape& output_shape_;\n  T* output_data_;\n  int thread_start_;\n  int thread_end_;\n  int thread_dim_;\n};\n\ninline void DepthwiseConvPerChannel(\n    const DepthwiseParams& params, const int32* output_multiplier,\n    const int32* output_shift, const RuntimeShape& input_shape,\n    const int8* input_data, const RuntimeShape& filter_shape,\n    const int8* filter_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape, int8* output_data,\n    gemmlowp::GemmContext* gemmlowp_context = nullptr) {\n  ruy::profiler::ScopeLabel label(\"DepthwiseConvInt8\");\n\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n\n  const int output_batches = output_shape.Dims(0);\n  const int output_rows = output_shape.Dims(1);\n  int thread_count_batch = HowManyConvThreads(output_shape, filter_shape, 0);\n  int thread_count_row = HowManyConvThreads(output_shape, filter_shape, 1);\n  int thread_dim, thread_count, thread_dim_size;\n  if (thread_count_batch > thread_count_row) {\n    thread_dim = 0;\n    thread_dim_size = output_batches;\n    thread_count = thread_count_batch;\n  } else {\n    thread_dim = 1;\n    thread_dim_size = output_rows;\n    thread_count = thread_count_row;\n  }\n\n  const int max_threads =\n      gemmlowp_context ? gemmlowp_context->max_num_threads() : 1;\n  thread_count = std::max(1, std::min(thread_count, max_threads));\n\n  if (thread_count == 1) {\n    CpuBackendContext backend_context;\n    optimized_integer_ops::DepthwiseConvImpl(\n        params, output_multiplier, output_shift, input_shape, input_data,\n        filter_shape, filter_data, bias_shape, bias_data, output_shape,\n        output_data, /*thread_start=*/0,\n        /*thread_end=*/output_rows, /*thread_dim=*/1, backend_context);\n  } else {\n    std::vector<gemmlowp::Task*> tasks(thread_count);\n    int thread_start = 0;\n    for (int i = 0; i < thread_count; ++i) {\n      int thread_end =\n          thread_start + (thread_dim_size - thread_start) / (thread_count - i);\n      tasks[i] = new LegacyPerChannelDepthwiseConvWorkerTask<int8, int32>(\n          params, output_multiplier, output_shift, input_shape, input_data,\n          filter_shape, filter_data, bias_shape, bias_data, output_shape,\n          output_data, thread_start, thread_end, thread_dim);\n      thread_start = thread_end;\n    }\n    gemmlowp_context->workers_pool()->LegacyExecuteAndDestroyTasks(tasks);\n  }\n}\n\ninline void DepthwiseConv(\n    const DepthwiseParams& params, const RuntimeShape& input_shape,\n    const float* input_data, const RuntimeShape& filter_shape,\n    const float* filter_data, const RuntimeShape& bias_shape,\n    const float* bias_data, const RuntimeShape& output_shape,\n    float* output_data) {\n  DepthwiseConvImpl(params, input_shape, input_data, filter_shape, filter_data,\n                    bias_shape, bias_data, output_shape, output_data,\n                    CpuFlags(),\n                    /*thread_start=*/0,\n                    /*thread_end=*/output_shape.Dims(1), /*thread_dim=*/1);\n}\n\ninline void AddBiasAndEvalActivationFunction(const float* bias_data,\n                                             const Dims<4>& bias_dims,\n                                             float* array_data,\n                                             const Dims<4>& array_dims,\n                                             float output_activation_min,\n                                             float output_activation_max) {\n  AddBiasAndEvalActivationFunction(output_activation_min, output_activation_max,\n                                   DimsToShape(bias_dims), bias_data,\n                                   DimsToShape(array_dims), array_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid AddBiasAndEvalActivationFunction(const float* bias_data,\n                                      const Dims<4>& bias_dims,\n                                      float* array_data,\n                                      const Dims<4>& array_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  AddBiasAndEvalActivationFunction(bias_data, bias_dims, array_data, array_dims,\n                                   output_activation_min,\n                                   output_activation_max);\n}\n\ntemplate <typename Lhs, typename Rhs, typename Result>\nvoid Gemm(const Eigen::MatrixBase<Lhs>& lhs, const Eigen::MatrixBase<Rhs>& rhs,\n          Eigen::MatrixBase<Result>* result) {\n  if (rhs.cols() == 1) {\n    ruy::profiler::ScopeLabel label(\"GEMV\");\n    result->col(0).noalias() = lhs * rhs.col(0);\n  } else {\n    ruy::profiler::ScopeLabel label(\"GEMM\");\n    result->noalias() = lhs * rhs;\n  }\n}\n\ninline void FullyConnected(\n    const FullyConnectedParams& params, const RuntimeShape& input_shape,\n    const float* input_data, const RuntimeShape& weights_shape,\n    const float* weights_data, const RuntimeShape& bias_shape,\n    const float* optional_bias_data, const RuntimeShape& output_shape,\n    float* output_data) {\n  ruy::profiler::ScopeLabel label(\"FullyConnected\");\n  const float output_activation_min = params.float_activation_min;\n  const float output_activation_max = params.float_activation_max;\n\n  // TODO(b/62193649): this convoluted shape computation (determining\n  // input_rows from the weights_dims, then MapAsMatrixWithGivenNumberOfRows)\n  // is because the current --variable_batch hack consists in overwriting the\n  // 3rd dimension with the runtime batch size, as we don't keep track for each\n  // array of which dimension is the batch dimension in it.\n  // When that is fixed, this should become:\n  // const auto input_matrix_map =\n  //     MapAsMatrixWithFirstDimAsRows(input_data, input_dims);\n  const int dims_count = weights_shape.DimensionsCount();\n  const int input_rows = weights_shape.Dims(dims_count - 1);\n  const auto input_matrix_map =\n      MapAsMatrixWithGivenNumberOfRows(input_data, input_shape, input_rows);\n  const auto filter_matrix_map =\n      MapAsMatrixWithLastDimAsRows(weights_data, weights_shape);\n  auto output_matrix_map =\n      MapAsMatrixWithLastDimAsRows(output_data, output_shape);\n\n  Gemm(filter_matrix_map.transpose(), input_matrix_map, &output_matrix_map);\n\n  if (optional_bias_data != nullptr) {\n    AddBiasAndEvalActivationFunction(\n        output_activation_min, output_activation_max, bias_shape,\n        optional_bias_data, output_shape, output_data);\n  } else {\n    const int flat_size = output_shape.FlatSize();\n    for (int i = 0; i < flat_size; ++i) {\n      output_data[i] = ActivationFunctionWithMinMax(\n          output_data[i], output_activation_min, output_activation_max);\n    }\n  }\n}\n\ninline void FullyConnected(const float* input_data, const Dims<4>& input_dims,\n                           const float* weights_data,\n                           const Dims<4>& weights_dims, const float* bias_data,\n                           const Dims<4>& bias_dims,\n                           float output_activation_min,\n                           float output_activation_max, float* output_data,\n                           const Dims<4>& output_dims) {\n  tflite::FullyConnectedParams op_params;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n\n  FullyConnected(op_params, DimsToShape(input_dims), input_data,\n                 DimsToShape(weights_dims), weights_data,\n                 DimsToShape(bias_dims), bias_data, DimsToShape(output_dims),\n                 output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid FullyConnected(const float* input_data, const Dims<4>& input_dims,\n                    const float* weights_data, const Dims<4>& weights_dims,\n                    const float* bias_data, const Dims<4>& bias_dims,\n                    float* output_data, const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  FullyConnected(input_data, input_dims, weights_data, weights_dims, bias_data,\n                 bias_dims, output_activation_min, output_activation_max,\n                 output_data, output_dims);\n}\n\nstruct GemmlowpOutputPipeline {\n  typedef gemmlowp::VectorMap<const int32, gemmlowp::VectorShape::Col>\n      ColVectorMap;\n  typedef std::tuple<gemmlowp::OutputStageBiasAddition<ColVectorMap>,\n                     gemmlowp::OutputStageScaleInt32ByFixedPointAndExponent,\n                     gemmlowp::OutputStageClamp,\n                     gemmlowp::OutputStageSaturatingCastToUint8>\n      Pipeline;\n  static Pipeline MakeExp(const int32* bias_data, int output_rows,\n                          int32 output_offset, int32 output_multiplier,\n                          int output_left_shift, int32 output_activation_min,\n                          int32 output_activation_max) {\n    ColVectorMap bias_vector(bias_data, output_rows);\n    gemmlowp::OutputStageBiasAddition<ColVectorMap> bias_addition_stage;\n    bias_addition_stage.bias_vector = bias_vector;\n    gemmlowp::OutputStageScaleInt32ByFixedPointAndExponent quantize_down_stage;\n    quantize_down_stage.result_offset_after_shift = output_offset;\n    quantize_down_stage.result_fixedpoint_multiplier = output_multiplier;\n    quantize_down_stage.result_exponent = output_left_shift;\n    gemmlowp::OutputStageClamp clamp_stage;\n    clamp_stage.min = output_activation_min;\n    clamp_stage.max = output_activation_max;\n    gemmlowp::OutputStageSaturatingCastToUint8 saturating_cast_stage;\n    return std::make_tuple(bias_addition_stage, quantize_down_stage,\n                           clamp_stage, saturating_cast_stage);\n  }\n};\n\nstruct GemmlowpOutputPipelineInt8 {\n  typedef gemmlowp::VectorMap<const int32, gemmlowp::VectorShape::Col>\n      ColVectorMap;\n  typedef std::tuple<gemmlowp::OutputStageBiasAddition<ColVectorMap>,\n                     gemmlowp::OutputStageScaleInt32ByFixedPointAndExponent,\n                     gemmlowp::OutputStageClamp,\n                     gemmlowp::OutputStageSaturatingCastToInt8>\n      Pipeline;\n  static Pipeline MakeExp(const int32* bias_data, int output_rows,\n                          int32 output_offset, int32 output_multiplier,\n                          int output_left_shift, int32 output_activation_min,\n                          int32 output_activation_max) {\n    ColVectorMap bias_vector(bias_data, output_rows);\n    gemmlowp::OutputStageBiasAddition<ColVectorMap> bias_addition_stage;\n    bias_addition_stage.bias_vector = bias_vector;\n    gemmlowp::OutputStageScaleInt32ByFixedPointAndExponent quantize_down_stage;\n    quantize_down_stage.result_offset_after_shift = output_offset;\n    quantize_down_stage.result_fixedpoint_multiplier = output_multiplier;\n    quantize_down_stage.result_exponent = output_left_shift;\n    gemmlowp::OutputStageClamp clamp_stage;\n    clamp_stage.min = output_activation_min;\n    clamp_stage.max = output_activation_max;\n    gemmlowp::OutputStageSaturatingCastToInt8 saturating_cast_stage;\n    return std::make_tuple(bias_addition_stage, quantize_down_stage,\n                           clamp_stage, saturating_cast_stage);\n  }\n};\n\n#ifdef USE_NEON\ninline void LegacyFullyConnectedAsGEMVWorkerImpl(\n    const RuntimeShape& input_shape, const uint8* input_data,\n    int32 input_offset, const RuntimeShape& filter_shape,\n    const uint8* filter_data, int32 filter_offset,\n    const RuntimeShape& bias_shape, const int32* bias_data, int32 output_offset,\n    int32 output_multiplier, int output_shift, int32 output_activation_min,\n    int32 output_activation_max, const RuntimeShape& output_shape,\n    uint8* output_data, int row_start, int row_end) {\n  ruy::profiler::ScopeLabel label(\"FullyConnectedAsGEMV/8bit\");\n  TFLITE_DCHECK_GE(input_shape.DimensionsCount(), 1);\n  TFLITE_DCHECK_GE(filter_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_GE(output_shape.DimensionsCount(), 1);\n  const int output_dim_count = output_shape.DimensionsCount();\n  TFLITE_DCHECK_EQ(FlatSizeSkipDim(output_shape, output_dim_count - 1), 1);\n  const int input_size = FlatSizeSkipDim(input_shape, 0);\n  static constexpr int kPeel = 4;\n  const bool shift_left = (output_shift > 0);\n  for (int k = 0; k < input_size; k += 64) {\n    optimized_ops_preload_l1_stream(input_data + k);\n  }\n  for (int k = 0; k < kPeel * input_size; k += 64) {\n    optimized_ops_preload_l1_stream(filter_data + k);\n  }\n\n  TFLITE_DCHECK_GE(row_end - row_start, kPeel);\n\n  for (int out = row_start; out < row_end; out += kPeel) {\n    out = std::min(out, row_end - kPeel);\n    int32x4_t acc0 = vdupq_n_s32(0);\n    int32x4_t acc1 = acc0;\n    int32x4_t acc2 = acc0;\n    int32x4_t acc3 = acc0;\n    const int16x8_t input_offset_vec = vdupq_n_s16(input_offset);\n    const int16x8_t filter_offset_vec = vdupq_n_s16(filter_offset);\n    int in = 0;\n    for (; in <= input_size - 16; in += 16) {\n      const uint8x16_t input_val_u8 = vld1q_u8(input_data + in);\n      const uint8* filter_ptr = filter_data + in + out * input_size;\n      uint8x16_t filter_val_u8_0 = vld1q_u8(filter_ptr);\n      optimized_ops_preload_l1_stream(filter_ptr + 64);\n      filter_ptr += input_size;\n      uint8x16_t filter_val_u8_1 = vld1q_u8(filter_ptr);\n      optimized_ops_preload_l1_stream(filter_ptr + 64);\n      filter_ptr += input_size;\n      uint8x16_t filter_val_u8_2 = vld1q_u8(filter_ptr);\n      optimized_ops_preload_l1_stream(filter_ptr + 64);\n      filter_ptr += input_size;\n      uint8x16_t filter_val_u8_3 = vld1q_u8(filter_ptr);\n      optimized_ops_preload_l1_stream(filter_ptr + 64);\n      int16x8_t input_val_0, input_val_1;\n      uint8x8_t low = vget_low_u8(input_val_u8);\n      uint8x8_t high = vget_high_u8(input_val_u8);\n      input_val_0 = vreinterpretq_s16_u16(vmovl_u8(low));\n      input_val_1 = vreinterpretq_s16_u16(vmovl_u8(high));\n      input_val_0 = vaddq_s16(input_val_0, input_offset_vec);\n      input_val_1 = vaddq_s16(input_val_1, input_offset_vec);\n      low = vget_low_u8(filter_val_u8_0);\n      high = vget_high_u8(filter_val_u8_0);\n      int16x8_t filter_val_0_0 = vreinterpretq_s16_u16(vmovl_u8(low));\n      int16x8_t filter_val_0_1 = vreinterpretq_s16_u16(vmovl_u8(high));\n      filter_val_0_0 = vaddq_s16(filter_val_0_0, filter_offset_vec);\n      filter_val_0_1 = vaddq_s16(filter_val_0_1, filter_offset_vec);\n      low = vget_low_u8(filter_val_u8_1);\n      high = vget_high_u8(filter_val_u8_1);\n      int16x8_t filter_val_1_0 = vreinterpretq_s16_u16(vmovl_u8(low));\n      int16x8_t filter_val_1_1 = vreinterpretq_s16_u16(vmovl_u8(high));\n      filter_val_1_0 = vaddq_s16(filter_val_1_0, filter_offset_vec);\n      filter_val_1_1 = vaddq_s16(filter_val_1_1, filter_offset_vec);\n      low = vget_low_u8(filter_val_u8_2);\n      high = vget_high_u8(filter_val_u8_2);\n      int16x8_t filter_val_2_0 = vreinterpretq_s16_u16(vmovl_u8(low));\n      int16x8_t filter_val_2_1 = vreinterpretq_s16_u16(vmovl_u8(high));\n      filter_val_2_0 = vaddq_s16(filter_val_2_0, filter_offset_vec);\n      filter_val_2_1 = vaddq_s16(filter_val_2_1, filter_offset_vec);\n      low = vget_low_u8(filter_val_u8_3);\n      high = vget_high_u8(filter_val_u8_3);\n      int16x8_t filter_val_3_0 = vreinterpretq_s16_u16(vmovl_u8(low));\n      int16x8_t filter_val_3_1 = vreinterpretq_s16_u16(vmovl_u8(high));\n      filter_val_3_0 = vaddq_s16(filter_val_3_0, filter_offset_vec);\n      filter_val_3_1 = vaddq_s16(filter_val_3_1, filter_offset_vec);\n      acc0 = vmlal_s16(acc0, vget_low_s16(filter_val_0_0),\n                       vget_low_s16(input_val_0));\n      acc1 = vmlal_s16(acc1, vget_low_s16(filter_val_1_0),\n                       vget_low_s16(input_val_0));\n      acc2 = vmlal_s16(acc2, vget_low_s16(filter_val_2_0),\n                       vget_low_s16(input_val_0));\n      acc3 = vmlal_s16(acc3, vget_low_s16(filter_val_3_0),\n                       vget_low_s16(input_val_0));\n      acc0 = vmlal_s16(acc0, vget_low_s16(filter_val_0_1),\n                       vget_low_s16(input_val_1));\n      acc1 = vmlal_s16(acc1, vget_low_s16(filter_val_1_1),\n                       vget_low_s16(input_val_1));\n      acc2 = vmlal_s16(acc2, vget_low_s16(filter_val_2_1),\n                       vget_low_s16(input_val_1));\n      acc3 = vmlal_s16(acc3, vget_low_s16(filter_val_3_1),\n                       vget_low_s16(input_val_1));\n      acc0 = vmlal_s16(acc0, vget_high_s16(filter_val_0_0),\n                       vget_high_s16(input_val_0));\n      acc1 = vmlal_s16(acc1, vget_high_s16(filter_val_1_0),\n                       vget_high_s16(input_val_0));\n      acc2 = vmlal_s16(acc2, vget_high_s16(filter_val_2_0),\n                       vget_high_s16(input_val_0));\n      acc3 = vmlal_s16(acc3, vget_high_s16(filter_val_3_0),\n                       vget_high_s16(input_val_0));\n      acc0 = vmlal_s16(acc0, vget_high_s16(filter_val_0_1),\n                       vget_high_s16(input_val_1));\n      acc1 = vmlal_s16(acc1, vget_high_s16(filter_val_1_1),\n                       vget_high_s16(input_val_1));\n      acc2 = vmlal_s16(acc2, vget_high_s16(filter_val_2_1),\n                       vget_high_s16(input_val_1));\n      acc3 = vmlal_s16(acc3, vget_high_s16(filter_val_3_1),\n                       vget_high_s16(input_val_1));\n    }\n    for (; in <= input_size - 8; in += 8) {\n      const uint8x8_t input_val_u8 = vld1_u8(input_data + in);\n      const uint8* filter_ptr = filter_data + in + out * input_size;\n      uint8x8_t filter_val_u8_0 = vld1_u8(filter_ptr);\n      filter_ptr += input_size;\n      uint8x8_t filter_val_u8_1 = vld1_u8(filter_ptr);\n      filter_ptr += input_size;\n      uint8x8_t filter_val_u8_2 = vld1_u8(filter_ptr);\n      filter_ptr += input_size;\n      uint8x8_t filter_val_u8_3 = vld1_u8(filter_ptr);\n      int16x8_t input_val = vreinterpretq_s16_u16(vmovl_u8(input_val_u8));\n      input_val = vaddq_s16(input_val, input_offset_vec);\n      int16x8_t filter_val_0 = vreinterpretq_s16_u16(vmovl_u8(filter_val_u8_0));\n      filter_val_0 = vaddq_s16(filter_val_0, filter_offset_vec);\n      int16x8_t filter_val_1 = vreinterpretq_s16_u16(vmovl_u8(filter_val_u8_1));\n      filter_val_1 = vaddq_s16(filter_val_1, filter_offset_vec);\n      int16x8_t filter_val_2 = vreinterpretq_s16_u16(vmovl_u8(filter_val_u8_2));\n      filter_val_2 = vaddq_s16(filter_val_2, filter_offset_vec);\n      int16x8_t filter_val_3 = vreinterpretq_s16_u16(vmovl_u8(filter_val_u8_3));\n      filter_val_3 = vaddq_s16(filter_val_3, filter_offset_vec);\n      acc0 =\n          vmlal_s16(acc0, vget_low_s16(filter_val_0), vget_low_s16(input_val));\n      acc1 =\n          vmlal_s16(acc1, vget_low_s16(filter_val_1), vget_low_s16(input_val));\n      acc2 =\n          vmlal_s16(acc2, vget_low_s16(filter_val_2), vget_low_s16(input_val));\n      acc3 =\n          vmlal_s16(acc3, vget_low_s16(filter_val_3), vget_low_s16(input_val));\n      acc0 = vmlal_s16(acc0, vget_high_s16(filter_val_0),\n                       vget_high_s16(input_val));\n      acc1 = vmlal_s16(acc1, vget_high_s16(filter_val_1),\n                       vget_high_s16(input_val));\n      acc2 = vmlal_s16(acc2, vget_high_s16(filter_val_2),\n                       vget_high_s16(input_val));\n      acc3 = vmlal_s16(acc3, vget_high_s16(filter_val_3),\n                       vget_high_s16(input_val));\n    }\n    if (in < input_size) {\n      int32 buf[16];\n      vst1q_s32(buf + 0, acc0);\n      vst1q_s32(buf + 4, acc1);\n      vst1q_s32(buf + 8, acc2);\n      vst1q_s32(buf + 12, acc3);\n      for (; in < input_size; in++) {\n        int lane = (in + 8 - input_size) % 4;\n        const int32 input_val = input_data[in] + input_offset;\n        for (int k = 0; k < kPeel; k++) {\n          int32 filter_val =\n              filter_data[in + (out + k) * input_size] + filter_offset;\n          buf[lane + 4 * k] += filter_val * input_val;\n        }\n      }\n      acc0 = vld1q_s32(buf + 0);\n      acc1 = vld1q_s32(buf + 4);\n      acc2 = vld1q_s32(buf + 8);\n      acc3 = vld1q_s32(buf + 12);\n    }\n\n    // Horizontally reduce accumulators\n    int32x2_t pairwise_reduced_acc_0 =\n        vpadd_s32(vget_low_s32(acc0), vget_high_s32(acc0));\n    int32x2_t pairwise_reduced_acc_1 =\n        vpadd_s32(vget_low_s32(acc1), vget_high_s32(acc1));\n    int32x2_t pairwise_reduced_acc_2 =\n        vpadd_s32(vget_low_s32(acc2), vget_high_s32(acc2));\n    int32x2_t pairwise_reduced_acc_3 =\n        vpadd_s32(vget_low_s32(acc3), vget_high_s32(acc3));\n    const int32x2_t reduced_lo =\n        vpadd_s32(pairwise_reduced_acc_0, pairwise_reduced_acc_1);\n    const int32x2_t reduced_hi =\n        vpadd_s32(pairwise_reduced_acc_2, pairwise_reduced_acc_3);\n    int32x4_t reduced = vcombine_s32(reduced_lo, reduced_hi);\n    // Add bias values.\n    int32x4_t bias_vec = vld1q_s32(bias_data + out);\n    reduced = vaddq_s32(reduced, bias_vec);\n    if (shift_left) {\n      const int32 multiplier_power_of_two = 1 << output_shift;\n      reduced = vmulq_n_s32(reduced, multiplier_power_of_two);\n      reduced = vqrdmulhq_n_s32(reduced, output_multiplier);\n    } else {\n      // Multiply by the fixed-point multiplier.\n      reduced = vqrdmulhq_n_s32(reduced, output_multiplier);\n      // Rounding-shift-right.\n      using gemmlowp::RoundingDivideByPOT;\n      reduced = RoundingDivideByPOT(reduced, -output_shift);\n    }\n    // Add the output offset.\n    const int32x4_t output_offset_vec = vdupq_n_s32(output_offset);\n    reduced = vaddq_s32(reduced, output_offset_vec);\n    // Narrow values down to 16 bit signed.\n    const int16x4_t res16 = vqmovn_s32(reduced);\n    // Narrow values down to 8 bit unsigned, saturating.\n    uint8x8_t res8 = vqmovun_s16(vcombine_s16(res16, res16));\n    // Apply the clamping from the activation function\n    res8 = vmax_u8(res8, vdup_n_u8(output_activation_min));\n    res8 = vmin_u8(res8, vdup_n_u8(output_activation_max));\n    // Store results to destination.\n    vst1_lane_u8(output_data + out + 0, res8, 0);\n    vst1_lane_u8(output_data + out + 1, res8, 1);\n    vst1_lane_u8(output_data + out + 2, res8, 2);\n    vst1_lane_u8(output_data + out + 3, res8, 3);\n  }\n}\n\nstruct LegacyFullyConnectedAsGEMVWorkerTask : public gemmlowp::Task {\n  LegacyFullyConnectedAsGEMVWorkerTask(\n      const RuntimeShape& input_shape, const uint8* input_data,\n      int32 input_offset, const RuntimeShape& filter_shape,\n      const uint8* filter_data, int32 filter_offset,\n      const RuntimeShape& bias_shape, const int32* bias_data,\n      int32 output_offset, int32 output_multiplier, int output_shift,\n      int32 output_activation_min, int32 output_activation_max,\n      const RuntimeShape& output_shape, uint8* output_data, int row_start,\n      int row_end)\n      : input_shape_(input_shape),\n        input_data_(input_data),\n        input_offset_(input_offset),\n        filter_shape_(filter_shape),\n        filter_data_(filter_data),\n        filter_offset_(filter_offset),\n        bias_shape_(bias_shape),\n        bias_data_(bias_data),\n        output_offset_(output_offset),\n        output_multiplier_(output_multiplier),\n        output_shift_(output_shift),\n        output_activation_min_(output_activation_min),\n        output_activation_max_(output_activation_max),\n        output_shape_(output_shape),\n        output_data_(output_data),\n        row_start_(row_start),\n        row_end_(row_end) {}\n\n  void Run() override {\n    LegacyFullyConnectedAsGEMVWorkerImpl(\n        input_shape_, input_data_, input_offset_, filter_shape_, filter_data_,\n        filter_offset_, bias_shape_, bias_data_, output_offset_,\n        output_multiplier_, output_shift_, output_activation_min_,\n        output_activation_max_, output_shape_, output_data_, row_start_,\n        row_end_);\n  }\n\n  const RuntimeShape& input_shape_;\n  const uint8* input_data_;\n  int32 input_offset_;\n  const RuntimeShape& filter_shape_;\n  const uint8* filter_data_;\n  int32 filter_offset_;\n  const RuntimeShape& bias_shape_;\n  const int32* bias_data_;\n  int32 output_offset_;\n  int32 output_multiplier_;\n  int output_shift_;\n  int32 output_activation_min_;\n  int32 output_activation_max_;\n  const RuntimeShape& output_shape_;\n  uint8* output_data_;\n  int row_start_;\n  int row_end_;\n};\n\ninline void FullyConnectedAsGEMV(\n    const RuntimeShape& input_shape, const uint8* input_data,\n    int32 input_offset, const RuntimeShape& filter_shape,\n    const uint8* filter_data, int32 filter_offset,\n    const RuntimeShape& bias_shape, const int32* bias_data, int32 output_offset,\n    int32 output_multiplier, int output_shift, int32 output_activation_min,\n    int32 output_activation_max, const RuntimeShape& output_shape,\n    uint8* output_data, gemmlowp::GemmContext* gemmlowp_context) {\n  const int output_dim_count = output_shape.DimensionsCount();\n  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);\n  const int output_rows = output_shape.Dims(output_dim_count - 1);\n  const int input_size = FlatSizeSkipDim(input_shape, 0);\n  static constexpr int kKernelRows = 4;\n  const int thread_count = gemmlowp::HowManyThreads<kKernelRows>(\n      gemmlowp_context->max_num_threads(), output_rows, batches, input_size);\n  if (thread_count == 1) {\n    // Single-thread case: do the computation on the current thread, don't\n    // use a threadpool\n    LegacyFullyConnectedAsGEMVWorkerImpl(\n        input_shape, input_data, input_offset, filter_shape, filter_data,\n        filter_offset, bias_shape, bias_data, output_offset, output_multiplier,\n        output_shift, output_activation_min, output_activation_max,\n        output_shape, output_data, 0, output_rows);\n    return;\n  }\n\n  // Multi-threaded case: use the gemmlowp context's threadpool.\n  TFLITE_DCHECK_GT(thread_count, 1);\n  std::vector<gemmlowp::Task*> tasks(thread_count);\n  const int kRowsPerWorker = gemmlowp::RoundUp<kKernelRows>(\n      gemmlowp::CeilQuotient(output_rows, thread_count));\n  int row_start = 0;\n  for (int i = 0; i < thread_count; ++i) {\n    int row_end = std::min(output_rows, row_start + kRowsPerWorker);\n    tasks[i] = new LegacyFullyConnectedAsGEMVWorkerTask(\n        input_shape, input_data, input_offset, filter_shape, filter_data,\n        filter_offset, bias_shape, bias_data, output_offset, output_multiplier,\n        output_shift, output_activation_min, output_activation_max,\n        output_shape, output_data, row_start, row_end);\n    row_start = row_end;\n  }\n  TFLITE_DCHECK_EQ(row_start, output_rows);\n  gemmlowp_context->workers_pool()->LegacyExecuteAndDestroyTasks(tasks);\n}\n#endif  // USE_NEON\n\ninline void FullyConnected(\n    const FullyConnectedParams& params, const RuntimeShape& input_shape,\n    const uint8* input_data, const RuntimeShape& filter_shape,\n    const uint8* filter_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape,\n    uint8* output_data, gemmlowp::GemmContext* gemmlowp_context) {\n  ruy::profiler::ScopeLabel label(\"FullyConnected/8bit\");\n  const int32 input_offset = params.input_offset;\n  const int32 filter_offset = params.weights_offset;\n  const int32 output_offset = params.output_offset;\n  const int32 output_multiplier = params.output_multiplier;\n  const int output_shift = params.output_shift;\n  const int32 output_activation_min = params.quantized_activation_min;\n  const int32 output_activation_max = params.quantized_activation_max;\n  TFLITE_DCHECK_GE(filter_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_GE(output_shape.DimensionsCount(), 1);\n  // TODO(b/62193649): This really should be:\n  //     const int batches = ArraySize(output_dims, 1);\n  // but the current --variable_batch hack consists in overwriting the 3rd\n  // dimension with the runtime batch size, as we don't keep track for each\n  // array of which dimension is the batch dimension in it.\n  const int output_dim_count = output_shape.DimensionsCount();\n  const int filter_dim_count = filter_shape.DimensionsCount();\n  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);\n#ifdef USE_NEON\n  if (batches == 1) {\n    const int output_size = MatchingDim(filter_shape, filter_dim_count - 2,\n                                        output_shape, output_dim_count - 1);\n    if (output_size >= 4) {\n      return FullyConnectedAsGEMV(\n          input_shape, input_data, input_offset, filter_shape, filter_data,\n          filter_offset, bias_shape, bias_data, output_offset,\n          output_multiplier, output_shift, output_activation_min,\n          output_activation_max, output_shape, output_data, gemmlowp_context);\n    }\n  }\n#endif  // USE_NEON\n  const int filter_rows = filter_shape.Dims(filter_dim_count - 2);\n  const int filter_cols = filter_shape.Dims(filter_dim_count - 1);\n  TFLITE_DCHECK_EQ(filter_shape.FlatSize(), filter_rows * filter_cols);\n  const int output_rows = output_shape.Dims(output_dim_count - 1);\n  TFLITE_DCHECK_EQ(output_rows, filter_rows);\n  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_rows);\n\n  gemmlowp::MatrixMap<const uint8, gemmlowp::MapOrder::RowMajor> filter_matrix(\n      filter_data, output_rows, filter_cols, filter_cols);\n  gemmlowp::MatrixMap<const uint8, gemmlowp::MapOrder::ColMajor> input_matrix(\n      input_data, filter_cols, batches, filter_cols);\n  gemmlowp::MatrixMap<uint8, gemmlowp::MapOrder::ColMajor> output_matrix(\n      output_data, output_rows, batches, output_rows);\n  const auto& output_pipeline = GemmlowpOutputPipeline::MakeExp(\n      bias_data, output_rows, output_offset, output_multiplier, output_shift,\n      output_activation_min, output_activation_max);\n  gemmlowp::GemmWithOutputPipeline<uint8, uint8,\n                                   gemmlowp::L8R8WithLhsNonzeroBitDepthParams>(\n      gemmlowp_context, filter_matrix, input_matrix, &output_matrix,\n      filter_offset, input_offset, output_pipeline);\n}\n\n#ifdef GEMMLOWP_NEON\n// In the common case of batch size 1, a fully-connected node degenerates\n// to a matrix*vector product. LSTM cells contain a fully-connected node;\n// when quantized, this becomes a special type of GEMV operation where\n// the output is 16bit-quantized, thus needs its own special path.\ninline void GEMVForLstmCell(const RuntimeShape& input_shape,\n                            const uint8* input_data,\n                            const RuntimeShape& weights_shape,\n                            const uint8* weights_data, uint8 weights_zero_point,\n                            const RuntimeShape& bias_shape,\n                            const int32* bias_data, int32 accum_multiplier,\n                            int accum_shift, const RuntimeShape& output_shape,\n                            int16* output_data) {\n  ruy::profiler::ScopeLabel label(\"GEMVForLstmCell\");\n  TFLITE_DCHECK_GE(input_shape.DimensionsCount(), 1);\n  TFLITE_DCHECK_GE(weights_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_GE(output_shape.DimensionsCount(), 1);\n  const int output_dim_count = output_shape.DimensionsCount();\n  const int weights_dim_count = weights_shape.DimensionsCount();\n  TFLITE_DCHECK_EQ(FlatSizeSkipDim(output_shape, output_dim_count - 1), 1);\n  const int input_size = FlatSizeSkipDim(input_shape, 0);\n  const int output_size = MatchingDim(weights_shape, weights_dim_count - 2,\n                                      output_shape, output_dim_count - 1);\n  // This special fast path for quantized LSTM cells does not try to support\n  // odd sizes that we haven't encountered in any LSTM cell, that would\n  // require special code (that would go untested until any LSTM cell\n  // exercises it). We just guard our assumptions about size evenness with\n  // the following assertions.\n  TFLITE_DCHECK(!(output_size % 4));\n  TFLITE_DCHECK(!(input_size % 8));\n  const int32* bias_ptr = bias_data;\n  int16* output_ptr = output_data;\n  for (int out = 0; out < output_size; out += 4) {\n    int32x4_t acc_0 = vdupq_n_s32(0);\n    int32x4_t acc_1 = vdupq_n_s32(0);\n    int32x4_t acc_2 = vdupq_n_s32(0);\n    int32x4_t acc_3 = vdupq_n_s32(0);\n    const int16x8_t input_offset_vec = vdupq_n_s16(-128);\n    const int16x8_t weights_offset_vec = vdupq_n_s16(-weights_zero_point);\n    int in = 0;\n    // Handle 16 levels of depth at a time.\n    for (; in <= input_size - 16; in += 16) {\n      const uint8x16_t input_val_u8 = vld1q_u8(input_data + in);\n      const uint8* weights_ptr = weights_data + in + out * input_size;\n      uint8x16_t weights_val_u8_0 = vld1q_u8(weights_ptr + 0 * input_size);\n      uint8x16_t weights_val_u8_1 = vld1q_u8(weights_ptr + 1 * input_size);\n      uint8x16_t weights_val_u8_2 = vld1q_u8(weights_ptr + 2 * input_size);\n      uint8x16_t weights_val_u8_3 = vld1q_u8(weights_ptr + 3 * input_size);\n      int16x8_t input_val_0, input_val_1;\n      const uint8x8_t low = vget_low_u8(input_val_u8);\n      const uint8x8_t high = vget_high_u8(input_val_u8);\n      input_val_0 = vreinterpretq_s16_u16(vmovl_u8(low));\n      input_val_1 = vreinterpretq_s16_u16(vmovl_u8(high));\n      input_val_0 = vaddq_s16(input_val_0, input_offset_vec);\n      input_val_1 = vaddq_s16(input_val_1, input_offset_vec);\n      int16x8_t weights_val_0_0, weights_val_1_0, weights_val_2_0,\n          weights_val_3_0;\n      int16x8_t weights_val_0_1, weights_val_1_1, weights_val_2_1,\n          weights_val_3_1;\n      weights_val_0_0 = vaddq_s16(\n          vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(weights_val_u8_0))),\n          weights_offset_vec);\n      weights_val_0_1 = vaddq_s16(\n          vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(weights_val_u8_0))),\n          weights_offset_vec);\n      weights_val_1_0 = vaddq_s16(\n          vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(weights_val_u8_1))),\n          weights_offset_vec);\n      weights_val_1_1 = vaddq_s16(\n          vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(weights_val_u8_1))),\n          weights_offset_vec);\n      weights_val_2_0 = vaddq_s16(\n          vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(weights_val_u8_2))),\n          weights_offset_vec);\n      weights_val_2_1 = vaddq_s16(\n          vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(weights_val_u8_2))),\n          weights_offset_vec);\n      weights_val_3_0 = vaddq_s16(\n          vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(weights_val_u8_3))),\n          weights_offset_vec);\n      weights_val_3_1 = vaddq_s16(\n          vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(weights_val_u8_3))),\n          weights_offset_vec);\n      acc_0 = vmlal_s16(acc_0, vget_low_s16(weights_val_0_0),\n                        vget_low_s16(input_val_0));\n      acc_1 = vmlal_s16(acc_1, vget_low_s16(weights_val_1_0),\n                        vget_low_s16(input_val_0));\n      acc_2 = vmlal_s16(acc_2, vget_low_s16(weights_val_2_0),\n                        vget_low_s16(input_val_0));\n      acc_3 = vmlal_s16(acc_3, vget_low_s16(weights_val_3_0),\n                        vget_low_s16(input_val_0));\n      acc_0 = vmlal_s16(acc_0, vget_high_s16(weights_val_0_0),\n                        vget_high_s16(input_val_0));\n      acc_1 = vmlal_s16(acc_1, vget_high_s16(weights_val_1_0),\n                        vget_high_s16(input_val_0));\n      acc_2 = vmlal_s16(acc_2, vget_high_s16(weights_val_2_0),\n                        vget_high_s16(input_val_0));\n      acc_3 = vmlal_s16(acc_3, vget_high_s16(weights_val_3_0),\n                        vget_high_s16(input_val_0));\n      acc_0 = vmlal_s16(acc_0, vget_low_s16(weights_val_0_1),\n                        vget_low_s16(input_val_1));\n      acc_1 = vmlal_s16(acc_1, vget_low_s16(weights_val_1_1),\n                        vget_low_s16(input_val_1));\n      acc_2 = vmlal_s16(acc_2, vget_low_s16(weights_val_2_1),\n                        vget_low_s16(input_val_1));\n      acc_3 = vmlal_s16(acc_3, vget_low_s16(weights_val_3_1),\n                        vget_low_s16(input_val_1));\n      acc_0 = vmlal_s16(acc_0, vget_high_s16(weights_val_0_1),\n                        vget_high_s16(input_val_1));\n      acc_1 = vmlal_s16(acc_1, vget_high_s16(weights_val_1_1),\n                        vget_high_s16(input_val_1));\n      acc_2 = vmlal_s16(acc_2, vget_high_s16(weights_val_2_1),\n                        vget_high_s16(input_val_1));\n      acc_3 = vmlal_s16(acc_3, vget_high_s16(weights_val_3_1),\n                        vget_high_s16(input_val_1));\n    }\n    // Handle 8 levels of depth at a time.\n    for (; in < input_size; in += 8) {\n      const uint8x8_t input_val_u8 = vld1_u8(input_data + in);\n      const uint8* weights_ptr = weights_data + in + out * input_size;\n      uint8x8_t weights_val_u8_0 = vld1_u8(weights_ptr + 0 * input_size);\n      uint8x8_t weights_val_u8_1 = vld1_u8(weights_ptr + 1 * input_size);\n      uint8x8_t weights_val_u8_2 = vld1_u8(weights_ptr + 2 * input_size);\n      uint8x8_t weights_val_u8_3 = vld1_u8(weights_ptr + 3 * input_size);\n      int16x8_t input_val;\n      input_val = vreinterpretq_s16_u16(vmovl_u8(input_val_u8));\n      input_val = vaddq_s16(input_val, input_offset_vec);\n      int16x8_t weights_val_0, weights_val_1, weights_val_2, weights_val_3;\n      weights_val_0 =\n          vaddq_s16(vreinterpretq_s16_u16(vmovl_u8(weights_val_u8_0)),\n                    weights_offset_vec);\n      weights_val_1 =\n          vaddq_s16(vreinterpretq_s16_u16(vmovl_u8(weights_val_u8_1)),\n                    weights_offset_vec);\n      weights_val_2 =\n          vaddq_s16(vreinterpretq_s16_u16(vmovl_u8(weights_val_u8_2)),\n                    weights_offset_vec);\n      weights_val_3 =\n          vaddq_s16(vreinterpretq_s16_u16(vmovl_u8(weights_val_u8_3)),\n                    weights_offset_vec);\n      acc_0 = vmlal_s16(acc_0, vget_low_s16(weights_val_0),\n                        vget_low_s16(input_val));\n      acc_1 = vmlal_s16(acc_1, vget_low_s16(weights_val_1),\n                        vget_low_s16(input_val));\n      acc_2 = vmlal_s16(acc_2, vget_low_s16(weights_val_2),\n                        vget_low_s16(input_val));\n      acc_3 = vmlal_s16(acc_3, vget_low_s16(weights_val_3),\n                        vget_low_s16(input_val));\n      acc_0 = vmlal_s16(acc_0, vget_high_s16(weights_val_0),\n                        vget_high_s16(input_val));\n      acc_1 = vmlal_s16(acc_1, vget_high_s16(weights_val_1),\n                        vget_high_s16(input_val));\n      acc_2 = vmlal_s16(acc_2, vget_high_s16(weights_val_2),\n                        vget_high_s16(input_val));\n      acc_3 = vmlal_s16(acc_3, vget_high_s16(weights_val_3),\n                        vget_high_s16(input_val));\n    }\n    // Horizontally reduce accumulators\n    int32x2_t pairwise_reduced_acc_0, pairwise_reduced_acc_1,\n        pairwise_reduced_acc_2, pairwise_reduced_acc_3;\n    pairwise_reduced_acc_0 =\n        vpadd_s32(vget_low_s32(acc_0), vget_high_s32(acc_0));\n    pairwise_reduced_acc_1 =\n        vpadd_s32(vget_low_s32(acc_1), vget_high_s32(acc_1));\n    pairwise_reduced_acc_2 =\n        vpadd_s32(vget_low_s32(acc_2), vget_high_s32(acc_2));\n    pairwise_reduced_acc_3 =\n        vpadd_s32(vget_low_s32(acc_3), vget_high_s32(acc_3));\n    const int32x2_t reduced_lo =\n        vpadd_s32(pairwise_reduced_acc_0, pairwise_reduced_acc_1);\n    const int32x2_t reduced_hi =\n        vpadd_s32(pairwise_reduced_acc_2, pairwise_reduced_acc_3);\n    int32x4_t reduced = vcombine_s32(reduced_lo, reduced_hi);\n    // Add bias values.\n    int32x4_t bias_vec = vld1q_s32(bias_ptr);\n    bias_ptr += 4;\n    reduced = vaddq_s32(reduced, bias_vec);\n    int left_shift = accum_shift > 0 ? accum_shift : 0;\n    int right_shift = accum_shift > 0 ? 0 : -accum_shift;\n    reduced = vshlq_s32(reduced, vdupq_n_s32(left_shift));\n    // Multiply by the fixed-point multiplier.\n    reduced = vqrdmulhq_n_s32(reduced, accum_multiplier);\n    // Rounding-shift-right.\n    using gemmlowp::RoundingDivideByPOT;\n    reduced = RoundingDivideByPOT(reduced, right_shift);\n    // Narrow values down to 16 bit signed.\n    const int16x4_t res16 = vqmovn_s32(reduced);\n    vst1_s16(output_ptr, res16);\n    output_ptr += 4;\n  }\n}\n#endif\n\n#ifdef GEMMLOWP_NEON\ninline void GEMVForLstmCellWithSymmetricRange(\n    const RuntimeShape& input_shape, const uint8* input_data,\n    const RuntimeShape& weights_shape, const uint8* weights_data,\n    const RuntimeShape& bias_shape, const int32* bias_data,\n    int32 accum_multiplier, int accum_shift, const RuntimeShape& output_shape,\n    int16* output_data) {\n  ruy::profiler::ScopeLabel label(\"GEMVForLstmCellWithSymmetricRange\");\n  TFLITE_DCHECK_GE(input_shape.DimensionsCount(), 1);\n  TFLITE_DCHECK_GE(weights_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_GE(output_shape.DimensionsCount(), 1);\n  const int output_dim_count = output_shape.DimensionsCount();\n  const int weights_dim_count = weights_shape.DimensionsCount();\n  TFLITE_DCHECK_EQ(FlatSizeSkipDim(output_shape, output_dim_count - 1), 1);\n  const int input_size = FlatSizeSkipDim(input_shape, 0);\n  const int output_size = MatchingDim(weights_shape, weights_dim_count - 2,\n                                      output_shape, output_dim_count - 1);\n  // This special fast path for quantized LSTM cells does not try to support\n  // odd sizes that we haven't encountered in any LSTM cell, that would\n  // require special code (that would go untested until any LSTM cell\n  // exercises it). We just guard our assumptions about size evenness with\n  // the following assertions.\n  TFLITE_DCHECK(!(output_size % 4));\n  TFLITE_DCHECK(!(input_size % 64));\n  const int32* bias_ptr = bias_data;\n  int16* output_ptr = output_data;\n  const uint8x16_t signbit = vdupq_n_u8(0x80);\n  for (int in = 0; in < input_size; in += 32) {\n    optimized_ops_preload_l1_keep(input_data + in);\n  }\n  const int left_shift = accum_shift > 0 ? accum_shift : 0;\n  const int right_shift = accum_shift > 0 ? 0 : -accum_shift;\n  for (int out = 0; out < output_size; out += 4) {\n    // Load the bias values\n    int32x4_t bias_vec = vld1q_s32(bias_ptr);\n    bias_ptr += 4;\n\n    // Clear accumulators. We use 2 accumulator registers per row,\n    // for 4 rows. row_accumRN is the N-th accumulator for row R.\n    int32x4_t row_accum00 = vdupq_n_s32(0);\n    int32x4_t row_accum01 = vdupq_n_s32(0);\n    int32x4_t row_accum10 = vdupq_n_s32(0);\n    int32x4_t row_accum11 = vdupq_n_s32(0);\n    int32x4_t row_accum20 = vdupq_n_s32(0);\n    int32x4_t row_accum21 = vdupq_n_s32(0);\n    int32x4_t row_accum30 = vdupq_n_s32(0);\n    int32x4_t row_accum31 = vdupq_n_s32(0);\n\n    // kReadAhead parametrizes how far ahead we prefetch weights into L1 cache.\n    const int kReadAhead = 512;\n    // Prefetch the first weights values.\n    for (int k = 0; k < kReadAhead; k += 64) {\n      optimized_ops_preload_l1_stream(weights_data + (out + 0) * input_size +\n                                      k);\n      optimized_ops_preload_l1_stream(weights_data + (out + 1) * input_size +\n                                      k);\n      optimized_ops_preload_l1_stream(weights_data + (out + 2) * input_size +\n                                      k);\n      optimized_ops_preload_l1_stream(weights_data + (out + 3) * input_size +\n                                      k);\n    }\n    // Loop along the rows, handling 64 bytes per iteration because that's\n    // cache line size on most current ARM-architecture CPUs.\n    for (int in = 0; in < input_size; in += 64) {\n      // Prefetch some future weights values.\n      optimized_ops_preload_l1_stream(weights_data + (out + 0) * input_size +\n                                      in + kReadAhead);\n      optimized_ops_preload_l1_stream(weights_data + (out + 1) * input_size +\n                                      in + kReadAhead);\n      optimized_ops_preload_l1_stream(weights_data + (out + 2) * input_size +\n                                      in + kReadAhead);\n      optimized_ops_preload_l1_stream(weights_data + (out + 3) * input_size +\n                                      in + kReadAhead);\n\n      // We will use 2 local 16-bit accumulators per row, for 2 rows.\n      // See below (*) for the rationale of processing only 2 rows at a time.\n      // local_accumRN is the N-th local accumulator for row R.\n      int16x8_t local_accum00;\n      int16x8_t local_accum01;\n      int16x8_t local_accum10;\n      int16x8_t local_accum11;\n\n      // Load 64 bytes of input activations values. Convert to signed int8\n      // by flipping the sign bit (i.e. subtracting 128, the required\n      // zero_point value).\n      int8x16_t input0 = vreinterpretq_s8_u8(\n          veorq_u8(signbit, vld1q_u8(input_data + in + 16 * 0)));\n      int8x16_t input1 = vreinterpretq_s8_u8(\n          veorq_u8(signbit, vld1q_u8(input_data + in + 16 * 1)));\n      int8x16_t input2 = vreinterpretq_s8_u8(\n          veorq_u8(signbit, vld1q_u8(input_data + in + 16 * 2)));\n      int8x16_t input3 = vreinterpretq_s8_u8(\n          veorq_u8(signbit, vld1q_u8(input_data + in + 16 * 3)));\n\n      // Beginning of the core accumulation. Notice how while we have 4\n      // rows to process, this code is taking care of only 2 rows at a time,\n      // thus being divided into two parts looking similar (\"Rows 0 and 1\" and\n      // \"Rows 2 and 3\").\n      //\n      // (*) The rationale for handling only 2 rows at a time is to avoid\n      // cache aliasing issues on 4-way set-associative L1-cache CPUs, such\n      // as Cortex-A53. With sufficiently large, power-of-two matrix dimensions,\n      // we may find ourselves in a situation where rows alias each other in\n      // the L1 cache, and moreover may also mutually alias with the input\n      // activations. If we try to load 4 rows at a time, together with the\n      // input activations, that may be 5 mutually-aliasing vectors, resulting\n      // in constant mutual eviction from L1 cache. Handling 2 rows at a time\n      // here largely mitigates these issues, and seems at least to be very\n      // effective on Cortex-A53:\n      //                          Before       After\n      // big (Cortex-A73)         2.85 ms      2.85 ms\n      // little (Cortex-A53)      11.0 ms      5.16 ms\n\n      // Rows 0 and 1:\n      // Load 64 bytes of weights values from each row. Convert to signed int8\n      // by flipping the sign bit (i.e. subtracting 128, the required\n      // zero_point value).\n      int8x16_t weights00 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 0) * input_size + in + 16 * 0)));\n      int8x16_t weights01 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 0) * input_size + in + 16 * 1)));\n      int8x16_t weights02 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 0) * input_size + in + 16 * 2)));\n      int8x16_t weights03 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 0) * input_size + in + 16 * 3)));\n      int8x16_t weights10 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 1) * input_size + in + 16 * 0)));\n      int8x16_t weights11 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 1) * input_size + in + 16 * 1)));\n      int8x16_t weights12 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 1) * input_size + in + 16 * 2)));\n      int8x16_t weights13 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 1) * input_size + in + 16 * 3)));\n      // Multiply-accumulate into local 16-bit accumulators.\n      // We can accumulate two products without overflow because weights are\n      // required to never be -128, so each product is at most 127^2 in absolute\n      // value.\n      local_accum00 = vmull_s8(vget_low_s8(weights00), vget_low_s8(input0));\n      local_accum01 = vmull_s8(vget_low_s8(weights01), vget_low_s8(input1));\n      local_accum10 = vmull_s8(vget_low_s8(weights10), vget_low_s8(input0));\n      local_accum11 = vmull_s8(vget_low_s8(weights11), vget_low_s8(input1));\n      local_accum00 = vmlal_s8(local_accum00, vget_high_s8(weights00),\n                               vget_high_s8(input0));\n      local_accum01 = vmlal_s8(local_accum01, vget_high_s8(weights01),\n                               vget_high_s8(input1));\n      local_accum10 = vmlal_s8(local_accum10, vget_high_s8(weights10),\n                               vget_high_s8(input0));\n      local_accum11 = vmlal_s8(local_accum11, vget_high_s8(weights11),\n                               vget_high_s8(input1));\n      // Pairwise add and accumulate into 32-bit accumulators\n      row_accum00 = vpadalq_s16(row_accum00, local_accum00);\n      row_accum01 = vpadalq_s16(row_accum01, local_accum01);\n      row_accum10 = vpadalq_s16(row_accum10, local_accum10);\n      row_accum11 = vpadalq_s16(row_accum11, local_accum11);\n      // Multiply-accumulate into local 16-bit accumulators.\n      // We can accumulate two products without overflow because weights are\n      // required to never be -128, so each product is at most 127^2 in absolute\n      // value.\n      local_accum00 = vmull_s8(vget_low_s8(weights02), vget_low_s8(input2));\n      local_accum01 = vmull_s8(vget_low_s8(weights03), vget_low_s8(input3));\n      local_accum10 = vmull_s8(vget_low_s8(weights12), vget_low_s8(input2));\n      local_accum11 = vmull_s8(vget_low_s8(weights13), vget_low_s8(input3));\n      local_accum00 = vmlal_s8(local_accum00, vget_high_s8(weights02),\n                               vget_high_s8(input2));\n      local_accum01 = vmlal_s8(local_accum01, vget_high_s8(weights03),\n                               vget_high_s8(input3));\n      local_accum10 = vmlal_s8(local_accum10, vget_high_s8(weights12),\n                               vget_high_s8(input2));\n      local_accum11 = vmlal_s8(local_accum11, vget_high_s8(weights13),\n                               vget_high_s8(input3));\n      // Pairwise add and accumulate into 32-bit accumulators\n      row_accum00 = vpadalq_s16(row_accum00, local_accum00);\n      row_accum01 = vpadalq_s16(row_accum01, local_accum01);\n      row_accum10 = vpadalq_s16(row_accum10, local_accum10);\n      row_accum11 = vpadalq_s16(row_accum11, local_accum11);\n\n      // Rows 2 and 3:\n      // Load 64 bytes of weights values from each row. Convert to signed int8\n      // by flipping the sign bit (i.e. subtracting 128, the required\n      // zero_point value).\n      weights00 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 2) * input_size + in + 16 * 0)));\n      weights01 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 2) * input_size + in + 16 * 1)));\n      weights02 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 2) * input_size + in + 16 * 2)));\n      weights03 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 2) * input_size + in + 16 * 3)));\n      weights10 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 3) * input_size + in + 16 * 0)));\n      weights11 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 3) * input_size + in + 16 * 1)));\n      weights12 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 3) * input_size + in + 16 * 2)));\n      weights13 = vreinterpretq_s8_u8(veorq_u8(\n          signbit,\n          vld1q_u8(weights_data + (out + 3) * input_size + in + 16 * 3)));\n      // Multiply-accumulate into local 16-bit accumulators.\n      // We can accumulate two products without overflow because weights are\n      // required to never be -128, so each product is at most 127^2 in absolute\n      // value.\n      local_accum00 = vmull_s8(vget_low_s8(weights00), vget_low_s8(input0));\n      local_accum01 = vmull_s8(vget_low_s8(weights01), vget_low_s8(input1));\n      local_accum10 = vmull_s8(vget_low_s8(weights10), vget_low_s8(input0));\n      local_accum11 = vmull_s8(vget_low_s8(weights11), vget_low_s8(input1));\n      local_accum00 = vmlal_s8(local_accum00, vget_high_s8(weights00),\n                               vget_high_s8(input0));\n      local_accum01 = vmlal_s8(local_accum01, vget_high_s8(weights01),\n                               vget_high_s8(input1));\n      local_accum10 = vmlal_s8(local_accum10, vget_high_s8(weights10),\n                               vget_high_s8(input0));\n      local_accum11 = vmlal_s8(local_accum11, vget_high_s8(weights11),\n                               vget_high_s8(input1));\n      // Pairwise add and accumulate into 32-bit accumulators\n      row_accum20 = vpadalq_s16(row_accum20, local_accum00);\n      row_accum21 = vpadalq_s16(row_accum21, local_accum01);\n      row_accum30 = vpadalq_s16(row_accum30, local_accum10);\n      row_accum31 = vpadalq_s16(row_accum31, local_accum11);\n      // Multiply-accumulate into local 16-bit accumulators.\n      // We can accumulate two products without overflow because weights are\n      // required to never be -128, so each product is at most 127^2 in absolute\n      // value.\n      local_accum00 = vmull_s8(vget_low_s8(weights02), vget_low_s8(input2));\n      local_accum01 = vmull_s8(vget_low_s8(weights03), vget_low_s8(input3));\n      local_accum10 = vmull_s8(vget_low_s8(weights12), vget_low_s8(input2));\n      local_accum11 = vmull_s8(vget_low_s8(weights13), vget_low_s8(input3));\n      local_accum00 = vmlal_s8(local_accum00, vget_high_s8(weights02),\n                               vget_high_s8(input2));\n      local_accum01 = vmlal_s8(local_accum01, vget_high_s8(weights03),\n                               vget_high_s8(input3));\n      local_accum10 = vmlal_s8(local_accum10, vget_high_s8(weights12),\n                               vget_high_s8(input2));\n      local_accum11 = vmlal_s8(local_accum11, vget_high_s8(weights13),\n                               vget_high_s8(input3));\n      // Pairwise add and accumulate into 32-bit accumulators\n      row_accum20 = vpadalq_s16(row_accum20, local_accum00);\n      row_accum21 = vpadalq_s16(row_accum21, local_accum01);\n      row_accum30 = vpadalq_s16(row_accum30, local_accum10);\n      row_accum31 = vpadalq_s16(row_accum31, local_accum11);\n    }\n\n    row_accum00 = vaddq_s32(row_accum00, row_accum01);\n    row_accum10 = vaddq_s32(row_accum10, row_accum11);\n    row_accum20 = vaddq_s32(row_accum20, row_accum21);\n    row_accum30 = vaddq_s32(row_accum30, row_accum31);\n    // Horizontally reduce accumulators\n    int32x2_t pairwise_reduced_acc_0, pairwise_reduced_acc_1,\n        pairwise_reduced_acc_2, pairwise_reduced_acc_3;\n    pairwise_reduced_acc_0 =\n        vpadd_s32(vget_low_s32(row_accum00), vget_high_s32(row_accum00));\n    pairwise_reduced_acc_1 =\n        vpadd_s32(vget_low_s32(row_accum10), vget_high_s32(row_accum10));\n    pairwise_reduced_acc_2 =\n        vpadd_s32(vget_low_s32(row_accum20), vget_high_s32(row_accum20));\n    pairwise_reduced_acc_3 =\n        vpadd_s32(vget_low_s32(row_accum30), vget_high_s32(row_accum30));\n    const int32x2_t reduced_lo =\n        vpadd_s32(pairwise_reduced_acc_0, pairwise_reduced_acc_1);\n    const int32x2_t reduced_hi =\n        vpadd_s32(pairwise_reduced_acc_2, pairwise_reduced_acc_3);\n    int32x4_t reduced = vcombine_s32(reduced_lo, reduced_hi);\n    // Add bias values.\n    reduced = vaddq_s32(reduced, bias_vec);\n    reduced = vshlq_s32(reduced, vdupq_n_s32(left_shift));\n    // Multiply by the fixed-point multiplier.\n    reduced = vqrdmulhq_n_s32(reduced, accum_multiplier);\n    // Rounding-shift-right.\n    using gemmlowp::RoundingDivideByPOT;\n    reduced = RoundingDivideByPOT(reduced, right_shift);\n    // Narrow values down to 16 bit signed.\n    const int16x4_t res16 = vqmovn_s32(reduced);\n    vst1_s16(output_ptr, res16);\n    output_ptr += 4;\n  }\n}\n#endif\n\ninline void FullyConnected(\n    const FullyConnectedParams& params, const RuntimeShape& input_shape,\n    const uint8* input_data, const RuntimeShape& filter_shape,\n    const uint8* filter_data, const RuntimeShape& bias_shape,\n    const int32* bias_data_int32, const RuntimeShape& output_shape,\n    int16* output_data, gemmlowp::GemmContext* gemmlowp_context) {\n  ruy::profiler::ScopeLabel label(\"FullyConnected/Uint8Int16\");\n  const int32 input_offset = params.input_offset;\n  const int32 filter_offset = params.weights_offset;\n  const int32 output_offset = params.output_offset;\n  const int32 output_multiplier = params.output_multiplier;\n  const int output_shift = params.output_shift;\n  const int32 output_activation_min = params.quantized_activation_min;\n  const int32 output_activation_max = params.quantized_activation_max;\n  // This is a copy of the reference implementation. We do not currently have a\n  // properly optimized version.\n  (void)gemmlowp_context;  // only used in properly optimized code.\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n  TFLITE_DCHECK_EQ(output_offset, 0);\n  TFLITE_DCHECK_GE(filter_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_GE(output_shape.DimensionsCount(), 1);\n\n  // TODO(b/62193649): This really should be:\n  //     const int batches = ArraySize(output_dims, 1);\n  // but the current --variable_batch hack consists in overwriting the 3rd\n  // dimension with the runtime batch size, as we don't keep track for each\n  // array of which dimension is the batch dimension in it.\n  const int output_dim_count = output_shape.DimensionsCount();\n  const int filter_dim_count = filter_shape.DimensionsCount();\n  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);\n  const int output_depth = MatchingDim(filter_shape, filter_dim_count - 2,\n                                       output_shape, output_dim_count - 1);\n  const int accum_depth = filter_shape.Dims(filter_dim_count - 1);\n\n  // Implementation of the fully connected node suited to the inside of an LSTM\n  // cell. The operands are 8-bit integers, the accumulators are internally\n  // 32bit integers, and the output is 16-bit fixed-point with 3 integer bits so\n  // the output range is [-2^3, 2^3] == [-8, 8]. The rationale for that\n  // is explained in the function comment above.\n#ifdef GEMMLOWP_NEON\n  if (batches == 1 && input_offset == -128 && output_activation_min == -32768 &&\n      output_activation_max == 32767) {\n    if (filter_offset == -128 && !(output_depth % 4) && !(accum_depth % 64)) {\n      GEMVForLstmCellWithSymmetricRange(\n          input_shape, input_data, filter_shape, filter_data, bias_shape,\n          bias_data_int32, output_multiplier, output_shift, output_shape,\n          output_data);\n      return;\n    }\n    if (!(output_depth % 4) && !(accum_depth % 8)) {\n      GEMVForLstmCell(input_shape, input_data, filter_shape, filter_data,\n                      filter_offset, bias_shape, bias_data_int32,\n                      output_multiplier, output_shift, output_shape,\n                      output_data);\n      return;\n    }\n  }\n#endif\n  gemmlowp::MatrixMap<const uint8, gemmlowp::MapOrder::RowMajor> weights_matrix(\n      filter_data, output_depth, accum_depth);\n  gemmlowp::MatrixMap<const uint8, gemmlowp::MapOrder::ColMajor> input_matrix(\n      input_data, accum_depth, batches);\n  gemmlowp::MatrixMap<int16, gemmlowp::MapOrder::ColMajor> output_matrix(\n      output_data, output_depth, batches);\n  typedef gemmlowp::VectorMap<const int32, gemmlowp::VectorShape::Col>\n      ColVectorMap;\n  ColVectorMap bias_vector(bias_data_int32, output_depth);\n  gemmlowp::OutputStageBiasAddition<ColVectorMap> bias_addition_stage;\n  bias_addition_stage.bias_vector = bias_vector;\n  gemmlowp::OutputStageScaleInt32ByFixedPointAndExponent scale_stage;\n  scale_stage.result_offset_after_shift = 0;\n  scale_stage.result_fixedpoint_multiplier = output_multiplier;\n  // Note that this shift is negated wrt ordinary FC.\n  scale_stage.result_exponent = output_shift;\n  gemmlowp::OutputStageClamp clamp_stage;\n  clamp_stage.min = output_activation_min;\n  clamp_stage.max = output_activation_max;\n  gemmlowp::OutputStageSaturatingCastToInt16 saturating_cast_int16_stage;\n  auto output_pipeline =\n      std::make_tuple(bias_addition_stage, scale_stage, clamp_stage,\n                      saturating_cast_int16_stage);\n  gemmlowp::GemmWithOutputPipeline<uint8, int16,\n                                   gemmlowp::L8R8WithLhsNonzeroBitDepthParams>(\n      gemmlowp_context, weights_matrix, input_matrix, &output_matrix,\n      filter_offset, input_offset, output_pipeline);\n}\n\ninline void FullyConnected(const uint8* input_data, const Dims<4>& input_dims,\n                           int32 input_offset, const uint8* filter_data,\n                           const Dims<4>& filter_dims, int32 filter_offset,\n                           const int32* bias_data, const Dims<4>& bias_dims,\n                           int32 output_offset, int32 output_multiplier,\n                           int output_shift, int32 output_activation_min,\n                           int32 output_activation_max, uint8* output_data,\n                           const Dims<4>& output_dims,\n                           gemmlowp::GemmContext* gemmlowp_context) {\n  tflite::FullyConnectedParams op_params;\n  op_params.input_offset = input_offset;\n  op_params.weights_offset = filter_offset;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n\n  FullyConnected(op_params, DimsToShape(input_dims), input_data,\n                 DimsToShape(filter_dims), filter_data, DimsToShape(bias_dims),\n                 bias_data, DimsToShape(output_dims), output_data,\n                 gemmlowp_context);\n}\n\ninline void FullyConnected(\n    const uint8* input_data, const Dims<4>& input_dims, int32 input_offset,\n    const uint8* filter_data, const Dims<4>& filter_dims, int32 filter_offset,\n    const int32* bias_data_int32, const Dims<4>& bias_dims, int32 output_offset,\n    int32 output_multiplier, int output_shift, int32 output_activation_min,\n    int32 output_activation_max, int16* output_data, const Dims<4>& output_dims,\n    gemmlowp::GemmContext* gemmlowp_context) {\n  tflite::FullyConnectedParams op_params;\n  op_params.input_offset = input_offset;\n  op_params.weights_offset = filter_offset;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n\n  FullyConnected(op_params, DimsToShape(input_dims), input_data,\n                 DimsToShape(filter_dims), filter_data, DimsToShape(bias_dims),\n                 bias_data_int32, DimsToShape(output_dims), output_data,\n                 gemmlowp_context);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid FullyConnected(const uint8* input_data, const Dims<4>& input_dims,\n                    int32 input_offset, const uint8* filter_data,\n                    const Dims<4>& filter_dims, int32 filter_offset,\n                    const int32* bias_data, const Dims<4>& bias_dims,\n                    int32 output_offset, int32 output_multiplier,\n                    int output_shift, int32 output_activation_min,\n                    int32 output_activation_max, uint8* output_data,\n                    const Dims<4>& output_dims,\n                    gemmlowp::GemmContext* gemmlowp_context) {\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  FullyConnected(input_data, input_dims, input_offset, filter_data, filter_dims,\n                 filter_offset, bias_data, bias_dims, output_offset,\n                 output_multiplier, output_shift, output_activation_min,\n                 output_activation_max, output_data, output_dims,\n                 gemmlowp_context);\n}\n\n#ifdef USE_NEON\ninline void LegacyInt8FullyConnectedAsGEMVWorkerImpl(\n    const RuntimeShape& input_shape, const int8_t* input_data,\n    int32 input_offset, const RuntimeShape& filter_shape,\n    const int8_t* filter_data, int32 filter_offset,\n    const RuntimeShape& bias_shape, const int32* bias_data, int32 output_offset,\n    int32 output_multiplier, int output_shift, int32 output_activation_min,\n    int32 output_activation_max, const RuntimeShape& output_shape,\n    int8_t* output_data, int row_start, int row_end) {\n  ruy::profiler::ScopeLabel label(\"FullyConnectedAsGEMVInt8/8bit\");\n  TFLITE_DCHECK_GE(input_shape.DimensionsCount(), 1);\n  TFLITE_DCHECK_GE(filter_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_GE(output_shape.DimensionsCount(), 1);\n  const int output_dim_count = output_shape.DimensionsCount();\n  TFLITE_DCHECK_EQ(FlatSizeSkipDim(output_shape, output_dim_count - 1), 1);\n  const int input_size = FlatSizeSkipDim(input_shape, 0);\n  static constexpr int kPeel = 4;\n  const bool shift_left = (output_shift > 0);\n  TFLITE_DCHECK_GE(row_end - row_start, kPeel);\n\n  for (int out = row_start; out < row_end; out += kPeel) {\n    out = std::min(out, row_end - kPeel);\n    int32x4_t acc0 = vdupq_n_s32(0);\n    int32x4_t acc1 = acc0;\n    int32x4_t acc2 = acc0;\n    int32x4_t acc3 = acc0;\n    const int16x8_t input_offset_vec = vdupq_n_s16(input_offset);\n    const int16x8_t filter_offset_vec = vdupq_n_s16(filter_offset);\n    int in = 0;\n    for (; in <= input_size - 16; in += 16) {\n      const int8x16_t input_val_s8 = vld1q_s8(input_data + in);\n      const int8_t* filter_ptr = filter_data + in + out * input_size;\n      int8x16_t filter_val_s8_0 = vld1q_s8(filter_ptr);\n      filter_ptr += input_size;\n      int8x16_t filter_val_s8_1 = vld1q_s8(filter_ptr);\n      filter_ptr += input_size;\n      int8x16_t filter_val_s8_2 = vld1q_s8(filter_ptr);\n      filter_ptr += input_size;\n      int8x16_t filter_val_s8_3 = vld1q_s8(filter_ptr);\n      int16x8_t input_val_0, input_val_1;\n      int8x8_t low = vget_low_s8(input_val_s8);\n      int8x8_t high = vget_high_s8(input_val_s8);\n      input_val_0 = vmovl_s8(low);\n      input_val_1 = vmovl_s8(high);\n      input_val_0 = vaddq_s16(input_val_0, input_offset_vec);\n      input_val_1 = vaddq_s16(input_val_1, input_offset_vec);\n      low = vget_low_s8(filter_val_s8_0);\n      high = vget_high_s8(filter_val_s8_0);\n      int16x8_t filter_val_0_0 = vmovl_s8(low);\n      int16x8_t filter_val_0_1 = vmovl_s8(high);\n      filter_val_0_0 = vaddq_s16(filter_val_0_0, filter_offset_vec);\n      filter_val_0_1 = vaddq_s16(filter_val_0_1, filter_offset_vec);\n      low = vget_low_s8(filter_val_s8_1);\n      high = vget_high_s8(filter_val_s8_1);\n      int16x8_t filter_val_1_0 = vmovl_s8(low);\n      int16x8_t filter_val_1_1 = vmovl_s8(high);\n      filter_val_1_0 = vaddq_s16(filter_val_1_0, filter_offset_vec);\n      filter_val_1_1 = vaddq_s16(filter_val_1_1, filter_offset_vec);\n      low = vget_low_s8(filter_val_s8_2);\n      high = vget_high_s8(filter_val_s8_2);\n      int16x8_t filter_val_2_0 = vmovl_s8(low);\n      int16x8_t filter_val_2_1 = vmovl_s8(high);\n      filter_val_2_0 = vaddq_s16(filter_val_2_0, filter_offset_vec);\n      filter_val_2_1 = vaddq_s16(filter_val_2_1, filter_offset_vec);\n      low = vget_low_s8(filter_val_s8_3);\n      high = vget_high_s8(filter_val_s8_3);\n      int16x8_t filter_val_3_0 = vmovl_s8(low);\n      int16x8_t filter_val_3_1 = vmovl_s8(high);\n      filter_val_3_0 = vaddq_s16(filter_val_3_0, filter_offset_vec);\n      filter_val_3_1 = vaddq_s16(filter_val_3_1, filter_offset_vec);\n      acc0 = vmlal_s16(acc0, vget_low_s16(filter_val_0_0),\n                       vget_low_s16(input_val_0));\n      acc1 = vmlal_s16(acc1, vget_low_s16(filter_val_1_0),\n                       vget_low_s16(input_val_0));\n      acc2 = vmlal_s16(acc2, vget_low_s16(filter_val_2_0),\n                       vget_low_s16(input_val_0));\n      acc3 = vmlal_s16(acc3, vget_low_s16(filter_val_3_0),\n                       vget_low_s16(input_val_0));\n      acc0 = vmlal_s16(acc0, vget_low_s16(filter_val_0_1),\n                       vget_low_s16(input_val_1));\n      acc1 = vmlal_s16(acc1, vget_low_s16(filter_val_1_1),\n                       vget_low_s16(input_val_1));\n      acc2 = vmlal_s16(acc2, vget_low_s16(filter_val_2_1),\n                       vget_low_s16(input_val_1));\n      acc3 = vmlal_s16(acc3, vget_low_s16(filter_val_3_1),\n                       vget_low_s16(input_val_1));\n      acc0 = vmlal_s16(acc0, vget_high_s16(filter_val_0_0),\n                       vget_high_s16(input_val_0));\n      acc1 = vmlal_s16(acc1, vget_high_s16(filter_val_1_0),\n                       vget_high_s16(input_val_0));\n      acc2 = vmlal_s16(acc2, vget_high_s16(filter_val_2_0),\n                       vget_high_s16(input_val_0));\n      acc3 = vmlal_s16(acc3, vget_high_s16(filter_val_3_0),\n                       vget_high_s16(input_val_0));\n      acc0 = vmlal_s16(acc0, vget_high_s16(filter_val_0_1),\n                       vget_high_s16(input_val_1));\n      acc1 = vmlal_s16(acc1, vget_high_s16(filter_val_1_1),\n                       vget_high_s16(input_val_1));\n      acc2 = vmlal_s16(acc2, vget_high_s16(filter_val_2_1),\n                       vget_high_s16(input_val_1));\n      acc3 = vmlal_s16(acc3, vget_high_s16(filter_val_3_1),\n                       vget_high_s16(input_val_1));\n    }\n    for (; in <= input_size - 8; in += 8) {\n      const int8x8_t input_val_s8 = vld1_s8(input_data + in);\n      const int8_t* filter_ptr = filter_data + in + out * input_size;\n      int8x8_t filter_val_s8_0 = vld1_s8(filter_ptr);\n      filter_ptr += input_size;\n      int8x8_t filter_val_s8_1 = vld1_s8(filter_ptr);\n      filter_ptr += input_size;\n      int8x8_t filter_val_s8_2 = vld1_s8(filter_ptr);\n      filter_ptr += input_size;\n      int8x8_t filter_val_s8_3 = vld1_s8(filter_ptr);\n      int16x8_t input_val = vmovl_s8(input_val_s8);\n      input_val = vaddq_s16(input_val, input_offset_vec);\n      int16x8_t filter_val_0 = vmovl_s8(filter_val_s8_0);\n      filter_val_0 = vaddq_s16(filter_val_0, filter_offset_vec);\n      int16x8_t filter_val_1 = vmovl_s8(filter_val_s8_1);\n      filter_val_1 = vaddq_s16(filter_val_1, filter_offset_vec);\n      int16x8_t filter_val_2 = vmovl_s8(filter_val_s8_2);\n      filter_val_2 = vaddq_s16(filter_val_2, filter_offset_vec);\n      int16x8_t filter_val_3 = vmovl_s8(filter_val_s8_3);\n      filter_val_3 = vaddq_s16(filter_val_3, filter_offset_vec);\n      acc0 =\n          vmlal_s16(acc0, vget_low_s16(filter_val_0), vget_low_s16(input_val));\n      acc1 =\n          vmlal_s16(acc1, vget_low_s16(filter_val_1), vget_low_s16(input_val));\n      acc2 =\n          vmlal_s16(acc2, vget_low_s16(filter_val_2), vget_low_s16(input_val));\n      acc3 =\n          vmlal_s16(acc3, vget_low_s16(filter_val_3), vget_low_s16(input_val));\n      acc0 = vmlal_s16(acc0, vget_high_s16(filter_val_0),\n                       vget_high_s16(input_val));\n      acc1 = vmlal_s16(acc1, vget_high_s16(filter_val_1),\n                       vget_high_s16(input_val));\n      acc2 = vmlal_s16(acc2, vget_high_s16(filter_val_2),\n                       vget_high_s16(input_val));\n      acc3 = vmlal_s16(acc3, vget_high_s16(filter_val_3),\n                       vget_high_s16(input_val));\n    }\n    if (in < input_size) {\n      int32 buf[16];\n      vst1q_s32(buf + 0, acc0);\n      vst1q_s32(buf + 4, acc1);\n      vst1q_s32(buf + 8, acc2);\n      vst1q_s32(buf + 12, acc3);\n      for (; in < input_size; in++) {\n        int lane = (in + 8 - input_size) % 4;\n        const int32 input_val = input_data[in] + input_offset;\n        for (int k = 0; k < kPeel; k++) {\n          int32 filter_val =\n              filter_data[in + (out + k) * input_size] + filter_offset;\n          buf[lane + 4 * k] += filter_val * input_val;\n        }\n      }\n      acc0 = vld1q_s32(buf + 0);\n      acc1 = vld1q_s32(buf + 4);\n      acc2 = vld1q_s32(buf + 8);\n      acc3 = vld1q_s32(buf + 12);\n    }\n\n    // Horizontally reduce accumulators\n    int32x2_t pairwise_reduced_acc_0 =\n        vpadd_s32(vget_low_s32(acc0), vget_high_s32(acc0));\n    int32x2_t pairwise_reduced_acc_1 =\n        vpadd_s32(vget_low_s32(acc1), vget_high_s32(acc1));\n    int32x2_t pairwise_reduced_acc_2 =\n        vpadd_s32(vget_low_s32(acc2), vget_high_s32(acc2));\n    int32x2_t pairwise_reduced_acc_3 =\n        vpadd_s32(vget_low_s32(acc3), vget_high_s32(acc3));\n    const int32x2_t reduced_lo =\n        vpadd_s32(pairwise_reduced_acc_0, pairwise_reduced_acc_1);\n    const int32x2_t reduced_hi =\n        vpadd_s32(pairwise_reduced_acc_2, pairwise_reduced_acc_3);\n    int32x4_t reduced = vcombine_s32(reduced_lo, reduced_hi);\n    // Add bias values.\n    int32x4_t bias_vec = vld1q_s32(bias_data + out);\n    reduced = vaddq_s32(reduced, bias_vec);\n    if (shift_left) {\n      const int32 multiplier_power_of_two = 1 << output_shift;\n      reduced = vmulq_n_s32(reduced, multiplier_power_of_two);\n      reduced = vqrdmulhq_n_s32(reduced, output_multiplier);\n    } else {\n      // Multiply by the fixed-point multiplier.\n      reduced = vqrdmulhq_n_s32(reduced, output_multiplier);\n      // Rounding-shift-right.\n      using gemmlowp::RoundingDivideByPOT;\n      reduced = RoundingDivideByPOT(reduced, -output_shift);\n    }\n    // Add the output offset.\n    const int32x4_t output_offset_vec = vdupq_n_s32(output_offset);\n    reduced = vaddq_s32(reduced, output_offset_vec);\n    // Narrow values down to 16 bit signed.\n    const int16x4_t res16 = vqmovn_s32(reduced);\n    // Narrow values down to 8 bit signed, saturating.\n    int8x8_t res8 = vqmovn_s16(vcombine_s16(res16, res16));\n    // Apply the clamping from the activation function\n    res8 = vmax_s8(res8, vdup_n_s8(output_activation_min));\n    res8 = vmin_s8(res8, vdup_n_s8(output_activation_max));\n    // Store results to destination.\n    vst1_lane_s8(output_data + out + 0, res8, 0);\n    vst1_lane_s8(output_data + out + 1, res8, 1);\n    vst1_lane_s8(output_data + out + 2, res8, 2);\n    vst1_lane_s8(output_data + out + 3, res8, 3);\n  }\n}\n\nstruct LegacyInt8FullyConnectedAsGEMVWorkerTask : public gemmlowp::Task {\n  LegacyInt8FullyConnectedAsGEMVWorkerTask(\n      const RuntimeShape& input_shape, const int8_t* input_data,\n      int32 input_offset, const RuntimeShape& filter_shape,\n      const int8_t* filter_data, int32 filter_offset,\n      const RuntimeShape& bias_shape, const int32* bias_data,\n      int32 output_offset, int32 output_multiplier, int output_shift,\n      int32 output_activation_min, int32 output_activation_max,\n      const RuntimeShape& output_shape, int8_t* output_data, int row_start,\n      int row_end)\n      : input_shape_(input_shape),\n        input_data_(input_data),\n        input_offset_(input_offset),\n        filter_shape_(filter_shape),\n        filter_data_(filter_data),\n        filter_offset_(filter_offset),\n        bias_shape_(bias_shape),\n        bias_data_(bias_data),\n        output_offset_(output_offset),\n        output_multiplier_(output_multiplier),\n        output_shift_(output_shift),\n        output_activation_min_(output_activation_min),\n        output_activation_max_(output_activation_max),\n        output_shape_(output_shape),\n        output_data_(output_data),\n        row_start_(row_start),\n        row_end_(row_end) {}\n\n  void Run() override {\n    LegacyInt8FullyConnectedAsGEMVWorkerImpl(\n        input_shape_, input_data_, input_offset_, filter_shape_, filter_data_,\n        filter_offset_, bias_shape_, bias_data_, output_offset_,\n        output_multiplier_, output_shift_, output_activation_min_,\n        output_activation_max_, output_shape_, output_data_, row_start_,\n        row_end_);\n  }\n\n  const RuntimeShape& input_shape_;\n  const int8_t* input_data_;\n  int32 input_offset_;\n  const RuntimeShape& filter_shape_;\n  const int8_t* filter_data_;\n  int32 filter_offset_;\n  const RuntimeShape& bias_shape_;\n  const int32* bias_data_;\n  int32 output_offset_;\n  int32 output_multiplier_;\n  int output_shift_;\n  int32 output_activation_min_;\n  int32 output_activation_max_;\n  const RuntimeShape& output_shape_;\n  int8_t* output_data_;\n  int row_start_;\n  int row_end_;\n};\n\ninline void LegacyInt8FullyConnectedAsGEMV(\n    const RuntimeShape& input_shape, const int8_t* input_data,\n    int32 input_offset, const RuntimeShape& filter_shape,\n    const int8_t* filter_data, int32 filter_offset,\n    const RuntimeShape& bias_shape, const int32* bias_data, int32 output_offset,\n    int32 output_multiplier, int output_shift, int32 output_activation_min,\n    int32 output_activation_max, const RuntimeShape& output_shape,\n    int8_t* output_data, gemmlowp::GemmContext* gemmlowp_context) {\n  const int output_dim_count = output_shape.DimensionsCount();\n  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);\n  const int output_rows = output_shape.Dims(output_dim_count - 1);\n  const int input_size = FlatSizeSkipDim(input_shape, 0);\n  static constexpr int kKernelRows = 4;\n  const int thread_count = gemmlowp::HowManyThreads<kKernelRows>(\n      gemmlowp_context->max_num_threads(), output_rows, batches, input_size);\n  if (thread_count == 1) {\n    // Single-thread case: do the computation on the current thread, don't\n    // use a threadpool\n    LegacyInt8FullyConnectedAsGEMVWorkerImpl(\n        input_shape, input_data, input_offset, filter_shape, filter_data,\n        filter_offset, bias_shape, bias_data, output_offset, output_multiplier,\n        output_shift, output_activation_min, output_activation_max,\n        output_shape, output_data, 0, output_rows);\n    return;\n  }\n\n  // Multi-threaded case: use the gemmlowp context's threadpool.\n  TFLITE_DCHECK_GT(thread_count, 1);\n  std::vector<LegacyInt8FullyConnectedAsGEMVWorkerTask> tasks;\n  // TODO(b/131746020) don't create new heap allocations every time.\n  // At least we make it a single heap allocation by using reserve().\n  tasks.reserve(thread_count);\n  const int kRowsPerWorker = gemmlowp::RoundUp<kKernelRows>(\n      gemmlowp::CeilQuotient(output_rows, thread_count));\n  int row_start = 0;\n  for (int i = 0; i < thread_count; ++i) {\n    int row_end = std::min(output_rows, row_start + kRowsPerWorker);\n    tasks.emplace_back(input_shape, input_data, input_offset, filter_shape,\n                       filter_data, filter_offset, bias_shape, bias_data,\n                       output_offset, output_multiplier, output_shift,\n                       output_activation_min, output_activation_max,\n                       output_shape, output_data, row_start, row_end);\n    row_start = row_end;\n  }\n  TFLITE_DCHECK_EQ(row_start, output_rows);\n  gemmlowp_context->workers_pool()->Execute(tasks.size(), tasks.data());\n}\n#endif  // USE_NEON\n\ninline void FullyConnected(\n    const FullyConnectedParams& params, const RuntimeShape& input_shape,\n    const int8* input_data, const RuntimeShape& filter_shape,\n    const int8* filter_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape, int8* output_data,\n    gemmlowp::GemmContext* gemmlowp_context) {\n  ruy::profiler::ScopeLabel label(\"FullyConnectedInt8/8bit\");\n\n#ifdef USE_NEON\n  const int32 input_offset = params.input_offset;\n  const int32 filter_offset = params.weights_offset;\n  const int32 output_offset = params.output_offset;\n  const int32 output_multiplier = params.output_multiplier;\n  const int output_shift = params.output_shift;\n  const int32 output_activation_min = params.quantized_activation_min;\n  const int32 output_activation_max = params.quantized_activation_max;\n  TFLITE_DCHECK_GE(filter_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_GE(output_shape.DimensionsCount(), 1);\n  // TODO(b/62193649): This really should be:\n  //     const int batches = ArraySize(output_dims, 1);\n  // but the current --variable_batch hack consists in overwriting the 3rd\n  // dimension with the runtime batch size, as we don't keep track for each\n  // array of which dimension is the batch dimension in it.\n  const int output_dim_count = output_shape.DimensionsCount();\n  const int filter_dim_count = filter_shape.DimensionsCount();\n  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);\n  if (batches == 1) {\n    const int output_size = MatchingDim(filter_shape, filter_dim_count - 2,\n                                        output_shape, output_dim_count - 1);\n    if (output_size >= 4) {\n      return LegacyInt8FullyConnectedAsGEMV(\n          input_shape, input_data, input_offset, filter_shape, filter_data,\n          filter_offset, bias_shape, bias_data, output_offset,\n          output_multiplier, output_shift, output_activation_min,\n          output_activation_max, output_shape, output_data, gemmlowp_context);\n    }\n  }\n#endif  // USE_NEON\n\n#ifdef GEMMLOWP_NEON\n  const int filter_rows = filter_shape.Dims(filter_dim_count - 2);\n  const int filter_cols = filter_shape.Dims(filter_dim_count - 1);\n  TFLITE_DCHECK_EQ(filter_shape.FlatSize(), filter_rows * filter_cols);\n  const int output_rows = output_shape.Dims(output_dim_count - 1);\n  TFLITE_DCHECK_EQ(output_rows, filter_rows);\n  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_rows);\n\n  gemmlowp::MatrixMap<const int8, gemmlowp::MapOrder::RowMajor> filter_matrix(\n      filter_data, output_rows, filter_cols, filter_cols);\n  gemmlowp::MatrixMap<const int8, gemmlowp::MapOrder::ColMajor> input_matrix(\n      input_data, filter_cols, batches, filter_cols);\n  gemmlowp::MatrixMap<int8, gemmlowp::MapOrder::ColMajor> output_matrix(\n      output_data, output_rows, batches, output_rows);\n  const auto& output_pipeline = GemmlowpOutputPipelineInt8::MakeExp(\n      bias_data, output_rows, output_offset, output_multiplier, output_shift,\n      output_activation_min, output_activation_max);\n\n  gemmlowp::GemmWithOutputPipeline<\n      int8, int8, gemmlowp::SignedL8R8WithLhsNonzeroBitDepthParams>(\n      gemmlowp_context, filter_matrix, input_matrix, &output_matrix,\n      filter_offset, input_offset, output_pipeline);\n  return;\n#endif  // GEMMLOWP_NEON\n\n  // If both GEMMLOWP_NEON && NEON paths are skipped, fallback to reference\n  // implementation.\n  reference_integer_ops::FullyConnected(params, input_shape, input_data,\n                                        filter_shape, filter_data, bias_shape,\n                                        bias_data, output_shape, output_data);\n}\n\nstruct LegacyShuffledFullyConnectedWorkerTask : gemmlowp::Task {\n  LegacyShuffledFullyConnectedWorkerTask(const uint8* input_data,\n                                         const int8* shuffled_weights_data,\n                                         int batches, int output_depth,\n                                         int output_stride, int accum_depth,\n                                         const int32* bias_data,\n                                         int32 output_multiplier,\n                                         int output_shift, int16* output_data)\n      : input_data_(input_data),\n        shuffled_weights_data_(shuffled_weights_data),\n        batches_(batches),\n        output_depth_(output_depth),\n        output_stride_(output_stride),\n        accum_depth_(accum_depth),\n        bias_data_(bias_data),\n        output_multiplier_(output_multiplier),\n        output_shift_(output_shift),\n        output_data_(output_data) {}\n\n  void Run() override {\n    ShuffledFullyConnectedWorkerImpl(\n        input_data_, shuffled_weights_data_, batches_, output_depth_,\n        output_stride_, accum_depth_, bias_data_, output_multiplier_,\n        output_shift_, output_data_);\n  }\n\n  const uint8* input_data_;\n  const int8* shuffled_weights_data_;\n  int batches_;\n  int output_depth_;\n  int output_stride_;\n  int accum_depth_;\n  const int32* bias_data_;\n  int32 output_multiplier_;\n  int output_shift_;\n  int16* output_data_;\n};\n\ninline void ShuffledFullyConnected(\n    const FullyConnectedParams& params, const RuntimeShape& input_shape,\n    const uint8* input_data, const RuntimeShape& weights_shape,\n    const uint8* shuffled_weights_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape,\n    int16* output_data, uint8* shuffled_input_workspace_data,\n    gemmlowp::GemmContext* gemmlowp_context) {\n  ruy::profiler::ScopeLabel label(\"ShuffledFullyConnected/8bit\");\n  const int32 output_multiplier = params.output_multiplier;\n  const int output_shift = params.output_shift;\n  const int32 output_activation_min = params.quantized_activation_min;\n  const int32 output_activation_max = params.quantized_activation_max;\n  (void)gemmlowp_context;  // only used in optimized code.\n  TFLITE_DCHECK_EQ(output_activation_min, -32768);\n  TFLITE_DCHECK_EQ(output_activation_max, 32767);\n  TFLITE_DCHECK_GE(input_shape.DimensionsCount(), 1);\n  TFLITE_DCHECK_GE(weights_shape.DimensionsCount(), 2);\n  TFLITE_DCHECK_GE(output_shape.DimensionsCount(), 1);\n  // TODO(b/62193649): This really should be:\n  //     const int batches = ArraySize(output_dims, 1);\n  // but the current --variable_batch hack consists in overwriting the 3rd\n  // dimension with the runtime batch size, as we don't keep track for each\n  // array of which dimension is the batch dimension in it.\n  const int output_dim_count = output_shape.DimensionsCount();\n  const int weights_dim_count = weights_shape.DimensionsCount();\n  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);\n  const int output_depth = MatchingDim(weights_shape, weights_dim_count - 2,\n                                       output_shape, output_dim_count - 1);\n  const int accum_depth = weights_shape.Dims(weights_dim_count - 1);\n  TFLITE_DCHECK((accum_depth % 16) == 0);\n  TFLITE_DCHECK((output_depth % 4) == 0);\n  // Shuffled weights have had their sign bit (0x80) pre-flipped (xor'd)\n  // so that just reinterpreting them as int8 values is equivalent to\n  // subtracting 128 from them, thus implementing for free the subtraction of\n  // the zero_point value 128.\n  const int8* int8_shuffled_weights_data =\n      reinterpret_cast<const int8*>(shuffled_weights_data);\n\n  // Shuffling and xoring of input activations into the workspace buffer\n  if (batches == 1) {\n#ifdef USE_NEON\n    const uint8x16_t signbit = vdupq_n_u8(0x80);\n    for (int i = 0; i < accum_depth; i += 16) {\n      uint8x16_t val = vld1q_u8(input_data + i);\n      val = veorq_u8(val, signbit);\n      vst1q_u8(shuffled_input_workspace_data + i, val);\n    }\n#else\n    for (int i = 0; i < accum_depth; i++) {\n      shuffled_input_workspace_data[i] = input_data[i] ^ 0x80;\n    }\n#endif\n  } else if (batches == 4) {\n    uint8* shuffled_input_workspace_ptr = shuffled_input_workspace_data;\n    int c = 0;\n#ifdef USE_NEON\n    const uint8x16_t signbit = vdupq_n_u8(0x80);\n    for (c = 0; c < accum_depth; c += 16) {\n      const uint8* src_data_ptr = input_data + c;\n      uint8x16_t val0 = vld1q_u8(src_data_ptr + 0 * accum_depth);\n      uint8x16_t val1 = vld1q_u8(src_data_ptr + 1 * accum_depth);\n      uint8x16_t val2 = vld1q_u8(src_data_ptr + 2 * accum_depth);\n      uint8x16_t val3 = vld1q_u8(src_data_ptr + 3 * accum_depth);\n      val0 = veorq_u8(val0, signbit);\n      val1 = veorq_u8(val1, signbit);\n      val2 = veorq_u8(val2, signbit);\n      val3 = veorq_u8(val3, signbit);\n      vst1q_u8(shuffled_input_workspace_ptr + 0, val0);\n      vst1q_u8(shuffled_input_workspace_ptr + 16, val1);\n      vst1q_u8(shuffled_input_workspace_ptr + 32, val2);\n      vst1q_u8(shuffled_input_workspace_ptr + 48, val3);\n      shuffled_input_workspace_ptr += 64;\n    }\n#else\n    for (c = 0; c < accum_depth; c += 16) {\n      for (int b = 0; b < 4; b++) {\n        const uint8* src_data_ptr = input_data + b * accum_depth + c;\n        for (int j = 0; j < 16; j++) {\n          uint8 src_val = *src_data_ptr++;\n          // Flip the sign bit, so that the kernel will only need to\n          // reinterpret these uint8 values as int8, getting for free the\n          // subtraction of the zero_point value 128.\n          uint8 dst_val = src_val ^ 0x80;\n          *shuffled_input_workspace_ptr++ = dst_val;\n        }\n      }\n    }\n#endif\n  } else {\n    TFLITE_DCHECK(false);\n    return;\n  }\n\n  static constexpr int kKernelRows = 4;\n  const int thread_count = gemmlowp::HowManyThreads<kKernelRows>(\n      gemmlowp_context->max_num_threads(), output_depth, batches, accum_depth);\n  if (thread_count == 1) {\n    // Single-thread case: do the computation on the current thread, don't\n    // use a threadpool\n    ShuffledFullyConnectedWorkerImpl(\n        shuffled_input_workspace_data, int8_shuffled_weights_data, batches,\n        output_depth, output_depth, accum_depth, bias_data, output_multiplier,\n        output_shift, output_data);\n    return;\n  }\n\n  // Multi-threaded case: use the gemmlowp context's threadpool.\n  TFLITE_DCHECK_GT(thread_count, 1);\n  std::vector<gemmlowp::Task*> tasks(thread_count);\n  const int kRowsPerWorker = gemmlowp::RoundUp<kKernelRows>(\n      gemmlowp::CeilQuotient(output_depth, thread_count));\n  int row_start = 0;\n  for (int i = 0; i < thread_count; i++) {\n    int row_end = std::min(output_depth, row_start + kRowsPerWorker);\n    tasks[i] = new LegacyShuffledFullyConnectedWorkerTask(\n        shuffled_input_workspace_data,\n        int8_shuffled_weights_data + row_start * accum_depth, batches,\n        row_end - row_start, output_depth, accum_depth, bias_data + row_start,\n        output_multiplier, output_shift, output_data + row_start);\n    row_start = row_end;\n  }\n  TFLITE_DCHECK_EQ(row_start, output_depth);\n  gemmlowp_context->workers_pool()->LegacyExecuteAndDestroyTasks(tasks);\n}\n\ninline void ShuffledFullyConnected(\n    const uint8* input_data, const Dims<4>& input_dims,\n    const uint8* shuffled_weights_data, const Dims<4>& weights_dims,\n    const int32* bias_data, const Dims<4>& bias_dims, int32 output_multiplier,\n    int output_shift, int32 output_activation_min, int32 output_activation_max,\n    int16* output_data, const Dims<4>& output_dims,\n    uint8* shuffled_input_workspace_data,\n    gemmlowp::GemmContext* gemmlowp_context) {\n  tflite::FullyConnectedParams op_params;\n  op_params.output_multiplier = output_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n\n  ShuffledFullyConnected(op_params, DimsToShape(input_dims), input_data,\n                         DimsToShape(weights_dims), shuffled_weights_data,\n                         DimsToShape(bias_dims), bias_data,\n                         DimsToShape(output_dims), output_data,\n                         shuffled_input_workspace_data, gemmlowp_context);\n}\n\ntemplate <typename T>\ninline void ExtractPatchIntoBufferColumn(\n    const Dims<4>& input_dims, int w, int h, int b, int kheight, int kwidth,\n    int stride_width, int stride_height, int pad_width, int pad_height,\n    int in_width, int in_height, int in_depth, int single_buffer_length,\n    int buffer_id, const T* in_data, T* conv_buffer_data, uint8 zero_byte) {\n  ExtractPatchIntoBufferColumn(\n      DimsToShape(input_dims), w, h, b, kheight, kwidth, stride_width,\n      stride_height, pad_width, pad_height, in_width, in_height, in_depth,\n      single_buffer_length, buffer_id, in_data, conv_buffer_data, zero_byte);\n}\n\ntemplate <typename T>\nvoid DilatedIm2col(const T* input_data, const Dims<4>& input_dims,\n                   const Dims<4>& filter_dims, int stride_width,\n                   int stride_height, int dilation_width_factor,\n                   int dilation_height_factor, int pad_width, int pad_height,\n                   const Dims<4>& output_dims, uint8 zero_byte,\n                   T* im2col_data) {\n  tflite::ConvParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n  op_params.dilation_width_factor = dilation_width_factor;\n  op_params.dilation_height_factor = dilation_height_factor;\n\n  DilatedIm2col(op_params, zero_byte, DimsToShape(input_dims), input_data,\n                DimsToShape(filter_dims), DimsToShape(output_dims),\n                im2col_data);\n}\n\ntemplate <typename T>\nvoid Im2col(const T* input_data, const Dims<4>& input_dims, int stride_width,\n            int stride_height, int pad_width, int pad_height, int kheight,\n            int kwidth, uint8 zero_byte, T* output_data,\n            const Dims<4>& output_dims) {\n  tflite::ConvParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n  op_params.dilation_width_factor = 1;\n  op_params.dilation_height_factor = 1;\n\n  Im2col(op_params, kheight, kwidth, zero_byte, DimsToShape(input_dims),\n         input_data, DimsToShape(output_dims), output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <typename T>\nvoid Im2col(const T* input_data, const Dims<4>& input_dims, int stride,\n            int pad_width, int pad_height, int kheight, int kwidth,\n            uint8 zero_byte, T* output_data, const Dims<4>& output_dims) {\n  Im2col(input_data, input_dims, stride, stride, pad_width, pad_height, kheight,\n         kwidth, zero_byte, output_data, output_dims);\n}\n\ninline void Conv(const ConvParams& params, const RuntimeShape& input_shape,\n                 const float* input_data, const RuntimeShape& filter_shape,\n                 const float* filter_data, const RuntimeShape& bias_shape,\n                 const float* bias_data, const RuntimeShape& output_shape,\n                 float* output_data, const RuntimeShape& im2col_shape,\n                 float* im2col_data) {\n  const int stride_width = params.stride_width;\n  const int stride_height = params.stride_height;\n  const int dilation_width_factor = params.dilation_width_factor;\n  const int dilation_height_factor = params.dilation_height_factor;\n  const float output_activation_min = params.float_activation_min;\n  const float output_activation_max = params.float_activation_max;\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n\n  (void)im2col_data;\n  (void)im2col_shape;\n  ruy::profiler::ScopeLabel label(\"Conv\");\n\n  // NB: the float 0.0f value is represented by all zero bytes.\n  const uint8 float_zero_byte = 0x00;\n  const float* gemm_input_data = nullptr;\n  const RuntimeShape* gemm_input_shape = nullptr;\n  const int filter_width = filter_shape.Dims(2);\n  const int filter_height = filter_shape.Dims(1);\n  const bool need_dilated_im2col =\n      dilation_width_factor != 1 || dilation_height_factor != 1;\n  const bool need_im2col = stride_width != 1 || stride_height != 1 ||\n                           filter_width != 1 || filter_height != 1;\n  if (need_dilated_im2col) {\n    DilatedIm2col(params, float_zero_byte, input_shape, input_data,\n                  filter_shape, output_shape, im2col_data);\n    gemm_input_data = im2col_data;\n    gemm_input_shape = &im2col_shape;\n  } else if (need_im2col) {\n    TFLITE_DCHECK(im2col_data);\n    Im2col(params, filter_height, filter_width, float_zero_byte, input_shape,\n           input_data, im2col_shape, im2col_data);\n    gemm_input_data = im2col_data;\n    gemm_input_shape = &im2col_shape;\n  } else {\n    // TODO(aselle): We need to make sure to not send im2col if it is not\n    // needed.\n    TFLITE_DCHECK(!im2col_data);\n    gemm_input_data = input_data;\n    gemm_input_shape = &input_shape;\n  }\n\n  // The following code computes matrix multiplication c = a * transponse(b)\n  // with CBLAS, where:\n  // * `a` is a matrix with dimensions (m, k).\n  // * `b` is a matrix with dimensions (n, k), so transpose(b) is (k, n).\n  // * `c` is a matrix with dimensions (m, n).\n  // The naming of variables are aligned with CBLAS specification here.\n  const float* a = gemm_input_data;\n  const float* b = filter_data;\n  float* c = output_data;\n  const int gemm_input_dims = gemm_input_shape->DimensionsCount();\n  int m = FlatSizeSkipDim(*gemm_input_shape, gemm_input_dims - 1);\n  int n = output_shape.Dims(3);\n  int k = gemm_input_shape->Dims(gemm_input_dims - 1);\n\n#if defined(TF_LITE_USE_CBLAS) && defined(__APPLE__)\n  // The stride of matrix a, b and c respectively.\n  int stride_a = k;\n  int stride_b = k;\n  int stride_c = n;\n\n  cblas_sgemm(CblasRowMajor, CblasNoTrans, CblasTrans, m, n, k, 1.0f, a,\n              stride_a, b, stride_b, 0.0f, c, stride_c);\n#else\n  // When an optimized CBLAS implementation is not available, fall back\n  // to using Eigen.\n  typedef Eigen::Matrix<float, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>\n      Matrix;\n  typedef Eigen::Map<Matrix> MatrixRef;\n  typedef Eigen::Map<const Matrix> ConstMatrixRef;\n\n  MatrixRef matrix_c(c, m, n);\n  ConstMatrixRef matrix_a(a, m, k);\n  ConstMatrixRef matrix_b(b, n, k);\n\n  // The following special casing for when a or b is a vector is required\n  // as Eigen seem to fail to make this optimization on its own.\n  if (n == 1) {\n    ruy::profiler::ScopeLabel label(\"GEMV\");\n    matrix_c.col(0).noalias() = matrix_a * matrix_b.row(0).transpose();\n  } else if (m == 1) {\n    ruy::profiler::ScopeLabel label(\"GEMV\");\n    matrix_c.row(0).noalias() = matrix_a.row(0) * matrix_b.transpose();\n  } else {\n    ruy::profiler::ScopeLabel label(\"GEMM\");\n    matrix_c.noalias() = matrix_a * matrix_b.transpose();\n  }\n\n#endif  //  defined(TF_LITE_USE_CBLAS) && defined(__APPLE__)\n\n  optimized_ops::AddBiasAndEvalActivationFunction(\n      output_activation_min, output_activation_max, bias_shape, bias_data,\n      output_shape, output_data);\n}\n\ninline void Conv(const float* input_data, const Dims<4>& input_dims,\n                 const float* filter_data, const Dims<4>& filter_dims,\n                 const float* bias_data, const Dims<4>& bias_dims,\n                 int stride_width, int stride_height, int dilation_width_factor,\n                 int dilation_height_factor, int pad_width, int pad_height,\n                 float output_activation_min, float output_activation_max,\n                 float* output_data, const Dims<4>& output_dims,\n                 float* im2col_data, const Dims<4>& im2col_dims) {\n  tflite::ConvParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n  op_params.dilation_width_factor = dilation_width_factor;\n  op_params.dilation_height_factor = dilation_height_factor;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n\n  Conv(op_params, DimsToShape(input_dims), input_data, DimsToShape(filter_dims),\n       filter_data, DimsToShape(bias_dims), bias_data, DimsToShape(output_dims),\n       output_data, DimsToShape(im2col_dims), im2col_data);\n}\n\ninline void HybridConv(const int8_t* input_data, const Dims<4>& input_dims,\n                       const int8_t* filter_data, const Dims<4>& filter_dims,\n                       const float* bias_data, const Dims<4>& bias_dims,\n                       int stride_width, int stride_height, int pad_width,\n                       int pad_height, float* scaling_factors_ptr,\n                       float output_activation_min, float output_activation_max,\n                       int32_t* scratch_data, const Dims<4>& scratch_dims,\n                       float* output_data, const Dims<4>& output_dims,\n                       int8_t* im2col_data, const Dims<4>& im2col_dims,\n                       CpuBackendContext* context) {\n  tflite::ConvParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n\n  HybridConv(op_params, scaling_factors_ptr, DimsToShape(input_dims),\n             input_data, DimsToShape(filter_dims), filter_data,\n             DimsToShape(bias_dims), bias_data, DimsToShape(scratch_dims),\n             scratch_data, DimsToShape(output_dims), output_data,\n             DimsToShape(im2col_dims), im2col_data, context);\n}\n\ntemplate <FusedActivationFunctionType Ac>\nvoid Conv(const float* input_data, const Dims<4>& input_dims,\n          const float* filter_data, const Dims<4>& filter_dims,\n          const float* bias_data, const Dims<4>& bias_dims, int stride_width,\n          int stride_height, int dilation_width_factor,\n          int dilation_height_factor, int pad_width, int pad_height,\n          float* output_data, const Dims<4>& output_dims, float* im2col_data,\n          const Dims<4>& im2col_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  Conv(input_data, input_dims, filter_data, filter_dims, bias_data, bias_dims,\n       stride_width, stride_height, dilation_width_factor,\n       dilation_height_factor, pad_width, pad_height, output_activation_min,\n       output_activation_max, output_data, output_dims, im2col_data,\n       im2col_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid Conv(const float* input_data, const Dims<4>& input_dims,\n          const float* filter_data, const Dims<4>& filter_dims,\n          const float* bias_data, const Dims<4>& bias_dims, int stride_width,\n          int stride_height, int pad_width, int pad_height, float* output_data,\n          const Dims<4>& output_dims, float* im2col_data,\n          const Dims<4>& im2col_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  Conv(input_data, input_dims, filter_data, filter_dims, bias_data, bias_dims,\n       stride_width, stride_height, 1, 1, pad_width, pad_height,\n       output_activation_min, output_activation_max, output_data, output_dims,\n       im2col_data, im2col_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid Conv(const float* input_data, const Dims<4>& input_dims,\n          const float* filter_data, const Dims<4>& filter_dims,\n          const float* bias_data, const Dims<4>& bias_dims, int stride,\n          int pad_width, int pad_height, float* output_data,\n          const Dims<4>& output_dims, float* im2col_data,\n          const Dims<4>& im2col_dims) {\n  Conv<Ac>(input_data, input_dims, filter_data, filter_dims, bias_data,\n           bias_dims, stride, stride, 1, 1, pad_width, pad_height, output_data,\n           output_dims, im2col_data, im2col_dims);\n}\n\ninline void Conv(const ConvParams& params, const RuntimeShape& input_shape,\n                 const uint8* input_data, const RuntimeShape& filter_shape,\n                 const uint8* filter_data, const RuntimeShape& bias_shape,\n                 const int32* bias_data, const RuntimeShape& output_shape,\n                 uint8* output_data, const RuntimeShape& im2col_shape,\n                 uint8* im2col_data, gemmlowp::GemmContext* gemmlowp_context) {\n  ruy::profiler::ScopeLabel label(\"Conv/8bit\");\n  const int stride_width = params.stride_width;\n  const int stride_height = params.stride_height;\n  const int dilation_width_factor = params.dilation_width_factor;\n  const int dilation_height_factor = params.dilation_height_factor;\n  const int32 input_offset = params.input_offset;\n  const int32 filter_offset = params.weights_offset;\n  const int32 output_offset = params.output_offset;\n  const int32 output_multiplier = params.output_multiplier;\n  const int output_shift = params.output_shift;\n  const int32 output_activation_min = params.quantized_activation_min;\n  const int32 output_activation_max = params.quantized_activation_max;\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n\n  const uint8* gemm_input_data = nullptr;\n  const RuntimeShape* gemm_input_shape = nullptr;\n  const int filter_width = filter_shape.Dims(2);\n  const int filter_height = filter_shape.Dims(1);\n  const bool need_dilated_im2col =\n      dilation_width_factor != 1 || dilation_height_factor != 1;\n  const bool need_im2col = stride_width != 1 || stride_height != 1 ||\n                           filter_width != 1 || filter_height != 1;\n  if (need_dilated_im2col) {\n    TFLITE_DCHECK(im2col_data);\n    const int input_zero_point = -input_offset;\n    TFLITE_DCHECK_GE(input_zero_point, 0);\n    TFLITE_DCHECK_LE(input_zero_point, 255);\n    DilatedIm2col(params, input_zero_point, input_shape, input_data,\n                  filter_shape, output_shape, im2col_data);\n    gemm_input_data = im2col_data;\n    gemm_input_shape = &im2col_shape;\n  } else if (need_im2col) {\n    TFLITE_DCHECK(im2col_data);\n    const int input_zero_point = -input_offset;\n    TFLITE_DCHECK_GE(input_zero_point, 0);\n    TFLITE_DCHECK_LE(input_zero_point, 255);\n    Im2col(params, filter_height, filter_width, input_zero_point, input_shape,\n           input_data, im2col_shape, im2col_data);\n    gemm_input_data = im2col_data;\n    gemm_input_shape = &im2col_shape;\n  } else {\n    TFLITE_DCHECK(!im2col_data);\n    gemm_input_data = input_data;\n    gemm_input_shape = &input_shape;\n  }\n\n  const int gemm_input_rows = gemm_input_shape->Dims(3);\n  // Using FlatSizeSkipDim causes segfault in some contexts (see b/79927784).\n  // The root cause has not yet been identified though. Same applies below for\n  // the other calls commented out. This is a partial rollback of cl/196819423.\n  // const int gemm_input_cols = FlatSizeSkipDim(*gemm_input_shape, 3);\n  const int gemm_input_cols = gemm_input_shape->Dims(0) *\n                              gemm_input_shape->Dims(1) *\n                              gemm_input_shape->Dims(2);\n  const int filter_rows = filter_shape.Dims(0);\n  // See b/79927784.\n  // const int filter_cols = FlatSizeSkipDim(filter_shape, 0);\n  const int filter_cols =\n      filter_shape.Dims(1) * filter_shape.Dims(2) * filter_shape.Dims(3);\n  const int output_rows = output_shape.Dims(3);\n  // See b/79927784.\n  // const int output_cols = FlatSizeSkipDim(output_shape, 3);\n  const int output_cols =\n      output_shape.Dims(0) * output_shape.Dims(1) * output_shape.Dims(2);\n  TFLITE_DCHECK_EQ(output_rows, filter_rows);\n  TFLITE_DCHECK_EQ(output_cols, gemm_input_cols);\n  TFLITE_DCHECK_EQ(filter_cols, gemm_input_rows);\n  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_rows);\n\n#ifdef USE_NEON\n  if (gemm_input_cols == 1 && output_rows >= 4) {\n    RuntimeShape fc_filter_shape{\n        filter_shape.Dims(0),\n        filter_shape.Dims(filter_shape.DimensionsCount() - 1)};\n\n    return FullyConnectedAsGEMV(\n        *gemm_input_shape, gemm_input_data, input_offset, fc_filter_shape,\n        filter_data, filter_offset, bias_shape, bias_data, output_offset,\n        output_multiplier, output_shift, output_activation_min,\n        output_activation_max, output_shape, output_data, gemmlowp_context);\n  }\n#endif\n\n  gemmlowp::MatrixMap<const uint8, gemmlowp::MapOrder::RowMajor> filter_matrix(\n      filter_data, filter_rows, filter_cols);\n  gemmlowp::MatrixMap<const uint8, gemmlowp::MapOrder::ColMajor> input_matrix(\n      gemm_input_data, gemm_input_rows, gemm_input_cols);\n  gemmlowp::MatrixMap<uint8, gemmlowp::MapOrder::ColMajor> output_matrix(\n      output_data, output_rows, output_cols);\n  const auto& output_pipeline = GemmlowpOutputPipeline::MakeExp(\n      bias_data, output_rows, output_offset, output_multiplier, output_shift,\n      output_activation_min, output_activation_max);\n  gemmlowp::GemmWithOutputPipeline<uint8, uint8,\n                                   gemmlowp::L8R8WithLhsNonzeroBitDepthParams>(\n      gemmlowp_context, filter_matrix, input_matrix, &output_matrix,\n      filter_offset, input_offset, output_pipeline);\n}\n\ninline void Conv(const uint8* input_data, const Dims<4>& input_dims,\n                 int32 input_offset, const uint8* filter_data,\n                 const Dims<4>& filter_dims, int32 filter_offset,\n                 const int32* bias_data, const Dims<4>& bias_dims,\n                 int stride_width, int stride_height, int dilation_width_factor,\n                 int dilation_height_factor, int pad_width, int pad_height,\n                 int32 output_offset, int32 output_multiplier, int output_shift,\n                 int32 output_activation_min, int32 output_activation_max,\n                 uint8* output_data, const Dims<4>& output_dims,\n                 uint8* im2col_data, const Dims<4>& im2col_dims,\n                 gemmlowp::GemmContext* gemmlowp_context) {\n  tflite::ConvParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n  op_params.dilation_width_factor = dilation_width_factor;\n  op_params.dilation_height_factor = dilation_height_factor;\n  op_params.input_offset = input_offset;\n  op_params.weights_offset = filter_offset;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  // Legacy ops used mixed left and right shifts. Now all are +ve-means-left.\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n\n  Conv(op_params, DimsToShape(input_dims), input_data, DimsToShape(filter_dims),\n       filter_data, DimsToShape(bias_dims), bias_data, DimsToShape(output_dims),\n       output_data, DimsToShape(im2col_dims), im2col_data, gemmlowp_context);\n}\n\ninline void Conv(const uint8* input_data, const Dims<4>& input_dims,\n                 int32 input_offset, const uint8* filter_data,\n                 const Dims<4>& filter_dims, int32 filter_offset,\n                 const int32* bias_data, const Dims<4>& bias_dims,\n                 int stride_width, int stride_height, int pad_width,\n                 int pad_height, int32 output_offset, int32 output_multiplier,\n                 int output_shift, int32 output_activation_min,\n                 int32 output_activation_max, uint8* output_data,\n                 const Dims<4>& output_dims, uint8* im2col_data,\n                 const Dims<4>& im2col_dims,\n                 gemmlowp::GemmContext* gemmlowp_context) {\n  Conv(input_data, input_dims, input_offset, filter_data, filter_dims,\n       filter_offset, bias_data, bias_dims, stride_width, stride_height, 1, 1,\n       pad_width, pad_height, output_offset, output_multiplier, output_shift,\n       output_activation_min, output_activation_max, output_data, output_dims,\n       im2col_data, im2col_dims, gemmlowp_context);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\ninline void Conv(const uint8* input_data, const Dims<4>& input_dims,\n                 int32 input_offset, const uint8* filter_data,\n                 const Dims<4>& filter_dims, int32 filter_offset,\n                 const int32* bias_data, const Dims<4>& bias_dims,\n                 int stride_width, int stride_height, int pad_width,\n                 int pad_height, int32 output_offset, int32 output_multiplier,\n                 int output_shift, int32 output_activation_min,\n                 int32 output_activation_max, uint8* output_data,\n                 const Dims<4>& output_dims, uint8* im2col_data,\n                 const Dims<4>& im2col_dims,\n                 gemmlowp::GemmContext* gemmlowp_context) {\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n  Conv(input_data, input_dims, input_offset, filter_data, filter_dims,\n       filter_offset, bias_data, bias_dims, stride_width, stride_height,\n       pad_width, pad_height, output_offset, output_multiplier, output_shift,\n       output_activation_min, output_activation_max, output_data, output_dims,\n       im2col_data, im2col_dims, gemmlowp_context);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid Conv(const uint8* input_data, const Dims<4>& input_dims,\n          int32 input_offset, const uint8* filter_data,\n          const Dims<4>& filter_dims, int32 filter_offset,\n          const int32* bias_data, const Dims<4>& bias_dims, int stride,\n          int pad_width, int pad_height, int32 output_offset,\n          int32 output_multiplier, int output_shift,\n          int32 output_activation_min, int32 output_activation_max,\n          uint8* output_data, const Dims<4>& output_dims, uint8* im2col_data,\n          const Dims<4>& im2col_dims, gemmlowp::GemmContext* gemmlowp_context) {\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  Conv(input_data, input_dims, input_offset, filter_data, filter_dims,\n       filter_offset, bias_data, bias_dims, stride, stride, pad_width,\n       pad_height, output_offset, output_multiplier, output_shift,\n       output_activation_min, output_activation_max, output_data, output_dims,\n       im2col_data, im2col_dims, gemmlowp_context);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac, typename T>\nvoid Im2col(const T* input_data, const Dims<4>& input_dims, int stride,\n            int pad_width, int pad_height, int kheight, int kwidth,\n            uint8 zero_byte, T* output_data, const Dims<4>& output_dims) {\n  Im2col(input_data, input_dims, stride, stride, pad_width, pad_height, kheight,\n         kwidth, zero_byte, output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid ConvAsGemm(const float* input_data, const Dims<4>& input_dims,\n                const float* filter_data, const Dims<4>& filter_dims,\n                const float* bias_data, const Dims<4>& bias_dims,\n                float* output_data, const Dims<4>& output_dims) {\n  ruy::profiler::ScopeLabel label(\"ConvAsGemm\");\n\n  const auto input_matrix_map =\n      MapAsMatrixWithFirstDimAsRows(input_data, input_dims);\n  const auto filter_matrix_map =\n      MapAsMatrixWithLastDimAsCols(filter_data, filter_dims);\n  auto output_matrix_map =\n      MapAsMatrixWithFirstDimAsRows(output_data, output_dims);\n\n  Gemm(filter_matrix_map.transpose(), input_matrix_map, &output_matrix_map);\n\n  AddBiasAndEvalActivationFunction<Ac>(bias_data, bias_dims, output_data,\n                                       output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid ConvAsGemm(const uint8* input_data, const Dims<4>& input_dims,\n                int32 input_offset, const uint8* filter_data,\n                const Dims<4>& filter_dims, int32 filter_offset,\n                const int32* bias_data, const Dims<4>& bias_dims,\n                int32 output_offset, int32 output_multiplier, int output_shift,\n                int32 output_activation_min, int32 output_activation_max,\n                uint8* output_data, const Dims<4>& output_dims,\n                gemmlowp::GemmContext* gemmlowp_context) {\n  ruy::profiler::ScopeLabel label(\"ConvAsGemm/8bit\");\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  const int input_rows = input_dims.sizes[0];\n  const int input_cols = FlatSizeSkipDim(input_dims, 0);\n  const int filter_rows = filter_dims.sizes[3];\n  const int filter_cols = FlatSizeSkipDim(filter_dims, 3);\n  const int output_rows = output_dims.sizes[0];\n  const int output_cols = FlatSizeSkipDim(output_dims, 0);\n  TFLITE_DCHECK_EQ(output_rows, filter_rows);\n  TFLITE_DCHECK_EQ(output_cols, input_cols);\n  TFLITE_DCHECK_EQ(filter_cols, input_rows);\n  TFLITE_DCHECK_EQ(bias_dims.sizes[0], output_rows);\n  TFLITE_DCHECK_EQ(bias_dims.sizes[1], 1);\n  TFLITE_DCHECK_EQ(bias_dims.sizes[2], 1);\n  TFLITE_DCHECK_EQ(bias_dims.sizes[3], 1);\n  gemmlowp::MatrixMap<const uint8, gemmlowp::MapOrder::RowMajor> filter_matrix(\n      filter_data, output_rows, filter_cols, filter_cols);\n  gemmlowp::MatrixMap<const uint8, gemmlowp::MapOrder::ColMajor> input_matrix(\n      input_data, filter_cols, output_cols, filter_cols);\n  gemmlowp::MatrixMap<uint8, gemmlowp::MapOrder::ColMajor> output_matrix(\n      output_data, output_rows, output_cols, output_rows);\n  const auto& output_pipeline = GemmlowpOutputPipeline::MakeExp(\n      bias_data, output_rows, output_offset, output_multiplier, -output_shift,\n      output_activation_min, output_activation_max);\n  gemmlowp::GemmWithOutputPipeline<uint8, uint8,\n                                   gemmlowp::L8R8WithLhsNonzeroBitDepthParams>(\n      gemmlowp_context, filter_matrix, input_matrix, &output_matrix,\n      filter_offset, input_offset, output_pipeline);\n}\n\ninline void TransposeConv(\n    const ConvParams& params, const RuntimeShape& input_shape,\n    const float* input_data, const RuntimeShape& filter_shape,\n    const float* filter_data, const RuntimeShape& output_shape,\n    float* output_data, const RuntimeShape& im2col_shape, float* im2col_data) {\n  ruy::profiler::ScopeLabel label(\"TransposeConv\");\n  // Note we could use transposed weights with forward conv for unstrided\n  // cases. But we are already getting good performance with this code as-is.\n  TFLITE_DCHECK(im2col_data);\n  TransposeIm2col(params, 0, input_shape, input_data, filter_shape,\n                  output_shape, im2col_data);\n\n  const auto im2col_matrix_map =\n      MapAsMatrixWithLastDimAsRows(im2col_data, im2col_shape);\n  const auto filter_matrix_map =\n      MapAsMatrixWithFirstDimAsCols(filter_data, filter_shape);\n  auto output_matrix_map =\n      MapAsMatrixWithLastDimAsRows(output_data, output_shape);\n\n  Gemm(filter_matrix_map.transpose(), im2col_matrix_map, &output_matrix_map);\n}\n\ninline void TransposeConv(const float* input_data, const Dims<4>& input_dims,\n                          const float* filter_data, const Dims<4>& filter_dims,\n                          int stride_width, int stride_height, int pad_width,\n                          int pad_height, float* output_data,\n                          const Dims<4>& output_dims, float* im2col_data,\n                          const Dims<4>& im2col_dims) {\n  tflite::ConvParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n\n  TransposeConv(op_params, DimsToShape(input_dims), input_data,\n                DimsToShape(filter_dims), filter_data, DimsToShape(output_dims),\n                output_data, DimsToShape(im2col_dims), im2col_data);\n}\n\ninline void TransposeConvV2(\n    const ConvParams& params, const RuntimeShape& input_shape,\n    const float* input_data, const RuntimeShape& hwoi_ordered_filter_shape,\n    const float* hwoi_ordered_filter_data, const RuntimeShape& output_shape,\n    float* output_data, const RuntimeShape& col2im_shape, float* col2im_data,\n    CpuBackendContext* cpu_backend_context) {\n  TransposeConvV2(params, input_shape, input_data, hwoi_ordered_filter_shape,\n                  hwoi_ordered_filter_data, /*bias_shape*/ RuntimeShape(),\n                  /*bias_data*/ nullptr, output_shape, output_data,\n                  col2im_shape, col2im_data, cpu_backend_context);\n}\n\ntemplate <typename T>\nvoid TransposeIm2col(const T* input_data, const Dims<4>& input_dims,\n                     const Dims<4>& filter_dims, int stride_width,\n                     int stride_height, int pad_width, int pad_height,\n                     const Dims<4>& output_dims, uint8 zero_byte,\n                     T* im2col_data) {\n  tflite::ConvParams op_params;\n  // Padding type is ignored, but still set.\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = pad_width;\n  op_params.padding_values.height = pad_height;\n  op_params.stride_width = stride_width;\n  op_params.stride_height = stride_height;\n\n  TransposeIm2col(op_params, zero_byte, DimsToShape(input_dims), input_data,\n                  DimsToShape(filter_dims), DimsToShape(output_dims),\n                  im2col_data);\n}\n\ninline void LstmCell(\n    const LstmCellParams& params, const RuntimeShape& unextended_input_shape,\n    const float* input_data, const RuntimeShape& unextended_prev_activ_shape,\n    const float* prev_activ_data, const RuntimeShape& weights_shape,\n    const float* weights_data, const RuntimeShape& unextended_bias_shape,\n    const float* bias_data, const RuntimeShape& unextended_prev_state_shape,\n    const float* prev_state_data,\n    const RuntimeShape& unextended_output_state_shape, float* output_state_data,\n    const RuntimeShape& unextended_output_activ_shape, float* output_activ_data,\n    const RuntimeShape& unextended_concat_temp_shape, float* concat_temp_data,\n    const RuntimeShape& unextended_activ_temp_shape, float* activ_temp_data) {\n  ruy::profiler::ScopeLabel label(\"LstmCell\");\n  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_prev_activ_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_bias_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_prev_state_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_output_state_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_output_activ_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_concat_temp_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_activ_temp_shape.DimensionsCount(), 4);\n  const RuntimeShape input_shape =\n      RuntimeShape::ExtendedShape(4, unextended_input_shape);\n  const RuntimeShape prev_activ_shape =\n      RuntimeShape::ExtendedShape(4, unextended_prev_activ_shape);\n  const RuntimeShape bias_shape =\n      RuntimeShape::ExtendedShape(4, unextended_bias_shape);\n  const RuntimeShape prev_state_shape =\n      RuntimeShape::ExtendedShape(4, unextended_prev_state_shape);\n  const RuntimeShape output_state_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_state_shape);\n  const RuntimeShape output_activ_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_activ_shape);\n  const RuntimeShape concat_temp_shape =\n      RuntimeShape::ExtendedShape(4, unextended_concat_temp_shape);\n  const RuntimeShape activ_temp_shape =\n      RuntimeShape::ExtendedShape(4, unextended_activ_temp_shape);\n  TFLITE_DCHECK_GE(weights_shape.DimensionsCount(), 2);\n\n  const int weights_dim_count = weights_shape.DimensionsCount();\n  MatchingDim(  // batches\n      input_shape, 0, prev_activ_shape, 0, prev_state_shape, 0,\n      output_state_shape, 0, output_activ_shape, 0);\n  MatchingDim(  // height\n      input_shape, 1, prev_activ_shape, 1, prev_state_shape, 1,\n      output_state_shape, 1, output_activ_shape, 1);\n  MatchingDim(  // width\n      input_shape, 2, prev_activ_shape, 2, prev_state_shape, 2,\n      output_state_shape, 2, output_activ_shape, 2);\n  const int input_depth = input_shape.Dims(3);\n  const int prev_activ_depth = prev_activ_shape.Dims(3);\n  const int total_input_depth = prev_activ_depth + input_depth;\n  TFLITE_DCHECK_EQ(weights_shape.Dims(weights_dim_count - 1),\n                   total_input_depth);\n  TFLITE_DCHECK_EQ(FlatSizeSkipDim(bias_shape, 3), 1);\n  const int intern_activ_depth =\n      MatchingDim(weights_shape, weights_dim_count - 2, bias_shape, 3);\n  TFLITE_DCHECK_EQ(weights_shape.FlatSize(),\n                   intern_activ_depth * total_input_depth);\n  TFLITE_DCHECK_EQ(intern_activ_depth % 4, 0);\n  const int output_depth =\n      MatchingDim(prev_state_shape, 3, prev_activ_shape, 3, output_state_shape,\n                  3, output_activ_shape, 3);\n  TFLITE_DCHECK_EQ(output_depth, intern_activ_depth / 4);\n\n  // Concatenate prev_activ and input data together\n  std::vector<float const*> concat_input_arrays_data;\n  std::vector<RuntimeShape const*> concat_input_arrays_shapes;\n  concat_input_arrays_data.push_back(input_data);\n  concat_input_arrays_data.push_back(prev_activ_data);\n  concat_input_arrays_shapes.push_back(&input_shape);\n  concat_input_arrays_shapes.push_back(&prev_activ_shape);\n  tflite::ConcatenationParams concat_params;\n  concat_params.axis = 3;\n  concat_params.inputs_count = concat_input_arrays_data.size();\n  Concatenation(concat_params, &(concat_input_arrays_shapes[0]),\n                &(concat_input_arrays_data[0]), concat_temp_shape,\n                concat_temp_data);\n\n  // Fully connected\n  tflite::FullyConnectedParams fc_params;\n  fc_params.float_activation_min = std::numeric_limits<float>::lowest();\n  fc_params.float_activation_max = std::numeric_limits<float>::max();\n  FullyConnected(fc_params, concat_temp_shape, concat_temp_data, weights_shape,\n                 weights_data, bias_shape, bias_data, activ_temp_shape,\n                 activ_temp_data);\n\n  // Map raw arrays to Eigen arrays so we can use Eigen's optimized array\n  // operations.\n  ArrayMap<float> activ_temp_map =\n      MapAsArrayWithLastDimAsRows(activ_temp_data, activ_temp_shape);\n  auto input_gate_sm = activ_temp_map.block(0 * output_depth, 0, output_depth,\n                                            activ_temp_map.cols());\n  auto new_input_sm = activ_temp_map.block(1 * output_depth, 0, output_depth,\n                                           activ_temp_map.cols());\n  auto forget_gate_sm = activ_temp_map.block(2 * output_depth, 0, output_depth,\n                                             activ_temp_map.cols());\n  auto output_gate_sm = activ_temp_map.block(3 * output_depth, 0, output_depth,\n                                             activ_temp_map.cols());\n  ArrayMap<const float> prev_state_map =\n      MapAsArrayWithLastDimAsRows(prev_state_data, prev_state_shape);\n  ArrayMap<float> output_state_map =\n      MapAsArrayWithLastDimAsRows(output_state_data, output_state_shape);\n  ArrayMap<float> output_activ_map =\n      MapAsArrayWithLastDimAsRows(output_activ_data, output_activ_shape);\n\n  // Combined memory state and final output calculation\n  ruy::profiler::ScopeLabel label2(\"MemoryStateAndFinalOutput\");\n  output_state_map =\n      input_gate_sm.unaryExpr(Eigen::internal::scalar_logistic_op<float>()) *\n          new_input_sm.tanh() +\n      forget_gate_sm.unaryExpr(Eigen::internal::scalar_logistic_op<float>()) *\n          prev_state_map;\n  output_activ_map =\n      output_gate_sm.unaryExpr(Eigen::internal::scalar_logistic_op<float>()) *\n      output_state_map.tanh();\n}\n\ninline void LstmCell(const float* input_data, const Dims<4>& input_dims,\n                     const float* prev_activ_data,\n                     const Dims<4>& prev_activ_dims, const float* weights_data,\n                     const Dims<4>& weights_dims, const float* bias_data,\n                     const Dims<4>& bias_dims, const float* prev_state_data,\n                     const Dims<4>& prev_state_dims, float* output_state_data,\n                     const Dims<4>& output_state_dims, float* output_activ_data,\n                     const Dims<4>& output_activ_dims, float* concat_temp_data,\n                     const Dims<4>& concat_temp_dims, float* activ_temp_data,\n                     const Dims<4>& activ_temp_dims) {\n  tflite::LstmCellParams op_params;\n  // Float LSTM cell does not need parameters to be set: leave untouched.\n\n  LstmCell(op_params, DimsToShape(input_dims), input_data,\n           DimsToShape(prev_activ_dims), prev_activ_data,\n           DimsToShape(weights_dims), weights_data, DimsToShape(bias_dims),\n           bias_data, DimsToShape(prev_state_dims), prev_state_data,\n           DimsToShape(output_state_dims), output_state_data,\n           DimsToShape(output_activ_dims), output_activ_data,\n           DimsToShape(concat_temp_dims), concat_temp_data,\n           DimsToShape(activ_temp_dims), activ_temp_data);\n}\n\ntemplate <int StateIntegerBits>\ninline void LstmCell(\n    const LstmCellParams& params, const RuntimeShape& unextended_input_shape,\n    const uint8* input_data_uint8,\n    const RuntimeShape& unextended_prev_activ_shape,\n    const uint8* prev_activ_data_uint8, const RuntimeShape& weights_shape,\n    const uint8* weights_data_uint8, const RuntimeShape& unextended_bias_shape,\n    const int32* bias_data_int32,\n    const RuntimeShape& unextended_prev_state_shape,\n    const int16* prev_state_data_int16,\n    const RuntimeShape& unextended_output_state_shape,\n    int16* output_state_data_int16,\n    const RuntimeShape& unextended_output_activ_shape,\n    uint8* output_activ_data_uint8,\n    const RuntimeShape& unextended_concat_temp_shape,\n    uint8* concat_temp_data_uint8,\n    const RuntimeShape& unextended_activ_temp_shape,\n    int16* activ_temp_data_int16, gemmlowp::GemmContext* gemmlowp_context) {\n  ruy::profiler::ScopeLabel label(\n      \"LstmCell/quantized (8bit external, 16bit internal)\");\n  int32 weights_zero_point = params.weights_zero_point;\n  int32 accum_multiplier = params.accum_multiplier;\n  int accum_shift = params.accum_shift;\n  TFLITE_DCHECK_LE(unextended_input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_prev_activ_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_bias_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_prev_state_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_output_state_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_output_activ_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_concat_temp_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_activ_temp_shape.DimensionsCount(), 4);\n  const RuntimeShape input_shape =\n      RuntimeShape::ExtendedShape(4, unextended_input_shape);\n  const RuntimeShape prev_activ_shape =\n      RuntimeShape::ExtendedShape(4, unextended_prev_activ_shape);\n  const RuntimeShape bias_shape =\n      RuntimeShape::ExtendedShape(4, unextended_bias_shape);\n  const RuntimeShape prev_state_shape =\n      RuntimeShape::ExtendedShape(4, unextended_prev_state_shape);\n  const RuntimeShape output_state_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_state_shape);\n  const RuntimeShape output_activ_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_activ_shape);\n  const RuntimeShape concat_temp_shape =\n      RuntimeShape::ExtendedShape(4, unextended_concat_temp_shape);\n  const RuntimeShape activ_temp_shape =\n      RuntimeShape::ExtendedShape(4, unextended_activ_temp_shape);\n  TFLITE_DCHECK_GE(weights_shape.DimensionsCount(), 2);\n\n  // Gather dimensions information, and perform consistency checks.\n  const int weights_dim_count = weights_shape.DimensionsCount();\n  const int outer_size = MatchingFlatSizeSkipDim(\n      input_shape, 3, prev_activ_shape, prev_state_shape, output_state_shape,\n      output_activ_shape);\n  const int input_depth = input_shape.Dims(3);\n  const int prev_activ_depth = prev_activ_shape.Dims(3);\n  const int total_input_depth = prev_activ_depth + input_depth;\n  TFLITE_DCHECK_EQ(weights_shape.Dims(weights_dim_count - 1),\n                   total_input_depth);\n  const int intern_activ_depth =\n      MatchingDim(weights_shape, weights_dim_count - 2, bias_shape, 3);\n  TFLITE_DCHECK_EQ(weights_shape.FlatSize(),\n                   intern_activ_depth * total_input_depth);\n  TFLITE_DCHECK_EQ(FlatSizeSkipDim(bias_shape, 3), 1);\n  TFLITE_DCHECK_EQ(intern_activ_depth % 4, 0);\n  const int output_depth =\n      MatchingDim(prev_state_shape, 3, prev_activ_shape, 3, output_state_shape,\n                  3, output_activ_shape, 3);\n  TFLITE_DCHECK_EQ(output_depth, intern_activ_depth / 4);\n  const int fc_batches = FlatSizeSkipDim(activ_temp_shape, 3);\n  const int fc_output_depth =\n      MatchingDim(weights_shape, weights_dim_count - 2, activ_temp_shape, 3);\n  const int fc_accum_depth = total_input_depth;\n  TFLITE_DCHECK_EQ(fc_output_depth, 4 * output_depth);\n\n  // Depth-concatenate prev_activ and input data together.\n  uint8 const* concat_input_arrays_data[2] = {input_data_uint8,\n                                              prev_activ_data_uint8};\n  const RuntimeShape* concat_input_arrays_shapes[2] = {&input_shape,\n                                                       &prev_activ_shape};\n  tflite::ConcatenationParams concat_params;\n  concat_params.axis = 3;\n  concat_params.inputs_count = 2;\n  Concatenation(concat_params, concat_input_arrays_shapes,\n                concat_input_arrays_data, concat_temp_shape,\n                concat_temp_data_uint8);\n\n  // Implementation of the fully connected node inside the LSTM cell.\n  // The operands are 8-bit integers, the accumulators are internally 32bit\n  // integers, and the output is 16-bit fixed-point with 3 integer bits so\n  // the output range is [-2^3, 2^3] == [-8, 8]. The rationale for that\n  // is explained in the function comment above.\n  bool gemm_already_performed = false;\n#ifdef GEMMLOWP_NEON\n  if (fc_batches == 1 && !(fc_output_depth % 4) && !(fc_accum_depth % 8)) {\n    GEMVForLstmCell(concat_temp_shape, concat_temp_data_uint8, weights_shape,\n                    weights_data_uint8, weights_zero_point, bias_shape,\n                    bias_data_int32, accum_multiplier, accum_shift,\n                    activ_temp_shape, activ_temp_data_int16);\n    gemm_already_performed = true;\n  }\n#endif\n  if (!gemm_already_performed) {\n    gemmlowp::MatrixMap<const uint8, gemmlowp::MapOrder::RowMajor>\n        weights_matrix(weights_data_uint8, fc_output_depth, fc_accum_depth);\n    gemmlowp::MatrixMap<const uint8, gemmlowp::MapOrder::ColMajor> input_matrix(\n        concat_temp_data_uint8, fc_accum_depth, fc_batches);\n    gemmlowp::MatrixMap<int16, gemmlowp::MapOrder::ColMajor> output_matrix(\n        activ_temp_data_int16, fc_output_depth, fc_batches);\n    typedef gemmlowp::VectorMap<const int32, gemmlowp::VectorShape::Col>\n        ColVectorMap;\n    ColVectorMap bias_vector(bias_data_int32, fc_output_depth);\n    gemmlowp::OutputStageBiasAddition<ColVectorMap> bias_addition_stage;\n    bias_addition_stage.bias_vector = bias_vector;\n    gemmlowp::OutputStageScaleInt32ByFixedPointAndExponent scale_stage;\n    scale_stage.result_offset_after_shift = 0;\n    scale_stage.result_fixedpoint_multiplier = accum_multiplier;\n    scale_stage.result_exponent = accum_shift;\n    gemmlowp::OutputStageSaturatingCastToInt16 saturating_cast_int16_stage;\n    auto output_pipeline = std::make_tuple(bias_addition_stage, scale_stage,\n                                           saturating_cast_int16_stage);\n    gemmlowp::GemmWithOutputPipeline<\n        uint8, int16, gemmlowp::L8R8WithLhsNonzeroBitDepthParams>(\n        gemmlowp_context, weights_matrix, input_matrix, &output_matrix,\n        -weights_zero_point, -128, output_pipeline);\n  }\n\n  // Rest of the LSTM cell: tanh and logistic math functions, and some adds\n  // and muls, all done in 16-bit fixed-point.\n  const int16* input_gate_input_ptr = activ_temp_data_int16;\n  const int16* input_modulation_gate_input_ptr =\n      activ_temp_data_int16 + output_depth;\n  const int16* forget_gate_input_ptr = activ_temp_data_int16 + 2 * output_depth;\n  const int16* output_gate_input_ptr = activ_temp_data_int16 + 3 * output_depth;\n  const int16* prev_state_ptr = prev_state_data_int16;\n  int16* output_state_data_ptr = output_state_data_int16;\n  uint8* output_activ_data_ptr = output_activ_data_uint8;\n\n  for (int b = 0; b < outer_size; ++b) {\n    int c = 0;\n#ifdef GEMMLOWP_NEON\n    for (; c <= output_depth - 8; c += 8) {\n      // Define the fixed-point data types that we will use here. All use\n      // int16 as the underlying integer type i.e. all are 16-bit fixed-point.\n      // They only differ by the number of integral vs. fractional bits,\n      // determining the range of values that they can represent.\n      //\n      // F0 uses 0 integer bits, range [-1, 1].\n      // This is the return type of math functions such as tanh, logistic,\n      // whose range is in [-1, 1].\n      using F0 = gemmlowp::FixedPoint<int16x8_t, 0>;\n      // F3 uses 3 integer bits, range [-8, 8].\n      // This is the range of the previous fully-connected node's output,\n      // which is our input here.\n      using F3 = gemmlowp::FixedPoint<int16x8_t, 3>;\n      // FS uses StateIntegerBits integer bits, range [-2^StateIntegerBits,\n      // 2^StateIntegerBits]. It's used to represent the internal state, whose\n      // number of integer bits is currently dictated by the model. See comment\n      // on the StateIntegerBits template parameter above.\n      using FS = gemmlowp::FixedPoint<int16x8_t, StateIntegerBits>;\n      // Implementation of input gate, using fixed-point logistic function.\n      F3 input_gate_input = F3::FromRaw(vld1q_s16(input_gate_input_ptr));\n      input_gate_input_ptr += 8;\n      F0 input_gate_output = gemmlowp::logistic(input_gate_input);\n      // Implementation of input modulation gate, using fixed-point tanh\n      // function.\n      F3 input_modulation_gate_input =\n          F3::FromRaw(vld1q_s16(input_modulation_gate_input_ptr));\n      input_modulation_gate_input_ptr += 8;\n      F0 input_modulation_gate_output =\n          gemmlowp::tanh(input_modulation_gate_input);\n      // Implementation of forget gate, using fixed-point logistic function.\n      F3 forget_gate_input = F3::FromRaw(vld1q_s16(forget_gate_input_ptr));\n      forget_gate_input_ptr += 8;\n      F0 forget_gate_output = gemmlowp::logistic(forget_gate_input);\n      // Implementation of output gate, using fixed-point logistic function.\n      F3 output_gate_input = F3::FromRaw(vld1q_s16(output_gate_input_ptr));\n      output_gate_input_ptr += 8;\n      F0 output_gate_output = gemmlowp::logistic(output_gate_input);\n      // Implementation of internal multiplication nodes, still in fixed-point.\n      F0 input_times_input_modulation =\n          input_gate_output * input_modulation_gate_output;\n      FS prev_state = FS::FromRaw(vld1q_s16(prev_state_ptr));\n      prev_state_ptr += 8;\n      FS prev_state_times_forget_state = forget_gate_output * prev_state;\n      // Implementation of internal addition node, saturating.\n      FS new_state = gemmlowp::SaturatingAdd(\n          gemmlowp::Rescale<StateIntegerBits>(input_times_input_modulation),\n          prev_state_times_forget_state);\n      // Implementation of last internal Tanh node, still in fixed-point.\n      // Since a Tanh fixed-point implementation is specialized for a given\n      // number or integer bits, and each specialization can have a substantial\n      // code size, and we already used above a Tanh on an input with 3 integer\n      // bits, and per the table in the above function comment there is no\n      // significant accuracy to be lost by clamping to [-8, +8] for a\n      // 3-integer-bits representation, let us just do that. This helps people\n      // porting this to targets where code footprint must be minimized.\n      F3 new_state_f3 = gemmlowp::Rescale<3>(new_state);\n      F0 output_activ_int16 = output_gate_output * gemmlowp::tanh(new_state_f3);\n      // Store the new internal state back to memory, as 16-bit integers.\n      // Note: here we store the original value with StateIntegerBits, not\n      // the rescaled 3-integer-bits value fed to tanh.\n      vst1q_s16(output_state_data_ptr, new_state.raw());\n      output_state_data_ptr += 8;\n      // Down-scale the output activations to 8-bit integers, saturating,\n      // and store back to memory.\n      int16x8_t rescaled_output_activ =\n          gemmlowp::RoundingDivideByPOT(output_activ_int16.raw(), 8);\n      int8x8_t int8_output_activ = vqmovn_s16(rescaled_output_activ);\n      uint8x8_t uint8_output_activ =\n          vadd_u8(vdup_n_u8(128), vreinterpret_u8_s8(int8_output_activ));\n      vst1_u8(output_activ_data_ptr, uint8_output_activ);\n      output_activ_data_ptr += 8;\n    }\n#endif\n    for (; c < output_depth; ++c) {\n      // Define the fixed-point data types that we will use here. All use\n      // int16 as the underlying integer type i.e. all are 16-bit fixed-point.\n      // They only differ by the number of integral vs. fractional bits,\n      // determining the range of values that they can represent.\n      //\n      // F0 uses 0 integer bits, range [-1, 1].\n      // This is the return type of math functions such as tanh, logistic,\n      // whose range is in [-1, 1].\n      using F0 = gemmlowp::FixedPoint<std::int16_t, 0>;\n      // F3 uses 3 integer bits, range [-8, 8].\n      // This is the range of the previous fully-connected node's output,\n      // which is our input here.\n      using F3 = gemmlowp::FixedPoint<std::int16_t, 3>;\n      // FS uses StateIntegerBits integer bits, range [-2^StateIntegerBits,\n      // 2^StateIntegerBits]. It's used to represent the internal state, whose\n      // number of integer bits is currently dictated by the model. See comment\n      // on the StateIntegerBits template parameter above.\n      using FS = gemmlowp::FixedPoint<std::int16_t, StateIntegerBits>;\n      // Implementation of input gate, using fixed-point logistic function.\n      F3 input_gate_input = F3::FromRaw(*input_gate_input_ptr++);\n      F0 input_gate_output = gemmlowp::logistic(input_gate_input);\n      // Implementation of input modulation gate, using fixed-point tanh\n      // function.\n      F3 input_modulation_gate_input =\n          F3::FromRaw(*input_modulation_gate_input_ptr++);\n      F0 input_modulation_gate_output =\n          gemmlowp::tanh(input_modulation_gate_input);\n      // Implementation of forget gate, using fixed-point logistic function.\n      F3 forget_gate_input = F3::FromRaw(*forget_gate_input_ptr++);\n      F0 forget_gate_output = gemmlowp::logistic(forget_gate_input);\n      // Implementation of output gate, using fixed-point logistic function.\n      F3 output_gate_input = F3::FromRaw(*output_gate_input_ptr++);\n      F0 output_gate_output = gemmlowp::logistic(output_gate_input);\n      // Implementation of internal multiplication nodes, still in fixed-point.\n      F0 input_times_input_modulation =\n          input_gate_output * input_modulation_gate_output;\n      FS prev_state = FS::FromRaw(*prev_state_ptr++);\n      FS prev_state_times_forget_state = forget_gate_output * prev_state;\n      // Implementation of internal addition node, saturating.\n      FS new_state = gemmlowp::SaturatingAdd(\n          gemmlowp::Rescale<StateIntegerBits>(input_times_input_modulation),\n          prev_state_times_forget_state);\n      // Implementation of last internal Tanh node, still in fixed-point.\n      // Since a Tanh fixed-point implementation is specialized for a given\n      // number or integer bits, and each specialization can have a substantial\n      // code size, and we already used above a Tanh on an input with 3 integer\n      // bits, and per the table in the above function comment there is no\n      // significant accuracy to be lost by clamping to [-8, +8] for a\n      // 3-integer-bits representation, let us just do that. This helps people\n      // porting this to targets where code footprint must be minimized.\n      F3 new_state_f3 = gemmlowp::Rescale<3>(new_state);\n      F0 output_activ_int16 = output_gate_output * gemmlowp::tanh(new_state_f3);\n      // Store the new internal state back to memory, as 16-bit integers.\n      // Note: here we store the original value with StateIntegerBits, not\n      // the rescaled 3-integer-bits value fed to tanh.\n      *output_state_data_ptr++ = new_state.raw();\n      // Down-scale the output activations to 8-bit integers, saturating,\n      // and store back to memory.\n      int16 rescaled_output_activ =\n          gemmlowp::RoundingDivideByPOT(output_activ_int16.raw(), 8);\n      int16 clamped_output_activ =\n          std::max<int16>(-128, std::min<int16>(127, rescaled_output_activ));\n      *output_activ_data_ptr++ = 128 + clamped_output_activ;\n    }\n    input_gate_input_ptr += 3 * output_depth;\n    input_modulation_gate_input_ptr += 3 * output_depth;\n    forget_gate_input_ptr += 3 * output_depth;\n    output_gate_input_ptr += 3 * output_depth;\n  }\n}\n\ntemplate <int StateIntegerBits>\nvoid LstmCell(const uint8* input_data_uint8, const Dims<4>& input_dims,\n              const uint8* prev_activ_data_uint8,\n              const Dims<4>& prev_activ_dims, const uint8* weights_data_uint8,\n              const Dims<4>& weights_dims, const int32* bias_data_int32,\n              const Dims<4>& bias_dims, const int16* prev_state_data_int16,\n              const Dims<4>& prev_state_dims, int16* output_state_data_int16,\n              const Dims<4>& output_state_dims, uint8* output_activ_data_uint8,\n              const Dims<4>& output_activ_dims, uint8* concat_temp_data_uint8,\n              const Dims<4>& concat_temp_dims, int16* activ_temp_data_int16,\n              const Dims<4>& activ_temp_dims, int32 weights_zero_point,\n              int32 accum_multiplier, int accum_shift,\n              gemmlowp::GemmContext* gemmlowp_context) {\n  tflite::LstmCellParams op_params;\n  op_params.weights_zero_point = weights_zero_point;\n  op_params.accum_multiplier = accum_multiplier;\n  op_params.accum_shift = accum_shift;\n\n  LstmCell<StateIntegerBits>(\n      op_params, DimsToShape(input_dims), input_data_uint8,\n      DimsToShape(prev_activ_dims), prev_activ_data_uint8,\n      DimsToShape(weights_dims), weights_data_uint8, DimsToShape(bias_dims),\n      bias_data_int32, DimsToShape(prev_state_dims), prev_state_data_int16,\n      DimsToShape(output_state_dims), output_state_data_int16,\n      DimsToShape(output_activ_dims), output_activ_data_uint8,\n      DimsToShape(concat_temp_dims), concat_temp_data_uint8,\n      DimsToShape(activ_temp_dims), activ_temp_data_int16, gemmlowp_context);\n}\n\ntemplate <typename T>\nvoid BroadcastDiv(const T* input1_data, const Dims<4>& input1_dims,\n                  const T* input2_data, const Dims<4>& input2_dims,\n                  T output_activation_min, T output_activation_max,\n                  T* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  SetActivationParams(output_activation_min, output_activation_max, &op_params);\n\n  BroadcastDivSlow(op_params, DimsToShape(input1_dims), input1_data,\n                   DimsToShape(input2_dims), input2_data,\n                   DimsToShape(output_dims), output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\nvoid L2Normalization(const float* input_data, const RuntimeShape& input_shape,\n                     float* output_data, const RuntimeShape& output_shape) {\n  static_assert(Ac == FusedActivationFunctionType::kNone, \"\");\n  tflite::L2NormalizationParams op_params;\n  // No params need to be set for float, but reserved in signature for future\n  // activations.\n\n  L2Normalization(op_params, input_shape, input_data, output_shape,\n                  output_data);\n}\n\ninline void L2Normalization(const uint8* input_data,\n                            const RuntimeShape& input_shape,\n                            int32 input_zero_point, uint8* output_data,\n                            const RuntimeShape& output_shape) {\n  tflite::L2NormalizationParams op_params;\n  op_params.input_zero_point = input_zero_point;\n\n  L2Normalization(op_params, input_shape, input_data, output_shape,\n                  output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\nvoid L2Normalization(const float* input_data, const Dims<4>& input_dims,\n                     float* output_data, const Dims<4>& output_dims) {\n  L2Normalization<Ac>(input_data, DimsToShape(input_dims), output_data,\n                      DimsToShape(output_dims));\n}\n\ninline void L2Normalization(const uint8* input_data, const Dims<4>& input_dims,\n                            int32 input_zero_point, uint8* output_data,\n                            const Dims<4>& output_dims) {\n  L2Normalization(input_data, DimsToShape(input_dims), input_zero_point,\n                  output_data, DimsToShape(output_dims));\n}\n\ninline void Relu(const float* input_data, const Dims<4>& input_dims,\n                 float* output_data, const Dims<4>& output_dims) {\n  Relu(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n       output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid Add(const float* input1_data, const Dims<4>& input1_dims,\n         const float* input2_data, const Dims<4>& input2_dims,\n         float* output_data, const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n\n  tflite::ArithmeticParams op_params;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n  Add(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\ninline void Add(int left_shift, const uint8* input1_data,\n                const Dims<4>& input1_dims, int32 input1_offset,\n                int32 input1_multiplier, int input1_shift,\n                const uint8* input2_data, const Dims<4>& input2_dims,\n                int32 input2_offset, int32 input2_multiplier, int input2_shift,\n                int32 output_offset, int32 output_multiplier, int output_shift,\n                int32 output_activation_min, int32 output_activation_max,\n                uint8* output_data, const Dims<4>& output_dims) {\n  constexpr int kReverseShift = -1;\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n\n  tflite::ArithmeticParams op_params;\n  op_params.left_shift = left_shift;\n  op_params.input1_offset = input1_offset;\n  op_params.input1_multiplier = input1_multiplier;\n  op_params.input1_shift = kReverseShift * input1_shift;\n  op_params.input2_offset = input2_offset;\n  op_params.input2_multiplier = input2_multiplier;\n  op_params.input2_shift = kReverseShift * input2_shift;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  Add(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\nvoid Add(const int32* input1_data, const Dims<4>& input1_dims,\n         const int32* input2_data, const Dims<4>& input2_dims,\n         int32* output_data, const Dims<4>& output_dims) {\n  ruy::profiler::ScopeLabel label(\"Add/int32\");\n  TFLITE_DCHECK(Ac == FusedActivationFunctionType::kNone);\n\n  tflite::ArithmeticParams op_params;\n  op_params.quantized_activation_min = std::numeric_limits<int32>::min();\n  op_params.quantized_activation_max = std::numeric_limits<int32>::max();\n  Add(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <typename T>\nvoid BroadcastAdd(const T* input1_data, const Dims<4>& input1_dims,\n                  const T* input2_data, const Dims<4>& input2_dims,\n                  T output_activation_min, T output_activation_max,\n                  T* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n  BroadcastAdd4DSlow(op_params, DimsToShape(input1_dims), input1_data,\n                     DimsToShape(input2_dims), input2_data,\n                     DimsToShape(output_dims), output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\ninline void BroadcastAdd(int left_shift, const uint8* input1_data,\n                         const Dims<4>& input1_dims, int32 input1_offset,\n                         int32 input1_multiplier, int input1_shift,\n                         const uint8* input2_data, const Dims<4>& input2_dims,\n                         int32 input2_offset, int32 input2_multiplier,\n                         int input2_shift, int32 output_offset,\n                         int32 output_multiplier, int output_shift,\n                         int32 output_activation_min,\n                         int32 output_activation_max, uint8* output_data,\n                         const Dims<4>& output_dims) {\n  constexpr int kReverseShift = -1;\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n\n  tflite::ArithmeticParams op_params;\n  op_params.left_shift = left_shift;\n  op_params.input1_offset = input1_offset;\n  op_params.input1_multiplier = input1_multiplier;\n  op_params.input1_shift = kReverseShift * input1_shift;\n  op_params.input2_offset = input2_offset;\n  op_params.input2_multiplier = input2_multiplier;\n  op_params.input2_shift = kReverseShift * input2_shift;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  BroadcastAdd4DSlow(op_params, DimsToShape(input1_dims), input1_data,\n                     DimsToShape(input2_dims), input2_data,\n                     DimsToShape(output_dims), output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\ninline void BroadcastAddFivefold(\n    int y0, int y1, int y2, int y3, int y4, int left_shift,\n    const uint8* input1_data, const Dims<4>& input1_dims, int32 input1_offset,\n    int32 input1_multiplier, int input1_shift, const uint8* input2_data,\n    const Dims<4>& input2_dims, int32 input2_offset, int32 input2_multiplier,\n    int input2_shift, int32 output_offset, int32 output_multiplier,\n    int output_shift, int32 output_activation_min, int32 output_activation_max,\n    uint8* output_data, const Dims<4>& output_dims) {\n  constexpr int kReverseShift = -1;\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n  tflite::ArithmeticParams op_params;\n  op_params.broadcast_category =\n      tflite::BroadcastableOpCategory::kFirstInputBroadcastsFast;\n  op_params.left_shift = left_shift;\n  op_params.input1_offset = input1_offset;\n  op_params.input1_multiplier = input1_multiplier;\n  op_params.input1_shift = kReverseShift * input1_shift;\n  op_params.input2_offset = input2_offset;\n  op_params.input2_multiplier = input2_multiplier;\n  op_params.input2_shift = kReverseShift * input2_shift;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  op_params.output_shift = kReverseShift * output_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  op_params.broadcast_shape[4] = y0;\n  op_params.broadcast_shape[3] = y1;\n  op_params.broadcast_shape[2] = y2;\n  op_params.broadcast_shape[1] = y3;\n  op_params.broadcast_shape[0] = y4;\n  BroadcastAddFivefold(op_params, DimsToShape(input1_dims), input1_data,\n                       DimsToShape(input2_dims), input2_data,\n                       DimsToShape(output_dims), output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac, typename T>\nvoid BroadcastAdd(const T* input1_data, const Dims<4>& input1_dims,\n                  const T* input2_data, const Dims<4>& input2_dims,\n                  T* output_data, const Dims<4>& output_dims) {\n  T output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n\n  BroadcastAdd(input1_data, input1_dims, input2_data, input2_dims,\n               output_activation_min, output_activation_max, output_data,\n               output_dims);\n}\n\ntemplate <FusedActivationFunctionType Ac>\ninline void Add(const int16* input1_data, const Dims<4>& input1_dims,\n                int input1_shift, const int16* input2_data,\n                const Dims<4>& input2_dims, int input2_shift,\n                int16 output_activation_min, int16 output_activation_max,\n                int16* output_data, const Dims<4>& output_dims) {\n  constexpr int kReverseShift = -1;\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, -32768);\n    TFLITE_DCHECK_EQ(output_activation_max, 32767);\n  }\n\n  tflite::ArithmeticParams op_params;\n  op_params.input1_shift = kReverseShift * input1_shift;\n  op_params.input2_shift = kReverseShift * input2_shift;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  Add(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ninline void Sub(const float* input1_data, const Dims<4>& input1_dims,\n                const float* input2_data, const Dims<4>& input2_dims,\n                float* output_data, const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(FusedActivationFunctionType::kNone,\n                      &output_activation_min, &output_activation_max);\n  tflite::ArithmeticParams op_params;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n  Sub(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <typename T>\nvoid Sub(const T* input1_data, const Dims<4>& input1_dims, const T* input2_data,\n         const Dims<4>& input2_dims, T* output_data,\n         const Dims<4>& output_dims) {\n  T output_activation_min, output_activation_max;\n  GetActivationMinMax(FusedActivationFunctionType::kNone,\n                      &output_activation_min, &output_activation_max);\n  tflite::ArithmeticParams op_params;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n  Sub(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ninline void BroadcastMul(const uint8* input1_data, const Dims<4>& input1_dims,\n                         int32 input1_offset, const uint8* input2_data,\n                         const Dims<4>& input2_dims, int32 input2_offset,\n                         int32 output_offset, int32 output_multiplier,\n                         int output_shift, int32 output_activation_min,\n                         int32 output_activation_max, uint8* output_data,\n                         const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  SetActivationParams(output_activation_min, output_activation_max, &op_params);\n  op_params.input1_offset = input1_offset;\n  op_params.input2_offset = input2_offset;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = output_multiplier;\n  op_params.output_shift = kReverseShift * output_shift;\n\n  BroadcastMul4DSlow(op_params, DimsToShape(input1_dims), input1_data,\n                     DimsToShape(input2_dims), input2_data,\n                     DimsToShape(output_dims), output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\ninline void BroadcastMul(const uint8* input1_data, const Dims<4>& input1_dims,\n                         int32 input1_offset, const uint8* input2_data,\n                         const Dims<4>& input2_dims, int32 input2_offset,\n                         int32 output_offset, int32 output_multiplier,\n                         int output_shift, int32 output_activation_min,\n                         int32 output_activation_max, uint8* output_data,\n                         const Dims<4>& output_dims) {\n  BroadcastMul(input1_data, input1_dims, input1_offset, input2_data,\n               input2_dims, input2_offset, output_offset, output_multiplier,\n               output_shift, output_activation_min, output_activation_max,\n               output_data, output_dims);\n}\n\ninline void AveragePool(const float* input_data, const Dims<4>& input_dims,\n                        int stride_width, int stride_height, int pad_width,\n                        int pad_height, int kwidth, int kheight,\n                        float output_activation_min,\n                        float output_activation_max, float* output_data,\n                        const Dims<4>& output_dims) {\n  tflite::PoolParams params;\n  params.stride_height = stride_height;\n  params.stride_width = stride_width;\n  params.filter_height = kheight;\n  params.filter_width = kwidth;\n  params.padding_values.height = pad_height;\n  params.padding_values.width = pad_width;\n  params.float_activation_min = output_activation_min;\n  params.float_activation_max = output_activation_max;\n  AveragePool(params, DimsToShape(input_dims), input_data,\n              DimsToShape(output_dims), output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid AveragePool(const float* input_data, const Dims<4>& input_dims,\n                 int stride_width, int stride_height, int pad_width,\n                 int pad_height, int kwidth, int kheight, float* output_data,\n                 const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n\n  AveragePool(input_data, input_dims, stride_width, stride_height, pad_width,\n              pad_height, kwidth, kheight, output_activation_min,\n              output_activation_max, output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid AveragePool(const float* input_data, const Dims<4>& input_dims, int stride,\n                 int pad_width, int pad_height, int filter_width,\n                 int filter_height, float* output_data,\n                 const Dims<4>& output_dims) {\n  AveragePool<Ac>(input_data, input_dims, stride, stride, pad_width, pad_height,\n                  filter_width, filter_height, output_data, output_dims);\n}\n\ninline void AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n                        int stride_width, int stride_height, int pad_width,\n                        int pad_height, int filter_width, int filter_height,\n                        int32 output_activation_min,\n                        int32 output_activation_max, uint8* output_data,\n                        const Dims<4>& output_dims) {\n  tflite::PoolParams params;\n  params.stride_height = stride_height;\n  params.stride_width = stride_width;\n  params.filter_height = filter_height;\n  params.filter_width = filter_width;\n  params.padding_values.height = pad_height;\n  params.padding_values.width = pad_width;\n  params.quantized_activation_min = output_activation_min;\n  params.quantized_activation_max = output_activation_max;\n  AveragePool(params, DimsToShape(input_dims), input_data,\n              DimsToShape(output_dims), output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n                 int stride_width, int stride_height, int pad_width,\n                 int pad_height, int filter_width, int filter_height,\n                 int32 output_activation_min, int32 output_activation_max,\n                 uint8* output_data, const Dims<4>& output_dims) {\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n  AveragePool(input_data, input_dims, stride_width, stride_height, pad_width,\n              pad_height, filter_width, filter_height, output_activation_min,\n              output_activation_max, output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid AveragePool(const uint8* input_data, const Dims<4>& input_dims, int stride,\n                 int pad_width, int pad_height, int filter_width,\n                 int filter_height, int32 output_activation_min,\n                 int32 output_activation_max, uint8* output_data,\n                 const Dims<4>& output_dims) {\n  AveragePool<Ac>(input_data, input_dims, stride, stride, pad_width, pad_height,\n                  filter_width, filter_height, output_activation_min,\n                  output_activation_max, output_data, output_dims);\n}\n\ninline void MaxPool(const float* input_data, const Dims<4>& input_dims,\n                    int stride_width, int stride_height, int pad_width,\n                    int pad_height, int kwidth, int kheight,\n                    float output_activation_min, float output_activation_max,\n                    float* output_data, const Dims<4>& output_dims) {\n  tflite::PoolParams params;\n  params.stride_height = stride_height;\n  params.stride_width = stride_width;\n  params.filter_height = kheight;\n  params.filter_width = kwidth;\n  params.padding_values.height = pad_height;\n  params.padding_values.width = pad_width;\n  params.float_activation_min = output_activation_min;\n  params.float_activation_max = output_activation_max;\n  MaxPool(params, DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n          output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid MaxPool(const float* input_data, const Dims<4>& input_dims,\n             int stride_width, int stride_height, int pad_width, int pad_height,\n             int kwidth, int kheight, float* output_data,\n             const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  MaxPool(input_data, input_dims, stride_width, stride_height, pad_width,\n          pad_height, kwidth, kheight, output_activation_min,\n          output_activation_max, output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid MaxPool(const float* input_data, const Dims<4>& input_dims, int stride,\n             int pad_width, int pad_height, int filter_width, int filter_height,\n             float* output_data, const Dims<4>& output_dims) {\n  MaxPool<Ac>(input_data, input_dims, stride, stride, pad_width, pad_height,\n              filter_width, filter_height, output_data, output_dims);\n}\n\ninline void MaxPool(const uint8* input_data, const Dims<4>& input_dims,\n                    int stride_width, int stride_height, int pad_width,\n                    int pad_height, int filter_width, int filter_height,\n                    int32 output_activation_min, int32 output_activation_max,\n                    uint8* output_data, const Dims<4>& output_dims) {\n  PoolParams params;\n  params.stride_height = stride_height;\n  params.stride_width = stride_width;\n  params.filter_height = filter_height;\n  params.filter_width = filter_width;\n  params.padding_values.height = pad_height;\n  params.padding_values.width = pad_width;\n  params.quantized_activation_min = output_activation_min;\n  params.quantized_activation_max = output_activation_max;\n  MaxPool(params, DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n          output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid MaxPool(const uint8* input_data, const Dims<4>& input_dims,\n             int stride_width, int stride_height, int pad_width, int pad_height,\n             int filter_width, int filter_height, int32 output_activation_min,\n             int32 output_activation_max, uint8* output_data,\n             const Dims<4>& output_dims) {\n  static_assert(Ac == FusedActivationFunctionType::kNone ||\n                    Ac == FusedActivationFunctionType::kRelu ||\n                    Ac == FusedActivationFunctionType::kRelu6 ||\n                    Ac == FusedActivationFunctionType::kRelu1,\n                \"\");\n  if (Ac == FusedActivationFunctionType::kNone) {\n    TFLITE_DCHECK_EQ(output_activation_min, 0);\n    TFLITE_DCHECK_EQ(output_activation_max, 255);\n  }\n  MaxPool(input_data, input_dims, stride_width, stride_height, pad_width,\n          pad_height, filter_width, filter_height, output_activation_min,\n          output_activation_max, output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid MaxPool(const uint8* input_data, const Dims<4>& input_dims, int stride,\n             int pad_width, int pad_height, int filter_width, int filter_height,\n             int32 output_activation_min, int32 output_activation_max,\n             uint8* output_data, const Dims<4>& output_dims) {\n  MaxPool<Ac>(input_data, input_dims, stride, stride, pad_width, pad_height,\n              filter_width, filter_height, output_activation_min,\n              output_activation_max, output_data, output_dims);\n}\n\ninline void L2Pool(const float* input_data, const Dims<4>& input_dims,\n                   int stride_width, int stride_height, int pad_width,\n                   int pad_height, int filter_width, int filter_height,\n                   float output_activation_min, float output_activation_max,\n                   float* output_data, const Dims<4>& output_dims) {\n  PoolParams params;\n  params.stride_height = stride_height;\n  params.stride_width = stride_width;\n  params.filter_height = filter_height;\n  params.filter_width = filter_width;\n  params.padding_values.height = pad_height;\n  params.padding_values.width = pad_width;\n  params.float_activation_min = output_activation_min;\n  params.float_activation_max = output_activation_max;\n  L2Pool(params, DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n         output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid L2Pool(const float* input_data, const Dims<4>& input_dims,\n            int stride_width, int stride_height, int pad_width, int pad_height,\n            int filter_width, int filter_height, float* output_data,\n            const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  L2Pool(input_data, input_dims, stride_width, stride_height, pad_width,\n         pad_height, filter_width, filter_height, output_activation_min,\n         output_activation_max, output_data, output_dims);\n}\n\n// legacy, for compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\nvoid L2Pool(const float* input_data, const Dims<4>& input_dims, int stride,\n            int pad_width, int pad_height, int filter_width, int filter_height,\n            float* output_data, const Dims<4>& output_dims) {\n  L2Pool<Ac>(input_data, input_dims, stride, stride, pad_width, pad_height,\n             filter_width, filter_height, output_data, output_dims);\n}\n\ninline void Softmax(const SoftmaxParams& params,\n                    const RuntimeShape& input_shape, const uint8* input_data,\n                    const RuntimeShape& output_shape, uint8* output_data) {\n  const int32 input_beta_multiplier = params.input_multiplier;\n  const int32 input_beta_left_shift = params.input_left_shift;\n  const int diff_min = params.diff_min;\n  // The representation chosen for the input to the exp() function is Q5.26.\n  // We need to leave extra space since values that we skip might be as large as\n  // -32 before multiplying by input_beta_multiplier, and therefore as large as\n  // -16 afterwards.  Note that exp(-8) is definitely not insignificant to\n  // accumulation, but exp(-16) definitely is.\n  static const int kScaledDiffIntegerBits = 5;\n  static const int kAccumulationIntegerBits = 12;\n  using FixedPointScaledDiff =\n      gemmlowp::FixedPoint<int32, kScaledDiffIntegerBits>;\n  using FixedPointAccum = gemmlowp::FixedPoint<int32, kAccumulationIntegerBits>;\n  using FixedPoint0 = gemmlowp::FixedPoint<int32, 0>;\n\n  ruy::profiler::ScopeLabel label(\"Softmax/8bit\");\n  const int trailing_dim = input_shape.DimensionsCount() - 1;\n  const int outer_size =\n      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);\n  const int depth =\n      MatchingDim(input_shape, trailing_dim, output_shape, trailing_dim);\n\n  for (int b = 0; b < outer_size; ++b) {\n    const uint8* input_data_ptr = input_data + b * depth;\n    uint8* output_data_ptr = output_data + b * depth;\n\n    // Determine the largest entry in the current row\n    uint8 max_in_row = 0;\n    {\n      int c = 0;\n#ifdef USE_NEON\n      uint8x16_t max16_0 = vdupq_n_u8(0);\n      uint8x16_t max16_1 = vdupq_n_u8(0);\n      for (; c <= depth - 32; c += 32) {\n        max16_0 = vmaxq_u8(max16_0, vld1q_u8(input_data_ptr + c + 0));\n        max16_1 = vmaxq_u8(max16_1, vld1q_u8(input_data_ptr + c + 16));\n      }\n      uint8x16_t max16 = vmaxq_u8(max16_0, max16_1);\n      if (c <= depth - 16) {\n        max16 = vmaxq_u8(max16, vld1q_u8(input_data_ptr + c));\n        c += 16;\n      }\n      uint8x8_t max8 = vmax_u8(vget_low_u8(max16), vget_high_u8(max16));\n      if (c <= depth - 8) {\n        max8 = vmax_u8(max8, vld1_u8(input_data_ptr + c));\n        c += 8;\n      }\n      uint8x8_t max4 = vmax_u8(max8, vext_u8(max8, max8, 4));\n      uint8x8_t max2 = vmax_u8(max4, vext_u8(max4, max4, 2));\n      uint8x8_t max1 = vpmax_u8(max2, max2);\n      max_in_row = vget_lane_u8(max1, 0);\n#endif\n      for (; c < depth; ++c) {\n        max_in_row = std::max(max_in_row, input_data_ptr[c]);\n      }\n    }\n\n#ifdef USE_NEON\n    using FixedPointAccumInt32x4 =\n        gemmlowp::FixedPoint<int32x4_t, kAccumulationIntegerBits>;\n    using FixedPointScaledDiffInt32x4 =\n        gemmlowp::FixedPoint<int32x4_t, kScaledDiffIntegerBits>;\n    using FixedPoint0Int32x4 = gemmlowp::FixedPoint<int32x4_t, 0>;\n    FixedPoint0Int32x4 input_beta_multiplier_f0 =\n        FixedPoint0Int32x4::FromScalarRaw(input_beta_multiplier);\n    int16x8_t max_in_row_s16 = vdupq_n_s16(max_in_row);\n#endif\n\n    // Compute the sum of exponentials of the differences of entries in the\n    // current row from the largest entry in the current row.\n    FixedPointAccum sum_of_exps = FixedPointAccum::Zero();\n    {\n      int c = 0;\n#ifdef USE_NEON\n      int32x4_t diff_min_s32 = vdupq_n_s32(diff_min);\n      FixedPointAccumInt32x4 sum_of_exps_0 = FixedPointAccumInt32x4::Zero();\n      FixedPointAccumInt32x4 sum_of_exps_1 = FixedPointAccumInt32x4::Zero();\n      FixedPointAccumInt32x4 zeros = FixedPointAccumInt32x4::Zero();\n      for (; c <= depth - 8; c += 8) {\n        uint16x8_t input_u16 = vmovl_u8(vld1_u8(input_data_ptr + c));\n        int16x8_t input_diff_s16 =\n            vsubq_s16(vreinterpretq_s16_u16(input_u16), max_in_row_s16);\n        int32x4_t input_diff_s32_0 = vmovl_s16(vget_low_s16(input_diff_s16));\n        int32x4_t input_diff_s32_1 = vmovl_s16(vget_high_s16(input_diff_s16));\n        int32x4_t mask_0 =\n            gemmlowp::MaskIfGreaterThanOrEqual(input_diff_s32_0, diff_min_s32);\n        int32x4_t mask_1 =\n            gemmlowp::MaskIfGreaterThanOrEqual(input_diff_s32_1, diff_min_s32);\n        FixedPointScaledDiffInt32x4 scaled_diff_0 =\n            input_beta_multiplier_f0 *\n            FixedPointScaledDiffInt32x4::FromRaw(\n                gemmlowp::ShiftLeft(input_diff_s32_0, input_beta_left_shift));\n        FixedPointScaledDiffInt32x4 scaled_diff_1 =\n            input_beta_multiplier_f0 *\n            FixedPointScaledDiffInt32x4::FromRaw(\n                gemmlowp::ShiftLeft(input_diff_s32_1, input_beta_left_shift));\n        FixedPointAccumInt32x4 exps_0 =\n            gemmlowp::Rescale<kAccumulationIntegerBits>(\n                exp_on_negative_values(scaled_diff_0));\n        FixedPointAccumInt32x4 exps_1 =\n            gemmlowp::Rescale<kAccumulationIntegerBits>(\n                exp_on_negative_values(scaled_diff_1));\n        FixedPointAccumInt32x4 masked_exps_0 =\n            SelectUsingMask(mask_0, exps_0, zeros);\n        FixedPointAccumInt32x4 masked_exps_1 =\n            SelectUsingMask(mask_1, exps_1, zeros);\n        sum_of_exps_0 = sum_of_exps_0 + masked_exps_0;\n        sum_of_exps_1 = sum_of_exps_1 + masked_exps_1;\n      }\n      int32x4_t sum_of_exps_reduced_4 = (sum_of_exps_0 + sum_of_exps_1).raw();\n      int32x2_t sum_of_exps_reduced_2 =\n          vadd_s32(vget_low_s32(sum_of_exps_reduced_4),\n                   vget_high_s32(sum_of_exps_reduced_4));\n      int32x2_t sum_of_exps_reduced_1 =\n          vpadd_s32(sum_of_exps_reduced_2, sum_of_exps_reduced_2);\n      sum_of_exps =\n          FixedPointAccum::FromRaw(vget_lane_s32(sum_of_exps_reduced_1, 0));\n#endif\n      for (; c < depth; ++c) {\n        int32 input_diff = static_cast<int32>(input_data_ptr[c]) - max_in_row;\n        if (input_diff >= diff_min) {\n          const int32 input_diff_rescaled =\n              MultiplyByQuantizedMultiplierGreaterThanOne(\n                  input_diff, input_beta_multiplier, input_beta_left_shift);\n          const FixedPointScaledDiff scaled_diff_f8 =\n              FixedPointScaledDiff::FromRaw(input_diff_rescaled);\n          sum_of_exps =\n              sum_of_exps + gemmlowp::Rescale<kAccumulationIntegerBits>(\n                                exp_on_negative_values(scaled_diff_f8));\n        }\n      }\n    }\n\n    // Compute the fixed-point multiplier and shift that we need to apply to\n    // perform a division by the above-computed sum-of-exponentials.\n    int num_bits_over_unit = 0;\n    FixedPoint0 shifted_scale = FixedPoint0::FromRaw(GetReciprocal(\n        sum_of_exps.raw(), kAccumulationIntegerBits, &num_bits_over_unit));\n\n    // Compute the quotients of exponentials of differences of entries in the\n    // current row from the largest entry, over the previously-computed sum of\n    // exponentials.\n    {\n      int c = 0;\n#ifdef USE_NEON\n      int16x8_t diff_min_s16 = vdupq_n_s16(diff_min);\n      for (; c <= depth - 8; c += 8) {\n        uint16x8_t input_u16 = vmovl_u8(vld1_u8(input_data_ptr + c));\n        int16x8_t input_diff_s16 =\n            vsubq_s16(vreinterpretq_s16_u16(input_u16), max_in_row_s16);\n        int32x4_t input_diff_s32_0 = vmovl_s16(vget_low_s16(input_diff_s16));\n        int32x4_t input_diff_s32_1 = vmovl_s16(vget_high_s16(input_diff_s16));\n        uint8x8_t mask = vmovn_u16(vcgeq_s16(input_diff_s16, diff_min_s16));\n        FixedPointScaledDiffInt32x4 scaled_diff_0 =\n            input_beta_multiplier_f0 *\n            FixedPointScaledDiffInt32x4::FromRaw(\n                gemmlowp::ShiftLeft(input_diff_s32_0, input_beta_left_shift));\n        FixedPointScaledDiffInt32x4 scaled_diff_1 =\n            input_beta_multiplier_f0 *\n            FixedPointScaledDiffInt32x4::FromRaw(\n                gemmlowp::ShiftLeft(input_diff_s32_1, input_beta_left_shift));\n        FixedPoint0Int32x4 exp_0 = exp_on_negative_values(scaled_diff_0);\n        FixedPoint0Int32x4 exp_1 = exp_on_negative_values(scaled_diff_1);\n        int32x4_t output_s32_0 = gemmlowp::RoundingDivideByPOT(\n            vqrdmulhq_n_s32(exp_0.raw(), shifted_scale.raw()),\n            num_bits_over_unit + 31 - 8);\n        int32x4_t output_s32_1 = gemmlowp::RoundingDivideByPOT(\n            vqrdmulhq_n_s32(exp_1.raw(), shifted_scale.raw()),\n            num_bits_over_unit + 31 - 8);\n        int16x8_t output_s16 =\n            vcombine_s16(vqmovn_s32(output_s32_0), vqmovn_s32(output_s32_1));\n        uint8x8_t output_u8 = vqmovun_s16(output_s16);\n        uint8x8_t masked_output = vbsl_u8(mask, output_u8, vdup_n_u8(0));\n        vst1_u8(output_data_ptr + c, masked_output);\n      }\n#endif\n      for (; c < depth; ++c) {\n        int32 input_diff = static_cast<int32>(input_data_ptr[c]) - max_in_row;\n        if (input_diff >= diff_min) {\n          const int32 input_diff_rescaled =\n              MultiplyByQuantizedMultiplierGreaterThanOne(\n                  input_diff, input_beta_multiplier, input_beta_left_shift);\n          const FixedPointScaledDiff scaled_diff_f8 =\n              FixedPointScaledDiff::FromRaw(input_diff_rescaled);\n\n          FixedPoint0 exp_in_0 = exp_on_negative_values(scaled_diff_f8);\n          int32 unsat_output = gemmlowp::RoundingDivideByPOT(\n              (shifted_scale * exp_in_0).raw(), num_bits_over_unit + 31 - 8);\n\n          output_data_ptr[c] = std::max(std::min(unsat_output, 255), 0);\n\n        } else {\n          output_data_ptr[c] = 0;\n        }\n      }\n    }\n  }\n}\n\ninline void Softmax(const float* input_data, const RuntimeShape& input_shape,\n                    float beta, float* output_data,\n                    const RuntimeShape& output_shape) {\n  SoftmaxParams params;\n  params.beta = beta;\n  Softmax(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Softmax(const float* input_data, const Dims<4>& input_dims,\n                    float beta, float* output_data,\n                    const Dims<4>& output_dims) {\n  Softmax(input_data, DimsToShape(input_dims), beta, output_data,\n          DimsToShape(output_dims));\n}\n\ninline void Softmax(const uint8* input_data, const RuntimeShape& input_shape,\n                    int32 input_beta_multiplier, int32 input_beta_left_shift,\n                    int diff_min, uint8* output_data,\n                    const RuntimeShape& output_shape) {\n  SoftmaxParams params;\n  params.input_multiplier = input_beta_multiplier;\n  params.input_left_shift = input_beta_left_shift;\n  params.diff_min = diff_min;\n  Softmax(params, input_shape, input_data, output_shape, output_data);\n}\ninline void Softmax(const uint8* input_data, const Dims<4>& input_dims,\n                    int32 input_beta_multiplier, int32 input_beta_left_shift,\n                    int diff_min, uint8* output_data,\n                    const Dims<4>& output_dims) {\n  Softmax(input_data, DimsToShape(input_dims), input_beta_multiplier,\n          input_beta_left_shift, diff_min, output_data,\n          DimsToShape(output_dims));\n}\n\ninline void LogSoftmax(const float* input_data, const RuntimeShape& input_shape,\n                       float* output_data, const RuntimeShape& output_shape) {\n  SoftmaxParams params;\n  // No params currently used for float LogSoftmax.\n  LogSoftmax(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void LogSoftmax(const float* input_data, const Dims<4>& input_dims,\n                       float* output_data, const Dims<4>& output_dims) {\n  LogSoftmax(input_data, DimsToShape(input_dims), output_data,\n             DimsToShape(output_dims));\n}\n\ninline void LogSoftmax(const uint8* input_data, const RuntimeShape& input_shape,\n                       int32 input_multiplier, int32 input_left_shift,\n                       int32 reverse_scaling_divisor,\n                       int32 reverse_scaling_right_shift, int diff_min,\n                       uint8* output_data, const RuntimeShape& output_shape) {\n  SoftmaxParams params;\n  params.input_multiplier = input_multiplier;\n  params.input_left_shift = input_left_shift;\n  params.reverse_scaling_divisor = reverse_scaling_divisor;\n  params.reverse_scaling_right_shift = reverse_scaling_right_shift;\n  params.diff_min = diff_min;\n  reference_ops::LogSoftmax(params, input_shape, input_data, output_shape,\n                            output_data);\n}\n\ninline void LogSoftmax(const uint8* input_data, const Dims<4>& input_dims,\n                       int32 input_multiplier, int32 input_left_shift,\n                       int32 reverse_scaling_divisor,\n                       int32 reverse_scaling_right_shift, int diff_min,\n                       uint8* output_data, const Dims<4>& output_dims) {\n  reference_ops::LogSoftmax(\n      input_data, DimsToShape(input_dims), input_multiplier, input_left_shift,\n      reverse_scaling_divisor, reverse_scaling_right_shift, diff_min,\n      output_data, DimsToShape(output_dims));\n}\n\ninline void Logistic(const LogisticParams& params,\n                     const RuntimeShape& input_shape, const uint8* input_data,\n                     const RuntimeShape& output_shape, uint8* output_data) {\n  ruy::profiler::ScopeLabel label(\"Logistic/Uint8\");\n  const int32 input_zero_point = params.input_zero_point;\n  const int32 input_range_radius = params.input_range_radius;\n  const int32 input_multiplier = params.input_multiplier;\n  const int input_left_shift = params.input_left_shift;\n  const int size = MatchingFlatSize(input_shape, output_shape);\n\n  int c = 0;\n#ifdef USE_NEON\n  // Handle 16 values at a time\n  for (; c <= size - 16; c += 16) {\n    // Read input uint8 values, cast to int16 and subtract input_zero_point\n    uint8x16_t input_val_u8 = vld1q_u8(input_data + c);\n    int16x8_t input_val_centered_0 =\n        vsubq_s16(vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(input_val_u8))),\n                  vdupq_n_s16(input_zero_point));\n    int16x8_t input_val_centered_1 =\n        vsubq_s16(vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(input_val_u8))),\n                  vdupq_n_s16(input_zero_point));\n\n    // Prepare the bit masks that we will use at the end to implement the logic\n    // that was expressed in the scalar code with branching:\n    //   if (input_val_centered < -input_range_radius) {\n    //     output_val = 0;\n    //   } else if (input_val_centered > input_range_radius) {\n    //     output_val = 255;\n    //   } else {\n    //     ...\n    uint16x8_t mask_rightclamp_0 =\n        vcgtq_s16(input_val_centered_0, vdupq_n_s16(input_range_radius));\n    uint16x8_t mask_rightclamp_1 =\n        vcgtq_s16(input_val_centered_1, vdupq_n_s16(input_range_radius));\n    uint16x8_t mask_leftclamp_0 =\n        vcgeq_s16(input_val_centered_0, vdupq_n_s16(-input_range_radius));\n    uint16x8_t mask_leftclamp_1 =\n        vcgeq_s16(input_val_centered_1, vdupq_n_s16(-input_range_radius));\n    uint8x16_t mask_rightclamp = vcombine_u8(vshrn_n_u16(mask_rightclamp_0, 8),\n                                             vshrn_n_u16(mask_rightclamp_1, 8));\n    uint8x16_t mask_leftclamp = vcombine_u8(vshrn_n_u16(mask_leftclamp_0, 8),\n                                            vshrn_n_u16(mask_leftclamp_1, 8));\n\n    // This performs what is expressed in the scalar code as\n    // const int32 input_val_rescaled =\n    //     MultiplyByQuantizedMultiplierGreaterThanOne(\n    //         input_val_centered, input_multiplier, input_left_shift);\n    int32x4_t input_val_rescaled_0 =\n        vshlq_s32(vmovl_s16(vget_low_s16(input_val_centered_0)),\n                  vdupq_n_s32(input_left_shift));\n    int32x4_t input_val_rescaled_1 =\n        vshlq_s32(vmovl_s16(vget_high_s16(input_val_centered_0)),\n                  vdupq_n_s32(input_left_shift));\n    int32x4_t input_val_rescaled_2 =\n        vshlq_s32(vmovl_s16(vget_low_s16(input_val_centered_1)),\n                  vdupq_n_s32(input_left_shift));\n    int32x4_t input_val_rescaled_3 =\n        vshlq_s32(vmovl_s16(vget_high_s16(input_val_centered_1)),\n                  vdupq_n_s32(input_left_shift));\n    input_val_rescaled_0 =\n        vqrdmulhq_n_s32(input_val_rescaled_0, input_multiplier);\n    input_val_rescaled_1 =\n        vqrdmulhq_n_s32(input_val_rescaled_1, input_multiplier);\n    input_val_rescaled_2 =\n        vqrdmulhq_n_s32(input_val_rescaled_2, input_multiplier);\n    input_val_rescaled_3 =\n        vqrdmulhq_n_s32(input_val_rescaled_3, input_multiplier);\n\n    // Invoke gemmlowp::logistic on FixedPoint wrapping int32x4_t\n    using FixedPoint4 = gemmlowp::FixedPoint<int32x4_t, 4>;\n    using FixedPoint0 = gemmlowp::FixedPoint<int32x4_t, 0>;\n    const FixedPoint4 input_val_f4_0 =\n        FixedPoint4::FromRaw(input_val_rescaled_0);\n    const FixedPoint4 input_val_f4_1 =\n        FixedPoint4::FromRaw(input_val_rescaled_1);\n    const FixedPoint4 input_val_f4_2 =\n        FixedPoint4::FromRaw(input_val_rescaled_2);\n    const FixedPoint4 input_val_f4_3 =\n        FixedPoint4::FromRaw(input_val_rescaled_3);\n    const FixedPoint0 output_val_f0_0 = gemmlowp::logistic(input_val_f4_0);\n    const FixedPoint0 output_val_f0_1 = gemmlowp::logistic(input_val_f4_1);\n    const FixedPoint0 output_val_f0_2 = gemmlowp::logistic(input_val_f4_2);\n    const FixedPoint0 output_val_f0_3 = gemmlowp::logistic(input_val_f4_3);\n\n    // Divide by 2^23 as in the scalar code\n    using gemmlowp::RoundingDivideByPOT;\n    int32x4_t output_val_s32_0 = RoundingDivideByPOT(output_val_f0_0.raw(), 23);\n    int32x4_t output_val_s32_1 = RoundingDivideByPOT(output_val_f0_1.raw(), 23);\n    int32x4_t output_val_s32_2 = RoundingDivideByPOT(output_val_f0_2.raw(), 23);\n    int32x4_t output_val_s32_3 = RoundingDivideByPOT(output_val_f0_3.raw(), 23);\n\n    // Cast output values to uint8, saturating\n    int16x8_t output_val_s16_0 = vcombine_s16(vqmovn_s32(output_val_s32_0),\n                                              vqmovn_s32(output_val_s32_1));\n    int16x8_t output_val_s16_1 = vcombine_s16(vqmovn_s32(output_val_s32_2),\n                                              vqmovn_s32(output_val_s32_3));\n    uint8x16_t output_val_u8 = vcombine_u8(vqmovun_s16(output_val_s16_0),\n                                           vqmovun_s16(output_val_s16_1));\n\n    // Perform the bit-masking with the bit masks computed at the beginning,\n    // see the comment there.\n    output_val_u8 = vorrq_u8(output_val_u8, mask_rightclamp);\n    output_val_u8 = vandq_u8(output_val_u8, mask_leftclamp);\n\n    // Store back to memory\n    vst1q_u8(output_data + c, output_val_u8);\n  }\n#endif\n  // Leftover loop: handle one value at a time with scalar code.\n  for (; c < size; ++c) {\n    const uint8 input_val_u8 = input_data[c];\n    const int32 input_val_centered =\n        static_cast<int32>(input_val_u8) - input_zero_point;\n    uint8 output_val;\n    if (input_val_centered < -input_range_radius) {\n      output_val = 0;\n    } else if (input_val_centered > input_range_radius) {\n      output_val = 255;\n    } else {\n      const int32 input_val_rescaled =\n          MultiplyByQuantizedMultiplierGreaterThanOne(\n              input_val_centered, input_multiplier, input_left_shift);\n      using FixedPoint4 = gemmlowp::FixedPoint<int32, 4>;\n      using FixedPoint0 = gemmlowp::FixedPoint<int32, 0>;\n      const FixedPoint4 input_val_f4 = FixedPoint4::FromRaw(input_val_rescaled);\n      const FixedPoint0 output_val_f0 = gemmlowp::logistic(input_val_f4);\n      using gemmlowp::RoundingDivideByPOT;\n      int32 output_val_s32 = RoundingDivideByPOT(output_val_f0.raw(), 23);\n      if (output_val_s32 == 256) {\n        output_val_s32 = 255;\n      }\n      TFLITE_DCHECK_GE(output_val_s32, 0);\n      TFLITE_DCHECK_LE(output_val_s32, 255);\n      output_val = static_cast<uint8>(output_val_s32);\n    }\n    output_data[c] = output_val;\n  }\n}\n\ninline void Logistic(const uint8* input_data, const RuntimeShape& input_shape,\n                     int32 input_zero_point, int32 input_range_radius,\n                     int32 input_multiplier, int input_left_shift,\n                     uint8* output_data, const RuntimeShape& output_shape) {\n  LogisticParams params;\n  params.input_zero_point = input_zero_point;\n  params.input_range_radius = input_range_radius;\n  params.input_multiplier = input_multiplier;\n  params.input_left_shift = input_left_shift;\n  Logistic(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Logistic(const float* input_data, const Dims<4>& input_dims,\n                     float* output_data, const Dims<4>& output_dims) {\n  Logistic(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n           output_data);\n}\n\ninline void Logistic(const uint8* input_data, const Dims<4>& input_dims,\n                     int32 input_zero_point, int32 input_range_radius,\n                     int32 input_multiplier, int input_left_shift,\n                     uint8* output_data, const Dims<4>& output_dims) {\n  Logistic(input_data, DimsToShape(input_dims), input_zero_point,\n           input_range_radius, input_multiplier, input_left_shift, output_data,\n           DimsToShape(output_dims));\n}\n\ninline void Logistic(const RuntimeShape& input_shape, const int16* input_data,\n                     const RuntimeShape& output_shape, int16* output_data) {\n  LogisticParams params;\n  // No params currently needed by int16 Logistic.\n  Logistic(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Logistic(const int16* input_data, const RuntimeShape& input_shape,\n                     int16* output_data, const RuntimeShape& output_shape) {\n  LogisticParams params;\n  // No params currently needed by int16 Logistic.\n  Logistic(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Logistic(const int16* input_data, const Dims<4>& input_dims,\n                     int16* output_data, const Dims<4>& output_dims) {\n  Logistic(input_data, DimsToShape(input_dims), output_data,\n           DimsToShape(output_dims));\n}\n\ninline void Tanh(const float* input_data, const Dims<4>& input_dims,\n                 float* output_data, const Dims<4>& output_dims) {\n  Tanh(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n       output_data);\n}\n\ninline void Tanh(const TanhParams& params, const RuntimeShape& input_shape,\n                 const uint8* input_data, const RuntimeShape& output_shape,\n                 uint8* output_data) {\n  // Note that this is almost the exact same code as in Logistic().\n  ruy::profiler::ScopeLabel label(\"Tanh\");\n  const int32 input_zero_point = params.input_zero_point;\n  const int32 input_range_radius = params.input_range_radius;\n  const int32 input_multiplier = params.input_multiplier;\n  const int input_left_shift = params.input_left_shift;\n  const int size = MatchingFlatSize(input_shape, output_shape);\n\n  int c = 0;\n  int32_t output_zero_point = 128;\n#ifdef USE_NEON\n  // Handle 16 values at a time\n  for (; c <= size - 16; c += 16) {\n    // Read input uint8 values, cast to int16 and subtract input_zero_point\n    uint8x16_t input_val_u8 = vld1q_u8(input_data + c);\n    int16x8_t input_val_centered_0 =\n        vsubq_s16(vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(input_val_u8))),\n                  vdupq_n_s16(input_zero_point));\n    int16x8_t input_val_centered_1 =\n        vsubq_s16(vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(input_val_u8))),\n                  vdupq_n_s16(input_zero_point));\n\n    // Prepare the bit masks that we will use at the end to implement the logic\n    // that was expressed in the scalar code with branching:\n    //   if (input_val_centered < -input_range_radius) {\n    //     output_val = 0;\n    //   } else if (input_val_centered > input_range_radius) {\n    //     output_val = 255;\n    //   } else {\n    //     ...\n    uint16x8_t mask_rightclamp_0 =\n        vcgtq_s16(input_val_centered_0, vdupq_n_s16(input_range_radius));\n    uint16x8_t mask_rightclamp_1 =\n        vcgtq_s16(input_val_centered_1, vdupq_n_s16(input_range_radius));\n    uint16x8_t mask_leftclamp_0 =\n        vcgeq_s16(input_val_centered_0, vdupq_n_s16(-input_range_radius));\n    uint16x8_t mask_leftclamp_1 =\n        vcgeq_s16(input_val_centered_1, vdupq_n_s16(-input_range_radius));\n    uint8x16_t mask_rightclamp = vcombine_u8(vshrn_n_u16(mask_rightclamp_0, 8),\n                                             vshrn_n_u16(mask_rightclamp_1, 8));\n    uint8x16_t mask_leftclamp = vcombine_u8(vshrn_n_u16(mask_leftclamp_0, 8),\n                                            vshrn_n_u16(mask_leftclamp_1, 8));\n\n    // This performs what is expressed in the scalar code as\n    // const int32 input_val_rescaled =\n    //     MultiplyByQuantizedMultiplierGreaterThanOne(\n    //         input_val_centered, input_multiplier, input_left_shift);\n    int32x4_t input_val_rescaled_0 =\n        vshlq_s32(vmovl_s16(vget_low_s16(input_val_centered_0)),\n                  vdupq_n_s32(input_left_shift));\n    int32x4_t input_val_rescaled_1 =\n        vshlq_s32(vmovl_s16(vget_high_s16(input_val_centered_0)),\n                  vdupq_n_s32(input_left_shift));\n    int32x4_t input_val_rescaled_2 =\n        vshlq_s32(vmovl_s16(vget_low_s16(input_val_centered_1)),\n                  vdupq_n_s32(input_left_shift));\n    int32x4_t input_val_rescaled_3 =\n        vshlq_s32(vmovl_s16(vget_high_s16(input_val_centered_1)),\n                  vdupq_n_s32(input_left_shift));\n    input_val_rescaled_0 =\n        vqrdmulhq_n_s32(input_val_rescaled_0, input_multiplier);\n    input_val_rescaled_1 =\n        vqrdmulhq_n_s32(input_val_rescaled_1, input_multiplier);\n    input_val_rescaled_2 =\n        vqrdmulhq_n_s32(input_val_rescaled_2, input_multiplier);\n    input_val_rescaled_3 =\n        vqrdmulhq_n_s32(input_val_rescaled_3, input_multiplier);\n\n    // Invoke gemmlowp::tanh on FixedPoint wrapping int32x4_t\n    using FixedPoint4 = gemmlowp::FixedPoint<int32x4_t, 4>;\n    using FixedPoint0 = gemmlowp::FixedPoint<int32x4_t, 0>;\n    const FixedPoint4 input_val_f4_0 =\n        FixedPoint4::FromRaw(input_val_rescaled_0);\n    const FixedPoint4 input_val_f4_1 =\n        FixedPoint4::FromRaw(input_val_rescaled_1);\n    const FixedPoint4 input_val_f4_2 =\n        FixedPoint4::FromRaw(input_val_rescaled_2);\n    const FixedPoint4 input_val_f4_3 =\n        FixedPoint4::FromRaw(input_val_rescaled_3);\n    const FixedPoint0 output_val_f0_0 = gemmlowp::tanh(input_val_f4_0);\n    const FixedPoint0 output_val_f0_1 = gemmlowp::tanh(input_val_f4_1);\n    const FixedPoint0 output_val_f0_2 = gemmlowp::tanh(input_val_f4_2);\n    const FixedPoint0 output_val_f0_3 = gemmlowp::tanh(input_val_f4_3);\n\n    // Divide by 2^24 as in the scalar code\n    using gemmlowp::RoundingDivideByPOT;\n    int32x4_t output_val_s32_0 = RoundingDivideByPOT(output_val_f0_0.raw(), 24);\n    int32x4_t output_val_s32_1 = RoundingDivideByPOT(output_val_f0_1.raw(), 24);\n    int32x4_t output_val_s32_2 = RoundingDivideByPOT(output_val_f0_2.raw(), 24);\n    int32x4_t output_val_s32_3 = RoundingDivideByPOT(output_val_f0_3.raw(), 24);\n\n    // Add the output zero point\n    int32x4_t output_zero_point_s32 = vdupq_n_s32(output_zero_point);\n    output_val_s32_0 = vaddq_s32(output_val_s32_0, output_zero_point_s32);\n    output_val_s32_1 = vaddq_s32(output_val_s32_1, output_zero_point_s32);\n    output_val_s32_2 = vaddq_s32(output_val_s32_2, output_zero_point_s32);\n    output_val_s32_3 = vaddq_s32(output_val_s32_3, output_zero_point_s32);\n\n    // Cast output values to uint8, saturating\n    int16x8_t output_val_s16_0 = vcombine_s16(vqmovn_s32(output_val_s32_0),\n                                              vqmovn_s32(output_val_s32_1));\n    int16x8_t output_val_s16_1 = vcombine_s16(vqmovn_s32(output_val_s32_2),\n                                              vqmovn_s32(output_val_s32_3));\n    uint8x16_t output_val_u8 = vcombine_u8(vqmovun_s16(output_val_s16_0),\n                                           vqmovun_s16(output_val_s16_1));\n\n    // Perform the bit-masking with the bit masks computed at the beginning,\n    // see the comment there.\n    output_val_u8 = vorrq_u8(output_val_u8, mask_rightclamp);\n    output_val_u8 = vandq_u8(output_val_u8, mask_leftclamp);\n\n    // Store back to memory\n    vst1q_u8(output_data + c, output_val_u8);\n  }\n#endif\n  // Leftover loop: handle one value at a time with scalar code.\n  for (; c < size; ++c) {\n    const uint8 input_val_u8 = input_data[c];\n    const int32 input_val_centered =\n        static_cast<int32>(input_val_u8) - input_zero_point;\n    uint8 output_val;\n    if (input_val_centered < -input_range_radius) {\n      output_val = 0;\n    } else if (input_val_centered > input_range_radius) {\n      output_val = 255;\n    } else {\n      const int32 input_val_rescaled =\n          MultiplyByQuantizedMultiplierGreaterThanOne(\n              input_val_centered, input_multiplier, input_left_shift);\n      using FixedPoint4 = gemmlowp::FixedPoint<int32, 4>;\n      using FixedPoint0 = gemmlowp::FixedPoint<int32, 0>;\n      const FixedPoint4 input_val_f4 = FixedPoint4::FromRaw(input_val_rescaled);\n      const FixedPoint0 output_val_f0 = gemmlowp::tanh(input_val_f4);\n      using gemmlowp::RoundingDivideByPOT;\n      int32 output_val_s32 = RoundingDivideByPOT(output_val_f0.raw(), 24);\n      output_val_s32 += output_zero_point;\n      if (output_val_s32 == 256) {\n        output_val_s32 = 255;\n      }\n      TFLITE_DCHECK_GE(output_val_s32, 0);\n      TFLITE_DCHECK_LE(output_val_s32, 255);\n      output_val = static_cast<uint8>(output_val_s32);\n    }\n    output_data[c] = output_val;\n  }\n}\n\ninline void Tanh(const uint8* input_data, const RuntimeShape& input_shape,\n                 int32 input_zero_point, int32 input_range_radius,\n                 int32 input_multiplier, int input_left_shift,\n                 uint8* output_data, const RuntimeShape& output_shape) {\n  TanhParams params;\n  params.input_zero_point = input_zero_point;\n  params.input_range_radius = input_range_radius;\n  params.input_multiplier = input_multiplier;\n  params.input_left_shift = input_left_shift;\n  Tanh(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Tanh(const uint8* input_data, const Dims<4>& input_dims,\n                 int32 input_zero_point, int32 input_range_radius,\n                 int32 input_multiplier, int input_left_shift,\n                 uint8* output_data, const Dims<4>& output_dims) {\n  Tanh(input_data, DimsToShape(input_dims), input_zero_point,\n       input_range_radius, input_multiplier, input_left_shift, output_data,\n       DimsToShape(output_dims));\n}\n\ninline void Tanh(const int16* input_data, const RuntimeShape& input_shape,\n                 int input_left_shift, int16* output_data,\n                 const RuntimeShape& output_shape) {\n  TanhParams params;\n  params.input_left_shift = input_left_shift;\n  Tanh(params, input_shape, input_data, output_shape, output_data);\n}\n\ninline void Tanh(const int16* input_data, const Dims<4>& input_dims,\n                 int input_left_shift, int16* output_data,\n                 const Dims<4>& output_dims) {\n  Tanh(input_data, DimsToShape(input_dims), input_left_shift, output_data,\n       DimsToShape(output_dims));\n}\n\ntemplate <typename T>\ninline void DepthToSpace(const T* input_data, const Dims<4>& input_dims,\n                         int block_size, T* output_data,\n                         const Dims<4>& output_dims) {\n  tflite::DepthToSpaceParams op_params;\n  op_params.block_size = block_size;\n\n  DepthToSpace(op_params, DimsToShape(input_dims), input_data,\n               DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\ninline void SpaceToDepth(const T* input_data, const Dims<4>& input_dims,\n                         int block_size, T* output_data,\n                         const Dims<4>& output_dims) {\n  tflite::SpaceToDepthParams op_params;\n  op_params.block_size = block_size;\n\n  SpaceToDepth(op_params, DimsToShape(input_dims), input_data,\n               DimsToShape(output_dims), output_data);\n}\n\ninline void Mul(const float* input1_data, const Dims<4>& input1_dims,\n                const float* input2_data, const Dims<4>& input2_dims,\n                float output_activation_min, float output_activation_max,\n                float* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n\n  Mul(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\nvoid Mul(const float* input1_data, const Dims<4>& input1_dims,\n         const float* input2_data, const Dims<4>& input2_dims,\n         float* output_data, const Dims<4>& output_dims) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n\n  Mul(input1_data, input1_dims, input2_data, input2_dims, output_activation_min,\n      output_activation_max, output_data, output_dims);\n}\n\ninline void Mul(const int32* input1_data, const Dims<4>& input1_dims,\n                const int32* input2_data, const Dims<4>& input2_dims,\n                int32 output_activation_min, int32 output_activation_max,\n                int32* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n\n  Mul(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <FusedActivationFunctionType Ac>\nvoid Mul(const int32* input1_data, const Dims<4>& input1_dims,\n         const int32* input2_data, const Dims<4>& input2_dims,\n         int32* output_data, const Dims<4>& output_dims) {\n  TFLITE_DCHECK(Ac == FusedActivationFunctionType::kNone);\n  tflite::ArithmeticParams op_params;\n  // No parameters needed.\n\n  MulNoActivation(op_params, DimsToShape(input1_dims), input1_data,\n                  DimsToShape(input2_dims), input2_data,\n                  DimsToShape(output_dims), output_data);\n}\n\ninline void Mul(const int16* input1_data, const Dims<4>& input1_dims,\n                const int16* input2_data, const Dims<4>& input2_dims,\n                int16* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  // No parameters needed.\n\n  Mul(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ninline void Mul(const int16* input1_data, const Dims<4>& input1_dims,\n                const int16* input2_data, const Dims<4>& input2_dims,\n                int32 output_offset, int32 output_activation_min,\n                int32 output_activation_max, uint8* output_data,\n                const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  op_params.output_offset = output_offset;\n  op_params.quantized_activation_min = output_activation_min;\n  op_params.quantized_activation_max = output_activation_max;\n\n  Mul(op_params, DimsToShape(input1_dims), input1_data,\n      DimsToShape(input2_dims), input2_data, DimsToShape(output_dims),\n      output_data);\n}\n\ntemplate <typename T>\nvoid BroadcastMul(const T* input1_data, const Dims<4>& input1_dims,\n                  const T* input2_data, const Dims<4>& input2_dims,\n                  T output_activation_min, T output_activation_max,\n                  T* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  SetActivationParams(output_activation_min, output_activation_max, &op_params);\n\n  BroadcastMul4DSlow(op_params, DimsToShape(input1_dims), input1_data,\n                     DimsToShape(input2_dims), input2_data,\n                     DimsToShape(output_dims), output_data);\n}\n\n// For compatibility with old checked-in code\ntemplate <FusedActivationFunctionType Ac>\ninline void BroadcastMul(const float* input1_data, const Dims<4>& input1_dims,\n                         const float* input2_data, const Dims<4>& input2_dims,\n                         float* output_data, const Dims<4>& output_dims) {\n  tflite::ArithmeticParams op_params;\n  float float_activation_min;\n  float float_activation_max;\n  GetActivationMinMax(Ac, &float_activation_min, &float_activation_max);\n  SetActivationParams(float_activation_min, float_activation_max, &op_params);\n\n  BroadcastMul4DSlow(op_params, DimsToShape(input1_dims), input1_data,\n                     DimsToShape(input2_dims), input2_data,\n                     DimsToShape(output_dims), output_data);\n}\n\ninline void LocalResponseNormalization(const float* input_data,\n                                       const Dims<4>& input_dims, int range,\n                                       float bias, float alpha, float beta,\n                                       float* output_data,\n                                       const Dims<4>& output_dims) {\n  tflite::LocalResponseNormalizationParams op_params;\n  op_params.range = range;\n  op_params.bias = bias;\n  op_params.alpha = alpha;\n  op_params.beta = beta;\n\n  LocalResponseNormalization(op_params, DimsToShape(input_dims), input_data,\n                             DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename SrcT, typename DstT>\nvoid Cast(const SrcT* input_data, const Dims<4>& input_dims, DstT* output_data,\n          const Dims<4>& output_dims) {\n  Cast(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n       output_data);\n}\n\ninline void Floor(const float* input_data, const Dims<4>& input_dims,\n                  float* output_data, const Dims<4>& output_dims) {\n  Floor(DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n        output_data);\n}\n\ninline void ResizeBilinear(const float* input_data, const Dims<4>& input_dims,\n                           const int32* output_size_data,\n                           const Dims<4>& output_size_dims, float* output_data,\n                           const Dims<4>& output_dims, bool align_corners) {\n  tflite::ResizeBilinearParams op_params;\n  op_params.align_corners = align_corners;\n  op_params.half_pixel_centers = false;\n  ResizeBilinear(op_params, DimsToShape(input_dims), input_data,\n                 DimsToShape(output_size_dims), output_size_data,\n                 DimsToShape(output_dims), output_data);\n}\n\ninline void ResizeBilinear(const uint8* input_data, const Dims<4>& input_dims,\n                           const int32* output_size_data,\n                           const Dims<4>& output_size_dims, uint8* output_data,\n                           const Dims<4>& output_dims, bool align_corners) {\n  tflite::ResizeBilinearParams op_params;\n  op_params.align_corners = align_corners;\n  op_params.half_pixel_centers = false;\n  ResizeBilinear(op_params, DimsToShape(input_dims), input_data,\n                 DimsToShape(output_size_dims), output_size_data,\n                 DimsToShape(output_dims), output_data);\n}\n\n// legacy, for compatibility with old checked-in code\ninline void ResizeBilinear(const float* input_data, const Dims<4>& input_dims,\n                           const int32* output_size_data,\n                           const Dims<4>& output_size_dims, float* output_data,\n                           const Dims<4>& output_dims) {\n  ResizeBilinear(input_data, input_dims, output_size_data, output_size_dims,\n                 output_data, output_dims, /*align_corners=*/false);\n}\n\n// legacy, for compatibility with old checked-in code\ninline void ResizeBilinear(const uint8* input_data, const Dims<4>& input_dims,\n                           const int32* output_size_data,\n                           const Dims<4>& output_size_dims, uint8* output_data,\n                           const Dims<4>& output_dims) {\n  ResizeBilinear(input_data, input_dims, output_size_data, output_size_dims,\n                 output_data, output_dims, /*align_corners=*/false);\n}\n\ntemplate <typename T>\ninline void BatchToSpaceND(const T* input_data, const Dims<4>& input_dims,\n                           const int32* block_shape_data,\n                           const Dims<4>& block_shape_dims,\n                           const int32* crops_data, const Dims<4>& crops_dims,\n                           T* output_data, const Dims<4>& output_dims) {\n  BatchToSpaceND(DimsToShape(input_dims), input_data,\n                 DimsToShape(block_shape_dims), block_shape_data,\n                 DimsToShape(crops_dims), crops_data, DimsToShape(output_dims),\n                 output_data);\n}\n\n// Legacy signature, function covered both Pad and PadV2.\ntemplate <typename T>\ninline void PadV2(const T* input_data, const Dims<4>& input_dims,\n                  const std::vector<int>& left_paddings,\n                  const std::vector<int>& right_paddings, T* output_data,\n                  const Dims<4>& output_dims, const T pad_value) {\n  TFLITE_DCHECK_EQ(left_paddings.size(), 4);\n  TFLITE_DCHECK_EQ(right_paddings.size(), 4);\n  tflite::PadParams op_params;\n  op_params.left_padding_count = 4;\n  op_params.right_padding_count = 4;\n  for (int i = 0; i < 4; ++i) {\n    op_params.left_padding[i] = left_paddings[3 - i];\n    op_params.right_padding[i] = right_paddings[3 - i];\n  }\n  const T pad_value_copy = pad_value;\n\n  Pad(op_params, DimsToShape(input_dims), input_data, &pad_value_copy,\n      DimsToShape(output_dims), output_data);\n}\n\n// Old Pad that calls legacy PadV2.\ntemplate <typename T>\ninline void Pad(const T* input_data, const Dims<4>& input_dims,\n                const std::vector<int>& left_paddings,\n                const std::vector<int>& right_paddings, T* output_data,\n                const Dims<4>& output_dims, const int32_t pad_value) {\n  const T converted_pad_value = static_cast<T>(pad_value);\n  PadV2<T>(input_data, input_dims, left_paddings, right_paddings, output_data,\n           output_dims, converted_pad_value);\n}\n\n// Old Pad that only padded with 0.\ntemplate <typename T>\ninline void Pad(const T* input_data, const Dims<4>& input_dims,\n                const std::vector<int>& left_paddings,\n                const std::vector<int>& right_paddings, T* output_data,\n                const Dims<4>& output_dims) {\n  const T pad_value = static_cast<T>(0);\n  PadV2<T>(input_data, input_dims, left_paddings, right_paddings, output_data,\n           output_dims, pad_value);\n}\n\ntemplate <typename T>\ninline void Slice(const T* input_data, const Dims<4>& input_dims,\n                  const std::vector<int>& begin, const std::vector<int>& size,\n                  T* output_data, const Dims<4>& output_dims) {\n  tflite::SliceParams op_params;\n  op_params.begin_count = 4;\n  op_params.size_count = 4;\n  for (int i = 0; i < 4; ++i) {\n    op_params.begin[i] = begin[3 - i];\n    op_params.size[i] = size[3 - i];\n  }\n\n  Slice(op_params, DimsToShape(input_dims), input_data,\n        DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\nvoid TensorFlowMinimum(const T* input1_data, const Dims<4>& input1_dims,\n                       const T* input2_data, T* output_data,\n                       const Dims<4>& output_dims) {\n  Minimum(DimsToShape(input1_dims), input1_data, input2_data,\n          DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\nvoid TensorFlowMaximum(const T* input1_data, const Dims<4>& input1_dims,\n                       const T* input2_data, T* output_data,\n                       const Dims<4>& output_dims) {\n  Maximum(DimsToShape(input1_dims), input1_data, input2_data,\n          DimsToShape(output_dims), output_data);\n}\n\ninline void Dequantize(const uint8* input_data, const Dims<4>& input_dims,\n                       int32 zero_point, double scale, float* output_data,\n                       const Dims<4>& output_dims) {\n  tflite::DequantizationParams op_params;\n  op_params.zero_point = zero_point;\n  op_params.scale = scale;\n\n  Dequantize(op_params, DimsToShape(input_dims), input_data,\n             DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T>\nvoid Transpose(const T* input, const Dims<4>& input_dims, T* output,\n               const Dims<4>& output_dims, const int* permuted_axes) {\n  TransposeParams params;\n  params.perm_count = 4;\n  for (int i = 0; i < 4; ++i) {\n    params.perm[i] = 3 - permuted_axes[3 - i];\n  }\n  Transpose(params, DimsToShape(input_dims), input, DimsToShape(output_dims),\n            output);\n}\n\ntemplate <typename T>\ninline void StridedSlice(const T* input_data, const Dims<4>& input_dims,\n                         int begin_mask, int end_mask, int shrink_axis_mask,\n                         const std::vector<int>& start_indices,\n                         const std::vector<int>& stop_indices,\n                         const std::vector<int>& strides, T* output_data,\n                         const Dims<4>& output_dims) {\n  TFLITE_DCHECK_EQ(start_indices.size(), 4);\n  auto op_params = strided_slice::BuildStridedSliceParams(\n      begin_mask, end_mask, shrink_axis_mask, start_indices, stop_indices,\n      strides);\n  reference_ops::StridedSliceReverseIndices(&op_params);\n\n  StridedSlice(op_params, DimsToShape(input_dims), input_data,\n               DimsToShape(output_dims), output_data);\n}\n\ntemplate <typename T1, typename T2, typename T3>\nvoid ArgMax(const T3* axis, const T1* input_data,\n            const tflite::Dims<4>& input_dims, T2* output_data,\n            const tflite::Dims<4>& output_dims) {\n  // Assumes the input always has 4 dimensions, and therefore,\n  // output always has three dimensions.\n  auto output_shape = RuntimeShape(\n      {output_dims.sizes[2], output_dims.sizes[1], output_dims.sizes[0]});\n  // Another way to interpret this is that output_dims.sizes[4] is always 1.\n  TFLITE_DCHECK_EQ(output_shape.FlatSize(),\n                   DimsToShape(output_dims).FlatSize());\n  // Legacy path only supported this.\n  TFLITE_DCHECK_EQ(axis[0], 3);\n  ArgMinMax(DimsToShape(input_dims), input_data, axis, output_shape,\n            output_data, /*is_arg_max=*/true);\n}\n\ntemplate <typename T1, typename T2, typename T3>\nvoid ArgMinMax(const T3* axis, const T1* input_data, const Dims<4>& input_dims,\n               T2* output_data, const Dims<4>& output_dims,\n               const bool is_arg_max) {\n  ArgMinMax(axis, DimsToShape(input_dims), input_data, DimsToShape(output_dims),\n            output_data, is_arg_max);\n}\n\n}  // namespace optimized_ops\n}  // namespace tflite\n#endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_OPTIMIZED_LEGACY_OPTIMIZED_OPS_H_"