"# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for various tensorflow.ops.tf.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\n\nfrom tensorflow.core.framework import node_def_pb2\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import importer\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.platform import test\n\n\n# TODO(zongheng): it'd be great to factor out this function and various random\n# SparseTensor gen funcs.\ndef _sparsify(x, thresh=0.5, index_dtype=np.int64):\n  x[x < thresh] = 0\n\n  non_zero = np.where(x)\n  x_indices = np.vstack(non_zero).astype(index_dtype).T\n  x_values = x[non_zero]\n  x_shape = x.shape\n\n  return sparse_tensor.SparseTensor(\n      indices=x_indices, values=x_values, dense_shape=x_shape), len(x_values)\n\n\nclass ShapeOpsTest(test.TestCase):\n\n  def _compareShape(self, x, use_gpu=False):\n    np_ans = np.array(np.shape(x))\n    with self.cached_session(use_gpu=use_gpu):\n      tf_ans = array_ops.shape(x)\n      tf_ans_64 = array_ops.shape(x, out_type=dtypes.int64)\n      result = self.evaluate(tf_ans)\n      result_64 = self.evaluate(tf_ans_64)\n    self.assertAllEqual(np_ans, result)\n    self.assertAllEqual(np_ans, result_64)\n    self.assertShapeEqual(np_ans, tf_ans)\n\n  def _compareShapeSparse(self, x_np, use_gpu=False):\n    np_ans = np.array(np.shape(x_np))\n    x_tf, unused_nnz = _sparsify(x_np)\n    with self.cached_session(use_gpu=use_gpu):\n      tf_ans = array_ops.shape(x_tf)\n      result = self.evaluate(tf_ans)\n    self.assertAllEqual(np_ans, result)\n    self.assertShapeEqual(np_ans, tf_ans)\n\n  def _compareShapeN(self, x, use_gpu=False):\n    np_ans = np.array(np.shape(x))\n    with self.cached_session(use_gpu=use_gpu) as sess:\n      tf_ans = array_ops.shape_n([x, x, x])\n      tf_ans_64 = array_ops.shape_n([x, x, x], out_type=dtypes.int64)\n      result = self.evaluate(tf_ans)\n      result_64 = self.evaluate(tf_ans_64)\n    for i in range(3):\n      self.assertAllEqual(np_ans, result[i])\n      self.assertAllEqual(np_ans, result_64[i])\n      self.assertShapeEqual(np_ans, tf_ans[i])\n\n  def _compareRank(self, x, use_gpu=False):\n    np_ans = np.asarray(np.ndim(x))\n    with self.cached_session(use_gpu=use_gpu):\n      tf_ans = array_ops.rank(x)\n      result = self.evaluate(tf_ans)\n    self.assertAllEqual(np_ans, result)\n    self.assertShapeEqual(np_ans, tf_ans)\n\n  def _compareRankSparse(self, x_np, use_gpu=False):\n    np_ans = np.asarray(np.ndim(x_np))\n    x_tf, unused_nnz = _sparsify(x_np)\n    with self.cached_session(use_gpu=use_gpu):\n      tf_ans = array_ops.rank(x_tf)\n      result = self.evaluate(tf_ans)\n    self.assertAllEqual(np_ans, result)\n    self.assertShapeEqual(np_ans, tf_ans)\n\n  def _compareSize(self, x, use_gpu=False):\n    np_ans = np.asarray(np.size(x))\n    with self.cached_session(use_gpu=use_gpu):\n      tf_ans = array_ops.size(x)\n      result = self.evaluate(tf_ans)\n      tf_ans_64 = array_ops.size(x, out_type=dtypes.int64)\n      result_64 = self.evaluate(tf_ans_64)\n    self.assertAllEqual(np_ans, result)\n    self.assertAllEqual(np_ans, result_64)\n    self.assertShapeEqual(np_ans, tf_ans)\n\n  def _compareSizeSparse(self, x_np, use_gpu=False):\n    np_ans = np.asarray(np.size(x_np))\n    x_tf, unused_nnz = _sparsify(x_np)\n    with self.cached_session(use_gpu=use_gpu):\n      tf_ans = array_ops.size(x_tf)\n      result = self.evaluate(tf_ans)\n    self.assertAllEqual(np_ans, result)\n    self.assertShapeEqual(np_ans, tf_ans)\n\n  def _testCpu(self, x):\n    self._compareShape(x, use_gpu=False)\n    self._compareShapeN(x, use_gpu=False)\n    self._compareRank(x, use_gpu=False)\n    self._compareSize(x, use_gpu=False)\n    self._compareShapeSparse(x, use_gpu=False)\n    self._compareRankSparse(x, use_gpu=False)\n    self._compareSizeSparse(x, use_gpu=False)\n\n  def _testGpu(self, x):\n    self._compareShape(x, use_gpu=True)\n    self._compareShapeN(x, use_gpu=True)\n    self._compareRank(x, use_gpu=True)\n    self._compareSize(x, use_gpu=True)\n    self._compareShapeSparse(x, use_gpu=True)\n    self._compareRankSparse(x, use_gpu=True)\n    self._compareSizeSparse(x, use_gpu=True)\n\n  def _testAll(self, x):\n    self._testCpu(x)\n    self._testGpu(x)\n\n  def testBasic(self):\n    self._testAll(np.random.randn(2))\n    self._testAll(np.random.randn(2, 3))\n    self._testAll(np.random.randn(2, 3, 5))\n    self._testAll(np.random.randn(2, 3, 5, 7))\n    self._testAll(np.random.randn(2, 3, 5, 7, 11))\n    self._testAll(np.random.randn(2, 3, 5, 7, 11, 13))\n\n  def testBool(self):\n    self._testAll(np.random.choice((False, True), size=(2,)))\n    self._testAll(np.random.choice((False, True), size=(2, 3)))\n    self._testAll(np.random.choice((False, True), size=(2, 3, 5)))\n    self._testAll(np.random.choice((False, True), size=(2, 3, 5, 7)))\n    self._testAll(np.random.choice((False, True), size=(2, 3, 5, 7, 11)))\n    self._testAll(np.random.choice((False, True), size=(2, 3, 5, 7, 11, 13)))\n\n  # Disabled because it takes too long to run, but manually verified\n  # as passing at time of writing.\n  def _test64BitOutput(self):\n    with self.cached_session():\n      inp = array_ops.zeros([2**31])\n      num_elements = array_ops.size_internal(\n          inp, optimize=False, out_type=dtypes.int64)\n      self.assertEqual(2**31, self.evaluate(num_elements))\n\n    # Too large for tf.int32 output.\n    with self.assertRaises(errors_impl.InvalidArgumentError):\n      with self.cached_session():\n        inp = array_ops.zeros([2**31])\n        num_elements = array_ops.size_internal(\n            inp, optimize=False, out_type=dtypes.int32)\n        self.assertEqual(2**31, self.evaluate(num_elements))\n\n  def _compareExpandDims(self, x, dim, use_gpu):\n    np_ans = np.expand_dims(x, axis=dim)\n    with self.cached_session(use_gpu=use_gpu):\n      tensor = array_ops.expand_dims(x, dim)\n      tf_ans = self.evaluate(tensor)\n    self.assertShapeEqual(np_ans, tensor)\n    self.assertAllEqual(np_ans, tf_ans)\n\n  def _compareExpandDimsAll(self, x, dim):\n    self._compareExpandDims(x, dim, False)\n    self._compareExpandDims(x, dim, True)\n\n  def testExpandDims(self):\n    self._compareExpandDimsAll(np.zeros([2]), 0)\n    self._compareExpandDimsAll(np.zeros([2]), 1)\n    self._compareExpandDimsAll(np.zeros([2]), -1)\n\n    self._compareExpandDimsAll(np.zeros([2, 3]), 0)\n    self._compareExpandDimsAll(np.zeros([2, 3]), 1)\n    self._compareExpandDimsAll(np.zeros([2, 3]), 2)\n    self._compareExpandDimsAll(np.zeros([2, 3]), -1)\n    self._compareExpandDimsAll(np.zeros([2, 3]), -2)\n\n    self._compareExpandDimsAll(np.zeros([2, 3, 5]), 0)\n    self._compareExpandDimsAll(np.zeros([2, 3, 5]), 1)\n    self._compareExpandDimsAll(np.zeros([2, 3, 5]), 2)\n    self._compareExpandDimsAll(np.zeros([2, 3, 5]), 3)\n\n    self._compareExpandDimsAll(np.zeros([2, 3, 5]), -1)\n    self._compareExpandDimsAll(np.zeros([2, 3, 5]), -2)\n    self._compareExpandDimsAll(np.zeros([2, 3, 5]), -3)\n    self._compareExpandDimsAll(np.zeros([2, 3, 5]), -4)\n\n  def testExpandDimsBool(self):\n    choice = lambda s: np.random.choice((False, True), size=s)\n    self._compareExpandDimsAll(choice([2]), 0)\n    self._compareExpandDimsAll(choice([2]), 1)\n    self._compareExpandDimsAll(choice([2]), -1)\n\n    self._compareExpandDimsAll(choice([2, 3]), 0)\n    self._compareExpandDimsAll(choice([2, 3]), 1)\n    self._compareExpandDimsAll(choice([2, 3]), 2)\n    self._compareExpandDimsAll(choice([2, 3]), -1)\n    self._compareExpandDimsAll(choice([2, 3]), -2)\n\n    self._compareExpandDimsAll(choice([2, 3, 5]), 0)\n    self._compareExpandDimsAll(choice([2, 3, 5]), 1)\n    self._compareExpandDimsAll(choice([2, 3, 5]), 2)\n    self._compareExpandDimsAll(choice([2, 3, 5]), 3)\n\n    self._compareExpandDimsAll(choice([2, 3, 5]), -1)\n    self._compareExpandDimsAll(choice([2, 3, 5]), -2)\n    self._compareExpandDimsAll(choice([2, 3, 5]), -3)\n    self._compareExpandDimsAll(choice([2, 3, 5]), -4)\n\n  @test_util.run_deprecated_v1\n  def testExpandDimsErrors(self):\n    with self.cached_session():\n      self.assertRaises(ValueError, array_ops.expand_dims,\n                        np.zeros([2, 3, 5]), -5)\n      self.assertRaises(ValueError, array_ops.expand_dims,\n                        [False, True, True], -5)\n      self.assertRaises(ValueError, array_ops.expand_dims,\n                        np.zeros([2, 3, 5]), 4)\n      self.assertRaises(ValueError, array_ops.expand_dims,\n                        [False, True, True], 4)\n\n  @test_util.run_deprecated_v1\n  def testExpandDimsGradient(self):\n    with self.cached_session():\n      inp = constant_op.constant(\n          np.random.rand(4, 2).astype(\"f\"), dtype=dtypes.float32)\n      squeezed = array_ops.expand_dims(inp, 1)\n\n      err = gradient_checker.compute_gradient_error(inp, [4, 2], squeezed,\n                                                    [4, 1, 2])\n    self.assertLess(err, 1e-3)\n\n  @test_util.run_deprecated_v1\n  def testExpandDimsScalar(self):\n    with self.cached_session():\n      inp = constant_op.constant(7)\n      self.assertAllEqual([7], array_ops.expand_dims(inp, 0))\n      self.assertAllEqual([7], array_ops.expand_dims(inp, -1))\n\n      inp = constant_op.constant(True)\n      self.assertAllEqual([True], array_ops.expand_dims(inp, 0))\n      self.assertAllEqual([True], array_ops.expand_dims(inp, -1))\n\n  def testExpandDimsDimType(self):\n    for dtype in [dtypes.int32, dtypes.int64]:\n      x = np.zeros([2])\n      np_ans = np.expand_dims(x, axis=0)\n      with self.cached_session():\n        tensor = array_ops.expand_dims(x, constant_op.constant(0, dtype))\n        tf_ans = self.evaluate(tensor)\n      self.assertShapeEqual(np_ans, tensor)\n      self.assertAllEqual(np_ans, tf_ans)\n\n  def _compareSqueeze(self, x, squeeze_dims, use_gpu):\n    with self.cached_session(use_gpu=use_gpu):\n      if squeeze_dims:\n        np_ans = np.squeeze(x, axis=tuple(squeeze_dims))\n        tensor = array_ops.squeeze(x, squeeze_dims)\n        tf_ans = self.evaluate(tensor)\n      else:\n        np_ans = np.squeeze(x)\n        tensor = array_ops.squeeze(x)\n        tf_ans = self.evaluate(tensor)\n    self.assertShapeEqual(np_ans, tensor)\n    self.assertAllEqual(np_ans, tf_ans)\n\n  def _compareSqueezeAll(self, x, squeeze_dims=None):\n    if squeeze_dims is None:\n      squeeze_dims = []\n    self._compareSqueeze(x, squeeze_dims, False)\n    self._compareSqueeze(x, squeeze_dims, True)\n\n  def testSqueeze(self):\n    # Nothing to squeeze.\n    self._compareSqueezeAll(np.zeros([2]))\n    self._compareSqueezeAll(np.zeros([2, 3]))\n\n    # Squeeze the middle element away.\n    self._compareSqueezeAll(np.zeros([2, 1, 2]))\n\n    # Squeeze on both ends.\n    self._compareSqueezeAll(np.zeros([1, 2, 1, 3, 1]))\n\n  def testSqueezeBool(self):\n    choice = lambda s: np.random.choice((False, True), size=s)\n    # Nothing to squeeze.\n    self._compareSqueezeAll(choice([2]))\n    self._compareSqueezeAll(choice([2, 3]))\n\n    # Squeeze the middle element away.\n    self._compareSqueezeAll(choice([2, 1, 2]))\n\n    # Squeeze on both ends.\n    self._compareSqueezeAll(choice([1, 2, 1, 3, 1]))\n\n  def testSqueezeSpecificDimension(self):\n    # Positive squeeze dim index.\n    self._compareSqueezeAll(np.zeros([1, 2, 1, 3, 1]), [0])\n    self._compareSqueezeAll(np.zeros([1, 2, 1, 3, 1]), [2, 4])\n    self._compareSqueezeAll(np.zeros([1, 2, 1, 3, 1]), [0, 4, 2])\n\n    # Negative squeeze dim index.\n    self._compareSqueezeAll(np.zeros([1, 2, 1, 3, 1]), [-1])\n    self._compareSqueezeAll(np.zeros([1, 2, 1, 3, 1]), [-3, -5])\n    self._compareSqueezeAll(np.zeros([1, 2, 1, 3, 1]), [-3, -5, -1])\n\n  def testSqueezeSpecificDimensionBool(self):\n    choice = lambda s: np.random.choice((False, True), size=s)\n    # Positive squeeze dim index.\n    self._compareSqueezeAll(choice([1, 2, 1, 3, 1]), [0])\n    self._compareSqueezeAll(choice([1, 2, 1, 3, 1]), [2, 4])\n    self._compareSqueezeAll(choice([1, 2, 1, 3, 1]), [0, 4, 2])\n\n    # Negative squeeze dim index.\n    self._compareSqueezeAll(choice([1, 2, 1, 3, 1]), [-1])\n    self._compareSqueezeAll(choice([1, 2, 1, 3, 1]), [-3, -5])\n    self._compareSqueezeAll(choice([1, 2, 1, 3, 1]), [-3, -5, -1])\n\n  def testSqueezeAllOnes(self):\n    # Numpy squeezes a 1 element tensor into a zero dimensional tensor.\n    # Verify that we do the same.\n    for use_gpu in [False, True]:\n      with self.cached_session(use_gpu=use_gpu):\n        tensor = array_ops.squeeze(np.zeros([1, 1, 1]), [])\n        self.assertEqual(np.shape(1), tensor.get_shape())\n        tf_ans = self.evaluate(tensor)\n        self.assertEqual(np.shape(1), tf_ans.shape)\n\n  def testSqueezeAllOnesBool(self):\n    # Numpy squeezes a 1 element tensor into a zero dimensional tensor.\n    # Verify that we do the same.\n    for use_gpu in [False, True]:\n      with self.cached_session(use_gpu=use_gpu):\n        tensor = array_ops.squeeze([[[False]]], [])\n        self.assertEqual(np.shape(1), tensor.get_shape())\n        tf_ans = self.evaluate(tensor)\n        self.assertEqual(np.shape(1), tf_ans.shape)\n\n  @test_util.run_deprecated_v1\n  def testSqueezeOnlyOnes(self):\n    for use_gpu in [False, True]:\n      with self.cached_session(use_gpu=use_gpu):\n        input_1x1x3 = np.zeros([1, 1, 3])\n        self._compareSqueezeAll(input_1x1x3)\n        self._compareSqueezeAll(input_1x1x3, [0])\n        self._compareSqueezeAll(input_1x1x3, [1])\n        self.assertRaises(ValueError, array_ops.squeeze, input_1x1x3, [2])\n\n  @test_util.run_deprecated_v1\n  def testSqueezeErrors(self):\n    for use_gpu in [False, True]:\n      with self.cached_session(use_gpu=use_gpu):\n        self.assertRaises(ValueError, array_ops.squeeze,\n                          np.zeros([1, 2, 1]), [-4])\n        self.assertRaises(ValueError, array_ops.squeeze,\n                          np.zeros([1, 2, 1]), [0, -4])\n        self.assertRaises(ValueError, array_ops.squeeze,\n                          np.zeros([1, 2, 1]), [3])\n        self.assertRaises(ValueError, array_ops.squeeze,\n                          np.zeros([1, 2, 1]), [2, 3])\n\n  @test_util.run_deprecated_v1\n  def testSqueezeGradient(self):\n    with self.cached_session():\n      inp = np.random.rand(4, 2).astype(\"f\")\n      a = array_ops.reshape(inp, [4, 1, 2])\n      squeezed = array_ops.squeeze(a, [])\n\n      err = gradient_checker.compute_gradient_error(a, [4, 1, 2], squeezed,\n                                                    [4, 2])\n    self.assertLess(err, 1e-3)\n\n  @test_util.run_deprecated_v1\n  def testSqueezeGradientWithSqueezeDims(self):\n    with self.cached_session():\n      inp = np.random.rand(4, 2).astype(\"f\")\n      a = array_ops.reshape(inp, [4, 1, 2, 1])\n      squeezed = array_ops.squeeze(a, [1])\n\n      err = gradient_checker.compute_gradient_error(a, [4, 1, 2, 1], squeezed,\n                                                    [4, 2, 1])\n    self.assertLess(err, 1e-3)\n\n  @test_util.run_deprecated_v1\n  def testSqueezeWithUnknownShape(self):\n    with self.cached_session():\n      a = array_ops.placeholder(dtypes.float32, shape=[2, None])\n\n      squeezed = array_ops.squeeze(a, [1])\n      self.assertEqual([2], squeezed.get_shape().as_list())\n\n      squeezed = array_ops.squeeze(a)\n      self.assertEqual(None, squeezed.get_shape())\n\n      self.assertRaises(ValueError, array_ops.squeeze, a, [0])\n      self.assertRaises(ValueError, array_ops.squeeze, a, [100])\n\n\nclass TileTest(test.TestCase, parameterized.TestCase):\n\n  def testScalar(self):\n    for use_gpu in False, True:\n      with self.cached_session(use_gpu=use_gpu):\n        a = constant_op.constant(7, shape=[], dtype=dtypes.float32)\n        tiled = array_ops.tile(a, [])\n        result = self.evaluate(tiled)\n      self.assertEqual(result.shape, ())\n      self.assertEqual([], tiled.get_shape())\n      self.assertEqual(7, result)\n\n  def testSimple(self):\n    # multiples could be int32 or int64\n    for dtype in [dtypes.int32, dtypes.int64]:\n      with self.cached_session():\n        inp = np.random.rand(4, 1).astype(np.float32)\n        a = constant_op.constant(inp)\n        tiled = array_ops.tile(a, constant_op.constant([1, 4], dtype=dtype))\n        result = self.evaluate(tiled)\n      self.assertEqual(result.shape, (4, 4))\n      self.assertEqual([4, 4], tiled.get_shape())\n      self.assertTrue((result == np.tile(inp, (1, 4))).all())\n\n  def testIdentityTileAndGrad(self):\n    with self.cached_session():\n      inp = np.random.rand(4, 1).astype(np.float32)\n      a = constant_op.constant(inp)\n      tiled = array_ops.tile(a, [1, 1])\n      result = self.evaluate(tiled)\n    self.assertEqual(result.shape, (4, 1))\n    self.assertEqual([4, 1], tiled.get_shape())\n    self.assertTrue((result == np.tile(inp, (1, 1))).all())\n\n  def testEmpty(self):\n    with self.cached_session():\n      inp = np.random.rand(2, 3).astype(np.float32)\n      a = constant_op.constant(inp)\n      tiled = array_ops.tile(a, [5, 0])\n      result = self.evaluate(tiled)\n    self.assertEqual(result.shape, (10, 0))\n    self.assertEqual([10, 0], tiled.get_shape())\n\n  @test_util.run_deprecated_v1\n  def testUnknownInputShape(self):\n    \"\"\"Importing can call _TileShape without shape of <multiples> known.\"\"\"\n    with self.cached_session():\n      inp = array_ops.placeholder(dtypes.float32)  # unknown shape\n      multiples = constant_op.constant([1, 2, 3, 4], dtype=np.int32)\n      tiled = array_ops.tile(inp, multiples)\n      gdef = tiled.graph.as_graph_def()\n\n      # Move the tile op to the start of the graph so that shapes of its inputs\n      # are not available when the shape function runs on import.\n      swapped = False\n      for i, n in enumerate(gdef.node):\n        if n.op == \"Tile\":\n          # Swap tile op to be first in gdef.node\n          assert i != 0\n          new_node = node_def_pb2.NodeDef()\n          new_node.CopyFrom(gdef.node[i])\n          gdef.node[i].CopyFrom(gdef.node[0])\n          gdef.node[0].CopyFrom(new_node)\n          swapped = True\n      assert swapped\n\n      tiled_imported, = importer.import_graph_def(\n          gdef, return_elements=[tiled.name])\n      self.assertEqual(4, tiled_imported.get_shape().ndims)\n\n  def testTypes(self):\n    types_to_test = {\n        \"bool\": (dtypes.bool, bool),\n        \"float32\": (dtypes.float32, float),\n        \"float64\": (dtypes.float64, float),\n        \"complex64\": (dtypes.complex64, complex),\n        \"complex128\": (dtypes.complex128, complex),\n        \"uint8\": (dtypes.uint8, int),\n        \"int8\": (dtypes.int8, int),\n        \"int16\": (dtypes.int16, int),\n        \"int32\": (dtypes.int32, int),\n        \"int64\": (dtypes.int64, int),\n        \"uint32\": (dtypes.uint32, int),\n        \"uint64\": (dtypes.uint64, int),\n        bytes: (dtypes.string, bytes)\n    }\n    for dtype_np, (dtype_tf, cast) in types_to_test.items():\n      with self.cached_session():\n        inp = np.random.rand(4, 1).astype(dtype_np)\n        a = constant_op.constant(\n            [cast(x) for x in inp.ravel(order=\"C\")],\n            shape=[4, 1],\n            dtype=dtype_tf)\n        tiled = array_ops.tile(a, [1, 4])\n        result = self.evaluate(tiled)\n      self.assertEqual(result.shape, (4, 4))\n      self.assertEqual([4, 4], tiled.get_shape())\n      self.assertAllEqual(result, np.tile(inp, (1, 4)))\n\n  @test_util.run_deprecated_v1\n  def testInvalidDim(self):\n    with self.cached_session():\n      inp = np.random.rand(4, 1).astype(\"f\")\n      a = constant_op.constant(\n          [float(x) for x in inp.ravel(order=\"C\")],\n          shape=[4, 1],\n          dtype=dtypes.float32)\n      # Wrong length of multiples.\n      with self.assertRaises(ValueError):\n        array_ops.tile(a, [1, 4, 2])\n      # Wrong rank for multiples.\n      with self.assertRaises(ValueError):\n        array_ops.tile(a, [[2, 3], [3, 4]]).eval()\n\n  def _RunAndVerifyResult(self, rank, use_gpu):\n    with self.cached_session(use_gpu=use_gpu):\n      # Random dims of given rank\n      input_shape = np.random.randint(1, 4, size=rank)\n      inp = np.random.rand(*input_shape).astype(\"f\")\n      a = constant_op.constant(\n          [float(x) for x in inp.ravel(order=\"C\")],\n          shape=input_shape,\n          dtype=dtypes.float32)\n      multiples = np.random.randint(1, 4, size=rank).astype(np.int32)\n      tiled = array_ops.tile(a, multiples)\n      result = self.evaluate(tiled)\n    self.assertTrue((np.array(multiples) * np.array(inp.shape) == np.array(\n        result.shape)).all())\n    self.assertAllEqual(result, np.tile(inp, tuple(multiples)))\n    self.assertShapeEqual(result, tiled)\n\n  def testRandom(self):\n    # test low rank, like 5\n    for _ in range(5):\n      self._RunAndVerifyResult(5, use_gpu=False)\n    for _ in range(5):\n      self._RunAndVerifyResult(5, use_gpu=True)\n    # test high rank, like 10\n    for _ in range(5):\n      self._RunAndVerifyResult(10, use_gpu=False)\n    for _ in range(5):\n      self._RunAndVerifyResult(10, use_gpu=True)\n\n  @parameterized.parameters(dtypes.int32, dtypes.int64)\n  @test_util.run_deprecated_v1\n  def testGradientSimpleReduction(self, multiples_dtype):\n    with self.cached_session():\n      inp = np.random.rand(4, 1).astype(\"f\")\n      a = constant_op.constant(\n          [float(x) for x in inp.flatten()], shape=[4, 1], dtype=dtypes.float32)\n      multiples = constant_op.constant([1, 4], dtype=multiples_dtype)\n      tiled = array_ops.tile(a, multiples)\n      grad_shape = [4, 4]\n      grad_inp = np.random.rand(*grad_shape).astype(\"f\")\n      grad_tensor = constant_op.constant(\n          [float(x) for x in grad_inp.flatten()], shape=grad_shape)\n      grad = gradients_impl.gradients([tiled], [a], [grad_tensor])[0]\n      self.assertShapeEqual(inp, grad)\n      result = self.evaluate(grad)\n    self.assertAllClose(np.sum(grad_inp, axis=1).reshape(4, 1), result, 1e-3)\n\n  @test_util.run_deprecated_v1\n  def testGradientStridedReduction(self):\n    with self.cached_session():\n      inp = np.random.rand(4, 2).astype(\"f\")\n      a = constant_op.constant(\n          [float(x) for x in inp.flatten()], shape=[4, 2], dtype=dtypes.float32)\n      tiled = array_ops.tile(a, [1, 2])\n      grad_shape = [4, 4]\n      grad_inp = np.random.rand(*grad_shape).astype(\"f\")\n      grad_tensor = constant_op.constant(\n          [float(x) for x in grad_inp.flatten()], shape=grad_shape)\n      grad = gradients_impl.gradients([tiled], [a], [grad_tensor])[0]\n      self.assertShapeEqual(inp, grad)\n      result = self.evaluate(grad)\n    expected_shape = [4, 2]\n    expected = np.zeros(expected_shape)\n    expected[:, 0] = grad_inp[:, 0] + grad_inp[:, 2]\n    expected[:, 1] = grad_inp[:, 1] + grad_inp[:, 3]\n    self.assertTrue((np.abs(expected - result) < 1e-3).all())\n\n  @test_util.run_deprecated_v1\n  def testGradientSimpleReductionOnGPU(self):\n    with self.session():\n      inp = np.random.rand(4, 1).astype(\"f\")\n      a = constant_op.constant(\n          [float(x) for x in inp.flatten()], shape=[4, 1], dtype=dtypes.float32)\n      tiled = array_ops.tile(a, [1, 4])\n      grad_shape = [4, 4]\n      grad_inp = np.random.rand(*grad_shape).astype(\"f\")\n      grad_tensor = constant_op.constant(\n          [float(x) for x in grad_inp.flatten()], shape=grad_shape)\n      grad = gradients_impl.gradients([tiled], [a], [grad_tensor])[0]\n      result = self.evaluate(grad)\n    self.assertAllClose(np.sum(grad_inp, axis=1).reshape(4, 1), result, 1e-3)\n\n  @test_util.run_deprecated_v1\n  def testGradientStridedReductionOnGPU(self):\n    with self.session():\n      inp = np.random.rand(4, 2).astype(\"f\")\n      a = constant_op.constant(\n          [float(x) for x in inp.flatten()], shape=[4, 2], dtype=dtypes.float32)\n      tiled = array_ops.tile(a, [1, 2])\n      grad_shape = [4, 4]\n      grad_inp = np.random.rand(*grad_shape).astype(\"f\")\n      grad_tensor = constant_op.constant(\n          [float(x) for x in grad_inp.flatten()], shape=grad_shape)\n      grad = gradients_impl.gradients([tiled], [a], [grad_tensor])[0]\n      result = self.evaluate(grad)\n    expected_shape = [4, 2]\n    expected = np.zeros(expected_shape)\n    expected[:, 0] = grad_inp[:, 0] + grad_inp[:, 2]\n    expected[:, 1] = grad_inp[:, 1] + grad_inp[:, 3]\n    self.assertAllClose(expected, result, 1e-3)\n\n  def _RunAndVerifyGradientResult(self, input_shape, multiples):\n    for use_gpu in False, True:\n      with self.cached_session(use_gpu=use_gpu):\n        # Random values\n        inp = np.asarray(np.random.rand(*input_shape))\n        a = constant_op.constant(inp, dtype=dtypes.float64)\n        tiled = array_ops.tile(a, multiples)\n        grad_shape = list(np.array(multiples) * np.array(inp.shape))\n        err = gradient_checker.compute_gradient_error(\n            a, list(input_shape), tiled, grad_shape, x_init_value=inp)\n      print(\"tile(float) error = \", err)\n      self.assertLess(err, 1e-3)\n\n  @test_util.run_deprecated_v1\n  def testGradientRandomScalar(self):\n    self._RunAndVerifyGradientResult([], [])\n\n  @test_util.run_deprecated_v1\n  def testGradientRandom(self):\n    self._RunAndVerifyGradientResult([2, 2, 1, 1, 3], [1, 1, 1, 1, 1])\n    self._RunAndVerifyGradientResult([2, 2, 1, 1, 3], [1, 2, 1, 3, 1])\n    self._RunAndVerifyGradientResult([2, 3, 1, 1, 3], [3, 1, 1, 2, 2])\n    self._RunAndVerifyGradientResult([2, 1, 3, 3, 2], [1, 3, 3, 1, 2])\n\n  @test_util.run_deprecated_v1\n  def testGradientStridedReductionGC(self):\n    with self.cached_session():\n      inp = np.random.rand(4, 2).astype(\"f\")\n      a = constant_op.constant(\n          [float(x) for x in inp.flatten()], shape=[4, 2], dtype=dtypes.float32)\n      tiled = array_ops.tile(a, [1, 2])\n      err = gradient_checker.compute_gradient_error(a, [4, 2], tiled, [4, 4])\n    self.assertLess(err, 1e-3)\n\n  @parameterized.parameters(dtypes.int32, dtypes.int64)\n  @test_util.run_deprecated_v1\n  def testGradientWithSparseGradWithRank1(self, multiples_dtype):\n    inputs = constant_op.constant([1.0, 2.0, 3.0, 4.0],\n                                  dtype=dtypes.float32)\n    multiples = constant_op.constant([3], dtype=dtypes.int64)\n    outputs = array_ops.gather(array_ops.tile(inputs, multiples),\n                               [1, 5, 9, 3, 7, 2, 2, 2])\n    with self.cached_session():\n      error = gradient_checker.compute_gradient_error(\n          inputs, inputs.get_shape().as_list(),\n          outputs, outputs.get_shape().as_list())\n      self.assertLess(error, 1e-4)\n\n  @test_util.run_deprecated_v1\n  def testGradientWithSparseGradWithRank3(self):\n    inputs = constant_op.constant([1.0, 2.0, 3.0, 4.0],\n                                  dtype=dtypes.float32)\n    inputs = array_ops.reshape(inputs, [-1, 1, 1])\n    outputs = array_ops.gather(array_ops.tile(inputs, [3, 4, 2]),\n                               [1, 5, 9, 3, 7, 2, 2, 2])\n    with self.cached_session():\n      error = gradient_checker.compute_gradient_error(\n          inputs, inputs.get_shape().as_list(),\n          outputs, outputs.get_shape().as_list())\n      self.assertLess(error, 1e-4)\n\n  @test_util.run_deprecated_v1\n  def testShapeFunctionEdgeCases(self):\n    # Unknown multiples shape.\n    inp = constant_op.constant(0.0, shape=[4, 4, 4, 4])\n    tiled = array_ops.tile(inp, array_ops.placeholder(dtypes.int32))\n    self.assertEqual([None, None, None, None], tiled.get_shape().as_list())\n\n    # Unknown input shape.\n    inp = array_ops.placeholder(dtypes.float32)\n    tiled = array_ops.tile(inp, [2, 2, 2, 2])\n    self.assertEqual([None, None, None, None], tiled.get_shape().as_list())\n\n    # Unknown input and multiples shape.\n    inp = array_ops.placeholder(dtypes.float32)\n    tiled = array_ops.tile(inp, array_ops.placeholder(dtypes.int32))\n    self.assertIs(None, tiled.get_shape().ndims)\n\n    # Known input and partially known multiples.\n    inp = constant_op.constant(0.0, shape=[1, 1])\n    tiled = array_ops.tile(inp, [array_ops.placeholder(dtypes.int32), 7])\n    self.assertEqual([None, 7], tiled.get_shape().as_list())\n\n    # Mismatched input rank and multiples length.\n    inp = array_ops.placeholder(dtypes.float32, shape=[None, None])\n    with self.assertRaises(ValueError):\n      tiled = array_ops.tile(\n          inp, array_ops.placeholder(\n              dtypes.int32, shape=[3]))\n\n\nif __name__ == \"__main__\":\n  test.main()"