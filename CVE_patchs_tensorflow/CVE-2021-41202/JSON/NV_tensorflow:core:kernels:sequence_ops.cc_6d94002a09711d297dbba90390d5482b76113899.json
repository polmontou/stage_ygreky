"/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/math_ops.cc.\n\n#include <cmath>\n\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/framework/types.h\"\n\nnamespace tensorflow {\n\nint32 GetValue(int32_t v) { return v; }\n\ntemplate <typename T>\nclass RangeOp : public OpKernel {\n public:\n  explicit RangeOp(OpKernelConstruction* context) : OpKernel(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& start_in = context->input(0);\n    const Tensor& limit_in = context->input(1);\n    const Tensor& delta_in = context->input(2);\n    // TODO(rmlarsen): Disallow legacy use of length-1 vectors as scalars.\n    OP_REQUIRES(context,\n                TensorShapeUtils::IsScalar(start_in.shape()) ||\n                    (TensorShapeUtils::IsVector(start_in.shape()) &&\n                     start_in.shape().dim_size(0) == 1),\n                errors::InvalidArgument(\"start must be a scalar, not shape \",\n                                        start_in.shape().DebugString()));\n    OP_REQUIRES(context,\n                TensorShapeUtils::IsScalar(limit_in.shape()) ||\n                    (TensorShapeUtils::IsVector(limit_in.shape()) &&\n                     limit_in.shape().dim_size(0) == 1),\n                errors::InvalidArgument(\"limit must be a scalar, not shape \",\n                                        limit_in.shape().DebugString()));\n    OP_REQUIRES(context,\n                TensorShapeUtils::IsScalar(delta_in.shape()) ||\n                    (TensorShapeUtils::IsVector(delta_in.shape()) &&\n                     delta_in.shape().dim_size(0) == 1),\n                errors::InvalidArgument(\"delta must be a scalar, not shape \",\n                                        delta_in.shape().DebugString()));\n    const T start = start_in.scalar<T>()();\n    const T limit = limit_in.scalar<T>()();\n    const T delta = delta_in.scalar<T>()();\n    OP_REQUIRES(context, delta != 0,\n                errors::InvalidArgument(\"Requires delta != 0: \", delta));\n    if (delta > 0) {\n      OP_REQUIRES(\n          context, start <= limit,\n          errors::InvalidArgument(\n              \"Requires start <= limit when delta > 0: \", start, \"/\", limit));\n    } else {\n      OP_REQUIRES(\n          context, start >= limit,\n          errors::InvalidArgument(\n              \"Requires start >= limit when delta < 0: \", start, \"/\", limit));\n    }\n    int64_t size = 0;\n    if (std::is_integral<T>::value) {\n      size = static_cast<int64>(\n          (std::abs(limit - start) + std::abs(delta) - 1) / std::abs(delta));\n    } else {\n      size = static_cast<int64>(std::ceil(std::abs((limit - start) / delta)));\n    }\n    Tensor* out = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, TensorShape({size}), &out));\n    auto flat = out->flat<T>();\n    T val = start;\n    for (int64_t i = 0; i < size; ++i) {\n      flat(i) = T(val);\n      val += delta;\n    }\n  }\n};\n\n#define REGISTER_KERNEL(DEV, TYPE)                           \\\n  REGISTER_KERNEL_BUILDER(Name(\"Range\")                      \\\n                              .Device(DEV)                   \\\n                              .HostMemory(\"start\")           \\\n                              .HostMemory(\"limit\")           \\\n                              .HostMemory(\"delta\")           \\\n                              .HostMemory(\"output\")          \\\n                              .TypeConstraint<TYPE>(\"Tidx\"), \\\n                          RangeOp<TYPE>);\n\n#define REGISTER_CPU_KERNEL(T) REGISTER_KERNEL(DEVICE_CPU, T)\n#define REGISTER_GPU_KERNEL(T) REGISTER_KERNEL(DEVICE_GPU, T)\n\nTF_CALL_float(REGISTER_CPU_KERNEL);\nTF_CALL_double(REGISTER_CPU_KERNEL);\nTF_CALL_int32(REGISTER_CPU_KERNEL);\nTF_CALL_int64(REGISTER_CPU_KERNEL);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nTF_CALL_float(REGISTER_GPU_KERNEL);\nTF_CALL_double(REGISTER_GPU_KERNEL);\nTF_CALL_int32(REGISTER_GPU_KERNEL);\nTF_CALL_int64(REGISTER_GPU_KERNEL);\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#undef REGISTER_KERNEL\n#undef REGISTER_CPU_KERNEL\n#undef REGISTER_GPU_KERNEL\n\ntemplate <typename T, typename Tnum>\nclass LinSpaceOp : public OpKernel {\n public:\n  explicit LinSpaceOp(OpKernelConstruction* context) : OpKernel(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& start_in = context->input(0);\n    const Tensor& stop_in = context->input(1);\n    const Tensor& num_in = context->input(2);\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(start_in.shape()),\n                errors::InvalidArgument(\"start must be a scalar, not shape \",\n                                        start_in.shape().DebugString()));\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(stop_in.shape()),\n                errors::InvalidArgument(\"stop must be a scalar, not shape \",\n                                        stop_in.shape().DebugString()));\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(num_in.shape()),\n                errors::InvalidArgument(\"num must be a scalar, not shape \",\n                                        num_in.shape().DebugString()));\n    const T start = start_in.scalar<T>()();\n    const T stop = stop_in.scalar<T>()();\n    const Tnum num = num_in.scalar<Tnum>()();\n    OP_REQUIRES(context, num > 0,\n                errors::InvalidArgument(\"Requires num > 0: \", num));\n    Tensor* out = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, TensorShape({num}), &out));\n    auto flat = out->flat<T>();\n    flat(0) = start;\n    if (num > 1) {\n      const T step = (stop - start) / (num - 1);\n      for (Tnum i = 1; i < num - 1; ++i) flat(i) = start + step * i;\n      // Ensure final value == stop; float arithmetic won't guarantee this.\n      flat(num - 1) = stop;\n    }\n  }\n};\n\n#define REGISTER_KERNEL(DEV, T, Tidx)                       \\\n  REGISTER_KERNEL_BUILDER(Name(\"LinSpace\")                  \\\n                              .Device(DEV)                  \\\n                              .TypeConstraint<T>(\"T\")       \\\n                              .TypeConstraint<Tidx>(\"Tidx\") \\\n                              .HostMemory(\"start\")          \\\n                              .HostMemory(\"stop\")           \\\n                              .HostMemory(\"num\")            \\\n                              .HostMemory(\"output\"),        \\\n                          LinSpaceOp<T, Tidx>);\n\n#define REGISTER_KERNEL_ALL_NUMS(dev, T) \\\n  REGISTER_KERNEL(dev, T, int32);        \\\n  REGISTER_KERNEL(dev, T, int64_t)\n\n#define REGISTER_CPU_KERNEL(T) REGISTER_KERNEL_ALL_NUMS(DEVICE_CPU, T)\nTF_CALL_float(REGISTER_CPU_KERNEL);\nTF_CALL_double(REGISTER_CPU_KERNEL);\n\n// NOTE(touts): We register the op on GPU but it still runs on CPU\n// because its inputs and outputs are tagged as HostMemory.\n#define REGISTER_GPU_KERNEL(T) REGISTER_KERNEL_ALL_NUMS(DEVICE_GPU, T)\nTF_CALL_float(REGISTER_GPU_KERNEL);\nTF_CALL_double(REGISTER_GPU_KERNEL);\n#undef REGISTER_GPU_KERNEL\n\n\n#undef REGISTER_CPU_KERNEL\n#undef REGISTER_KERNEL_ALL_NUMS\n#undef REGISTER_KERNEL\n\n}  // namespace tensorflow"