"diff --git a/tensorflow/core/kernels/xent_op.cc b/tensorflow/core/kernels/xent_op.cc\nindex 2c252b5f21e..7d8ad52c895 100644\n--- a/tensorflow/core/kernels/xent_op.cc\n+++ b/tensorflow/core/kernels/xent_op.cc\n@@ -46,7 +46,8 @@ class SoftmaxXentWithLogitsOp : public OpKernel {\n     TensorShape shape_in = logits_in.shape();\n \n     BCast bcast(BCast::FromShape(logits_in.shape()),\n-                BCast::FromShape(labels_in.shape()));\n+                BCast::FromShape(labels_in.shape()),\n+                /*fewer_dims_optimization=*/false);\n     if (!logits_in.IsSameSize(labels_in)) {\n       OP_REQUIRES(context, bcast.IsValid(),\n                   errors::InvalidArgument(\n@@ -88,20 +89,12 @@ class SoftmaxXentWithLogitsOp : public OpKernel {\n                                 {0}, 1, shape_in, &back_out));\n     if (shape_in.dim_size(0) > 0) {\n       functor::XentFunctor<Device, T> functor;\n-      if (logits_in.IsSameSize(labels_in)) {\n-        functor(context->eigen_device<Device>(), shape_in.AsEigenDSizes<2>(),\n-                Eigen::array<Eigen::DenseIndex, 2>{1, 1},\n-                Eigen::array<Eigen::DenseIndex, 2>{1, 1}, logits_in.matrix<T>(),\n-                labels_in.matrix<T>(), scratch.matrix<T>(), loss_out->vec<T>(),\n-                back_out->matrix<T>());\n-      } else {\n-        functor(context->eigen_device<Device>(), shape_in.AsEigenDSizes<2>(),\n-                BCast::ToIndexArray<2>(bcast.x_bcast()),\n-                BCast::ToIndexArray<2>(bcast.y_bcast()),\n-                logits_in.template shaped<T, 2>(bcast.x_reshape()),\n-                labels_in.template shaped<T, 2>(bcast.y_reshape()),\n-                scratch.matrix<T>(), loss_out->vec<T>(), back_out->matrix<T>());\n-      }\n+      functor(context->eigen_device<Device>(), shape_in.AsEigenDSizes<2>(),\n+              BCast::ToIndexArray<2>(bcast.x_bcast()),\n+              BCast::ToIndexArray<2>(bcast.y_bcast()),\n+              logits_in.template shaped<T, 2>(bcast.x_reshape()),\n+              labels_in.template shaped<T, 2>(bcast.y_reshape()),\n+              scratch.matrix<T>(), loss_out->vec<T>(), back_out->matrix<T>());\n     }\n   }\n };"