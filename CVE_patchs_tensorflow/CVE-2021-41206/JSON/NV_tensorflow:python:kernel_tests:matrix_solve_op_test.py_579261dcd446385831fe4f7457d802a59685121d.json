"# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for tensorflow.ops.math_ops.matrix_solve.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom tensorflow.python.client import session\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import linalg_ops\nfrom tensorflow.python.ops import stateless_random_ops\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.platform import benchmark\nfrom tensorflow.python.platform import test\n\n\nclass MatrixSolveOpTest(test.TestCase):\n\n  def _verifySolve(self, x, y, batch_dims=None):\n    for np_type in [np.float32, np.float64, np.complex64, np.complex128]:\n      if np_type == np.float32 or np_type == np.complex64:\n        tol = 1e-5\n      else:\n        tol = 1e-12\n      for adjoint in False, True:\n        if np_type in (np.float32, np.float64):\n          a = x.real.astype(np_type)\n          b = y.real.astype(np_type)\n          a_np = np.transpose(a) if adjoint else a\n        else:\n          a = x.astype(np_type)\n          b = y.astype(np_type)\n          a_np = np.conj(np.transpose(a)) if adjoint else a\n        if batch_dims is not None:\n          a = np.tile(a, batch_dims + [1, 1])\n          a_np = np.tile(a_np, batch_dims + [1, 1])\n          b = np.tile(b, batch_dims + [1, 1])\n        np_ans = np.linalg.solve(a_np, b)\n        for use_placeholder in set((False, not context.executing_eagerly())):\n          if use_placeholder:\n            a_ph = array_ops.placeholder(dtypes.as_dtype(np_type))\n            b_ph = array_ops.placeholder(dtypes.as_dtype(np_type))\n            tf_ans = linalg_ops.matrix_solve(a_ph, b_ph, adjoint=adjoint)\n            with self.cached_session() as sess:\n              out = sess.run(tf_ans, {a_ph: a, b_ph: b})\n          else:\n            tf_ans = linalg_ops.matrix_solve(a, b, adjoint=adjoint)\n            out = self.evaluate(tf_ans)\n            self.assertEqual(tf_ans.get_shape(), out.shape)\n          self.assertEqual(np_ans.shape, out.shape)\n          self.assertAllClose(np_ans, out, atol=tol, rtol=tol)\n\n  def _generateMatrix(self, m, n):\n    matrix = (np.random.normal(-5, 5,\n                               m * n).astype(np.complex128).reshape([m, n]))\n    matrix.imag = (np.random.normal(-5, 5, m * n).astype(np.complex128).reshape(\n        [m, n]))\n    return matrix\n\n  @test_util.run_in_graph_and_eager_modes(use_gpu=True)\n  def testSolve(self):\n    for n in 1, 2, 4, 9:\n      matrix = self._generateMatrix(n, n)\n      for nrhs in 1, 2, n:\n        rhs = self._generateMatrix(n, nrhs)\n        self._verifySolve(matrix, rhs)\n\n  @test_util.run_in_graph_and_eager_modes(use_gpu=True)\n  def testSolveBatch(self):\n    for n in 2, 5:\n      matrix = self._generateMatrix(n, n)\n      for nrhs in 1, n:\n        rhs = self._generateMatrix(n, nrhs)\n        for batch_dims in [[2], [2, 2], [7, 4]]:\n          self._verifySolve(matrix, rhs, batch_dims=batch_dims)\n\n  @test_util.run_in_graph_and_eager_modes(use_gpu=True)\n  def testNonSquareMatrix(self):\n    # When the solve of a non-square matrix is attempted we should return\n    # an error\n    with self.assertRaises((ValueError, errors_impl.InvalidArgumentError)):\n      matrix = constant_op.constant([[1., 2., 3.], [3., 4., 5.]])\n      self.evaluate(linalg_ops.matrix_solve(matrix, matrix))\n\n  @test_util.run_in_graph_and_eager_modes(use_gpu=True)\n  def testWrongDimensions(self):\n    # The matrix and right-hand sides should have the same number of rows.\n    matrix = constant_op.constant([[1., 0.], [0., 1.]])\n    rhs = constant_op.constant([[1., 0.]])\n    with self.assertRaises((ValueError, errors_impl.InvalidArgumentError)):\n      self.evaluate(linalg_ops.matrix_solve(matrix, rhs))\n\n    # The matrix and right-hand side should have the same batch dimensions\n    matrix = np.random.normal(size=(2, 6, 2, 2))\n    rhs = np.random.normal(size=(2, 3, 2, 2))\n    with self.assertRaises((ValueError, errors_impl.InvalidArgumentError)):\n      self.evaluate(linalg_ops.matrix_solve(matrix, rhs))\n\n  def testNotInvertible(self):\n    # The input should be invertible.\n    with self.assertRaisesOpError(\"Input matrix is not invertible.\"):\n      # All rows of the matrix below add to zero\n      matrix = constant_op.constant([[1., 0., -1.], [-1., 1., 0.],\n                                     [0., -1., 1.]])\n      self.evaluate(linalg_ops.matrix_solve(matrix, matrix))\n\n  @test_util.run_in_graph_and_eager_modes(use_gpu=True)\n  def testConcurrent(self):\n    seed = [42, 24]\n    matrix_shape = [3, 3]\n    all_ops = []\n    for adjoint_ in False, True:\n      lhs1 = stateless_random_ops.stateless_random_normal(\n          matrix_shape, seed=seed)\n      lhs2 = stateless_random_ops.stateless_random_normal(\n          matrix_shape, seed=seed)\n      rhs1 = stateless_random_ops.stateless_random_normal(\n          matrix_shape, seed=seed)\n      rhs2 = stateless_random_ops.stateless_random_normal(\n          matrix_shape, seed=seed)\n      s1 = linalg_ops.matrix_solve(lhs1, rhs1, adjoint=adjoint_)\n      s2 = linalg_ops.matrix_solve(lhs2, rhs2, adjoint=adjoint_)\n      all_ops += [s1, s2]\n    val = self.evaluate(all_ops)\n    for i in range(0, len(all_ops), 2):\n      self.assertAllEqual(val[i], val[i + 1])\n\n\nclass MatrixSolveBenchmark(test.Benchmark):\n\n  matrix_shapes = [\n      (4, 4),\n      (10, 10),\n      (16, 16),\n      (101, 101),\n      (256, 256),\n      (1001, 1001),\n      (1024, 1024),\n      (2048, 2048),\n      (513, 4, 4),\n      (513, 16, 16),\n      (513, 256, 256),\n  ]\n\n  def _GenerateTestData(self, matrix_shape, num_rhs):\n    batch_shape = matrix_shape[:-2]\n    matrix_shape = matrix_shape[-2:]\n    assert matrix_shape[0] == matrix_shape[1]\n    n = matrix_shape[0]\n    matrix = (np.ones(matrix_shape).astype(np.float32) /\n              (2.0 * n) + np.diag(np.ones(n).astype(np.float32)))\n    rhs = np.ones([n, num_rhs]).astype(np.float32)\n    matrix = variables.Variable(\n        np.tile(matrix, batch_shape + (1, 1)), trainable=False)\n    rhs = variables.Variable(\n        np.tile(rhs, batch_shape + (1, 1)), trainable=False)\n    return matrix, rhs\n\n  def benchmarkMatrixSolveOp(self):\n    run_gpu_test = test.is_gpu_available(True)\n    for adjoint in False, True:\n      for matrix_shape in self.matrix_shapes:\n        for num_rhs in 1, 2, matrix_shape[-1]:\n\n          with ops.Graph().as_default(), \\\n              session.Session(config=benchmark.benchmark_config()) as sess, \\\n              ops.device(\"/cpu:0\"):\n            matrix, rhs = self._GenerateTestData(matrix_shape, num_rhs)\n            x = linalg_ops.matrix_solve(matrix, rhs, adjoint=adjoint)\n            self.evaluate(variables.global_variables_initializer())\n            self.run_op_benchmark(\n                sess,\n                control_flow_ops.group(x),\n                min_iters=25,\n                store_memory_usage=False,\n                name=(\"matrix_solve_cpu_shape_{matrix_shape}_num_rhs_{num_rhs}_\"\n                      \"adjoint_{adjoint}\").format(\n                          matrix_shape=matrix_shape,\n                          num_rhs=num_rhs,\n                          adjoint=adjoint))\n\n          if run_gpu_test:\n            with ops.Graph().as_default(), \\\n                session.Session(config=benchmark.benchmark_config()) as sess, \\\n                ops.device(\"/gpu:0\"):\n              matrix, rhs = self._GenerateTestData(matrix_shape, num_rhs)\n              x = linalg_ops.matrix_solve(matrix, rhs, adjoint=adjoint)\n              self.evaluate(variables.global_variables_initializer())\n              self.run_op_benchmark(\n                  sess,\n                  control_flow_ops.group(x),\n                  min_iters=25,\n                  store_memory_usage=False,\n                  name=(\"matrix_solve_gpu_shape_{matrix_shape}_num_rhs_\"\n                        \"{num_rhs}_adjoint_{adjoint}\").format(\n                            matrix_shape=matrix_shape, num_rhs=num_rhs,\n                            adjoint=adjoint))\n\n\nif __name__ == \"__main__\":\n  test.main()"