"# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for tensorflow.ops.linalg.linalg_impl.tridiagonal_matmul.\"\"\"\n\nimport itertools\n\nimport numpy as np\n\nfrom tensorflow.python.client import session\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import gradient_checker_v2\nfrom tensorflow.python.ops import linalg_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.ops.linalg import linalg_impl\nfrom tensorflow.python.platform import benchmark\nfrom tensorflow.python.platform import test\n\n\nclass TridiagonalMulOpTest(test.TestCase):\n\n  def _testAllFormats(self,\n                      superdiag,\n                      maindiag,\n                      subdiag,\n                      rhs,\n                      expected,\n                      dtype=dtypes.float64):\n    superdiag_extended = np.pad(superdiag, [0, 1], 'constant')\n    subdiag_extended = np.pad(subdiag, [1, 0], 'constant')\n    diags_compact = np.stack([superdiag_extended, maindiag, subdiag_extended])\n    diags_matrix = np.diag(superdiag, 1) + np.diag(maindiag, 0) + np.diag(\n        subdiag, -1)\n\n    diags_sequence = (constant_op.constant(superdiag_extended, dtype),\n                      constant_op.constant(maindiag, dtype),\n                      constant_op.constant(subdiag_extended, dtype))\n    diags_compact = constant_op.constant(diags_compact, dtype)\n    diags_matrix = constant_op.constant(diags_matrix, dtype)\n    rhs = constant_op.constant(rhs, dtype)\n\n    rhs_batch = array_ops.stack([rhs, 2 * rhs])\n    diags_compact_batch = array_ops.stack([diags_compact, 2 * diags_compact])\n    diags_matrix_batch = array_ops.stack([diags_matrix, 2 * diags_matrix])\n    diags_sequence_batch = [array_ops.stack([x, 2 * x]) for x in diags_sequence]\n\n    results = [\n        linalg_impl.tridiagonal_matmul(\n            diags_sequence, rhs, diagonals_format='sequence'),\n        linalg_impl.tridiagonal_matmul(\n            diags_compact, rhs, diagonals_format='compact'),\n        linalg_impl.tridiagonal_matmul(\n            diags_matrix, rhs, diagonals_format='matrix')\n    ]\n    results_batch = [\n        linalg_impl.tridiagonal_matmul(\n            diags_sequence_batch, rhs_batch, diagonals_format='sequence'),\n        linalg_impl.tridiagonal_matmul(\n            diags_compact_batch, rhs_batch, diagonals_format='compact'),\n        linalg_impl.tridiagonal_matmul(\n            diags_matrix_batch, rhs_batch, diagonals_format='matrix')\n    ]\n\n    with self.cached_session():\n      results = self.evaluate(results)\n      results_batch = self.evaluate(results_batch)\n\n    expected = np.array(expected)\n    expected_batch = np.stack([expected, 4 * expected])\n    for result in results:\n      self.assertAllClose(result, expected)\n    for result in results_batch:\n      self.assertAllClose(result, expected_batch)\n\n  def _makeTridiagonalMatrix(self, superdiag, maindiag, subdiag):\n    super_pad = [[0, 0], [0, 1], [1, 0]]\n    sub_pad = [[0, 0], [1, 0], [0, 1]]\n\n    super_part = array_ops.pad(array_ops.matrix_diag(superdiag), super_pad)\n    main_part = array_ops.matrix_diag(maindiag)\n    sub_part = array_ops.pad(array_ops.matrix_diag(subdiag), sub_pad)\n    return super_part + main_part + sub_part\n\n  def _randomComplexArray(self, shape):\n    np.random.seed(43)\n    return (np.random.uniform(-10, 10, shape) +\n            np.random.uniform(-10, 10, shape) * 1j)\n\n  def _gradientTest(self, diags, rhs, dtype=dtypes.float64):\n\n    def reference_matmul(diags, rhs):\n      matrix = self._makeTridiagonalMatrix(diags[..., 0, :-1], diags[..., 1, :],\n                                           diags[..., 2, 1:])\n      return math_ops.matmul(matrix, rhs)\n\n    diags = constant_op.constant(diags, dtype=dtype)\n    rhs = constant_op.constant(rhs, dtype=dtype)\n    with self.cached_session():\n      grad_reference, _ = gradient_checker_v2.compute_gradient(\n          reference_matmul, [diags, rhs])\n      grad_theoretical, grad_numerical = gradient_checker_v2.compute_gradient(\n          linalg_impl.tridiagonal_matmul, [diags, rhs])\n    self.assertAllClose(grad_theoretical, grad_numerical)\n    self.assertAllClose(grad_theoretical, grad_reference)\n\n  def test2x2(self):\n    self._testAllFormats([1], [2, 3], [4], [[2, 1], [4, 3]], [[8, 5], [20, 13]])\n\n  def test3x3(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      self._testAllFormats([1, 2], [1, 2, 1], [2, 1], [[1, 1], [2, 2], [3, 3]],\n                           [[3, 3], [12, 12], [5, 5]],\n                           dtype=dtype)\n\n  def testComplex(self):\n    for dtype in [dtypes.complex64, dtypes.complex128]:\n      self._testAllFormats([1j, 1j], [1, -1, 0], [1j, 1j],\n                           np.array([[1, 1j], [1, 1j], [1, 1j]]),\n                           [[1 + 1j, -1 + 1j], [-1 + 2j, -2 - 1j], [1j, -1]],\n                           dtype=dtype)\n\n  def testBatch(self):\n    b = 20\n    m = 10\n    n = 15\n    superdiag = self._randomComplexArray((b, m - 1))\n    maindiag = self._randomComplexArray((b, m))\n    subdiag = self._randomComplexArray((b, m - 1))\n    rhs = self._randomComplexArray((b, m, n))\n    matrix = np.stack([np.diag(superdiag[i], 1) + \\\n                       np.diag(maindiag[i], 0) + \\\n                       np.diag(subdiag[i], -1) for i in range(b)])\n    expected_result = np.matmul(matrix, rhs)\n    result = linalg_impl.tridiagonal_matmul(\n        constant_op.constant(matrix, dtype=dtypes.complex128),\n        constant_op.constant(rhs, dtype=dtypes.complex128),\n        diagonals_format='matrix')\n\n    with self.cached_session():\n      result = self.evaluate(result)\n\n    self.assertAllClose(result, expected_result)\n\n  def testGradientSmall(self):\n    self._gradientTest([[[1, 2, 0], [1, 2, 3], [0, 1, 2]]],\n                       [[[1, 2], [3, 4], [5, 6]]],\n                       dtype=dtypes.float64)\n\n  def testGradientComplexSmall(self):\n    self._gradientTest(\n        np.array([[[1 + 1j, 2j, 0], [1 + 2j, 2j, 3 + 0j], [0, 1j, 2 + 0j]]]),\n        np.array([[[1j, 2 + 0j], [3 + 1j, 4j], [5j, 6 + 3j]]]),\n        dtype=dtypes.complex128)\n\n  def testGradientComplexWithBatches(self):\n    b = 5\n    m = 10\n    n = 15\n    diags = self._randomComplexArray((b, 3, m))\n    rhs = self._randomComplexArray((b, m, n))\n    self._gradientTest(diags, rhs, dtype=dtypes.complex128)\n\n  def _testErrorWithShapesEager(self, exception_regex, superdiag_shape,\n                                maindiag_shape, subdiag_shape, rhs_shape):\n    with context.eager_mode():\n      superdiag = array_ops.ones(superdiag_shape)\n      maindiag = array_ops.ones(maindiag_shape)\n      subdiag = array_ops.ones(subdiag_shape)\n      rhs = array_ops.ones(rhs_shape)\n      with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n                                  exception_regex):\n        linalg_ops.tridiagonal_mat_mul(superdiag, maindiag, subdiag, rhs)\n\n  def testInvalidShapesEagerGpu(self):\n    if not test.is_gpu_available():\n      self.skipTest('Test requires GPU')\n    self._testErrorWithShapesEager('Input must have rank >= 2, but got ',\n                                   [2], [2], [2], [2])\n    self._testErrorWithShapesEager(\n        'superdiag must have same rank as rhs, but got 3 and 2',\n        [2, 1, 2], [2, 1], [2, 1], [2, 2])\n    self._testErrorWithShapesEager(\n        'maindiag must have same outer dimensions as rhs, but for index 0, got '\n        '3 and 2',\n        [2, 1, 2], [3, 1, 2], [2, 1, 2], [2, 2, 2])\n    self._testErrorWithShapesEager(\n        \"subdiag's second-to-last dimension must be 1, but got 3\",\n        [2, 1, 2], [2, 1, 2], [2, 3, 2], [2, 2, 2])\n    self._testErrorWithShapesEager(\n        \"subdiag's last dimension size must be rhs's second-to-last dimension \"\n        \"size, but got 3 and 2\",\n        [2, 1, 2], [2, 1, 2], [2, 1, 3], [2, 2, 2])\n\n  # Benchmark\n  class TridiagonalMatMulBenchmark(test.Benchmark):\n    sizes = [(100000, 1, 1), (1000000, 1, 1), (10000000, 1, 1), (100000, 10, 1),\n             (100000, 100, 1), (10000, 1, 100), (10000, 1, 1000),\n             (10000, 1, 10000)]\n\n    def baseline(self, upper, diag, lower, vec):\n      diag_part = array_ops.expand_dims(diag, -1) * vec\n      lower_part = array_ops.pad(\n          array_ops.expand_dims(lower[:, 1:], -1) * vec[:, :-1, :],\n          [[0, 0], [1, 0], [0, 0]])\n      upper_part = array_ops.pad(\n          array_ops.expand_dims(upper[:, :-1], -1) * vec[:, 1:, :],\n          [[0, 0], [0, 1], [0, 0]])\n      return lower_part + diag_part + upper_part\n\n    def _generateData(self, batch_size, m, n, seed=42):\n      np.random.seed(seed)\n      data = np.random.normal(size=(batch_size, m, 3 + n))\n      return (variables.Variable(data[:, :, 0], dtype=dtypes.float64),\n              variables.Variable(data[:, :, 1], dtype=dtypes.float64),\n              variables.Variable(data[:, :, 2], dtype=dtypes.float64),\n              variables.Variable(data[:, :, 3:], dtype=dtypes.float64))\n\n    def benchmarkTridiagonalMulOp(self):\n      devices = [('/cpu:0', 'cpu')]\n      if test.is_gpu_available(cuda_only=True):\n        devices += [('/gpu:0', 'gpu')]\n\n      for device_option, size_option in itertools.product(devices, self.sizes):\n        device_id, device_name = device_option\n        m, batch_size, n = size_option\n\n        with ops.Graph().as_default(), \\\n            session.Session(config=benchmark.benchmark_config()) as sess, \\\n            ops.device(device_id):\n          upper, diag, lower, vec = self._generateData(batch_size, m, n)\n          x1 = self.baseline(upper, diag, lower, vec)\n          x2 = linalg_impl.tridiagonal_matmul((upper, diag, lower),\n                                              vec,\n                                              diagonals_format='sequence')\n\n          self.evaluate(variables.global_variables_initializer())\n          self.run_op_benchmark(\n              sess,\n              control_flow_ops.group(x1),\n              min_iters=10,\n              store_memory_usage=False,\n              name=('tridiagonal_matmul_baseline_%s'\n                    '_batch_size_%d_m_%d_n_%d' %\n                    (device_name, batch_size, m, n)))\n\n          self.run_op_benchmark(\n              sess,\n              control_flow_ops.group(x2),\n              min_iters=10,\n              store_memory_usage=False,\n              name=('tridiagonal_matmul_%s_batch_size_%d_m_%d_n_%d' %\n                    (device_name, batch_size, m, n)))\n\n\nif __name__ == '__main__':\n  test.main()"