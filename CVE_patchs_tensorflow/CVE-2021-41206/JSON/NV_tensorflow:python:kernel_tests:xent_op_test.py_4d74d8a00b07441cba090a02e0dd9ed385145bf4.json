"# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for SoftmaxCrossEntropyWithLogits op.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport itertools\nimport sys\n\nimport numpy as np\n\nfrom tensorflow.python.client import session\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.kernel_tests import xent_op_test_base\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gen_nn_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.platform import test\n\n\nclass XentOpTest(xent_op_test_base.XentOpTestBase):\n\n  @test_util.run_deprecated_v1\n  def testRankTooLarge(self):\n    for dtype in np.float16, np.float32:\n      np_features = np.array([[[1., 1., 1., 1.]], [[1., 2., 3.,\n                                                    4.]]]).astype(dtype)\n      np_labels = np.array([[[0., 0., 0., 1.]], [[0., .5, .5,\n                                                  0.]]]).astype(dtype)\n      self.assertRaisesRegex(ValueError, \"rank 2, but is rank 3\",\n                             gen_nn_ops.softmax_cross_entropy_with_logits,\n                             np_features, np_labels)\n\n  def testFeaturesBroadcast(self):\n    np_f = np.array([[1., 2., 3., 4.],\n                     [1., 2., 3., 4.]]).astype(np.float32)\n    np_l = np.array([[0., 0., 0., 1.],\n                     [0., .5, .5, 0.]]).astype(np.float32)\n    np_loss, np_gradient = self._npXent(labels=np_l, logits=np_f)\n    tf_f = constant_op.constant(\n        np.array([[1., 2., 3., 4.]]).astype(np.float32))\n    tf_l = constant_op.constant(\n        np.array([[0., 0., 0., 1.], [0., .5, .5, 0.]]).astype(np.float32))\n    tf_loss, tf_gradient = gen_nn_ops.softmax_cross_entropy_with_logits(\n        tf_f, tf_l)\n    self.assertAllCloseAccordingToType(np_loss, tf_loss)\n    self.assertAllCloseAccordingToType(np_gradient, tf_gradient)\n\n    tf_f = constant_op.constant(np.array([[1.]]).astype(np.float32))\n    tf_l = constant_op.constant(np.array([[1.], [1.]]).astype(np.float32))\n    tf_loss, tf_gradient = gen_nn_ops.softmax_cross_entropy_with_logits(\n        tf_f, tf_l)\n    self.assertAllClose([0, 0], tf_loss)\n    self.assertAllCloseAccordingToType([[0], [0]], tf_gradient)\n\n  @test_util.run_deprecated_v1\n  def testNotMatrix(self):\n    with self.cached_session():\n      with self.assertRaises(ValueError):\n        gen_nn_ops.softmax_cross_entropy_with_logits([0., 1., 2., 3.],\n                                                     [0., 1., 0., 1.])\n\n\nclass XentBenchmark(test.Benchmark):\n\n  def benchmarkZeroDimension(self):\n    for (m, n, p, use_gpu) in itertools.product(\n        [128],\n        [10, 100, 1000, 10000, 100000],\n        [0.001, 0.01, 0.5, 0.99, 1.0],\n        [False]):\n      k = int(p * n)\n      if k == 0:\n        continue\n      name = \"zero_dimension_m_%d_n_%d_k_%g_use_gpu_%s\" % (m, n, k, use_gpu)\n      device = \"/%s:0\" % (\"gpu\" if use_gpu else \"cpu\")\n      with ops.Graph().as_default():\n        with ops.device(device):\n          labels = array_ops.zeros([0, 2, 4], dtype=dtypes.float32)\n          logits = array_ops.zeros([0, 2, 4], dtype=dtypes.float32)\n          op = nn_ops.softmax_cross_entropy_with_logits(\n              labels=labels, logits=logits)\n        with session.Session() as sess:\n          r = self.run_op_benchmark(sess, op, min_iters=100, name=name)\n          gb_processed_input = m * n / 1.0e9\n          throughput = gb_processed_input / r[\"wall_time\"]\n          print(\"Benchmark: %s \\t wall_time: %0.03g s \\t \"\n                \"Throughput: %0.03g GB/s\" % (name, r[\"wall_time\"], throughput))\n          sys.stdout.flush()\n\n  def benchmarkSingleClass(self):\n    for (m, n, p, use_gpu) in itertools.product(\n        [128],\n        [10, 100, 1000, 10000, 100000],\n        [0.001, 0.01, 0.5, 0.99, 1.0],\n        [False]):\n      k = int(p * n)\n      if k == 0:\n        continue\n      name = \"single_class_m_%d_n_%d_k_%g_use_gpu_%s\" % (m, n, k, use_gpu)\n      device = \"/%s:0\" % (\"gpu\" if use_gpu else \"cpu\")\n      with ops.Graph().as_default():\n        with ops.device(device):\n          labels = constant_op.constant([[1.], [-1.], [0.]],\n                                        dtype=dtypes.float32)\n          logits = constant_op.constant([[-1.], [0.], [1.]],\n                                        dtype=dtypes.float32)\n          op = nn_ops.softmax_cross_entropy_with_logits(\n              labels=labels, logits=logits)\n        with session.Session() as sess:\n          r = self.run_op_benchmark(sess, op, min_iters=100, name=name)\n          gb_processed_input = m * n / 1.0e9\n          throughput = gb_processed_input / r[\"wall_time\"]\n          print(\"Benchmark: %s \\t wall_time: %0.03g s \\t \"\n                \"Throughput: %0.03g GB/s\" % (name, r[\"wall_time\"], throughput))\n          sys.stdout.flush()\n\n\nif __name__ == \"__main__\":\n  test.main()"