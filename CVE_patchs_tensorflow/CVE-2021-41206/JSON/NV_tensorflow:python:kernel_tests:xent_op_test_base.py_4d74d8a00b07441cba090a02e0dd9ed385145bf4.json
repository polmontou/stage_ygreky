"# Copyright 2015-2021 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for SoftmaxCrossEntropyWithLogits op.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nimport numpy as np\n\nfrom tensorflow.python.eager import backprop\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\n# The following import is required to register the gradient function.\nfrom tensorflow.python.ops.nn_grad import _SoftmaxCrossEntropyWithLogitsGrad  # pylint: disable=unused-import\nfrom tensorflow.python.platform import test\n\n\nclass XentOpTestBase(test.TestCase):\n\n  def _opDeterminismEnabled(self):\n    deterministic_ops = os.getenv(\"TF_DETERMINISTIC_OPS\", \"0\")\n    return deterministic_ops in (\"1\", \"true\")\n\n  def _opFwdBwd(self, labels, logits, axis=-1):\n    \"\"\" Runs the op-under-test both forwards and backwards.\"\"\"\n    logits = ops.convert_to_tensor(logits)  # needed for the gradient tape\n    with backprop.GradientTape() as tape:\n      tape.watch(logits)\n      loss = nn_ops.softmax_cross_entropy_with_logits(\n          labels=labels, logits=logits, dim=axis)\n    return loss, tape.gradient(loss, logits)\n\n  def _npXent(self, labels, logits, dim=-1):\n    if dim == -1:\n      dim = len(logits.shape) - 1\n    one_only_on_dim = list(logits.shape)\n    one_only_on_dim[dim] = 1\n    e = np.exp(logits - np.reshape(np.amax(logits, axis=dim), one_only_on_dim))\n    probs = e / np.reshape(np.sum(e, axis=dim), one_only_on_dim)\n    bp = (probs - labels)\n    l = -np.sum(labels * np.log(probs + 1.0e-20), axis=dim)\n    return l, bp\n\n  # TODO(b/123860949): The values are constant folded for XLA, so placeholders\n  # are needed.\n  def _testXent2D(self,\n                  np_labels,\n                  np_logits,\n                  with_placeholders=False,\n                  expected_gradient=None):\n    np_loss, np_gradient = self._npXent(labels=np_labels, logits=np_logits)\n    if expected_gradient is not None:\n      np_gradient = expected_gradient\n    with self.cached_session() as sess:\n      if with_placeholders:\n        logits_placeholder = array_ops.placeholder(np_logits.dtype)\n        labels_placeholder = array_ops.placeholder(np_labels.dtype)\n        loss, gradient = self._opFwdBwd(labels_placeholder, logits_placeholder)\n        tf_loss, tf_gradient = sess.run([loss, gradient],\n                                        feed_dict={\n                                            labels_placeholder: np_labels,\n                                            logits_placeholder: np_logits\n                                        })\n      else:\n        loss, gradient = self._opFwdBwd(np_labels, np_logits)\n        tf_loss, tf_gradient = self.evaluate([loss, gradient])\n    self.assertAllCloseAccordingToType(np_loss, tf_loss, half_rtol=1e-2)\n    self.assertAllCloseAccordingToType(np_gradient, tf_gradient)\n\n  def _testXentND(self, np_labels, np_logits, dim=-1):\n    np_loss, _ = self._npXent(np_labels, np_logits, dim=dim)\n    loss = nn_ops.softmax_cross_entropy_with_logits(\n        labels=np_labels, logits=np_logits, dim=dim)\n    tf_loss = self.evaluate(loss)\n    self.assertAllCloseAccordingToType(np_loss, tf_loss)\n\n  def _testSingleClass(self, expected_gradient=[[2.0], [1.0], [0.0], [0.0]]):\n    for dtype in np.float16, np.float32:\n      loss, gradient = self._opFwdBwd(\n          labels=np.array([[-1.], [0.], [1.], [1.]]).astype(dtype),\n          logits=np.array([[1.], [-1.], [0.], [1.]]).astype(dtype))\n      self.assertAllClose([0.0, 0.0, 0.0, 0.0], loss)\n      self.assertAllClose(expected_gradient, gradient)\n\n  def testSingleClass(self):\n    \"\"\"This method is structured to be easily overridden by a child class.\"\"\"\n    self._testSingleClass()\n\n  def testNpXent(self):\n    # We create 2 batches of logits for testing.\n    # batch 0 is the boring uniform distribution: 1, 1, 1, 1, with target 3.\n    # batch 1 has a bit of difference: 1, 2, 3, 4, with soft targets (1, 2).\n    logits = [[1., 1., 1., 1.], [1., 2., 3., 4.]]\n    labels = [[0., 0., 0., 1.], [0., .5, .5, 0.]]\n\n    # For batch 0, we expect the uniform distribution: 0.25, 0.25, 0.25, 0.25\n    # With a hard target 3, the gradient is [0.25, 0.25, 0.25, -0.75]\n    # The loss for this batch is -log(0.25) = 1.386\n    #\n    # For batch 1, we have:\n    # exp(0) = 1\n    # exp(1) = 2.718\n    # exp(2) = 7.389\n    # exp(3) = 20.085\n    # SUM = 31.192\n    # So we have as probabilities:\n    # exp(0) / SUM = 0.032\n    # exp(1) / SUM = 0.087\n    # exp(2) / SUM = 0.237\n    # exp(3) / SUM = 0.644\n    # With a soft target (1, 2), the gradient is\n    # [0.032, 0.087 - 0.5 = -0.413, 0.237 - 0.5 = -0.263, 0.644]\n    # The loss for this batch is [0.5 * -log(0.087), 0.5 * -log(0.237)]\n    # = [1.3862, 1.9401]\n    np_loss, np_gradient = self._npXent(np.array(labels), np.array(logits))\n    self.assertAllClose(\n        np.array([[0.25, 0.25, 0.25, -0.75], [0.0321, -0.4129, -0.2632,\n                                              0.6439]]),\n        np_gradient,\n        rtol=1.e-3,\n        atol=1.e-3)\n    self.assertAllClose(\n        np.array([1.3862, 1.9401]), np_loss, rtol=1.e-3, atol=1.e-3)\n\n  # TODO(b/123860949): The values are constant folded for XLA, so placeholders\n  # are needed.\n  @test_util.run_deprecated_v1\n  def _testLabelsBroadcast(self, uniform_labels_gradient):\n    labels = np.array([[0., 0., 0., 1.]]).astype(np.float16)\n    logits = np.array([[1., 1., 1., 1.], [1., 2., 3., 4.]]).astype(np.float16)\n    self._testXent2D(labels, logits, with_placeholders=True)\n    labels = np.array([[1.]]).astype(np.float16)\n    logits = np.array([[1.], [2.]]).astype(np.float16)\n    self._testXent2D(labels, logits, with_placeholders=True)\n    labels = np.array([[0.], [2.], [0.25]]).astype(np.float16)\n    logits = np.array([[1., 1., 1., 1.], [1., 2., 3., 4.],\n                       [1., 2., 3., 4.]]).astype(np.float16)\n    self._testXent2D(\n        labels,\n        logits,\n        with_placeholders=True,\n        expected_gradient=uniform_labels_gradient)\n\n  def testLabelsBroadcast(self):\n    \"\"\"This method is structured to be easily overridden by a child class.\"\"\"\n    self._testLabelsBroadcast(uniform_labels_gradient=[[\n        0.25, 0.25, 0.25, 0.25\n    ], [-1.968, -1.913, -1.763, -1.355], [-0.218, -0.163, -0.013, 0.394]])\n\n  @test_util.run_deprecated_v1\n  def testShapeMismatch(self):\n    with self.cached_session():\n      with self.assertRaises(ValueError):\n        self._opFwdBwd(\n            labels=[[0., 1., 0.], [1., 0., 0.]], logits=[[0., 1.], [2., 3.]])\n\n  def testHalf(self):\n    labels = np.array([[0., 0., 0., 1.], [0., .5, .5, 0.]]).astype(np.float16)\n    logits = np.array([[1., 1., 1., 1.], [1., 2., 3., 4.]]).astype(np.float16)\n    self._testXent2D(labels, logits)\n\n  def testFloat(self):\n    labels = np.array([[0., 0., 0., 1.], [0., .5, .5, 0.]]).astype(np.float32)\n    logits = np.array([[1., 1., 1., 1.], [1., 2., 3., 4.]]).astype(np.float32)\n    self._testXent2D(labels, logits)\n\n  def testDouble(self):\n    labels = np.array([[0., 0., 0., 1.], [0., .5, .5, 0.]]).astype(np.float64)\n    logits = np.array([[1., 1., 1., 1.], [1., 2., 3., 4.]]).astype(np.float64)\n    self._testXent2D(labels, logits)\n\n  @test_util.run_deprecated_v1\n  def testGradient(self):\n    with self.cached_session() as sess:\n      labels = constant_op.constant(\n          [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5],\n          shape=[3, 4],\n          dtype=dtypes.float64,\n          name=\"labels\")\n      logits = constant_op.constant(\n          [0.1, 0.2, 0.3, 0.4, 0.1, 0.4, 0.9, 1.6, 0.1, 0.8, 2.7, 6.4],\n          shape=[3, 4],\n          dtype=dtypes.float64,\n          name=\"logits\")\n      x = nn_ops.softmax_cross_entropy_with_logits(\n          labels=labels, logits=logits, name=\"xent\")\n      err = gradient_checker.compute_gradient_error(logits, [3, 4], x, [3])\n\n      # Check that no extra computation gets performed. When only the first\n      # derivative is requested, the second derivative must not be computed.\n      # So when there is no second derivative, there is no `BatchMatMul` op\n      # in the graph.\n      op_names = [\n          op.op_def.name for op in sess.graph.get_operations() if op.op_def\n      ]\n      self.assertNotIn(\"BatchMatMul\", op_names)\n      self.assertNotIn(\"BatchMatMulV2\", op_names)\n\n    self.assertLess(err, 5e-8)\n\n  @test_util.run_deprecated_v1\n  def testGradientLabelWithV2(self):\n    with self.cached_session():\n      labels = constant_op.constant(\n          [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5],\n          shape=[3, 4],\n          dtype=dtypes.float64,\n          name=\"labels\")\n      logits = constant_op.constant(\n          [0.1, 0.2, 0.3, 0.4, 0.1, 0.4, 0.9, 1.6, 0.1, 0.8, 2.7, 6.4],\n          shape=[3, 4],\n          dtype=dtypes.float64,\n          name=\"logits\")\n      x = nn_ops.softmax_cross_entropy_with_logits_v2(\n          labels=labels, logits=logits, name=\"xent\")\n      err = gradient_checker.compute_gradient_error(labels, [3, 4], x, [3])\n\n    self.assertLess(err, 5e-8)\n\n  @test_util.run_deprecated_v1\n  def testSecondGradient(self):\n    with self.cached_session() as sess:\n      labels = constant_op.constant([\n          0.0, 0.0, 1.0 / 3, 0.0, 1.0 / 3, 0.0, 0.0, 0.0, 0.0, 0.5 / 3, 0.0,\n          0.5 / 3\n      ],\n                                    shape=[12],\n                                    dtype=dtypes.float64,\n                                    name=\"labels\")\n      logits = constant_op.constant(\n          [0.1, 0.2, 0.3, 0.4, 0.1, 0.4, 0.9, 1.6, 0.1, 0.8, 2.7, 6.4],\n          shape=[12],\n          dtype=dtypes.float64,\n          name=\"logits\")\n      x = nn_ops.softmax_cross_entropy_with_logits(\n          labels=labels, logits=logits, name=\"xent\")\n      loss = math_ops.reduce_sum(x)\n\n      gradients = gradients_impl.gradients(loss, [logits])[0]\n\n      err = gradient_checker.compute_gradient_error(logits, [12], gradients,\n                                                    [12])\n\n      if not self._opDeterminismEnabled():\n        # Check how second derivative is calculated.\n        # (it is equivalent to a `BatchMatMul` op being in the graph because of\n        # the implementation in SoftmaxCrossEntropyWithLogitsGrad)\n        op_names = [\n            op.op_def.name for op in sess.graph.get_operations() if op.op_def\n        ]\n        self.assertIn(\"BatchMatMulV2\", op_names)\n\n    self.assertLess(err, 5e-8)\n\n  def test3D(self):\n    labels = np.array([[[0., 0., 0., 1.], [0., 1., 0., 0.]],\n                       [[0., 0.5, 0.5, 0.], [0.5, 0.5, 0., 0.]],\n                       [[0., 1., 0., 0.], [0., 0., 1., 0.]]]).astype(np.float32)\n    logits = np.array([[[1., 1., 1., 1.], [1., 2., 3., 4.]],\n                       [[2., 3., 4., 5.], [6., 7., 8., 9.]],\n                       [[5., 4., 3., 2.], [1., 2., 3., 4.]]]).astype(np.float32)\n    self._testXentND(labels, logits, dim=0)\n    self._testXentND(labels, logits, dim=1)\n    self._testXentND(labels, logits, dim=-1)\n\n  def testZeroDimension(self):\n    labels = np.zeros([0, 2, 4]).astype(np.float32)\n    logits = np.zeros([0, 2, 4]).astype(np.float32)\n    np_loss, _ = self._npXent(labels=labels, logits=logits)\n    loss = nn_ops.softmax_cross_entropy_with_logits(\n        labels=labels, logits=logits)\n    tf_loss = self.evaluate(loss)\n    self.assertAllEqual(np_loss, tf_loss)"