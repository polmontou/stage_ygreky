"/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/linalg_ops.cc.\n#if GOOGLE_CUDA\n#define EIGEN_USE_GPU\n#endif\n\n#include <numeric>\n\n#include \"third_party/eigen3/Eigen/Core\"\n#include \"third_party/eigen3/Eigen/LU\"\n#include \"tensorflow/core/framework/kernel_def_builder.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/kernels/linalg/linalg_ops_common.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/platform/macros.h\"\n#include \"tensorflow/core/platform/types.h\"\n\n#if GOOGLE_CUDA\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/kernels/transpose_functor.h\"\n#include \"tensorflow/core/util/cuda_solvers.h\"\n#endif\n\nnamespace tensorflow {\n\nstatic const char kErrMsg[] = \"Input matrix is not invertible.\";\n\ntemplate <class Scalar>\nclass MatrixSolveOp : public LinearAlgebraOp<Scalar> {\n public:\n  INHERIT_LINALG_TYPEDEFS(Scalar);\n\n  explicit MatrixSolveOp(OpKernelConstruction* context) : Base(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"adjoint\", &adjoint_));\n  }\n\n  void ValidateInputMatrixShapes(\n      OpKernelContext* context,\n      const TensorShapes& input_matrix_shapes) const final {\n    Base::ValidateSquareSolver(context, input_matrix_shapes);\n  }\n\n  TensorShapes GetOutputMatrixShapes(\n      const TensorShapes& input_matrix_shapes) const final {\n    return TensorShapes({TensorShape({input_matrix_shapes[0].dim_size(1),\n                                      input_matrix_shapes[1].dim_size(1)})});\n  }\n\n  int64 GetCostPerUnit(const TensorShapes& input_matrix_shapes) const final {\n    double rows = static_cast<double>(input_matrix_shapes[0].dim_size(0));\n    double num_rhss = static_cast<double>(input_matrix_shapes[1].dim_size(1));\n    double cost = rows * rows * (rows + num_rhss);\n    return cost >= static_cast<double>(kint64max) ? kint64max\n                                                  : static_cast<int64>(cost);\n  }\n\n  bool EnableInputForwarding() const final { return false; }\n\n  void ComputeMatrix(OpKernelContext* context, const ConstMatrixMaps& inputs,\n                     MatrixMaps* outputs) final {\n    const ConstMatrixMap& matrix = inputs[0];\n    const ConstMatrixMap& rhs = inputs[1];\n    if (matrix.rows() == 0 || matrix.cols() == 0 || rhs.cols() == 0) {\n      // To be consistent with the MatrixInverse op, we define the solution for\n      // an empty set of equation as the empty matrix.\n      return;\n    }\n    Eigen::PartialPivLU<Matrix> lu_decomposition(matrix.rows());\n    if (adjoint_) {\n      // TODO(rmlarsen): For Eigen 3.2, this creates a temporary copy.\n      // Make sure to backport: https://bitbucket.org/eigen/eigen/commits/\n      // bd2219a74c96dfe3f6bc2c23588749e36d2d8173\n      lu_decomposition.compute(matrix.adjoint());\n    } else {\n      lu_decomposition.compute(matrix);\n    }\n\n    // PartialPivLU cannot give strong guarantees on invertibility,\n    // but we can at least guard against exact zero pivots. This can occur as\n    // a result of basic user mistakes such providing integer valued\n    // matrices that are exactly singular, or due to underflow if this\n    // code is run with denormals being flushed to zero.\n    const RealScalar min_abs_pivot =\n        lu_decomposition.matrixLU().diagonal().cwiseAbs().minCoeff();\n    OP_REQUIRES(context, min_abs_pivot > RealScalar(0),\n                errors::InvalidArgument(kErrMsg));\n\n    // TODO(rmlarsen): Add check based on condition number estimation.\n    // The necessary changes to Eigen are in\n    // https://bitbucket.org/eigen/eigen/pull-requests/174/\n    // add-matrix-condition-number-estimation/diff\n    outputs->at(0) = lu_decomposition.solve(rhs);\n  }\n\n private:\n  bool adjoint_;\n\n  TF_DISALLOW_COPY_AND_ASSIGN(MatrixSolveOp);\n};\n\n#if GOOGLE_CUDA\ntypedef Eigen::GpuDevice GPUDevice;\n\ntemplate <class Scalar>\nclass MatrixSolveOpGpu : public AsyncOpKernel {\n public:\n  explicit MatrixSolveOpGpu(OpKernelConstruction* context)\n      : AsyncOpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"adjoint\", &adjoint_));\n  }\n\n  void ComputeAsync(OpKernelContext* context, DoneCallback done) final {\n    const Tensor& input = context->input(0);\n    const Tensor& rhs = context->input(1);\n    const int ndims = input.dims();\n    const int64 n = input.dim_size(ndims - 1);\n    const int64 nrhs = rhs.dim_size(ndims - 1);\n    // Validate inputs.\n    OP_REQUIRES_ASYNC(\n        context, ndims >= 2,\n        errors::InvalidArgument(\"Input must have rank >= 2, got \", ndims),\n        done);\n    OP_REQUIRES_ASYNC(context, rhs.dims() == ndims,\n                      errors::InvalidArgument(\n                          \"Input and right-hand side must have same rank, got \",\n                          ndims, \" != \", rhs.dims()),\n                      done);\n    OP_REQUIRES_ASYNC(\n        context, input.dim_size(ndims - 2) == n,\n        errors::InvalidArgument(\"Input matrices must be squares, got\",\n                                input.dim_size(ndims - 2), \" != \", n),\n        done);\n    OP_REQUIRES_ASYNC(context, rhs.dim_size(ndims - 2) == n,\n                      errors::InvalidArgument(\n                          \"Input matrix and right-hand side must have the \"\n                          \"same number of rows, got\",\n                          n, \" != \", rhs.dim_size(ndims - 2)),\n                      done);\n\n    // Allocate output.\n    Tensor* output;\n    OP_REQUIRES_OK_ASYNC(\n        context,\n        context->forward_input_or_allocate_output({1}, 0, rhs.shape(), &output),\n        done);\n\n    // To be consistent with the MatrixInverse op, we define the solution for\n    // an empty set of equations as the empty matrix.\n    if (input.NumElements() == 0 || rhs.NumElements() == 0) {\n      done();\n      return;\n    }\n\n    // TODO(rmlarsen): Convert to std::make_unique when available.\n    std::unique_ptr<CudaSolver> solver(new CudaSolver(context));\n\n    // Make a copy of the input for the factorization step, or, if adjoint_ is\n    // false, try to reuse the input buffer if this op owns it exclusively.\n    Tensor input_copy;\n    const GPUDevice& device = context->eigen_device<GPUDevice>();\n    if (adjoint_) {\n      // For the adjoint case, it is simpler to always make a transposed copy up\n      // front.\n      OP_REQUIRES_OK_ASYNC(\n          context,\n          solver->allocate_scoped_tensor(DataTypeToEnum<Scalar>::value,\n                                         input.shape(), &input_copy),\n          done);\n      OP_REQUIRES_OK_ASYNC(context,\n                           DoMatrixTranspose(device, input, &input_copy), done);\n    } else {\n      OP_REQUIRES_OK_ASYNC(\n          context,\n          solver->forward_input_or_allocate_scoped_tensor(\n              {0}, DataTypeToEnum<Scalar>::value, input.shape(), &input_copy),\n          done);\n      if (!input.SharesBufferWith(input_copy)) {\n        device.memcpy(input_copy.flat<Scalar>().data(),\n                      input.flat<Scalar>().data(),\n                      input.NumElements() * sizeof(Scalar));\n      }\n    }\n    auto input_copy_reshaped = input_copy.template flat_inner_dims<Scalar, 3>();\n    const int64 batch_size = input_copy_reshaped.dimension(0);\n\n    // Allocate pivots on the device.\n    Tensor pivots;\n    OP_REQUIRES_OK_ASYNC(\n        context,\n        solver->allocate_scoped_tensor(DataTypeToEnum<int>::value,\n                                       TensorShape{batch_size, n}, &pivots),\n        done);\n    auto pivots_mat = pivots.template matrix<int>();\n\n    // 1. Compute the partially pivoted LU factorization(s) of the\n    // matrix/matrices.\n    std::vector<DeviceLapackInfo> dev_info;\n    auto input_copy_ptrs = solver->GetScratchSpace<uint8>(\n        sizeof(Scalar*) * batch_size, \"input_copt_ptrs\",\n        /* on_host */ true);\n    const int kMaxMatrixSizeToBatchSizeRatio = 128;\n    const bool use_batched_solver =\n        n <= kMaxMatrixSizeToBatchSizeRatio * batch_size;\n    if (use_batched_solver) {\n      // For small matrices or large batch sizes, we use the batched interface\n      // from cuBlas.\n      const Scalar** input_copy_ptrs_base =\n          reinterpret_cast<const Scalar**>(input_copy_ptrs.mutable_data());\n      for (int batch = 0; batch < batch_size; ++batch) {\n        input_copy_ptrs_base[batch] = &input_copy_reshaped(batch, 0, 0);\n      }\n      dev_info.push_back(\n          solver->GetDeviceLapackInfo(batch_size, \"getrfBatched\"));\n      OP_REQUIRES_OK_ASYNC(\n          context,\n          solver->GetrfBatched(n, input_copy_ptrs_base, n, pivots_mat.data(),\n                               &dev_info.back(), batch_size),\n          done);\n    } else {\n      // For small batch sizes or large matrices, we use the non-batched\n      // interface from cuSolver, which is much faster for large matrices.\n      dev_info.push_back(solver->GetDeviceLapackInfo(batch_size, \"getrf\"));\n      for (int batch = 0; batch < batch_size; ++batch) {\n        OP_REQUIRES_OK_ASYNC(\n            context,\n            solver->Getrf(n, n, &input_copy_reshaped(batch, 0, 0), n,\n                          &pivots_mat(batch, 0), &dev_info.back()(batch)),\n            done);\n      }\n    }\n\n    // 2. Make a transposed copy of the right-hand sides. This is necessary\n    // because cuBLAS assumes column-major storage while TensorFlow TF uses\n    // row-major.\n    TensorShape transposed_rhs_shape(rhs.shape());\n    transposed_rhs_shape.RemoveLastDims(2);\n    transposed_rhs_shape.AddDim(nrhs);\n    transposed_rhs_shape.AddDim(n);\n    Tensor transposed_rhs;\n    OP_REQUIRES_OK_ASYNC(\n        context,\n        solver->allocate_scoped_tensor(DataTypeToEnum<Scalar>::value,\n                                       transposed_rhs_shape, &transposed_rhs),\n        done);\n    if (nrhs > 1) {\n      OP_REQUIRES_OK_ASYNC(\n          context, DoMatrixTranspose(device, rhs, &transposed_rhs), done);\n    } else {\n      device.memcpy(transposed_rhs.flat<Scalar>().data(),\n                    rhs.flat<Scalar>().data(),\n                    rhs.NumElements() * sizeof(Scalar));\n    }\n\n    // 3. Solve op(A) X = B (in column major form).\n    // We use a trick here: If adjoint_ is true, we converted A to column major\n    // form above. If adjoint is false then I leave A in row-major form and use\n    // trans_a = CUBLAS_OP_T to effectively transform it to column-major on the\n    // fly. (This means that we actually use the LU-factorization of A^T in that\n    // case, but that is equally good for solving AX=B). This way we save an\n    // explicit transpose in the more common case of adjoint_ == false.\n    auto input_copy_ptr_array = solver->GetScratchSpace<uint8>(\n        sizeof(Scalar*) * batch_size, \"input_copy_ptr_array\",\n        /* on_host */ true);\n    auto transposed_rhs_ptr_array = solver->GetScratchSpace<uint8>(\n        sizeof(Scalar*) * batch_size, \"transposed_rhs_ptr_array\",\n        /* on_host */ true);\n    auto transposed_rhs_reshaped =\n        transposed_rhs.template flat_inner_dims<Scalar, 3>();\n    if (use_batched_solver) {\n      const Scalar** input_copy_ptrs_base =\n          reinterpret_cast<const Scalar**>(input_copy_ptr_array.mutable_data());\n      const Scalar** transposed_rhs_ptrs_base =\n          reinterpret_cast<const Scalar**>(\n              transposed_rhs_ptr_array.mutable_data());\n      for (int batch = 0; batch < batch_size; ++batch) {\n        input_copy_ptrs_base[batch] = &input_copy_reshaped(batch, 0, 0);\n        transposed_rhs_ptrs_base[batch] = &transposed_rhs_reshaped(batch, 0, 0);\n      }\n      int host_info = 0;\n      OP_REQUIRES_OK_ASYNC(\n          context,\n          solver->GetrsBatched(adjoint_ ? CUBLAS_OP_C : CUBLAS_OP_T, n, nrhs,\n                               input_copy_ptrs_base, n, pivots_mat.data(),\n                               transposed_rhs_ptrs_base, n, &host_info,\n                               batch_size),\n          done);\n      OP_REQUIRES_ASYNC(\n          context, host_info == 0,\n          errors::InvalidArgument(\"The \", -host_info,\n                                  \"'th argument to cublas*getrsBatched had \"\n                                  \"an illegal value.\"),\n          done);\n    } else {\n      dev_info.push_back(solver->GetDeviceLapackInfo(batch_size, \"getrs\"));\n      for (int batch = 0; batch < batch_size; ++batch) {\n        OP_REQUIRES_OK_ASYNC(\n            context,\n            solver->Getrs(adjoint_ ? CUBLAS_OP_C : CUBLAS_OP_T, n, nrhs,\n                          &input_copy_reshaped(batch, 0, 0), n,\n                          &pivots_mat(batch, 0),\n                          &transposed_rhs_reshaped(batch, 0, 0), n,\n                          &dev_info.back()(batch)),\n            done);\n      }\n    }\n\n    // 4. Transpose X to get the final result in row-major form.\n    if (nrhs > 1) {\n      OP_REQUIRES_OK_ASYNC(\n          context, DoMatrixTranspose(device, transposed_rhs, output), done);\n    } else {\n      device.memcpy(output->flat<Scalar>().data(),\n                    transposed_rhs.flat<Scalar>().data(),\n                    transposed_rhs.NumElements() * sizeof(Scalar));\n    }\n\n    // Callback for checking info after kernels finish. Also capture the\n    // temporary Tensors/ScratchSpace so they don't get deallocated before the\n    // kernels run. TODO(rmlarsen): Use move capture once C++14 becomes\n    // available.\n    auto info_checker = [context, done, dev_info](\n                            const Status& status,\n                            const std::vector<HostLapackInfo>& host_infos) {\n      if (!status.ok() && errors::IsInvalidArgument(status) &&\n          !host_infos.empty()) {\n        for (int i = 0; i < host_infos[0].size(); ++i) {\n          // Match the CPU error message for singular matrices. Otherwise\n          // just print the original error message from the status below.\n          OP_REQUIRES_ASYNC(context, host_infos[0].data()[i] <= 0,\n                            errors::InvalidArgument(kErrMsg), done);\n        }\n      }\n      OP_REQUIRES_OK_ASYNC(context, status, done);\n      done();\n    };\n    CudaSolver::CheckLapackInfoAndDeleteSolverAsync(std::move(solver), dev_info,\n                                                    std::move(info_checker));\n  }\n\n private:\n  bool adjoint_;\n};\n\nREGISTER_LINALG_OP_GPU(\"MatrixSolve\", (MatrixSolveOpGpu<float>), float);\nREGISTER_LINALG_OP_GPU(\"MatrixSolve\", (MatrixSolveOpGpu<double>), double);\nREGISTER_LINALG_OP_GPU(\"MatrixSolve\", (MatrixSolveOpGpu<complex64>), complex64);\nREGISTER_LINALG_OP_GPU(\"MatrixSolve\", (MatrixSolveOpGpu<complex128>),\n                       complex128);\n\n#endif  // GOOGLE_CUDA\n\nREGISTER_LINALG_OP(\"MatrixSolve\", (MatrixSolveOp<float>), float);\nREGISTER_LINALG_OP(\"MatrixSolve\", (MatrixSolveOp<double>), double);\nREGISTER_LINALG_OP(\"MatrixSolve\", (MatrixSolveOp<complex64>), complex64);\nREGISTER_LINALG_OP(\"MatrixSolve\", (MatrixSolveOp<complex128>), complex128);\nREGISTER_LINALG_OP(\"BatchMatrixSolve\", (MatrixSolveOp<float>), float);\nREGISTER_LINALG_OP(\"BatchMatrixSolve\", (MatrixSolveOp<double>), double);\nREGISTER_LINALG_OP(\"BatchMatrixSolve\", (MatrixSolveOp<complex64>), complex64);\nREGISTER_LINALG_OP(\"BatchMatrixSolve\", (MatrixSolveOp<complex128>), complex128);\n}  // namespace tensorflow"