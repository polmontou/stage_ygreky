"/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/core/kernels/pooling_ops_common.h\"\n\n#include <vector>\n\n#include \"tensorflow/core/common_runtime/device.h\"\n#include \"tensorflow/core/framework/bounds_check.h\"\n#include \"tensorflow/core/framework/kernel_shape_util.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n\n#if GOOGLE_CUDA\n#include \"third_party/gpus/cudnn/cudnn.h\"\n#endif  // GOOGLE_CUDA\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#include \"tensorflow/core/kernels/conv_2d.h\"\n#include \"tensorflow/core/kernels/gpu_utils.h\"\n#if TENSORFLOW_USE_ROCM\n#include \"tensorflow/core/kernels/conv_ops_gpu.h\"\n#endif\n#include \"tensorflow/core/kernels/pooling_ops_common_gpu.h\"\n#include \"tensorflow/core/platform/stream_executor.h\"\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nnamespace tensorflow {\n\nnamespace {\n\ntemplate <typename T>\nstruct RawType {\n  using type = T;\n};\n\ntemplate <>\nstruct RawType<qint8> {\n  using type = int8;\n};\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\ntemplate <typename T>\nstruct PadInputWithNegativeInf {\n  Status operator()(const GPUDevice& d,\n                    typename TTypes<T, 4, int>::ConstTensor in,\n                    int input_pad_top, int input_pad_bottom, int input_pad_left,\n                    int input_pad_right, typename TTypes<T, 4, int>::Tensor out,\n                    TensorFormat format) {\n    T padding_value = -std::numeric_limits<T>::infinity();\n    functor::PadInput<GPUDevice, T, int, 4>()(\n        d, in, {{input_pad_top, input_pad_left}},\n        {{input_pad_bottom, input_pad_right}}, out, format, padding_value);\n    return Status::OK();\n  }\n};\n\ntemplate <>\nstruct PadInputWithNegativeInf<qint8> {\n  Status operator()(const GPUDevice& d,\n                    typename TTypes<qint8, 4, int>::ConstTensor in,\n                    int input_pad_top, int input_pad_bottom, int input_pad_left,\n                    int input_pad_right,\n                    typename TTypes<qint8, 4, int>::Tensor out,\n                    TensorFormat format) {\n    return errors::InvalidArgument(\n        \"Explicit padding not yet supported with qint8\");\n  }\n};\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n}  // namespace\n\nStatus CheckPaddingSize(int64_t window_rows, int64_t window_cols,\n                        int64_t pad_top, int64_t pad_bottom, int64_t pad_left,\n                        int64_t pad_right) {\n  if (!FastBoundsCheck(pad_top, window_rows)) {\n    return errors::InvalidArgument(\"Top padding \", pad_top,\n                                   \" needs to be smaller than the \"\n                                   \"window size \",\n                                   window_rows);\n  }\n  if (!FastBoundsCheck(pad_bottom, window_rows)) {\n    return errors::InvalidArgument(\"Bottom padding \", pad_bottom,\n                                   \" needs to be smaller than the \"\n                                   \"window size \",\n                                   window_rows);\n  }\n  if (!FastBoundsCheck(pad_left, window_cols)) {\n    return errors::InvalidArgument(\"Left padding \", pad_left,\n                                   \" needs to be smaller than the \"\n                                   \"window size \",\n                                   window_cols);\n  }\n  if (!FastBoundsCheck(pad_right, window_cols)) {\n    return errors::InvalidArgument(\"Right padding \", pad_right,\n                                   \" needs to be smaller than the \"\n                                   \"window size \",\n                                   window_cols);\n  }\n  return Status::OK();\n}\n\nPoolParameters::PoolParameters(OpKernelContext* context,\n                               const std::vector<int32>& ksize,\n                               const std::vector<int32>& stride,\n                               Padding padding,\n                               std::vector<int64_t> explicit_paddings,\n                               TensorFormat data_format,\n                               const TensorShape& tensor_in_shape) {\n  // For maxpooling, tensor_in should have 2 spatial dimensions.\n  // Note: the total number of dimensions could be 4 for NHWC, NCHW,\n  // or 5 for NCHW_VECT_C.\n  OP_REQUIRES(context,\n              GetTensorSpatialDims(tensor_in_shape.dims(), data_format) == 2,\n              errors::InvalidArgument(\n                  \"tensor_in_shape must have 2 spatial dimensions. \",\n                  tensor_in_shape.dims(), \" \", data_format));\n\n  this->data_format = data_format;\n  depth = GetTensorDim(tensor_in_shape, data_format, 'C') *\n          (data_format == FORMAT_NCHW_VECT_C ? 4 : 1);\n  tensor_in_cols = GetTensorDim(tensor_in_shape, data_format, 'W');\n  tensor_in_rows = GetTensorDim(tensor_in_shape, data_format, 'H');\n  tensor_in_batch = GetTensorDim(tensor_in_shape, data_format, 'N');\n  window_rows = GetTensorDim(ksize, data_format, 'H');\n  window_cols = GetTensorDim(ksize, data_format, 'W');\n  depth_window = GetTensorDim(ksize, data_format, 'C');\n  row_stride = GetTensorDim(stride, data_format, 'H');\n  col_stride = GetTensorDim(stride, data_format, 'W');\n  depth_stride = GetTensorDim(stride, data_format, 'C');\n\n  // We only support 2D pooling across width/height and depthwise\n  // pooling, not a combination.\n  OP_REQUIRES(context,\n              (depth_window == 1 || (window_rows == 1 && window_cols == 1)),\n              errors::Unimplemented(\n                  \"MaxPooling supports exactly one of pooling across depth \"\n                  \"or pooling across width/height.\"));\n  if (padding == Padding::EXPLICIT) {\n    OP_REQUIRES_OK(context, CheckValidPadding(padding, explicit_paddings,\n                                              /*num_dims=*/4, data_format));\n    GetExplicitPaddingForDim(explicit_paddings, data_format, 'H', &pad_top,\n                             &pad_bottom);\n    GetExplicitPaddingForDim(explicit_paddings, data_format, 'W', &pad_left,\n                             &pad_right);\n    OP_REQUIRES_OK(context, CheckPaddingSize(window_rows, window_cols, pad_top,\n                                             pad_bottom, pad_left, pad_right));\n  }\n\n  if (depth_window == 1) {\n    OP_REQUIRES_OK(context, GetWindowedOutputSizeVerbose(\n                                tensor_in_rows, window_rows, row_stride,\n                                padding, &out_height, &pad_top, &pad_bottom));\n    OP_REQUIRES_OK(context, GetWindowedOutputSizeVerbose(\n                                tensor_in_cols, window_cols, col_stride,\n                                padding, &out_width, &pad_left, &pad_right));\n    pad_depth = 0;\n    out_depth = depth;\n  } else {\n    OP_REQUIRES(context, depth_window > 0,\n                errors::InvalidArgument(\"depth_window must not be 0\"));\n    // Our current version of depthwise max pooling does not support\n    // any padding, and expects the depth_window to equal the\n    // depth_stride (no overlapping).\n    OP_REQUIRES(\n        context, depth % depth_window == 0,\n        errors::Unimplemented(\"Depthwise max pooling requires the depth \"\n                              \"window to evenly divide the input depth\"));\n    OP_REQUIRES(\n        context, depth_stride == depth_window,\n        errors::Unimplemented(\"Depthwise max pooling requires the depth \"\n                              \"window to equal the depth stride\"));\n\n    // The current version of depthwise max is only implemented on CPU.\n    OP_REQUIRES(context,\n                (DeviceType(static_cast<Device*>(context->device())\n                                ->attributes()\n                                .device_type()) == DeviceType(DEVICE_CPU)),\n                errors::Unimplemented(\"Depthwise max pooling is currently \"\n                                      \"only implemented for CPU devices.\"));\n\n    pad_depth = 0;\n    out_depth = depth / depth_window;\n  }\n}\n\nTensorShape PoolParameters::forward_output_shape() {\n  if (depth_window == 1) {\n    // Spatial pooling\n    return ShapeFromFormat(data_format, tensor_in_batch, out_height, out_width,\n                           depth);\n  } else {\n    // Depthwise pooling\n    return TensorShape(\n        {tensor_in_batch, tensor_in_rows, tensor_in_cols, out_depth});\n  }\n}\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\ntemplate <typename T>\nvoid DnnPoolingOp<T>::Compute(OpKernelContext* context,\n                              se::dnn::PoolingMode pooling_mode,\n                              const std::vector<int32>& size,\n                              const std::vector<int32>& stride, Padding padding,\n                              std::vector<int64_t> explicit_paddings,\n                              TensorFormat data_format, const Tensor& tensor_in,\n                              const TensorShape& tensor_out_shape,\n                              bool propagate_nans) {\n  Tensor* tensor_out = nullptr;\n  OP_REQUIRES_OK(context,\n                 context->allocate_output(0, tensor_out_shape, &tensor_out));\n  if (tensor_in.shape().num_elements() == 0) {\n    return;\n  }\n\n  PoolParameters params{\n      context,           size,        stride,           padding,\n      explicit_paddings, data_format, tensor_in.shape()};\n  if (!context->status().ok()) {\n    return;\n  }\n\n  int batch_size = params.tensor_in_batch;\n  int depth = params.depth;\n  int tensor_in_cols = params.tensor_in_cols;\n  int tensor_in_rows = params.tensor_in_rows;\n\n#if CUDNN_VERSION < 7300\n  /// Earlier versions do not support NHWC format, so we need to convert it\n  /// to NCHW before calling cudnn. We need to get rid of this once it is done\n  Tensor transformed_input;\n  if (data_format == FORMAT_NHWC) {\n    OP_REQUIRES_OK(context, context->allocate_temp(\n                                DataTypeToEnum<T>::value,\n                                ShapeFromFormat(FORMAT_NCHW, tensor_in.shape(),\n                                                data_format),\n                                &transformed_input));\n    functor::NHWCToNCHW<GPUDevice, T, 4>()(context->eigen_device<Device>(),\n                                           tensor_in.tensor<T, 4>(),\n                                           transformed_input.tensor<T, 4>());\n  } else {\n    transformed_input = tensor_in;\n  }\n  Tensor transformed_output;\n  if (data_format == FORMAT_NHWC) {\n    OP_REQUIRES_OK(context, context->allocate_temp(\n                                DataTypeToEnum<T>::value,\n                                ShapeFromFormat(FORMAT_NCHW, tensor_out_shape,\n                                                data_format),\n                                &transformed_output));\n  } else {\n    transformed_output = *tensor_out;\n  }\n  se::dnn::DataLayout data_layout = se::dnn::DataLayout::kBatchDepthYX;\n#else\n  Tensor transformed_input = tensor_in;\n  auto& transformed_output = *tensor_out;\n  se::dnn::DataLayout data_layout;\n  switch (data_format) {\n    case FORMAT_NHWC:\n      data_layout = se::dnn::DataLayout::kBatchYXDepth;\n      break;\n    case FORMAT_NCHW:\n      data_layout = se::dnn::DataLayout::kBatchDepthYX;\n      break;\n    case FORMAT_NCHW_VECT_C:\n      // NCHW_VECT_C is not supported by cudnnPoolingForward(), but can be\n      // emulated via NHWC.\n      data_layout = se::dnn::DataLayout::kBatchYXDepth;\n      batch_size *= depth / 4;\n      depth = 4;\n      break;\n    default:\n      OP_REQUIRES(context, false,\n                  errors::InvalidArgument(\"Unsupported format: \",\n                                          ToString(data_format)));\n  }\n#endif\n\n  int64_t vertical_padding = params.pad_top;\n  int64_t horizontal_padding = params.pad_left;\n\n  if (padding == EXPLICIT && (params.pad_top != params.pad_bottom ||\n                              params.pad_left != params.pad_right)) {\n    // cuDNN only supports padding the same amount on the left and right sides,\n    // and on the top and bottom sides. So we manually create a new padded\n    // input tensor such that we can pass it to cuDNN.\n    const int64_t common_padding_rows =\n        std::min(params.pad_top, params.pad_bottom);\n    const int64_t common_padding_cols =\n        std::min(params.pad_left, params.pad_right);\n\n    Tensor padded_input;\n    const int64_t padding_rows_diff =\n        std::abs(params.pad_top - params.pad_bottom);\n    const int64_t padding_cols_diff =\n        std::abs(params.pad_left - params.pad_right);\n\n    const int64_t new_in_rows = tensor_in_rows + padding_rows_diff;\n    const int64_t new_in_cols = tensor_in_cols + padding_cols_diff;\n\n    OP_REQUIRES_OK(\n        context,\n        context->allocate_temp(DataTypeToEnum<T>::value,\n                               ShapeFromFormat(data_format, batch_size,\n                                               new_in_rows, new_in_cols, depth),\n                               &padded_input));\n    const int64_t input_pad_top = params.pad_top - common_padding_rows;\n    const int64_t input_pad_bottom = params.pad_bottom - common_padding_rows;\n    const int64_t input_pad_left = params.pad_left - common_padding_cols;\n    const int64_t input_pad_right = params.pad_right - common_padding_cols;\n\n    bool in_bounds =\n        FastBoundsCheck(input_pad_top, std::numeric_limits<int>::max()) &&\n        FastBoundsCheck(input_pad_bottom, std::numeric_limits<int>::max()) &&\n        FastBoundsCheck(input_pad_left, std::numeric_limits<int>::max()) &&\n        FastBoundsCheck(input_pad_right, std::numeric_limits<int>::max());\n    if (!in_bounds) {\n      context->SetStatus(errors::InvalidArgument(\"Padding is too large.\"));\n      return;\n    }\n\n    // We need to call the const version of transformed_input.tensor()\n    const Tensor& const_transformed_input = transformed_input;\n    OP_REQUIRES_OK(\n        context,\n        PadInputWithNegativeInf<T>()(\n            context->eigen_device<GPUDevice>(),\n            To32Bit(const_transformed_input.tensor<T, 4>()),\n            static_cast<int>(input_pad_top), static_cast<int>(input_pad_bottom),\n            static_cast<int>(input_pad_left), static_cast<int>(input_pad_right),\n            To32Bit(padded_input.tensor<T, 4>()), data_format));\n    transformed_input = padded_input;\n    vertical_padding = common_padding_rows;\n    horizontal_padding = common_padding_cols;\n    tensor_in_rows = new_in_rows;\n    tensor_in_cols = new_in_cols;\n  }\n\n  se::dnn::PoolingDescriptor pooling_desc;\n  pooling_desc.set_pooling_mode(pooling_mode)\n      .set_window_height(params.window_rows)\n      .set_window_width(params.window_cols)\n      .set_vertical_stride(params.row_stride)\n      .set_horizontal_stride(params.col_stride)\n      .set_vertical_padding(vertical_padding)\n      .set_horizontal_padding(horizontal_padding)\n      .set_propagate_nans(propagate_nans);\n\n  se::dnn::BatchDescriptor input_desc;\n  input_desc.set_count(batch_size)\n      .set_height(tensor_in_rows)\n      .set_width(tensor_in_cols)\n      .set_feature_map_count(depth)\n      .set_layout(data_layout);\n\n  se::dnn::BatchDescriptor output_desc;\n  output_desc.set_count(batch_size)\n      .set_height(params.out_height)\n      .set_width(params.out_width)\n      .set_feature_map_count(depth)\n      .set_layout(data_layout);\n\n  auto input_data =\n      AsDeviceMemory(reinterpret_cast<const typename RawType<T>::type*>(\n                         transformed_input.template flat<T>().data()),\n                     transformed_input.template flat<T>().size());\n\n  auto output_data =\n      AsDeviceMemory(reinterpret_cast<const typename RawType<T>::type*>(\n                         transformed_output.template flat<T>().data()),\n                     transformed_output.template flat<T>().size());\n\n  auto* stream = context->op_device_context()->stream();\n  OP_REQUIRES(context, stream, errors::Internal(\"No GPU stream available.\"));\n\n#if TENSORFLOW_USE_ROCM\n  static int64 PoolingScratchSize = GetDnnWorkspaceLimit(\n      // default value is in bytes despite the name of the environment variable\n      \"TF_CUDNN_WORKSPACE_LIMIT_IN_MB\", 1LL << 32  // 4GB\n  );\n\n  DnnScratchAllocator scratch_allocator(PoolingScratchSize, context);\n  bool status =\n      stream\n          ->ThenPoolForward(pooling_desc, input_desc, input_data, output_desc,\n                            &output_data, &scratch_allocator)\n          .ok();\n#else\n  bool status = stream\n                    ->ThenPoolForward(pooling_desc, input_desc, input_data,\n                                      output_desc, &output_data)\n                    .ok();\n#endif\n  OP_REQUIRES(context, status,\n              errors::Internal(\"dnn PoolForward launch failed\"));\n#if CUDNN_VERSION < 7300\n  if (data_format == FORMAT_NHWC) {\n    /// Transform the output data from NCHW back to NHWC\n    auto toConstTensor = [](const Tensor& x) -> const Tensor { return x; };\n    using RT = typename RawType<T>::type;\n    functor::NCHWToNHWC<GPUDevice, RT, 4>()(\n        context->eigen_device<Device>(),\n        toConstTensor(transformed_output).template tensor<RT, 4>(),\n        tensor_out->tensor<RT, 4>());\n  }\n#endif\n}\n\n// Forward declarations of the functor specializations for GPU.\nnamespace functor {\n#define DECLARE_GPU_SPEC(T)                                             \\\n  template <>                                                           \\\n  void PadInput<GPUDevice, T, int, 4>::operator()(                      \\\n      const GPUDevice& d, typename TTypes<T, 4, int>::ConstTensor in,   \\\n      const std::array<int, 2>& padding_left,                           \\\n      const std::array<int, 2>& padding_right,                          \\\n      typename TTypes<T, 4, int>::Tensor out, TensorFormat data_format, \\\n      const T& padding_value);                                          \\\n  extern template struct PadInput<GPUDevice, T, int, 4>;\n\nDECLARE_GPU_SPEC(float);\nDECLARE_GPU_SPEC(Eigen::half);\nDECLARE_GPU_SPEC(double);\nDECLARE_GPU_SPEC(int32);\n}  // namespace functor\n\ntemplate <typename T>\nvoid DnnPoolingGradOp<T>::Compute(\n    OpKernelContext* context, se::dnn::PoolingMode pooling_mode,\n    const std::vector<int32>& size, const std::vector<int32>& stride,\n    Padding padding, std::vector<int64_t> explicit_paddings,\n    TensorFormat data_format, const Tensor* tensor_in, const Tensor* tensor_out,\n    const Tensor& out_backprop, const TensorShape& tensor_in_shape,\n    bool propagate_nans) {\n  CHECK((pooling_mode != se::dnn::PoolingMode::kMaximum) ||\n        (tensor_in && tensor_out))\n      << \"For MaxPoolGrad, both tensor_in and tensor_out needs to be \"\n         \"specified\";\n\n  Tensor* input_backprop = nullptr;\n  OP_REQUIRES_OK(context,\n                 context->allocate_output(0, tensor_in_shape, &input_backprop));\n  if (tensor_in_shape.num_elements() == 0) {\n    return;\n  }\n\n  PoolParameters params{context,           size,        stride,         padding,\n                        explicit_paddings, data_format, tensor_in_shape};\n  if (!context->status().ok()) {\n    return;\n  }\n\n  TensorFormat transformed_input_data_format = data_format;\n\n#if CUDNN_VERSION < 7300\n  /// For now, cudnn does not support NHWC format, so we need to convert it\n  /// to NCHW before calling cudnn. We need to get rid of this once it is done\n  Tensor transformed_input;\n  TensorShape transformed_input_shape;\n  if (data_format == FORMAT_NHWC || !tensor_in) {\n    transformed_input_shape =\n        ShapeFromFormat(FORMAT_NCHW, tensor_in_shape, data_format);\n    OP_REQUIRES_OK(context, context->allocate_temp(DataTypeToEnum<T>::value,\n                                                   transformed_input_shape,\n                                                   &transformed_input));\n  } else {\n    transformed_input = *tensor_in;\n  }\n  Tensor transformed_output;\n  TensorShape transformed_output_shape;\n  if (data_format == FORMAT_NHWC || !tensor_out) {\n    transformed_output_shape =\n        ShapeFromFormat(FORMAT_NCHW, out_backprop.shape(), data_format);\n    OP_REQUIRES_OK(context, context->allocate_temp(DataTypeToEnum<T>::value,\n                                                   transformed_output_shape,\n                                                   &transformed_output));\n  } else {\n    transformed_output = *tensor_out;\n  }\n  Tensor transformed_input_backprop;\n  if (data_format == FORMAT_NHWC) {\n    OP_REQUIRES_OK(context,\n                   context->allocate_temp(DataTypeToEnum<T>::value,\n                                          transformed_input_shape,\n                                          &transformed_input_backprop));\n  } else {\n    transformed_input_backprop = *input_backprop;\n  }\n  Tensor transformed_output_backprop;\n  if (data_format == FORMAT_NHWC) {\n    OP_REQUIRES_OK(context,\n                   context->allocate_temp(DataTypeToEnum<T>::value,\n                                          transformed_output_shape,\n                                          &transformed_output_backprop));\n  } else {\n    transformed_output_backprop = out_backprop;\n  }\n\n  if (data_format == FORMAT_NHWC) {\n    /// Convert the data from NHWC to NCHW if necessary.\n    if (tensor_in) {\n      // For AvgPoolGrad, the original input tensor is not necessary. However,\n      // cudnn still requires them to run, although they do not affect the\n      // results.\n      functor::NHWCToNCHW<GPUDevice, T, 4>()(context->eigen_device<Device>(),\n                                             tensor_in->tensor<T, 4>(),\n                                             transformed_input.tensor<T, 4>());\n      transformed_input_data_format = FORMAT_NCHW;\n    }\n    if (tensor_out) {\n      // For AvgPoolGrad, the original output tensor is not necessary. However,\n      // cudnn still requires them to run, although they do not affect the\n      // results.\n      functor::NHWCToNCHW<GPUDevice, T, 4>()(context->eigen_device<Device>(),\n                                             tensor_out->tensor<T, 4>(),\n                                             transformed_output.tensor<T, 4>());\n    }\n    functor::NHWCToNCHW<GPUDevice, T, 4>()(\n        context->eigen_device<Device>(), out_backprop.tensor<T, 4>(),\n        transformed_output_backprop.tensor<T, 4>());\n  }\n  se::dnn::DataLayout data_layout = se::dnn::DataLayout::kBatchDepthYX;\n#else\n  Tensor transformed_input;\n  if (!tensor_in) {\n    OP_REQUIRES_OK(context,\n                   context->allocate_temp(DataTypeToEnum<T>::value,\n                                          tensor_in_shape, &transformed_input));\n  } else {\n    transformed_input = *tensor_in;\n  }\n  Tensor transformed_output;\n  if (!tensor_out) {\n    OP_REQUIRES_OK(context, context->allocate_temp(DataTypeToEnum<T>::value,\n                                                   out_backprop.shape(),\n                                                   &transformed_output));\n  } else {\n    transformed_output = *tensor_out;\n  }\n  Tensor transformed_input_backprop = *input_backprop;\n  Tensor transformed_output_backprop = out_backprop;\n  se::dnn::DataLayout data_layout;\n  switch (data_format) {\n    case FORMAT_NHWC:\n      data_layout = se::dnn::DataLayout::kBatchYXDepth;\n      break;\n    case FORMAT_NCHW:\n      data_layout = se::dnn::DataLayout::kBatchDepthYX;\n      break;\n    default:\n      OP_REQUIRES(context, false,\n                  errors::InvalidArgument(\"Unsupported format: \",\n                                          ToString(data_format)));\n  }\n#endif  // CUDNN_VERSION < 7300\n\n  int64_t vertical_padding = params.pad_top;\n  int64_t horizontal_padding = params.pad_left;\n\n  int batch_size = params.tensor_in_batch;\n  int depth = params.depth;\n  int tensor_in_cols = params.tensor_in_cols;\n  int tensor_in_rows = params.tensor_in_rows;\n\n  int64_t input_pad_top = 0;\n  int64_t input_pad_bottom = 0;\n  int64_t input_pad_left = 0;\n  int64_t input_pad_right = 0;\n\n  Tensor transformed_and_padded_input_backprop;\n\n  if (padding == EXPLICIT && (params.pad_top != params.pad_bottom ||\n                              params.pad_left != params.pad_right)) {\n    // Pad the input in the same way we did during the forward pass, so that\n    // cuDNN or MIOpen receives the same input during the backward pass function\n    // as it did during the forward pass function.\n    const int64_t common_padding_rows =\n        std::min(params.pad_top, params.pad_bottom);\n    const int64_t common_padding_cols =\n        std::min(params.pad_left, params.pad_right);\n\n    Tensor padded_input;\n    const int64_t padding_rows_diff =\n        std::abs(params.pad_top - params.pad_bottom);\n    const int64_t padding_cols_diff =\n        std::abs(params.pad_left - params.pad_right);\n\n    const int64_t new_in_rows = tensor_in_rows + padding_rows_diff;\n    const int64_t new_in_cols = tensor_in_cols + padding_cols_diff;\n\n    VLOG(2) << \"Create new tensor: \"\n            << \" original rows=\" << tensor_in_rows\n            << \" original cols=\" << tensor_in_cols\n            << \" padding_rows=\" << new_in_rows\n            << \" padding_cols=\" << new_in_cols << \" depth= \" << depth\n            << \" batch_size=\" << batch_size << \" kernel_rows\"\n            << params.window_rows << \" kernel_col\" << params.window_cols\n            << \" stride_rows\" << params.row_stride;\n\n    OP_REQUIRES_OK(\n        context, context->allocate_temp(\n                     DataTypeToEnum<T>::value,\n                     ShapeFromFormat(transformed_input_data_format, batch_size,\n                                     new_in_rows, new_in_cols, depth),\n                     &padded_input));\n\n    OP_REQUIRES_OK(\n        context, context->allocate_temp(\n                     DataTypeToEnum<T>::value,\n                     ShapeFromFormat(transformed_input_data_format, batch_size,\n                                     new_in_rows, new_in_cols, depth),\n                     &transformed_and_padded_input_backprop));\n\n    input_pad_top = params.pad_top - common_padding_rows;\n    input_pad_bottom = params.pad_bottom - common_padding_rows;\n    input_pad_left = params.pad_left - common_padding_cols;\n    input_pad_right = params.pad_right - common_padding_cols;\n\n    bool in_bounds =\n        FastBoundsCheck(input_pad_top, std::numeric_limits<int>::max()) &&\n        FastBoundsCheck(input_pad_bottom, std::numeric_limits<int>::max()) &&\n        FastBoundsCheck(input_pad_left, std::numeric_limits<int>::max()) &&\n        FastBoundsCheck(input_pad_right, std::numeric_limits<int>::max());\n    if (!in_bounds) {\n      context->SetStatus(errors::InvalidArgument(\"Padding is too large.\"));\n      return;\n    }\n\n    // PadInputWithNegativeInf functor requires input to be a const.\n    const Tensor& const_transformed_input = transformed_input;\n    OP_REQUIRES_OK(\n        context,\n        PadInputWithNegativeInf<T>()(\n            context->eigen_device<GPUDevice>(),\n            To32Bit(const_transformed_input.tensor<T, 4>()),\n            static_cast<int>(input_pad_top), static_cast<int>(input_pad_bottom),\n            static_cast<int>(input_pad_left), static_cast<int>(input_pad_right),\n            To32Bit(padded_input.tensor<T, 4>()),\n            transformed_input_data_format));\n\n    transformed_input = padded_input;\n\n    vertical_padding = common_padding_rows;\n    horizontal_padding = common_padding_cols;\n    VLOG(2) << \"vertical padding set to: \" << vertical_padding\n            << \" horizontal padding set to: \" << horizontal_padding;\n    tensor_in_rows = new_in_rows;\n    tensor_in_cols = new_in_cols;\n  } else {\n    transformed_and_padded_input_backprop = transformed_input_backprop;\n  }\n\n  /// Get ready to call cudnn\n  se::dnn::PoolingDescriptor pooling_desc;\n  pooling_desc.set_pooling_mode(pooling_mode)\n      .set_window_height(params.window_rows)\n      .set_window_width(params.window_cols)\n      .set_vertical_stride(params.row_stride)\n      .set_horizontal_stride(params.col_stride)\n      .set_vertical_padding(vertical_padding)\n      .set_horizontal_padding(horizontal_padding)\n      .set_propagate_nans(propagate_nans);\n\n  se::dnn::BatchDescriptor orig_output_desc;\n  orig_output_desc.set_count(params.tensor_in_batch)\n      .set_height(params.out_height)\n      .set_width(params.out_width)\n      .set_feature_map_count(params.depth)\n      .set_layout(data_layout);\n\n  se::dnn::BatchDescriptor orig_input_desc;\n  orig_input_desc.set_count(params.tensor_in_batch)\n      .set_height(tensor_in_rows)\n      .set_width(tensor_in_cols)\n      .set_feature_map_count(params.depth)\n      .set_layout(data_layout);\n\n  auto orig_output_data =\n      AsDeviceMemory(transformed_output.template flat<T>().data(),\n                     transformed_output.template flat<T>().size());\n  auto orig_input_data =\n      AsDeviceMemory(transformed_input.template flat<T>().data(),\n                     transformed_input.template flat<T>().size());\n  auto output_backprop_data =\n      AsDeviceMemory(transformed_output_backprop.template flat<T>().data(),\n                     transformed_output_backprop.template flat<T>().size());\n  auto input_backprop_data = AsDeviceMemory(\n      transformed_and_padded_input_backprop.template flat<T>().data(),\n      transformed_and_padded_input_backprop.template flat<T>().size());\n\n  auto* stream = context->op_device_context()->stream();\n  OP_REQUIRES(context, stream, errors::Internal(\"No GPU stream available.\"));\n\n#if TENSORFLOW_USE_ROCM\n  static int64 PoolingScratchSize = GetDnnWorkspaceLimit(\n      // default value is in bytes despite the name of the environment variable\n      \"TF_CUDNN_WORKSPACE_LIMIT_IN_MB\", 1LL << 32  // 4GB\n  );\n\n  DnnScratchAllocator scratch_allocator(PoolingScratchSize, context);\n  bool status = stream\n                    ->ThenPoolBackward(pooling_desc, orig_input_desc,\n                                       orig_input_data, orig_output_desc,\n                                       orig_output_data, output_backprop_data,\n                                       &input_backprop_data, &scratch_allocator)\n                    .ok();\n#else\n  bool status =\n      stream\n          ->ThenPoolBackward(pooling_desc, orig_input_desc, orig_input_data,\n                             orig_output_desc, orig_output_data,\n                             output_backprop_data, &input_backprop_data)\n          .ok();\n#endif\n\n  OP_REQUIRES(context, status,\n              errors::Internal(\"dnn PoolBackward launch failed\"));\n\n  if (padding == EXPLICIT && (params.pad_top != params.pad_bottom ||\n                              params.pad_left != params.pad_right)) {\n    // Remove the padding that was added to the input shape above.\n    functor::PadInput<GPUDevice, T, int, 4>()(\n        context->eigen_device<GPUDevice>(),\n        To32Bit(const_cast<const Tensor&>(transformed_and_padded_input_backprop)\n                    .tensor<T, 4>()),\n        {{static_cast<int>(-input_pad_top), static_cast<int>(-input_pad_left)}},\n        {{static_cast<int>(-input_pad_bottom),\n          static_cast<int>(-input_pad_right)}},\n        To32Bit(transformed_input_backprop.template tensor<T, 4>()),\n        transformed_input_data_format, T{});\n  }\n\n#if CUDNN_VERSION < 7300\n  if (data_format == FORMAT_NHWC) {\n    /// Transform the output data from NCHW back to NHWC.\n    auto toConstTensor = [](const Tensor& x) -> const Tensor { return x; };\n    functor::NCHWToNHWC<GPUDevice, T, 4>()(\n        context->eigen_device<Device>(),\n        toConstTensor(transformed_input_backprop).template tensor<T, 4>(),\n        input_backprop->tensor<T, 4>());\n  }\n#endif  // CUDNN_VERSION < 7300\n}\n\n#define DEFINE_DNN_OPS(T)         \\\n  template class DnnPoolingOp<T>; \\\n  template class DnnPoolingGradOp<T>;\nTF_CALL_GPU_NUMBER_TYPES(DEFINE_DNN_OPS)\n\n#if CUDNN_VERSION >= 7300\ntemplate class DnnPoolingOp<qint8>;\n#endif\n\n#undef DEFINE_DNN_OPS\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n}  // namespace tensorflow"