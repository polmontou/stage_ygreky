"# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport functools\nimport itertools\nimport pickle\nimport re\nimport sys\nimport unittest\nimport weakref\n\nfrom absl.testing import parameterized\nfrom six.moves import range\n\nfrom tensorflow.python.autograph.core import converter\nfrom tensorflow.python.eager import backprop\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.eager import lift_to_graph\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import extension_type\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_spec\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.module import module\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import cond_v2\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import resource_variable_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.saved_model import save_context\nfrom tensorflow.python.saved_model import save_options\nfrom tensorflow.python.saved_model.load import load\nfrom tensorflow.python.saved_model.save import save\nfrom tensorflow.python.training.tracking.util import Checkpoint\n\n\ndef undecorated_function(x):\n  return x * 3.\n\n\nclass _HasDecoratedMethod(object):\n\n  @def_function.function\n  def f(self, x):\n    return x * 3.\n\n\nclass DefFunctionTest(test.TestCase, parameterized.TestCase):\n\n  def testNoVariables(self):\n\n    @def_function.function\n    def fn(x):\n      return 2 * x\n\n    self.assertAllEqual(fn(constant_op.constant(4.0)), 8.0)\n\n  def testFailIfVariablesAreCreatedMoreThanOnce(self):\n\n    @def_function.function\n    def fn(x):\n      return variables.Variable(1.0) + x\n\n    with self.assertRaises(ValueError):\n      fn(1.0)\n\n  def testFailIfVariablesAreCreatedMoreThanOnceNoWeakRef(self):\n    state = []\n\n    @def_function.function\n    def fn(x):\n      state.append(variables.Variable(1.0))\n      return state[-1] + x\n\n    with self.assertRaises(ValueError):\n      fn(1.0)\n\n  def testRange(self):\n\n    @def_function.function\n    def f(unused_x):\n      return 1.0\n\n    self.assertAllEqual(f(range(5)), 1.0)\n\n  def testCorrectVariableCreation(self):\n\n    state = []\n\n    @def_function.function\n    def fn(x):\n      if not state:\n        state.append(variables.Variable(2.0))\n      return state[0] * x\n\n    self.assertAllEqual(fn(constant_op.constant(1.0)), 2.0)\n    self.assertAllEqual(fn(constant_op.constant(3.0)), 6.0)\n\n  def testFunctionInitializer(self):\n\n    state = []\n\n    @def_function.function\n    def fn(x):\n      if not state:\n        state.append(variables.Variable(lambda: 2.0))\n      return state[0] * x\n\n    self.assertAllEqual(fn(constant_op.constant(1.0)), 2.0)\n\n  def testFunctionMultipleVariableInitializer(self):\n\n    state = []\n\n    @def_function.function\n    def fn(x):\n      if not state:\n        state.append(variables.Variable(lambda: 2.0))\n        state.append(variables.Variable(lambda: 5.0))\n      return state[0] * x, state[1] * x\n\n    self.assertAllEqual(fn(constant_op.constant(1.0)), [2.0, 5.0])\n\n  def testFunctionInitializationFunction(self):\n\n    state = []\n\n    @def_function.function\n    def fn(x):\n      if not state:\n        state.append(variables.Variable(2.0))\n      return state[0] * x\n\n    init_fn = fn.get_initialization_function(constant_op.constant(1.0))\n    self.assertLen(state, 1)\n    self.assertFalse(\n        resource_variable_ops.var_is_initialized_op(state[0].handle))\n    init_fn()\n    self.assertEqual(state[0].numpy(), 2.0)\n\n  def testVariableInitializerNotConstant(self):\n\n    state = []\n\n    @def_function.function\n    def fn(x):\n      if not state:\n        state.append(variables.Variable(2.0 * x))\n      return state[0] * x\n\n    self.assertAllEqual(fn(constant_op.constant(1.0)), 2.0)\n    self.assertAllEqual(fn(constant_op.constant(3.0)), 6.0)\n\n  def testLegacyGraphModeVariables(self):\n    with ops.Graph().as_default(), self.test_session() as sess:\n      state = []\n\n      @def_function.function\n      def fn(x):\n        if not state:\n          state.append(variables.Variable(2.0))\n        return state[0] * x\n\n      result = fn(3.0)\n\n      self.evaluate(variables.global_variables_initializer())\n      self.assertAllEqual(sess.run(state[0]), 2.0)\n      self.assertAllEqual(self.evaluate(result), 6.0)\n\n  def testLegacyGraphModeVariablesNonTrivialInitializer(self):\n    with ops.Graph().as_default(), self.test_session() as sess:\n      state = []\n\n      @def_function.function\n      def fn(x):\n        if not state:\n          two = constant_op.constant(2.0)\n          four = two * two\n          two_again = math_ops.sqrt(four)\n          state.append(variables.Variable(two_again + four))\n        return state[0] * x\n\n      result = fn(3.0)\n\n      self.evaluate(variables.global_variables_initializer())\n      self.assertAllEqual(sess.run(state[0]), 6.0)\n      self.assertAllEqual(self.evaluate(result), 18.0)\n\n  def testLegacyGraphModeInputDependentInitializerFails(self):\n    with ops.Graph().as_default():\n      state = []\n\n      @def_function.function\n      def fn(x):\n        if not state:\n          state.append(variables.Variable(2.0 * x))\n        return state[0] * x\n\n      with self.assertRaisesRegex(lift_to_graph.UnliftableError,\n                                  r'transitively.* mul .* x'):\n        fn(constant_op.constant(3.0))\n\n  def testMethod(self):\n\n    class MyModel(object):\n\n      def __init__(self):\n        self.var = None\n\n      @def_function.function\n      def apply(self, x):\n        if self.var is None:\n          self.var = variables.Variable(2.0)\n        return self.var * x\n\n    m0 = MyModel()\n    self.assertAllEqual(m0.apply(3.0), 6.0)\n    # Calling twice to exercise that we do not recreate variables.\n    m0.var.assign(3.0)\n    self.assertAllEqual(m0.apply(3.0), 9.0)\n\n    m1 = MyModel()\n    self.assertAllEqual(m1.apply(3.0), 6.0)\n\n  @unittest.expectedFailure\n  def testMethodAllowDynamicVariableWithoutGuards(self):\n\n    class Foo:\n\n      def __init__(self):\n        self._var = 0\n\n      def __call__(self, val):\n        self.compute(val)\n        return self._var\n\n      @def_function.function\n      def compute(self, val):\n        self._var = variables.Variable(val)\n\n    def_function.ALLOW_DYNAMIC_VARIABLE_CREATION = True\n    foo = Foo()\n    self.assertAllEqual(foo(0.3), 0.3)\n    self.assertAllEqual(\n        foo(0.9), 0.9, 'https://github.com/tensorflow/tensorflow/issues/27120')\n\n  def testMethodAllowDynamicVariable(self):\n\n    class Foo:\n\n      def __init__(self):\n        self._flag_keyed_vars = {}\n        self.trace_count = 0\n\n      def __call__(self, var_creation_flag):\n        self.compute(var_creation_flag)\n        return self._flag_keyed_vars[var_creation_flag]\n\n      @def_function.function\n      def compute(self, var_creation_flag):\n        self.trace_count += 1\n        if var_creation_flag not in self._flag_keyed_vars:\n          if var_creation_flag:\n            self._flag_keyed_vars[var_creation_flag] = variables.Variable(1.0)\n          else:\n            self._flag_keyed_vars[var_creation_flag] = variables.Variable(2.0)\n\n    def_function.ALLOW_DYNAMIC_VARIABLE_CREATION = True\n    foo = Foo()\n    self.assertAllEqual(foo(True), 1.0)\n    self.assertEqual(foo.trace_count, 2)\n    self.assertAllEqual(foo(True), 1.0)\n    self.assertEqual(foo.trace_count, 2)\n    self.assertAllEqual(foo(False), 2.0)\n    self.assertEqual(foo.trace_count, 3)\n\n  def testMethodNotAllowDynamicVariable(self):\n\n    class Foo:\n\n      def __init__(self):\n        self._flag_keyed_vars = {}\n        self.trace_count = 0\n\n      def __call__(self, var_creation_flag):\n        self.compute(var_creation_flag)\n        return self._flag_keyed_vars[var_creation_flag]\n\n      @def_function.function\n      def compute(self, var_creation_flag):\n        self.trace_count += 1\n        if var_creation_flag not in self._flag_keyed_vars:\n          if var_creation_flag:\n            self._flag_keyed_vars[var_creation_flag] = variables.Variable(1.0)\n          else:\n            self._flag_keyed_vars[var_creation_flag] = variables.Variable(2.0)\n\n    def_function.ALLOW_DYNAMIC_VARIABLE_CREATION = False\n    foo = Foo()\n    self.assertAllEqual(foo(True), 1.0)\n    self.assertEqual(foo.trace_count, 2)\n    self.assertAllEqual(foo(True), 1.0)\n    self.assertEqual(foo.trace_count, 2)\n    msg = 'singleton tf.Variable.*on the first call'\n    with self.assertRaisesRegex(ValueError, msg):\n      foo(False)\n    self.assertEqual(foo.trace_count, 3)\n\n  def testMethodExtensionType(self):\n\n    class MaskedTensor(extension_type.ExtensionType):\n      values: ops.Tensor\n      mask: ops.Tensor\n\n      @def_function.function\n      def with_default(self, default_value):\n        return array_ops.where_v2(self.mask, self.values, default_value)\n\n      @def_function.function\n      def sum(self):\n        # Use a loop & conditional to test that autograph works correctly.\n        result = 0\n        for i in range(array_ops.size(self.values)):\n          if self.mask[i]:\n            result += self.values[i]\n        return result\n\n    mt = MaskedTensor([1, 2, 3], [True, False, True])\n    self.assertAllEqual(mt.with_default(-1), [1, -1, 3])\n    self.assertAllEqual(mt.sum(), 4)\n\n  def test_functools_partial(self):\n    self.assertAllClose(\n        3.,\n        def_function.function(functools.partial(lambda x, y: x + y, 1.))(\n            constant_op.constant(2.)))\n\n  def test_functools_partial_new_default(self):\n    def f(x=3, y=7):\n      return x + y\n\n    func = def_function.function(functools.partial(f, y=6))\n    self.assertEqual(func().numpy(), 9)\n    self.assertEqual(func(y=8).numpy(), 11)\n\n  def test_functools_partial_keywords(self):\n    def f(x, y):\n      return x + y\n\n    func = def_function.function(\n        functools.partial(f, x=array_ops.zeros([1]), y=array_ops.zeros([1])))\n    self.assertAllEqual(func(), [0.0])\n\n  def test_functools_partial_single_positional(self):\n    def f(x, y):\n      return x + y\n\n    func = def_function.function(\n        functools.partial(f, constant_op.constant(1)))\n    self.assertAllEqual(func(5), 6)\n\n  def test_complicated_partial_with_defaults(self):\n\n    def identity(*args):\n      return args\n\n    def dynamic_unroll(core_fn,\n                       input_sequence,\n                       initial_state,\n                       sequence_length=None,\n                       parallel_iterations=1,\n                       swap_memory=False):\n      del core_fn\n      self.assertIs(None, sequence_length)\n      self.assertEqual(1, parallel_iterations)\n      self.assertTrue(swap_memory)\n      return input_sequence, initial_state\n\n    input_sequence = random_ops.random_uniform([1, 1, 1])\n    initial_state = random_ops.random_uniform([1, 1])\n\n    func = def_function.function(\n        functools.partial(dynamic_unroll, identity, swap_memory=True))\n    func(input_sequence, initial_state)\n\n  def test_unspecified_default_argument(self):\n    wrapped = def_function.function(\n        lambda x, y=2: x + y,\n        input_signature=[tensor_spec.TensorSpec((), dtypes.int32)])\n    self.assertEqual(3, wrapped(constant_op.constant(1)).numpy())\n\n  def test_concrete_function_from_signature(self):\n\n    @def_function.function(\n        input_signature=[tensor_spec.TensorSpec(None, dtypes.float32)])\n    def compute(x):\n      return 2. * x\n\n    concrete = compute.get_concrete_function()\n    self.assertAllClose(1., concrete(constant_op.constant(0.5)))\n    concrete = compute.get_concrete_function(\n        tensor_spec.TensorSpec(None, dtypes.float32))\n    self.assertAllClose(4., concrete(constant_op.constant(2.)))\n    signature_args, _ = concrete.structured_input_signature\n    self.assertEqual(signature_args,\n                     (tensor_spec.TensorSpec(\n                         None, dtypes.float32, name='x'),))\n\n  def testInputSignatureMissingTensorSpecsMethod(self):\n\n    class MyModule(module.Module):\n\n      def f1(self, arg1, arg2, arg3):\n        pass\n\n      def f2(self, arg1, arg2, arg3, **kwargs):\n        pass\n\n      def f3(self, arg1, arg2, arg3, arg4=4, **kwargs):\n        pass\n\n      def f4(self, arg1, arg2, arg3, *args):\n        pass\n\n      def f5(self, arg1, arg2, arg3, *args, **kwargs):\n        pass\n\n      def f6(self, arg1, arg4=4, **kwargs):\n        return arg1 + arg4\n\n    m = MyModule()\n    tf_func_dec = def_function.function(\n        input_signature=(tensor_spec.TensorSpec([], dtypes.int32),))\n    error_msg = 'TensorSpecs are still required.*arg2.*arg3'\n    with self.assertRaisesRegex(TypeError, error_msg):\n      tf_func_dec(m.f1)(1, 2, 3)\n\n    with self.assertRaisesRegex(TypeError, error_msg):\n      tf_func_dec(m.f2)(1, 2, 3)\n\n    with self.assertRaisesRegex(TypeError, error_msg):\n      tf_func_dec(m.f3)(1, 2, 3)\n\n    with self.assertRaisesRegex(TypeError, error_msg):\n      tf_func_dec(m.f4)(1, 2, 3)\n\n    with self.assertRaisesRegex(TypeError, error_msg):\n      tf_func_dec(m.f5)(1, 2, 3)\n\n    self.assertEqual(tf_func_dec(m.f6)(1).numpy(), 5)\n\n  def testInputSignatureMissingTensorSpecsFunction(self):\n    tf_func_dec = def_function.function(\n        input_signature=(tensor_spec.TensorSpec([], dtypes.int32),))\n    error_msg = 'TensorSpecs are still required.*arg2.*arg3'\n    # pylint: disable=unused-argument\n    def f1(arg1, arg2, arg3):\n      pass\n\n    with self.assertRaisesRegex(TypeError, error_msg):\n      tf_func_dec(f1)(1, 2, 3)\n\n    def f2(arg1, arg2, arg3, **kwargs):\n      pass\n\n    with self.assertRaisesRegex(TypeError, error_msg):\n      tf_func_dec(f2)(1, 2, 3)\n\n    def f3(arg1, arg2, arg3, arg4=4, **kwargs):\n      pass\n\n    with self.assertRaisesRegex(TypeError, error_msg):\n      tf_func_dec(f3)(1, 2, 3)\n\n    def f4(arg1, arg2, arg3, *args):\n      pass\n\n    with self.assertRaisesRegex(TypeError, error_msg):\n      tf_func_dec(f4)(1, 2, 3)\n\n    def f5(arg1, arg2, arg3, *args, **kwargs):\n      pass\n\n    with self.assertRaisesRegex(TypeError, error_msg):\n      tf_func_dec(f5)(1, 2, 3)\n    # pyline: enable=unused-argument\n\n    def f6(arg1, arg4=4, **kwargs):\n      return arg1 + arg4\n    self.assertEqual(tf_func_dec(f6)(1).numpy(), 5)\n\n  def testInputSignatureMissingTensorSpecsLambdaFunction(self):\n    tf_func_dec = def_function.function(\n        input_signature=(tensor_spec.TensorSpec([], dtypes.int32),))\n    error_msg = 'TensorSpecs are still required.*arg2.*arg3'\n    with self.assertRaisesRegex(TypeError, error_msg):\n      tf_func_dec(lambda ar1, arg2, arg3: None)(1, 2, 3)\n\n    with self.assertRaisesRegex(TypeError, error_msg):\n      tf_func_dec(lambda arg1, arg2, arg3, **kwargs: None)(1, 2, 3)\n\n    with self.assertRaisesRegex(TypeError, error_msg):\n      tf_func_dec(lambda arg1, arg2, arg3, arg4=4, **kwargs: None)(1, 2, 3)\n\n    with self.assertRaisesRegex(TypeError, error_msg):\n      tf_func_dec(lambda arg1, arg2, arg3, *args: None)(1, 2, 3)\n\n    with self.assertRaisesRegex(TypeError, error_msg):\n      tf_func_dec(lambda arg1, arg2, arg3, *args, **kwargs: None)(1, 2, 3)\n\n    self.assertEqual(\n        tf_func_dec(lambda arg1, arg4=4, **kwargs: arg1 + arg4)(1).numpy(), 5)\n\n  @parameterized.named_parameters(('_method', 'method'),\n                                  ('_function', 'function'),\n                                  ('_lambda_function', 'lambda_function'))\n  def testInputSignaturePartialFuncMissingTensorSpecs(self, func_type):\n    if func_type == 'method':\n      class MyModule(module.Module):\n\n        def f(self, arg1, arg2, arg3, arg4=4):\n          return arg1 + arg2 + arg3 + arg4\n      f = MyModule().f\n    elif func_type == 'function':\n      def f(arg1, arg2, arg3, arg4=4):\n        return arg1 + arg2 + arg3 + arg4\n    else:  # lambda_function\n      f = lambda arg1, arg2, arg3, arg4=4: arg1 + arg2 + arg3 + arg4\n\n    tf_func_dec = def_function.function(\n        input_signature=(tensor_spec.TensorSpec([], dtypes.int32),))\n    with self.assertRaisesRegex(TypeError,\n                                'TensorSpecs are still required.*arg3'):\n      tf_func_dec(functools.partial(f, 1))(2, 3)\n\n    with self.assertRaisesRegex(TypeError,\n                                'TensorSpecs are still required.*arg2.*arg3'):\n      tf_func_dec(functools.partial(f, arg4=5))(1, 2, 3)\n\n    with self.assertRaisesRegex(TypeError,\n                                'TensorSpecs are still required.*arg3'):\n      tf_func_dec(functools.partial(f, 1, arg4=5))(2, 3)\n\n    self.assertAllEqual(tf_func_dec(functools.partial(f, 1, 2, arg4=5))(3),\n                        array_ops.constant(11))\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_variable_naming(self):\n    class HasVars(module.Module):\n\n      def __init__(self):\n        self.x = None\n        self.y = None\n        self.z = None\n\n      @def_function.function\n      def make_x(self):\n        if self.x is None:\n          self.x = variables.Variable(1., name='v')\n\n      def make_y(self):\n        if self.y is None:\n          self.y = variables.Variable(1., name='v')\n\n      def make_z(self):\n        if self.z is None:\n          with ops.name_scope('z_scope', skip_on_eager=False):\n            self.z = variables.Variable(1., name='z')\n\n    root = HasVars()\n    root.make_x()\n    root.make_y()\n    root.make_z()\n    self.assertEqual('v:0', root.x.name)\n    self.assertEqual('z_scope/z:0', root.z.name)\n\n  def test_concrete_function_keyword_arguments(self):\n    @def_function.function\n    def f(x):\n      return x\n\n    conc = f.get_concrete_function(\n        tensor_spec.TensorSpec(None, dtypes.float32, 'y'))\n    conc(y=constant_op.constant(3.0))\n    signature_args, _ = conc.structured_input_signature\n    self.assertEqual('y', signature_args[0].name)\n\n    conc = f.get_concrete_function(tensor_spec.TensorSpec(None, dtypes.float32))\n    conc(x=constant_op.constant(3.0))\n    signature_args, _ = conc.structured_input_signature\n    self.assertEqual('x', signature_args[0].name)\n\n    @def_function.function\n    def g(x):\n      return x[0]\n\n    conc = g.get_concrete_function(\n        [tensor_spec.TensorSpec(None, dtypes.float32, 'z'), 2])\n    conc(z=constant_op.constant(3.0))\n    signature_args, _ = conc.structured_input_signature\n    self.assertEqual('z', signature_args[0][0].name)\n\n  def testRuntimeErrorNotSticky(self):\n\n    @def_function.function\n    def fail(i):\n      control_flow_ops.Assert(math_ops.equal(i, 0), ['ick'])\n\n    fail(constant_op.constant(0))  # OK\n    with self.assertRaises(errors.InvalidArgumentError):\n      fail(constant_op.constant(1))  # InvalidArgument: \"ick\"\n    fail(constant_op.constant(0))  # OK\n\n  def testUnderscoreName(self):\n\n    @def_function.function\n    def f(_):\n      return _ + _\n\n    self.assertAllEqual(2.0, f(constant_op.constant(1.0)))\n\n  def test_serialization_signature_cache(self):\n\n    @def_function.function\n    def f(x, y):\n      return x, y\n\n    f(constant_op.constant([[3., 4.]]), constant_op.constant([2.]))\n    f(constant_op.constant([[3, 4, 5]]), constant_op.constant([2]))\n\n    signatures_args = set()\n    concrete_functions = f._list_all_concrete_functions_for_serialization()\n    for concrete_function in concrete_functions:\n      args, kwargs = concrete_function.structured_input_signature\n      signatures_args.add(args)\n      self.assertEqual(dict(), kwargs)\n\n    self.assertEqual(\n        signatures_args,\n        set(((tensor_spec.TensorSpec([1, 2], dtypes.float32, name='x'),\n              tensor_spec.TensorSpec([1], dtypes.float32, name='y')),\n             (tensor_spec.TensorSpec([1, 3], dtypes.int32, name='x'),\n              tensor_spec.TensorSpec([1], dtypes.int32, name='y')))))\n\n  @test_util.assert_no_garbage_created\n  def testFunctionReferenceCycles(self):\n    fn = def_function.function(lambda x: 2. * x)\n    fn(constant_op.constant(4.0))\n    weak_fn = weakref.ref(fn)\n    del fn\n    # Tests that the weak reference we made to the function is now dead, which\n    # means the object has been deleted. This should be true as long as the\n    # function itself is not involved in a reference cycle.\n    self.assertIs(None, weak_fn())\n\n  @test_util.assert_no_garbage_created\n  def testMethodReferenceCycles(self):\n    has_decorated_method = _HasDecoratedMethod()\n    has_decorated_method.f(constant_op.constant(5.))\n    weak_fn = weakref.ref(has_decorated_method.f)\n    del has_decorated_method\n    # Tests that the weak reference we made to the function is now dead, which\n    # means the object has been deleted. This should be true as long as the\n    # function itself is not involved in a reference cycle.\n    self.assertIs(None, weak_fn())\n\n  @test_util.assert_no_new_pyobjects_executing_eagerly\n  def testErrorMessageWhenGraphTensorIsPassedToEager(self):\n\n    @def_function.function\n    def failing_function():\n      a = constant_op.constant(1.)\n\n      with ops.init_scope():\n        _ = a + a\n\n    with self.assertRaisesRegex(\n        TypeError, re.compile('def_function_test.*out of scope', re.DOTALL)):\n      failing_function()\n\n  def testSymbolicTensorIllegalCaptureCallTimeError(self):\n    x = None\n\n    @def_function.function\n    def f1(a):\n      nonlocal x\n      x = a\n      return a\n\n    @def_function.function\n    def f2(b):\n      return b + x\n\n    f1(constant_op.constant(1))\n    with self.assertRaisesRegex(\n        TypeError, re.compile('def_function_test.*out of scope', re.DOTALL)):\n      f2(constant_op.constant(2))\n\n  def testSymbolicTensorIllegalCaptureTraceTimeError(self):\n\n    @def_function.function\n    def f(inputs):\n      num_steps, _ = inputs.shape[:2]\n      outputs = []\n      for t in math_ops.range(num_steps):\n        outputs.append(inputs[t])\n      return outputs\n\n    with self.assertRaisesRegex(errors.InaccessibleTensorError, 'out of scope'):\n      f(array_ops.zeros(shape=(8, 42, 3)))\n\n  def testNonUniqueNamesGetConcreteFunction(self):\n    @def_function.function\n    def non_unique_arg_names(x, **kwargs):\n      a, b, c = x\n      d = kwargs['d']\n      return a + b + c + d\n\n    concrete = non_unique_arg_names.get_concrete_function(\n        (tensor_spec.TensorSpec(None, dtypes.float32),\n         tensor_spec.TensorSpec(None, dtypes.float32),\n         tensor_spec.TensorSpec(None, dtypes.float32)),\n        d=tensor_spec.TensorSpec(None, dtypes.float32))\n    self.assertAllClose(\n        10.,\n        concrete(x=constant_op.constant(1.),\n                 x_1=constant_op.constant(2.),\n                 x_2=constant_op.constant(3.),\n                 d=constant_op.constant(4.)))\n    self.assertAllClose(\n        10.,\n        concrete(constant_op.constant(1.),\n                 constant_op.constant(2.),\n                 constant_op.constant(3.),\n                 constant_op.constant(4.)))\n\n  def testVariableCreatorScope(self):\n    created_variables = []\n    captured_variables = []\n\n    @def_function.function\n    def f():\n      if not created_variables:\n        created_variables.append(variables.Variable(1.))\n      return created_variables[0] + 1.\n\n    def capture_creator(next_creator, **kwargs):\n      created = next_creator(**kwargs)\n      captured_variables.append(created)\n      return created\n\n    with variable_scope.variable_creator_scope(capture_creator):\n      f()\n    self.assertEqual(created_variables, captured_variables)\n\n  def testVarAlreadyInitializedNoClobbering(self):\n    v_holder = []\n\n    @def_function.function\n    def add_var(x):\n      if not v_holder:\n        v = variables.Variable([1., 2.])\n        v_holder.append(v)\n        already_initialized = variables.Variable(3.)\n        with ops.init_scope():\n          already_initialized.assign(10.)\n        v_holder.append(already_initialized)\n      return v_holder[0] + v_holder[1] + x\n\n    add_var.get_concrete_function(constant_op.constant(2.))\n    self.assertAllClose([13., 14.], add_var(constant_op.constant(2.)))\n\n  def testSameVariableTwice(self):\n    v = variables.Variable(1.0)\n\n    @def_function.function\n    def add(a, b):\n      return a + b\n\n    self.assertAllEqual(add(v, v), 2.0)\n\n  def testVariableUpdate(self):\n    v1 = variables.Variable(1.0)\n    v2 = variables.Variable(2.0)\n    v3 = variables.Variable(4, dtype=dtypes.int32)\n\n    trace_count = [0]\n\n    @def_function.function\n    def double_variable(x):\n      trace_count[0] += 1\n      x.assign_add(x.read_value())\n\n    self.assertEqual(trace_count[0], 0)\n    double_variable(v1)\n    self.assertEqual(trace_count[0], 1)\n    self.assertEqual(self.evaluate(v1), 2.0)\n    double_variable(v2)\n    # No retracing because v2's data type and shape are the same as v1\n    self.assertEqual(trace_count[0], 1)\n    self.assertEqual(self.evaluate(v2), 4.0)\n    double_variable(v3)\n    # Retracing because of data type change\n    self.assertEqual(trace_count[0], 2)\n    self.assertEqual(self.evaluate(v3), 8)\n\n  def testShapeCache(self):\n    @def_function.function\n    def func(x):\n      return 2 * x\n\n    func_a = func.get_concrete_function(\n        tensor_spec.TensorSpec([None], dtypes.int32))\n    func_b = func.get_concrete_function(\n        tensor_spec.TensorSpec([None], dtypes.int32))\n\n    self.assertIs(func_a, func_b)\n\n  def testCacheWithinSaveContext(self):\n\n    @def_function.function\n    def func(x):\n      return 2 * x\n\n    func_a = func.get_concrete_function(constant_op.constant(2.))\n    func_b = func.get_concrete_function(constant_op.constant(2.))\n\n    self.assertIs(func_a, func_b)\n\n    with save_context.save_context(\n        save_options.SaveOptions(experimental_variable_policy=save_options\n                                 .VariablePolicy.EXPAND_DISTRIBUTED_VARIABLES)):\n      func_c = func.get_concrete_function(constant_op.constant(2.))\n\n    with save_context.save_context(\n        save_options.SaveOptions(\n            experimental_variable_policy=save_options.VariablePolicy.NONE)):\n      func_d = func.get_concrete_function(constant_op.constant(2.))\n\n    self.assertIsNot(func_a, func_c)\n    self.assertIsNot(func_a, func_d)\n\n  def testInitializationInNestedCall(self):\n    v_holder = []\n\n    @def_function.function\n    def add_var(x):\n      if not v_holder:\n        v = variables.Variable([1., 2.])\n        v_holder.append(v)\n        already_initialized = variables.Variable(3.)\n        with ops.init_scope():\n          already_initialized.assign(10.)\n        v_holder.append(already_initialized)\n      return v_holder[0] + v_holder[1] + x\n\n    @def_function.function\n    def wrapper(x):\n      return add_var(x)\n\n    self.assertAllClose([13., 14.], wrapper(constant_op.constant(2.)))\n    v_holder[1].assign(11.)\n    self.assertAllClose([14., 15.], wrapper(constant_op.constant(2.)))\n\n  @test_util.run_gpu_only\n  def testDeviceAnnotationRespected(self):\n    a = []\n\n    @def_function.function()\n    def create_variable():\n      with ops.init_scope():\n        initial_value = random_ops.random_uniform(\n            (2, 2), maxval=1000000, dtype=dtypes.int64)\n\n      if not a:\n        with ops.device('CPU:0'):\n          a.append(resource_variable_ops.ResourceVariable(initial_value))\n\n      return a[0].read_value()\n\n    create_variable()\n    self.assertRegex(a[0].device, 'CPU')\n\n  @test_util.run_gpu_only\n  def testDeviceAnnotationForInitializerRespected(self):\n    a = []\n    initial_value = []\n\n    def initial_value_fn():\n      initial_value.append(random_ops.random_uniform((2, 3)))\n      return initial_value[0]\n\n    @def_function.function()\n    def create_variable():\n      with ops.init_scope():\n        if not a:\n          a.append(variables.Variable(initial_value_fn))\n\n    with ops.device('CPU:0'):\n      create_variable()\n    self.assertRegex(a[0].device, 'CPU')\n    self.assertRegex(initial_value[0].device, 'CPU')\n\n  def testDecorate(self):\n    func = def_function.function(lambda: 1)\n    def decorator(f):\n      return lambda: 1 + f()\n\n    func._decorate(decorator)\n    self.assertEqual(func().numpy(), 2)\n\n  @parameterized.parameters(*itertools.product(\n      (None, (tensor_spec.TensorSpec([]),)),  # input_signature\n      (True, False),                          # autograph\n      (None, converter.Feature.ALL),          # autograph_options\n      (None, 'foo.bar'),                      # implements\n      (None, True, False),                    # relax_shapes\n      (True, False),                          # compile\n      (True, False),                          # override_function\n  ))\n\n  def testClone(self, input_signature, autograph, autograph_options, implements,\n                relax_shapes, compile_, override_function):\n    original_py_function = lambda x: x\n\n    compile_ = False\n    func = def_function.function(\n        func=original_py_function,\n        input_signature=input_signature,\n        autograph=autograph,\n        experimental_implements=implements,\n        experimental_autograph_options=autograph_options,\n        experimental_relax_shapes=relax_shapes,\n        jit_compile=compile_)\n\n    if override_function:\n      cloned_py_function = lambda x: x + 1\n    else:\n      cloned_py_function = original_py_function\n\n    cloned = func._clone(python_function=cloned_py_function)\n\n    self.assertEqual(cloned_py_function, cloned._python_function)\n    self.assertEqual(func._name, cloned._name)\n    self.assertEqual(input_signature, cloned._input_signature)\n    self.assertEqual(autograph, cloned._autograph)\n    self.assertEqual(implements, cloned._implements)\n    self.assertEqual(autograph_options, cloned._experimental_autograph_options)\n    self.assertEqual(relax_shapes, cloned._experimental_relax_shapes)\n    self.assertEqual(compile_, cloned._jit_compile)\n\n    # This test does not run with XLA JIT support linked in so we can only check\n    # the output of the function if compile is disabled.\n    if not compile_:\n      x = array_ops.zeros([])\n      self.assertEqual(self.evaluate(cloned(x)),\n                       self.evaluate(cloned_py_function(x)))\n\n  def testLiftPlaceholderInitializedVariable(self):\n    with ops.Graph().as_default():\n      var_list = []\n\n      @def_function.function\n      def use_variable():\n        if not var_list:\n          initial_value = array_ops.placeholder(shape=[], dtype=dtypes.float32)\n          v = variables.Variable(initial_value)\n          var_list.append(v)\n        return var_list[0] + 1.\n\n      var_plus_one = use_variable()\n      with self.session() as session:\n        init_op = var_list[0].initializer\n        session.run(init_op, feed_dict={init_op.inputs[1]: 2.})\n        self.assertEqual(3., session.run(var_plus_one))\n\n  def testDecorate_rejectedAfterTrace(self):\n    func = def_function.function(lambda: 1)\n    self.assertEqual(func().numpy(), 1)\n    msg = 'Functions cannot be decorated after they have been traced.'\n    with self.assertRaisesRegex(ValueError, msg):\n      func._decorate(lambda f: f)\n\n  def testGetConcreteFunctionGraphLifetime(self):\n\n    @def_function.function\n    def func():\n      pass\n\n    graph = func.get_concrete_function().graph\n    del func\n\n    # If the graph is deleted, then an exception is raised on reading `captures`\n    self.assertEmpty(graph.captures)\n\n  @parameterized.parameters(*itertools.product(\n      (None, (tensor_spec.TensorSpec([]),)),  # input_signature\n      (True, False),  # autograph\n      (None, converter.Feature.ALL),  # autograph_options\n      (None, 'foo.bar'),  # implements\n      (None, True, False),  # relax_shapes\n  ))\n\n  def test_pickle(self, input_signature, autograph, autograph_options,\n                  implements, relax_shapes):\n    \"\"\"@function objects can be pickled and unpickled.\"\"\"\n    original_py_function = undecorated_function\n\n    func = def_function.function(\n        func=original_py_function,\n        input_signature=input_signature,\n        autograph=autograph,\n        experimental_implements=implements,\n        experimental_autograph_options=autograph_options,\n        experimental_relax_shapes=relax_shapes,\n    )\n\n    cloned = pickle.loads(pickle.dumps(func))\n\n    self.assertEqual(func._name, cloned._name)\n    self.assertEqual(input_signature, cloned._input_signature)\n    self.assertEqual(autograph, cloned._autograph)\n    self.assertEqual(implements, cloned._implements)\n    self.assertEqual(autograph_options, cloned._experimental_autograph_options)\n    self.assertEqual(relax_shapes, cloned._experimental_relax_shapes)\n\n    x = array_ops.ones([])\n    self.assertEqual(self.evaluate(cloned(x)), self.evaluate(func(x)))\n\n  def test_frequent_retracing_warning(self):\n    if sys.version_info[0] < 3:\n      self.skipTest('self.assertLogs() call is not available in Python 2.')\n\n    @def_function.function\n    def f(x):\n      return x\n\n    with self.assertLogs(level='WARN') as logs:\n      f(1)\n      f(2)\n      f(3)\n      f(4)\n      self.assertEmpty(logs.output)\n      f(5)\n\n    self.assertLen(logs.output, 1)\n    self.assertIn('Tracing is expensive', logs.output[0])\n\n  def test_frequent_retracing_warning_lambda(self):\n    if sys.version_info[0] < 3:\n      self.skipTest('self.assertLogs() call is not available in Python 2.')\n\n    f = def_function.function(lambda x: x)\n\n    with self.assertLogs(level='WARN') as logs:\n      f(1)\n      f(2)\n      f(3)\n      f(4)\n      f(5)\n\n    self.assertLen(logs.output, 1)\n    self.assertIn('Tracing is expensive', logs.output[0])\n\n  def test_frequent_retracing_warning_method(self):\n    if sys.version_info[0] < 3:\n      self.skipTest('self.assertLogs() call is not available in Python 2.')\n\n    class Foo(object):\n\n      @def_function.function\n      def f(self, x):\n        return x\n\n    f = Foo().f\n\n    with self.assertLogs(level='WARN') as logs:\n      f(1)\n      f(2)\n      f(3)\n      f(4)\n      f(5)\n\n    self.assertLen(logs.output, 1)\n    self.assertIn('Tracing is expensive', logs.output[0])\n\n  def test_frequent_retracing_warning_two_independent_tf_functions(self):\n    if sys.version_info[0] < 3:\n      self.skipTest('self.assertLogs() call is not available in Python 2.')\n\n    @def_function.function\n    def f(x):\n      return x\n\n    @def_function.function\n    def g(x):\n      return x\n\n    with self.assertLogs(level='WARN') as logs:\n      f(1)\n      f(2)\n      f(3)\n      f(4)\n      g(1)\n      g(2)\n      g(3)\n      g(4)\n      g(5)\n\n    self.assertLen(logs.output, 1)\n    self.assertIn('Tracing is expensive', logs.output[0])\n\n  def test_frequent_retracing_warning_nested(self):\n    if sys.version_info[0] < 3:\n      self.skipTest('self.assertLogs() call is not available in Python 2.')\n\n    @def_function.function\n    def inner(x):\n      return x + 1\n\n    @def_function.function\n    def outer1(x):\n      return inner(x) * 2\n\n    @def_function.function\n    def outer2(x):\n      return inner(x) * 3\n\n    with self.assertLogs(level='WARN') as logs:\n      inner(1)\n      inner(2)\n      inner(3)\n      inner(4)\n\n      outer1(5)\n      outer1(6)\n      outer1(7)\n      outer1(8)\n\n      outer2(9)\n      outer2(10)\n      outer2(11)\n      outer2(12)\n\n      self.assertEmpty(logs.output)\n\n      outer2(13)\n\n      self.assertLen(logs.output, 1)\n      self.assertIn('Tracing is expensive', logs.output[0])\n\n  def test_frequent_retracing_warning_on_reinstantiation(self):\n    if sys.version_info[0] < 3:\n      self.skipTest('self.assertLogs() call is not available in Python 2.')\n\n    with self.assertLogs(level='WARN') as logs:\n      for i in range(5):\n\n        @def_function.function\n        def f(x):\n          return x\n\n        f(i)\n\n        if i < 4:\n          self.assertEmpty(logs.output)\n\n    self.assertLen(logs.output, 1)\n    self.assertIn('Tracing is expensive', logs.output[0])\n\n  def test_restored_function_retracing_warning(self):\n\n    class Foo(Checkpoint):\n\n      @def_function.function\n      def __call__(self, x):\n        return x\n\n    f_flexible = Foo()\n    _ = f_flexible.__call__.get_concrete_function(\n        tensor_spec.TensorSpec(shape=[None], dtype=dtypes.int32))\n    tmp_dir = self.create_tempdir()\n    save(f_flexible, tmp_dir.full_path)\n    restored_f_flexible = load(tmp_dir.full_path)\n\n    f_fixed_shape = Foo()\n\n    with self.assertLogs(level='WARN') as logs:\n      restored_f_flexible(constant_op.constant([1], dtypes.int32))\n      restored_f_flexible(constant_op.constant([1, 2], dtypes.int32))\n      restored_f_flexible(constant_op.constant([1, 2, 3], dtypes.int32))\n      restored_f_flexible(constant_op.constant([1, 2, 3, 4], dtypes.int32))\n      restored_f_flexible(constant_op.constant([1, 2, 3, 4, 5], dtypes.int32))\n      self.assertEmpty(logs.output)\n\n      f_fixed_shape(constant_op.constant([1], dtypes.int32))\n      f_fixed_shape(constant_op.constant([1, 2], dtypes.int32))\n      f_fixed_shape(constant_op.constant([1, 2, 3], dtypes.int32))\n      f_fixed_shape(constant_op.constant([1, 2, 3, 4], dtypes.int32))\n      f_fixed_shape(constant_op.constant([1, 2, 3, 4, 5], dtypes.int32))\n      self.assertLen(logs.output, 1)\n      self.assertIn('Tracing is expensive', logs.output[0])\n\n  def test_retracing_warning_limits(self):\n\n    @def_function.function\n    def my_func(x):\n      return x\n\n    with self.assertLogs(level='WARN') as logs:\n      for i in range(10):\n        my_func(i)\n\n      self.assertLen(logs.output, 2)\n\n  def test_experimental_get_tracing_count_function(self):\n\n    @def_function.function\n    def double(a):\n      return a + a\n\n    double(constant_op.constant(1))\n    double(constant_op.constant(2))\n    self.assertAllEqual(double.experimental_get_tracing_count(), 1)\n    double(constant_op.constant('a'))\n    self.assertAllEqual(double.experimental_get_tracing_count(), 2)\n\n  def test_experimental_get_tracing_count_method(self):\n\n    class TestClass():\n\n      @def_function.function\n      def testDouble(self, a):\n        return a + a\n\n    obj1 = TestClass()\n    obj1.testDouble(constant_op.constant(1))\n    obj1.testDouble(constant_op.constant(2))\n    obj1.testDouble(constant_op.constant(1.1))\n    self.assertAllEqual(obj1.testDouble.experimental_get_tracing_count(), 2)\n    obj2 = TestClass()\n    obj2.testDouble(constant_op.constant(1))\n    obj2.testDouble(constant_op.constant(1.1))\n    obj2.testDouble(constant_op.constant('a'))\n    self.assertAllEqual(obj2.testDouble.experimental_get_tracing_count(), 3)\n    self.assertAllEqual(obj1.testDouble.experimental_get_tracing_count(), 2)\n\n  def test_recursive_tf_function(self):\n\n    @def_function.function\n    def recursive_fn(n):\n      if n > 0:\n        return recursive_fn(n - 1)\n      return 1\n\n    self.assertEqual(recursive_fn(5).numpy(), 1)\n\n  def test_recursive_tf_function_with_gradients(self):\n\n    @def_function.function\n    def recursive_fn(n, x):\n      if n > 0:\n        return n * recursive_fn(n - 1, x)\n      else:\n        return x\n\n    x = variables.Variable(1.0)\n    with backprop.GradientTape() as tape:\n      g = recursive_fn(5, x)\n\n    dg_dx = tape.gradient(g, x)\n    self.assertEqual(dg_dx.numpy(), 120)\n\n  def test_recursive_python_function(self):\n\n    def recursive_py_fn(n):\n      if n > 0:\n        return recursive_py_fn(n - 1)\n      return 1\n\n    @def_function.function\n    def recursive_fn(n):\n      return recursive_py_fn(n)\n\n    self.assertEqual(recursive_fn(5).numpy(), 1)\n\n  def test_recursive_python_function_with_gradients(self):\n\n    def recursive_py_fn(n, x):\n      if n > 0:\n        return n * recursive_py_fn(n - 1, x)\n      return x\n\n    @def_function.function\n    def recursive_fn(n, x):\n      return recursive_py_fn(n, x)\n\n    x = variables.Variable(1.0)\n    with backprop.GradientTape() as tape:\n      g = recursive_fn(5, x)\n\n    dg_dx = tape.gradient(g, x)\n    self.assertEqual(dg_dx.numpy(), 120)\n\n  def test_recursive_tf_function_call_each_other(self):\n\n    @def_function.function\n    def recursive_fn1(n):\n      if n <= 1:\n        return 1\n      return recursive_fn2(n - 1)\n\n    @def_function.function\n    def recursive_fn2(n):\n      if n <= 1:\n        return 2\n      return recursive_fn1(n - 1)\n\n    self.assertEqual(recursive_fn1(5).numpy(), 1)\n    self.assertEqual(recursive_fn1(6).numpy(), 2)\n    self.assertEqual(recursive_fn2(5).numpy(), 2)\n    self.assertEqual(recursive_fn2(6).numpy(), 1)\n\n  def test_recursive_tf_function_call_each_other_with_gradients(self):\n\n    @def_function.function\n    def recursive_fn1(n, x):\n      if n <= 1:\n        return x\n      return n * recursive_fn2(n - 1, x)\n\n    @def_function.function\n    def recursive_fn2(n, x):\n      if n <= 1:\n        return 2 * x\n      return n * recursive_fn1(n - 1, x)\n\n    x = variables.Variable(1.0)\n    with backprop.GradientTape() as tape:\n      g1 = recursive_fn1(5, x)\n\n    dg1_dx = tape.gradient(g1, x)\n    self.assertEqual(dg1_dx.numpy(), 120)\n\n    with backprop.GradientTape() as tape:\n      g2 = recursive_fn2(5, x)\n\n    dg2_dx = tape.gradient(g2, x)\n    self.assertEqual(dg2_dx.numpy(), 240)\n\n  def test_recursive_tf_function_with_cond(self):\n    @def_function.function(autograph=False)\n    def recursive_fn(n):\n      return cond_v2.cond_v2(n > 0, recursive_fn(n - 1), 1)\n\n    with self.assertRaises(RecursionError):\n      recursive_fn(constant_op.constant(5))\n\n\nif __name__ == '__main__':\n  ops.enable_eager_execution()\n  test.main()"