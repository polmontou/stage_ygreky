"# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\n\"\"\"Tests for tpu_function helpers.\"\"\"\n\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import importer\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.layers import convolutional\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import control_flow_util\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import special_math_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.tpu import tpu\nfrom tensorflow.python.tpu import tpu_feed\nfrom tensorflow.python.tpu import training_loop\nfrom tensorflow.python.tpu.ops import tpu_ops\n\n\nclass TPUContextTest(test.TestCase):\n\n  def testIsInContext(self):\n    \"\"\"Test that control_flow_util can check that we're in a TPU context.\"\"\"\n    with ops.Graph().as_default():\n      z1 = array_ops.identity(1)\n      pivot = control_flow_ops.no_op()\n      context = tpu.TPUReplicateContext(b\"context\", 1, pivot=pivot)\n      context.Enter()\n      z2 = array_ops.identity(1)\n      context.Exit()\n      self.assertFalse(control_flow_util.IsInXLAContext(z1.op))\n      self.assertTrue(control_flow_util.IsInXLAContext(z2.op))\n\n  def testHandlesNameCollision(self):\n    \"\"\"Test AddValue handles name collisions for ops from different graphs.\"\"\"\n    with ops.Graph().as_default():\n      z = array_ops.zeros([2, 3], name=\"a\")\n      assert z.name == \"a:0\", \"Expected: a:0, Found: %s\" % z.name\n\n      @def_function.function\n      def f():\n        pivot = control_flow_ops.no_op()\n        context = tpu.TPUReplicateContext(b\"context\", 1, pivot=pivot)\n        context.Enter()\n        array_ops.identity(z)  # Capture z.\n        z1 = array_ops.zeros([3, 2], name=\"a\")\n        assert z1.name == \"a:0\", \"Expected: a:0, Found: %s\" % z1.name\n        z2 = array_ops.zeros([3, 2], name=\"a\")\n        # Prior to fixing b/166794533 this would fail with a shape mismatch\n        # because context.AddValue would have cached `z` by its name which\n        # collides with z1's name.\n        result = z1 + z2\n        context.Exit()\n        return result\n\n      f.get_concrete_function()\n\n\nclass TPULayerRewriteTest(test.TestCase):\n\n  def testUsingInfeedQueueWithRegularizer(self):\n    \"\"\"Test that Layer regularizers can reference data created in loops.\"\"\"\n\n    with ops.Graph().as_default():\n\n      def make_regularizer(scale):\n        def regularizer(inputs):\n          return scale * math_ops.reduce_sum(math_ops.square(inputs))\n        return regularizer\n\n      def training_step(inputs, scale):\n        outputs = convolutional.conv2d(\n            inputs,\n            filters=16,\n            kernel_size=(3, 3),\n            data_format=\"channels_first\",\n            kernel_regularizer=make_regularizer(scale))\n        loss = math_ops.reduce_mean(math_ops.square(outputs))\n        return loss.op\n\n      inputs = array_ops.zeros(shape=(128, 32, 32, 16))\n      scale = array_ops.ones(shape=())\n      infeed = tpu_feed.InfeedQueue(\n          tuple_types=[dtypes.float32, dtypes.float32],\n          tuple_shapes=[inputs.shape, scale.shape])\n\n      def loop():\n        return training_loop.repeat(5, training_step, infeed_queue=infeed)\n\n      # This should not throw an error.\n      tpu.rewrite(loop)\n\n\nclass TPUGraphPruneTest(test.TestCase):\n\n  def test_prune_unconnected_ops(self):\n    with ops.Graph().as_default():\n      a = array_ops.placeholder(dtype=dtypes.float32, name=\"a\")\n      b = array_ops.placeholder(dtype=dtypes.float32, name=\"b\")\n      constant_op.constant(1.0, name=\"constant\")\n      x = variable_scope.get_variable(\n          name=\"x\",\n          dtype=dtypes.float32,\n          shape=[],\n          use_resource=True,\n          initializer=init_ops.constant_initializer(2.0))\n      y = variable_scope.get_variable(\n          name=\"y\",\n          dtype=dtypes.float32,\n          shape=[],\n          use_resource=True,\n          initializer=init_ops.constant_initializer(3.0))\n      math_ops.add(a, b)\n      math_ops.add(x, y)\n      graph_def = ops.get_default_graph().as_graph_def()\n\n      for node in graph_def.node:\n        # Attach a TPU_REPLICATE_ATTR to each node.\n        node.attr[tpu._TPU_REPLICATE_ATTR].s = b\"0\"\n        # Rewire placeholder \"a\" and variable \"y\" leaving them unconnected.\n        for (input_index, node_input) in enumerate(node.input):\n          if node_input == \"b\":\n            node.input[input_index] = \"constant\"\n          if node_input == \"y\":\n            node.input[input_index] = \"x\"\n\n    with ops.Graph().as_default() as graph:\n      # Reimport the graph and prune unconnected ops.\n      importer.import_graph_def(graph_def)\n      tpu.prune_unconnected_ops_from_xla(ops.get_default_graph())\n\n      # Verify that ops \"a\" and \"x\" still have TPU_REPLICATE_ATTR.\n      a = graph.get_operation_by_name(\"import/a\").get_attr(\n          tpu._TPU_REPLICATE_ATTR)\n      self.assertEqual(b\"0\", a)\n      x = graph.get_operation_by_name(\"import/x\").get_attr(\n          tpu._TPU_REPLICATE_ATTR)\n      self.assertEqual(b\"0\", x)\n      # Verify that ops \"b\" and \"y\" have TPU_REPLICATE_ATTR removed.\n      with self.assertRaisesRegex(\n          ValueError,\n          \"Operation \\'import/b\\' has no attr named \\'_tpu_replicate\\'\"):\n        graph.get_operation_by_name(\"import/b\").get_attr(\n            tpu._TPU_REPLICATE_ATTR)\n      with self.assertRaisesRegex(\n          ValueError,\n          \"Operation \\'import/y\\' has no attr named \\'_tpu_replicate\\'\"):\n        graph.get_operation_by_name(\"import/y\").get_attr(\n            tpu._TPU_REPLICATE_ATTR)\n\n\nclass TPUOpsTest(test.TestCase):\n\n  def test_all_to_all_zero_split_count(self):\n    with self.assertRaisesRegex(\n        ValueError, \"split_count 0 must at least be one\"):\n      tpu_ops.all_to_all(\n          x=[0.0, 0.1652, 0.6543],\n          group_assignment=[1, -1],\n          concat_dimension=0,\n          split_dimension=0,\n          split_count=0)\n\n  def test_all_to_all_group_assignment_wrong_shape(self):\n    with self.assertRaisesRegex(\n        ValueError, \"group_assignment must have rank 2\"):\n      tpu_ops.all_to_all(\n          x=[0.0, 0.1652, 0.6543],\n          group_assignment=[1, -1],\n          concat_dimension=0,\n          split_dimension=0,\n          split_count=2)\n\n  def test_all_to_all_split_count_not_equal_to_group_assignment_shape(self):\n    with self.assertRaisesRegex(\n        ValueError, \"split_count 1 must equal the size of the second dimension \"\n        \"of group_assignment 2\"):\n      tpu_ops.all_to_all(\n          x=[0.0, 0.1652, 0.6543],\n          group_assignment=[[0, 1], [2, 3]],\n          concat_dimension=0,\n          split_dimension=0,\n          split_count=1)\n\n  def test_all_to_all_split_count_not_divide_input_shape(self):\n    with self.assertRaisesRegex(\n        ValueError, \"input dimension 3 not divisible by split_count 2\"):\n      tpu_ops.all_to_all(\n          x=[[0.0], [0.1652], [0.6543]],\n          group_assignment=[[0, 1], [2, 3]],\n          concat_dimension=1,\n          split_dimension=0,\n          split_count=2)\n\n\ndef do_einsum():\n  a = array_ops.placeholder(dtype=dtypes.float32, name=\"a\", shape=[2, 3, 4])\n  b = array_ops.placeholder(dtype=dtypes.float32, name=\"b\", shape=[2, 4, 5])\n  return special_math_ops.einsum(\"abc,acd->abd\", a, b)\n\n\ndef find_einsum(g):\n  graph_def = g.as_graph_def()\n  for node in graph_def.node:\n    if node.op == \"Einsum\":\n      return True\n  return False\n\n\ndef find_xla_einsum(g):\n  graph_def = g.as_graph_def()\n  for node in graph_def.node:\n    if node.op == \"XlaEinsum\":\n      return True\n  return False\n\n\nclass TPUXlaEinsumTest(test.TestCase):\n\n  def test_tpu_rewrite_uses_xla_einsum(self):\n    with ops.Graph().as_default() as g:\n      tpu.rewrite(do_einsum)\n      self.assertTrue(find_einsum(g) or find_xla_einsum(g))\n\n  def test_default_does_not_use_xla_einsum(self):\n    with ops.Graph().as_default() as g:\n      do_einsum()\n      self.assertFalse(find_xla_einsum(g))\n\n\nif __name__ == \"__main__\":\n  test.main()"