"# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for V2 Collective Operations.\"\"\"\n\nimport os\nimport threading\nimport time\n\nfrom absl.testing import parameterized\n\nfrom tensorflow.python.compat import v2_compat\nfrom tensorflow.python.data.experimental.ops import testing as dataset_testing\nfrom tensorflow.python.data.ops import dataset_ops\nfrom tensorflow.python.distribute import combinations\nfrom tensorflow.python.distribute import test_util\nfrom tensorflow.python.eager import cancellation\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import check_ops\nfrom tensorflow.python.ops import collective_ops as _collective_ops\nfrom tensorflow.python.ops import resource_variable_ops\nfrom tensorflow.python.platform import test\n\n\nclass CollectiveOpsV1(object):\n  all_reduce = _collective_ops.all_reduce\n  all_gather = _collective_ops.all_gather\n  broadcast_send = _collective_ops.broadcast_send\n  broadcast_recv = _collective_ops.broadcast_recv\n\n\nclass CollectiveOpsV2(object):\n\n  @staticmethod\n  def all_reduce(t, group_size, group_key, instance_key, *args, **kwargs):\n    group_size = array_ops.identity(group_size)\n    group_key = array_ops.identity(group_key)\n    instance_key = array_ops.identity(instance_key)\n    return _collective_ops.all_reduce_v2(t, group_size, group_key, instance_key,\n                                         *args, **kwargs)\n\n  @staticmethod\n  def all_gather(t, group_size, group_key, instance_key, *args, **kwargs):\n    group_size = array_ops.identity(group_size)\n    group_key = array_ops.identity(group_key)\n    instance_key = array_ops.identity(instance_key)\n    return _collective_ops.all_gather_v2(t, group_size, group_key, instance_key,\n                                         *args, **kwargs)\n\n  @staticmethod\n  def broadcast_send(t, shape, dtype, group_size, group_key, instance_key,\n                     *args, **kwargs):\n    group_size = array_ops.identity(group_size)\n    group_key = array_ops.identity(group_key)\n    instance_key = array_ops.identity(instance_key)\n    return _collective_ops.broadcast_send_v2(t, group_size, group_key,\n                                             instance_key, *args, **kwargs)\n\n  @staticmethod\n  def broadcast_recv(shape, dtype, group_size, group_key, instance_key, *args,\n                     **kwargs):\n    group_size = array_ops.identity(group_size)\n    group_key = array_ops.identity(group_key)\n    instance_key = array_ops.identity(instance_key)\n    shape = array_ops.identity(shape)\n    return _collective_ops.broadcast_recv_v2(\n        shape, dtype, group_size, group_key, instance_key, *args, **kwargs)\n\n\ndevice_combination = (\n    combinations.combine(device='CPU', communication='RING', required_gpus=0) +\n    combinations.combine(\n        device='GPU', communication=['RING', 'NCCL'], required_gpus=2))\n\ncollective_op_combinations = combinations.combine(collective_op=[\n    combinations.NamedObject('all_reduce', CollectiveOpsV1.all_reduce),\n    combinations.NamedObject('all_reduce_v2', CollectiveOpsV2.all_reduce),\n    combinations.NamedObject('all_gather', CollectiveOpsV1.all_gather),\n    combinations.NamedObject('all_gather_v2', CollectiveOpsV2.all_gather)\n])\n\n\n@combinations.generate(\n    combinations.times(\n        combinations.combine(\n            collective_ops=[\n                combinations.NamedObject('v1', CollectiveOpsV1),\n                combinations.NamedObject('v2', CollectiveOpsV2)\n            ],\n            mode='eager'), device_combination))\nclass CollectiveOpsTest(test.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    _setup_context()\n    super().setUp()\n\n  def testReduce(self, collective_ops, device, communication):\n    dev0 = '/device:%s:0' % device\n    dev1 = '/device:%s:1' % device\n\n    @def_function.function\n    def run_all_reduce_1device():\n      with ops.device(dev0):\n        in_value = constant_op.constant([1.])\n        group_size = 1\n        group_key = 1\n        instance_key = 1\n        return collective_ops.all_reduce(\n            in_value,\n            group_size,\n            group_key,\n            instance_key,\n            communication_hint=communication)\n\n    @def_function.function\n    def run_all_reduce_2devices():\n      in_value = constant_op.constant([1.])\n      group_size = 2\n      group_key = 2\n      instance_key = 2\n      collectives = []\n      with ops.device(dev0):\n        collectives.append(\n            collective_ops.all_reduce(\n                in_value,\n                group_size,\n                group_key,\n                instance_key,\n                communication_hint=communication))\n      with ops.device(dev1):\n        collectives.append(\n            collective_ops.all_reduce(\n                in_value,\n                group_size,\n                group_key,\n                instance_key,\n                communication_hint=communication))\n      return collectives\n\n    self.assertAllClose(run_all_reduce_1device(), [1.], rtol=1e-5, atol=1e-5)\n    for result in run_all_reduce_2devices():\n      self.assertAllClose(result, [2.], rtol=1e-5, atol=1e-5)\n\n  def testGather(self, collective_ops, device, communication):\n    dev0 = '/device:%s:0' % device\n    dev1 = '/device:%s:1' % device\n\n    @def_function.function\n    def run_all_gather_1device():\n      with ops.device(dev0):\n        in_value = constant_op.constant([1.])\n        group_size = 1\n        group_key = 1\n        instance_key = 1\n        return collective_ops.all_gather(\n            in_value,\n            group_size,\n            group_key,\n            instance_key,\n            communication_hint=communication)\n\n    @def_function.function\n    def run_all_gather_2devices():\n      in_value = constant_op.constant([1.])\n      group_size = 2\n      group_key = 2\n      instance_key = 2\n      collectives = []\n      with ops.device(dev0):\n        collectives.append(\n            collective_ops.all_gather(\n                in_value,\n                group_size,\n                group_key,\n                instance_key,\n                communication_hint=communication))\n      with ops.device(dev1):\n        collectives.append(\n            collective_ops.all_gather(\n                in_value,\n                group_size,\n                group_key,\n                instance_key,\n                communication_hint=communication))\n      return collectives\n\n    self.assertAllClose(run_all_gather_1device(), [1.], rtol=1e-5, atol=1e-5)\n    for result in run_all_gather_2devices():\n      self.assertAllClose(result, [1., 1.], rtol=1e-5, atol=1e-5)\n\n  def testBroadcast(self, collective_ops, device, communication):\n    dev0 = '/device:%s:0' % device\n    dev1 = '/device:%s:1' % device\n\n    @def_function.function\n    def run_broadcast_2devices():\n      shape = [3]\n      in_value = constant_op.constant([1., 2., 3.], shape=shape)\n      group_size = 2\n      group_key = 2\n      instance_key = 2\n      collectives = []\n      with ops.device(dev0):\n        collectives.append(\n            collective_ops.broadcast_send(\n                in_value,\n                shape,\n                in_value.dtype,\n                group_size,\n                group_key,\n                instance_key,\n                communication_hint=communication))\n      with ops.device(dev1):\n        collectives.append(\n            collective_ops.broadcast_recv(\n                shape,\n                in_value.dtype,\n                group_size,\n                group_key,\n                instance_key,\n                communication_hint=communication))\n      return collectives\n\n    for result in run_broadcast_2devices():\n      self.assertAllClose(result, [1., 2., 3.], rtol=1e-5, atol=1e-5)\n\n  def testInstanceKeyScopedUnderGroupKey(self, collective_ops, device,\n                                         communication):\n    if device == 'GPU' and context.num_gpus() < 4:\n      self.skipTest('not enough GPU')\n\n    dev0 = '/device:%s:0' % device\n    dev1 = '/device:%s:1' % device\n    dev2 = '/device:%s:2' % device\n    dev3 = '/device:%s:3' % device\n\n    @def_function.function\n    def run_all_reduce_4devices_same_instance_key():\n      # Use a common instance key for both groups.\n      instance_key = 0\n      # We will create 2 groups each with 2 devices.\n      group_size = 2\n      # Group 0 comprises dev0 and dev1.\n      group0_key = 0\n      # Group 1 comprises dev2 and dev3.\n      group1_key = 1\n      collectives = []\n      with ops.device(dev0):\n        collectives.append(\n            collective_ops.all_reduce(\n                constant_op.constant(1.), group_size, group0_key, instance_key))\n      with ops.device(dev1):\n        collectives.append(\n            collective_ops.all_reduce(\n                constant_op.constant(2.), group_size, group0_key, instance_key))\n      with ops.device(dev2):\n        collectives.append(\n            collective_ops.all_reduce(\n                constant_op.constant(3.), group_size, group1_key, instance_key))\n      with ops.device(dev3):\n        collectives.append(\n            collective_ops.all_reduce(\n                constant_op.constant(4.), group_size, group1_key, instance_key))\n      return collectives\n\n    results = run_all_reduce_4devices_same_instance_key()\n    self.assertAllClose(results[0], 3., rtol=1e-5, atol=1e-5)\n    self.assertAllClose(results[1], 3., rtol=1e-5, atol=1e-5)\n    self.assertAllClose(results[2], 7., rtol=1e-5, atol=1e-5)\n    self.assertAllClose(results[3], 7., rtol=1e-5, atol=1e-5)\n\n  def testCollectiveGroupSizeOne(self, collective_ops, device, communication):\n    dev0 = '/device:%s:0' % device\n\n    group_size = 1\n    group_key = 100\n    in_value = [1., 2., 3., 4.]\n    in_tensor = constant_op.constant(in_value)\n\n    with ops.device(dev0):\n      reduced_tensor = collective_ops.all_reduce(\n          in_tensor,\n          group_size,\n          group_key,\n          instance_key=100,\n          communication_hint=communication)\n    self.assertAllEqual(in_value, reduced_tensor.numpy())\n\n    with ops.device(dev0):\n      gathered_tensor = collective_ops.all_gather(\n          in_tensor,\n          group_size,\n          group_key,\n          instance_key=200,\n          communication_hint=communication)\n    self.assertAllEqual(in_value, gathered_tensor.numpy())\n\n  def testCollectiveInvalidKey(self, collective_ops, device, communication):\n    dev0 = '/device:%s:0' % device\n\n    group_size = 1\n    group_key = 100\n    instance_key = 100\n    in_value = [1., 2., 3., 4.]\n    in_tensor = constant_op.constant(in_value)\n\n    with ops.device(dev0):\n      reduced_tensor = collective_ops.all_reduce(\n          in_tensor,\n          group_size,\n          group_key,\n          instance_key,\n          communication_hint=communication)\n    self.assertAllEqual(in_value, reduced_tensor.numpy())\n\n    with self.assertRaisesRegex(\n        errors.InternalError, 'instance 100 expected type 0 and data_type 1 but'\n        ' got type 2 and data_type 1'):\n      with ops.device(dev0):\n        collective_ops.all_gather(\n            in_tensor,\n            group_size,\n            group_key,\n            instance_key,\n            communication_hint=communication)\n\n  def testMultipleGroups(self, collective_ops, device, communication):\n    if device == 'GPU' and context.num_gpus() < 4:\n      self.skipTest('not enough GPU')\n\n    num_elements = 4\n\n    @def_function.function\n    def run_all_reduce(group_size, group_key):\n      instance_key = group_key\n      input_value = [float(group_key) for i in range(num_elements)]\n      collectives = []\n      for device_idx in range(group_size):\n        with ops.device('/{}:{}'.format(device, device_idx)):\n          input_tensor = constant_op.constant(input_value)\n          collectives.append(\n              collective_ops.all_reduce(\n                  input_tensor,\n                  group_size,\n                  group_key,\n                  instance_key,\n                  communication_hint=communication))\n      return collectives\n\n    def run_and_assert(group_size, group_key):\n      for reduced_tensor in run_all_reduce(group_size, group_key):\n        self.assertAllEqual(\n            [float(group_key) * group_size for i in range(num_elements)],\n            reduced_tensor.numpy())\n\n    run_and_assert(group_size=2, group_key=1)\n    run_and_assert(group_size=3, group_key=2)\n\n\n@combinations.generate(\n    combinations.times(\n        combinations.combine(\n            collective_ops=[\n                combinations.NamedObject('v2', CollectiveOpsV2)\n            ],\n            mode='eager',\n            max_subdivs_per_device=[-1, 0, 16]), device_combination))\nclass AllReduceWithSubdivisionsTest(test.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    _setup_context()\n    super().setUp()\n\n  def testReduce(self, collective_ops, device, communication,\n                 max_subdivs_per_device):\n    dev0 = '/device:%s:0' % device\n    dev1 = '/device:%s:1' % device\n\n    @def_function.function\n    def run_all_reduce_1device():\n      with ops.device(dev0):\n        in_value = constant_op.constant([1.])\n        group_size = 1\n        group_key = 1\n        instance_key = 1\n        if max_subdivs_per_device == -1:\n          return collective_ops.all_reduce(\n              in_value,\n              group_size,\n              group_key,\n              instance_key,\n              communication_hint=communication)\n        else:\n          return collective_ops.all_reduce(\n              in_value,\n              group_size,\n              group_key,\n              instance_key,\n              communication_hint=communication,\n              max_subdivs_per_device=max_subdivs_per_device)\n\n    @def_function.function\n    def run_all_reduce_2devices():\n      in_value = constant_op.constant([1.])\n      group_size = 2\n      group_key = 2\n      instance_key = 2\n      collectives = []\n      with ops.device(dev0):\n        collectives.append(\n            collective_ops.all_reduce(\n                in_value,\n                group_size,\n                group_key,\n                instance_key,\n                communication_hint=communication))\n      with ops.device(dev1):\n        collectives.append(\n            collective_ops.all_reduce(\n                in_value,\n                group_size,\n                group_key,\n                instance_key,\n                communication_hint=communication))\n      return collectives\n\n    self.assertAllClose(run_all_reduce_1device(), [1.], rtol=1e-5, atol=1e-5)\n    for result in run_all_reduce_2devices():\n      self.assertAllClose(result, [2.], rtol=1e-5, atol=1e-5)\n\n\n@combinations.generate(\n    combinations.combine(required_physical_gpus=2, mode='eager'))\nclass XlaTest(test.TestCase, parameterized.TestCase):\n\n  def testReduce(self):\n    device0 = '/device:GPU:0'\n    device1 = '/device:GPU:1'\n    group_size = 2\n    group_key = 100\n    instance_key = 100\n    results = []\n\n    def all_reduce(device):\n\n      @def_function.function(jit_compile=True)\n      def f():\n        return _collective_ops.all_reduce_v2([1.], group_size, group_key,\n                                             instance_key)\n\n      with ops.device(device):\n        results.append(f())\n\n    t0 = threading.Thread(target=all_reduce, args=(device0,))\n    t1 = threading.Thread(target=all_reduce, args=(device1,))\n    t0.start()\n    t1.start()\n    t0.join()\n    t1.join()\n\n    self.assertAllEqual(results, [[2.], [2.]])\n\n\n@combinations.generate(\n    combinations.times(collective_op_combinations, device_combination))\nclass AbortCollectiveOpsTest(test.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    _setup_context()\n    super().setUp()\n\n  def testAbortGroupParamsResolution(self, collective_op, device,\n                                     communication):\n    dev0 = '/device:%s:0' % device\n    dev1 = '/device:%s:1' % device\n    group_size = 2\n    group_key = 100\n    instance_key = 100\n    in_tensor = constant_op.constant([1.])\n\n    def abort_fn():\n      time.sleep(2)\n      context.context().abort_collective_ops(errors.UNAVAILABLE, 'peer down')\n\n    t = threading.Thread(target=abort_fn)\n    t.start()\n\n    with self.assertRaisesRegex(errors.UnavailableError, 'peer down'):\n      # This hangs on params resolution since we're only launching one\n      # collective for a group size of 2.\n      with ops.device(dev0):\n        collective_op(\n            in_tensor,\n            group_size,\n            group_key,\n            instance_key,\n            communication_hint=communication)\n\n    # After abortion, subsequent collectives should fail immediately.\n    with self.assertRaisesRegex(errors.UnavailableError, 'peer down'):\n      with ops.device(dev0):\n        collective_op(\n            in_tensor,\n            group_size,\n            group_key,\n            instance_key,\n            communication_hint=communication)\n\n    t.join()\n    # Reset the context in order to reset the collective executor.\n    _setup_context()\n\n    # After reset non-NCCL collectives should work.\n    def collective_fn():\n      for device in [dev0, dev1]:\n        with ops.device(device):\n          collective_op(\n              in_tensor,\n              group_size,\n              group_key,\n              instance_key,\n              communication_hint=communication)\n\n    def_function.function(collective_fn)()\n\n  def testAbortInstanceParamsResolution(self, collective_op, device,\n                                        communication):\n    dev0 = '/device:%s:0' % device\n    dev1 = '/device:%s:1' % device\n    group_size = 2\n    group_key = 100\n    instance_key = 100\n    in_tensor = constant_op.constant([1.])\n\n    def collective_fn():\n      for device in [dev0, dev1]:\n        with ops.device(device):\n          collective_op(\n              in_tensor,\n              group_size,\n              group_key,\n              instance_key,\n              communication_hint=communication)\n\n    # First perform a normal all-reduce to complete the group resolution.\n    def_function.function(collective_fn)()\n\n    def abort_fn():\n      time.sleep(2)\n      context.context().abort_collective_ops(errors.UNAVAILABLE, 'peer down')\n\n    t = threading.Thread(target=abort_fn)\n    t.start()\n\n    # Use a different instance key to trigger another instance resolution.\n    instance_key = 101\n    with self.assertRaisesRegex(errors.UnavailableError, 'peer down'):\n      # This hangs on params resolution since we're only launching one\n      # collective for a group size of 2.\n      with ops.device(dev0):\n        collective_op(\n            in_tensor,\n            group_size,\n            group_key,\n            instance_key,\n            communication_hint=communication)\n\n    # After abortion, subsequent collectives should fail immediately.\n    with self.assertRaisesRegex(errors.UnavailableError, 'peer down'):\n      with ops.device(dev0):\n        collective_op(\n            in_tensor,\n            group_size,\n            group_key,\n            instance_key,\n            communication_hint=communication)\n\n    context._reset_context()  # pylint: disable=protected-access\n    t.join()\n    # Reset the context in order to reset the collective executor.\n    _setup_context()\n\n    # After reset non-NCCL collectives should work.\n    def_function.function(collective_fn)()\n\n  def testAbortCommunication(self, collective_op, device, communication):\n    dev0 = '/device:%s:0' % device\n    dev1 = '/device:%s:1' % device\n    group_size = 2\n    group_key = 100\n    instance_key = 100\n    in_tensor = constant_op.constant([1.])\n\n    # First perform a normal collective to finish resolution.\n    def collective_fn():\n      for device in [dev0, dev1]:\n        with ops.device(device):\n          collective_op(\n              in_tensor,\n              group_size,\n              group_key,\n              instance_key,\n              communication_hint=communication)\n\n    def_function.function(collective_fn)()\n\n    # Launch a collective that hangs, and abort the collective executor after\n    # the launch.\n    def abort_fn():\n      time.sleep(2)\n      context.context().abort_collective_ops(errors.UNAVAILABLE, 'peer down')\n\n    t = threading.Thread(target=abort_fn)\n    t.start()\n\n    with self.assertRaisesRegex(errors.UnavailableError, 'peer down'):\n      with ops.device(dev0):\n        collective_op(\n            in_tensor,\n            group_size,\n            group_key,\n            instance_key,\n            communication_hint=communication)\n\n    # After abortion, subsequent collectives should fail immediately.\n    with self.assertRaisesRegex(errors.UnavailableError, 'peer down'):\n      with ops.device(dev0):\n        collective_op(\n            in_tensor,\n            group_size,\n            group_key,\n            instance_key,\n            communication_hint=communication)\n\n    # Reset the context in order to reset the collective executor.\n    t.join()\n    _setup_context()\n    def_function.function(collective_fn)()\n\n\nclass OpCancellationTest(test.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    _setup_context()\n    super().setUp()\n\n  @combinations.generate(\n      combinations.times(\n          combinations.combine(\n              collective_op=[\n                  combinations.NamedObject('all_reduce',\n                                           CollectiveOpsV1.all_reduce),\n                  combinations.NamedObject('all_reduce_v2',\n                                           CollectiveOpsV2.all_reduce),\n                  combinations.NamedObject('all_gather',\n                                           CollectiveOpsV1.all_gather),\n                  combinations.NamedObject('all_gather_v2',\n                                           CollectiveOpsV2.all_gather),\n              ],\n              mode='eager'), device_combination))\n  def testOpErrorNotAbortIfNoCollective(self, collective_op, device,\n                                        communication):\n    # Do not abort if there's no active collective ops. There could be\n    # exceptions like EOF which we expect users to catch, aborting collective\n    # ops on all op errors intervenes with this workflow.\n    dev0 = '/device:%s:0' % device\n    dev1 = '/device:%s:1' % device\n    group_size = 2\n    group_key = 100\n    instance_key = 100\n    dataset = dataset_ops.Dataset.from_tensors([1.])\n\n    @def_function.function\n    def collective_fn(in_tensor):\n      for device in [dev0, dev1]:\n        with ops.device(device):\n          collective_op(\n              in_tensor,\n              group_size,\n              group_key,\n              instance_key,\n              communication_hint=communication)\n\n    @def_function.function\n    def f():\n      iterator = iter(dataset)\n      collective_fn(next(iterator))\n      # This next(iterator) should raise EOF.\n      collective_fn(next(iterator))\n\n    with self.assertRaises(errors.OutOfRangeError):\n      f()\n    collective_fn(constant_op.constant([1.]))\n\n  @combinations.generate(\n      combinations.times(\n          combinations.combine(\n              collective_op=[\n                  combinations.NamedObject('all_reduce',\n                                           CollectiveOpsV1.all_reduce),\n                  combinations.NamedObject('all_gather',\n                                           CollectiveOpsV1.all_gather),\n              ],\n              mode='eager'), device_combination))\n  def testOpErrorAbortWithCollective(self, collective_op, device,\n                                     communication):\n    # Abort v1 collective ops if there're active collective ops at the time of\n    # an op error. This is due to the inability to cancel collective ops, and op\n    # errors may cause running collective ops to hang.\n    dev0 = '/device:%s:0' % device\n    group_size = 2\n    group_key = 100\n    instance_key = 100\n    in_tensor = constant_op.constant([1.])\n    # Make the dataset sleep a while so that the collective is being executed\n    # when the EOF happens.\n    dataset = dataset_ops.Dataset.from_tensors([1.]).apply(\n        dataset_testing.sleep(sleep_microseconds=200))\n\n    @def_function.function\n    def f():\n      # Launch a collective op that won't be able to finish to test abortion\n      # when other ops error.\n      with ops.device(dev0):\n        ret = collective_op(\n            in_tensor,\n            group_size,\n            group_key,\n            instance_key,\n            communication_hint=communication)\n      iterator = iter(dataset)\n      next(iterator)\n      # This should raise EOF.\n      next(iterator)\n      return ret\n\n    with self.assertRaises(errors.OutOfRangeError):\n      f()\n    # Now collective ops is aborted, subsequent collective ops should fail with\n    # the previous error.\n    with self.assertRaises(errors.CancelledError):\n      with ops.device(dev0):\n        collective_op(\n            in_tensor,\n            group_size,\n            group_key,\n            instance_key,\n            communication_hint=communication)\n\n  @combinations.generate(\n      combinations.times(\n          combinations.combine(\n              collective_op=[\n                  combinations.NamedObject('all_reduce_v2',\n                                           CollectiveOpsV2.all_reduce),\n                  combinations.NamedObject('all_gather_v2',\n                                           CollectiveOpsV2.all_gather),\n              ],\n              mode='eager'), device_combination))\n  def testOpErrorNotAbortWithCollective(self, collective_op, device,\n                                        communication):\n    # Do not abort v2 collective ops even if there're active collective ops at\n    # the time of an op error. We rely cancellation to terminate active\n    # collective ops.\n    dev0 = '/device:%s:0' % device\n    dev1 = '/device:%s:1' % device\n    group_size = 2\n    group_key = 100\n    instance_key = 100\n    in_tensor = constant_op.constant([1.])\n\n    @def_function.function\n    def collective_fn():\n      for device in [dev0, dev1]:\n        with ops.device(device):\n          collective_op(\n              in_tensor,\n              group_size,\n              group_key,\n              instance_key,\n              communication_hint=communication)\n\n    # Local params resolution cannot be cancelled yet, so we perform a normal\n    # collective so that the group is resolved.\n    collective_fn()\n\n    # Make the dataset sleep a while so that the collective is being executed\n    # when the EOF happens.\n    dataset = dataset_ops.Dataset.from_tensors([1.]).apply(\n        dataset_testing.sleep(sleep_microseconds=200))\n\n    @def_function.function\n    def f():\n      # Launch a collective op that won't be able to finish to test cancellation\n      # when other ops error.\n      with ops.device(dev0):\n        ret = collective_op(\n            in_tensor,\n            group_size,\n            group_key,\n            instance_key,\n            communication_hint=communication)\n      iterator = iter(dataset)\n      next(iterator)\n      # This should raise EOF.\n      next(iterator)\n      return ret\n\n    with self.assertRaises(errors.OutOfRangeError):\n      f()\n    # Collective ops shouldn't be aborted and new collectives should be able to\n    # proceed.\n    collective_fn()\n\n  @combinations.generate(\n      combinations.times(\n          combinations.combine(\n              collective_op=[\n                  combinations.NamedObject('all_reduce_v2',\n                                           CollectiveOpsV2.all_reduce),\n                  combinations.NamedObject('all_gather_v2',\n                                           CollectiveOpsV2.all_gather),\n              ],\n              mode='eager'), device_combination))\n  def testCancelDuringParamResolution(self, collective_op, device,\n                                      communication):\n    dev0 = '/device:%s:0' % device\n    dev1 = '/device:%s:1' % device\n    group_size = 2\n    group_key = 100\n    instance_key = 100\n    in_tensor = constant_op.constant([1.])\n    t1_cancellation_manager = cancellation.CancellationManager()\n    t2_cancellation_manager = cancellation.CancellationManager()\n\n    @def_function.function\n    def _collective_fn(x):\n      # Run an assertion to crash one of the two function executions running\n      # collectives. We explicitly cancel the other in response.\n      assert_op = check_ops.assert_equal(x, in_tensor)\n      with ops.control_dependencies([assert_op]):\n        return collective_op(\n            in_tensor,\n            group_size,\n            group_key,\n            instance_key,\n            communication_hint=communication)\n\n    collective_concrete = _collective_fn.get_concrete_function(in_tensor)\n\n    finish_mu = threading.Lock()\n    finishes = 0\n\n    def _placement_wrapper(device, x, my_cancellation, other_cancellation):\n      try:\n        with ops.device(device):\n          cancelable_collective = my_cancellation.get_cancelable_function(\n              collective_concrete)\n          return cancelable_collective(x)\n      except errors.InvalidArgumentError:\n        # `assert_equal` failed for this execution of the function. The other\n        # function would deadlock without cancellation.\n        other_cancellation.start_cancel()\n      except errors.CancelledError:\n        pass\n      nonlocal finishes\n      with finish_mu:\n        finishes += 1\n\n    t1 = threading.Thread(\n        target=_placement_wrapper,\n        args=(dev0, constant_op.constant([1.]), t1_cancellation_manager,\n              t2_cancellation_manager))\n    t2 = threading.Thread(\n        target=_placement_wrapper,\n        # Will cause the assertion to fail\n        args=(dev1, constant_op.constant([2.]), t2_cancellation_manager,\n              t1_cancellation_manager))\n    t1.start()\n    t2.start()\n    t1.join()\n    t2.join()\n    self.assertEqual(finishes, 2)\n\n\n@combinations.generate(\n    combinations.times(collective_op_combinations, device_combination))\nclass TimeoutTest(test.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    _setup_context()\n    super().setUp()\n\n  def testTimeout(self, collective_op, device, communication):\n    timeout = 1.5\n\n    @def_function.function\n    def run(group_size, reported_group_size=None):\n      group_key = 20\n      instance_key = 30\n      tensor = [1., 2., 3., 4.]\n      results = []\n      if reported_group_size is None:\n        reported_group_size = group_size\n      for i in range(group_size):\n        with ops.device('/{}:{}'.format(device, i)):\n          input_data = constant_op.constant(tensor)\n          result = collective_op(\n              input_data,\n              group_size=reported_group_size,\n              group_key=group_key,\n              instance_key=instance_key,\n              communication_hint=communication,\n              timeout=timeout)\n          results.append(result)\n      return results\n\n    run(2, 2)\n\n    start_time = time.time()\n    with self.assertRaisesRegex(errors.DeadlineExceededError,\n                                'Collective has timed out during execution'):\n      run(1, 2)\n    elapsed = time.time() - start_time\n    self.assertAllGreaterEqual(elapsed, timeout)\n\n  def testParamResolutionAfterTimeout(self, collective_op, device,\n                                      communication):\n    dev0 = '/device:%s:0' % device\n    dev1 = '/device:%s:1' % device\n    timeout = 1.5\n    group_key = 20\n    instance_key = 30\n    input_data = constant_op.constant([1., 2., 3., 4.])\n\n    # This timeout comes from param solution.\n    with self.assertRaisesRegex(\n        errors.DeadlineExceededError,\n        'Collective has timed out waiting for other workers'):\n      with ops.device(dev0):\n        collective_op(\n            input_data,\n            group_size=2,\n            group_key=group_key,\n            instance_key=instance_key,\n            communication_hint=communication,\n            timeout=timeout)\n\n    # We launch the second device after the first device times out. This is to\n    # simulate the situation when other workers are slow and the timeout is\n    # short. It should error immediately.\n    with self.assertRaisesRegex(\n        errors.DeadlineExceededError,\n        'Collective has timed out waiting for other workers'):\n      with ops.device(dev1):\n        collective_op(\n            input_data,\n            group_size=2,\n            group_key=group_key,\n            instance_key=instance_key,\n            communication_hint=communication)\n\n  def testExecutionAfterTimeout(self, collective_op, device, communication):\n    dev0 = '/device:%s:0' % device\n    dev1 = '/device:%s:1' % device\n    timeout = 1.5\n    group_key = 20\n    instance_key = 30\n    input_data = constant_op.constant([1., 2., 3., 4.])\n\n    @def_function.function\n    def run():\n      for device in [dev0, dev1]:\n        with ops.device(device):\n          collective_op(\n              input_data,\n              group_size=2,\n              group_key=group_key,\n              instance_key=instance_key,\n              communication_hint=communication,\n              timeout=timeout)\n\n    # Run a normal all-reduce to complete param resolution.\n    run()\n\n    with self.assertRaisesRegex(errors.DeadlineExceededError,\n                                'Collective has timed out during execution'):\n      with ops.device(dev0):\n        collective_op(\n            input_data,\n            group_size=2,\n            group_key=group_key,\n            instance_key=instance_key,\n            communication_hint=communication,\n            timeout=timeout)\n\n    # We launch the second device after the first device times out. This is to\n    # simulate the situation when other workers are slow and the timeout is\n    # short. It should error immediately.\n    with self.assertRaisesRegex(errors.DeadlineExceededError,\n                                'Collective has timed out during execution'):\n      with ops.device(dev1):\n        # No timeout.\n        collective_op(\n            input_data,\n            group_size=2,\n            group_key=group_key,\n            instance_key=instance_key,\n            communication_hint=communication)\n\n\nclass CommunicationHintTest(test.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    _setup_context()\n    super().setUp()\n\n  @combinations.generate(\n      combinations.times(collective_op_combinations,\n                         combinations.combine(required_gpus=[0, 1])))\n  def testNCCLFallbackOnCPU(self, collective_op):\n    # communication_hint=NCCL should work for CPU by falling back to RING. The\n    # test doesn't actually require GPU, only GPU builds. We specify\n    # required_gpus=1 so that it's tested with GPU builds.\n    dev0 = '/device:CPU:0'\n    dev1 = '/device:CPU:1'\n    group_key = 20\n    instance_key = 30\n    input_data = constant_op.constant([1., 2., 3., 4.])\n\n    @def_function.function\n    def run():\n      for device in [dev0, dev1]:\n        with ops.device(device):\n          collective_op(\n              input_data,\n              group_size=2,\n              group_key=group_key,\n              instance_key=instance_key,\n              communication_hint='NCCL')\n\n    run()\n\n\n@combinations.generate(\n    combinations.times(\n        combinations.combine(\n            collective_op=[\n                combinations.NamedObject('all_reduce_v2',\n                                         CollectiveOpsV2.all_reduce),\n                combinations.NamedObject('all_gather_v2',\n                                         CollectiveOpsV2.all_gather),\n            ],\n            mode='eager'), device_combination))\nclass OrderingTest(test.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    _setup_context()\n    super().setUp()\n\n  def testOrdering(self, collective_op, device, communication):\n    dev0 = '/device:%s:0' % device\n    dev1 = '/device:%s:1' % device\n    group_size = 2\n    group_key = 100\n    instance_key = 100\n    in_tensor = constant_op.constant([1.])\n\n    with ops.device(dev0):\n      token0 = resource_variable_ops.ResourceVariable(0.)\n    with ops.device(dev1):\n      token1 = resource_variable_ops.ResourceVariable(0.)\n\n    @def_function.function\n    def f():\n      # Launch the first collective with token.\n      with ops.device(dev0):\n        collective_op(\n            in_tensor,\n            group_size,\n            group_key,\n            instance_key,\n            ordering_token=token0.handle)\n      with ops.device(dev1):\n        collective_op(\n            in_tensor,\n            group_size,\n            group_key,\n            instance_key,\n            ordering_token=token1.handle)\n      # Launch the second collective without token.\n      with ops.device(dev0):\n        collective_op(in_tensor, group_size, group_key, instance_key)\n      with ops.device(dev1):\n        collective_op(in_tensor, group_size, group_key, instance_key)\n      # Launch the third collective with token.\n      with ops.device(dev0):\n        collective_op(\n            in_tensor,\n            group_size,\n            group_key,\n            instance_key,\n            ordering_token=token0.handle)\n      with ops.device(dev1):\n        collective_op(\n            in_tensor,\n            group_size,\n            group_key,\n            instance_key,\n            ordering_token=token1.handle)\n\n    graph = f.get_concrete_function().graph\n    for device in [dev0, dev1]:\n      # Try to find the third collective, which should have the first collective\n      # as a control input.\n      third = None\n      for op in graph.get_operations():\n        if (op.type.startswith('Collective') and op.device.endswith(device) and\n            op.control_inputs and\n            op.control_inputs[0].type.startswith('Collective')):\n          self.assertIsNone(third)\n          third = op\n      self.assertIsNotNone(third)\n      # Verify it's not the second collective by looking at the inputs.\n      self.assertTrue(any(v.dtype == dtypes.resource for v in third.inputs))\n      first = third.control_inputs[0]\n      self.assertEqual(third.device, first.device)\n      # Verify it's not the second collective by looking at the inputs.\n      self.assertTrue(any(v.dtype == dtypes.resource for v in first.inputs))\n      self.assertEmpty(first.control_inputs)\n\n\nclass InputPipelineTest(test.TestCase):\n\n  def setUp(self):\n    super().setUp()\n    _setup_context()\n\n  def testMap(self):\n    group_size = 2\n    group_key = 100\n    instance_key = 100\n\n    def create_dataset_and_fetch_one(t):\n      dataset = dataset_ops.Dataset.from_tensor_slices([t])\n\n      def reduce_fn(t):\n        return CollectiveOpsV2.all_reduce(\n            t,\n            group_size=group_size,\n            group_key=group_key,\n            instance_key=instance_key)\n\n      dataset = dataset.map(reduce_fn)\n      return next(iter(dataset))\n\n    @def_function.function\n    def f():\n      with ops.device('CPU:0'):\n        value0 = create_dataset_and_fetch_one([1.])\n      with ops.device('CPU:1'):\n        value1 = create_dataset_and_fetch_one([2.])\n      return value0, value1\n\n    self.assertAllEqual(self.evaluate(f()), [[3.], [3.]])\n\n\nclass CollectiveOpsV3Test(test.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super().setUp()\n    _setup_context()\n\n  def testGroupInitialization(self):\n    group_size = 2\n    group_key = 100\n\n    @def_function.function\n    def f():\n      with ops.device('CPU:0'):\n        _collective_ops.initialize_communicator(\n            group_key=group_key, rank=0, group_size=group_size)\n      with ops.device('CPU:1'):\n        _collective_ops.initialize_communicator(\n            group_key=group_key, rank=1, group_size=group_size)\n\n      # TODO(b/193864859): Add validation with reduction op.\n\n    self.evaluate(f())\n\n  @combinations.generate(device_combination)\n  def testAllReduceV3(self, device, communication):\n    group_size = 2\n    group_key = 101\n\n    dev0 = '/device:%s:0' % device\n    dev1 = '/device:%s:1' % device\n\n    @def_function.function\n    def run_all_reduce_2devices():\n      collectives = []\n      with ops.device(dev0):\n        group_handle0 = _collective_ops.initialize_communicator(\n            group_key=group_key,\n            rank=0,\n            group_size=group_size,\n            communication_hint=communication)\n        collectives.append(\n            _collective_ops.all_reduce_v3(\n                group_handle0, [1.0], reduction='Add'))\n      with ops.device(dev1):\n        group_handle1 = _collective_ops.initialize_communicator(\n            group_key=group_key,\n            rank=1,\n            group_size=group_size,\n            communication_hint=communication)\n        collectives.append(\n            _collective_ops.all_reduce_v3(\n                group_handle1, [2.0], reduction='Add'))\n      return collectives\n\n    for result in run_all_reduce_2devices():\n      self.assertAllClose(result, [3.], rtol=1e-5, atol=1e-5)\n\n  @combinations.generate(device_combination)\n  def testAllToAllV3(self, device, communication):\n    group_size = 2\n    group_key = 104\n\n    dev0 = '/device:%s:0' % device\n    dev1 = '/device:%s:1' % device\n\n    @def_function.function\n    def run_all_to_all_2devices():\n      collectives = []\n      with ops.device(dev0):\n        group_handle0 = _collective_ops.initialize_communicator(\n            group_key=group_key,\n            rank=0,\n            group_size=group_size,\n            communication_hint=communication)\n        collectives.append(\n            _collective_ops.all_to_all_v3(group_handle0, [1.0, 3.0]))\n      with ops.device(dev1):\n        group_handle1 = _collective_ops.initialize_communicator(\n            group_key=group_key,\n            rank=1,\n            group_size=group_size,\n            communication_hint=communication)\n        collectives.append(\n            _collective_ops.all_to_all_v3(group_handle1, [2.0, 4.0]))\n      return collectives\n\n    result = run_all_to_all_2devices()\n    self.assertAllClose(result[0], [1.0, 2.0], rtol=1e-5, atol=1e-5)\n    self.assertAllClose(result[1], [3.0, 4.0], rtol=1e-5, atol=1e-5)\n\n\ndef _setup_context():\n  context._reset_context()\n  test_util.set_logical_devices_to_at_least('CPU', 4)\n  context.ensure_initialized()\n\n\nif __name__ == '__main__':\n  os.environ['NCCL_DEBUG'] = 'INFO'\n  v2_compat.enable_v2_behavior()\n  test.main()"