"# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for fused_batch_norm related functionality in tensorflow.ops.nn.\"\"\"\n\nimport numpy as np\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_grad\nfrom tensorflow.python.ops import nn_impl\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.platform import test\n\n\nclass BatchNormalizationTest(test.TestCase):\n\n  def _batch_norm(self, x, mean, var, offset, scale, epsilon):\n    # We compute the batch norm manually in this function because\n    # nn_impl.batch_normalization does not support float16 yet.\n    # TODO(reedwm): Add float16 support to nn_impl.batch_normalization.\n    inv = math_ops.rsqrt(var + epsilon) * scale\n    y = math_ops.cast(x, scale.dtype) * inv + (offset - mean * inv)\n    return math_ops.cast(y, x.dtype)\n\n  def _inference_ref(self, x, scale, offset, mean, var, epsilon, data_format):\n    if data_format not in ['NHWC', 'NCHW', 'NDHWC', 'NCDHW']:\n      raise ValueError('data_format must be NCHW or NHWC for 4D tensors or'\n                       'NCDHW or NDHWC for 5D tensors, got %s.' % data_format)\n    if data_format == 'NCHW':\n      x = array_ops.transpose(x, [0, 2, 3, 1])\n    elif data_format == 'NCDHW':\n      x = array_ops.transpose(x, [0, 2, 3, 4, 1])\n    y = self._batch_norm(x, mean, var, offset, scale, epsilon)\n    if data_format == 'NCHW':\n      y = array_ops.transpose(y, [0, 3, 1, 2])\n    elif data_format == 'NCDHW':\n      y = array_ops.transpose(y, [0, 4, 1, 2, 3])\n    return self.evaluate(y)\n\n  def _test_inference(self,\n                      x_shape,\n                      x_dtype,\n                      scale_shape,\n                      scale_dtype,\n                      use_gpu=True,\n                      exponential_avg_factor=1.0,\n                      data_format='NHWC'):\n    np.random.seed(1)\n    x_val = np.random.random_sample(x_shape).astype(x_dtype)\n    scale_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n    offset_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n    mean_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n    var_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n\n    with self.cached_session(use_gpu=use_gpu) as sess:\n      x = constant_op.constant(x_val, name='x')\n      scale = constant_op.constant(scale_val, name='scale')\n      offset = constant_op.constant(offset_val, name='offset')\n      mean = constant_op.constant(mean_val, name='mean')\n      var = constant_op.constant(var_val, name='variance')\n      epsilon = 0.001\n      y, _, _ = nn_impl.fused_batch_norm(\n          x,\n          scale,\n          offset,\n          mean=mean,\n          variance=var,\n          epsilon=epsilon,\n          exponential_avg_factor=exponential_avg_factor,\n          data_format=data_format,\n          is_training=False)\n      y_val = self.evaluate(y)\n      y_ref = self._inference_ref(x, scale, offset, mean, var, epsilon,\n                                  data_format)\n    # An atol value of 1e-3 is too small for float16's, because some adjacent\n    # float16 values that y_val can take are greater than 1e-3 apart, e.g.\n    # 2.16602 and 2.16797.\n    atol = 2e-3 if x_dtype == np.float16 else 1e-3\n    self.assertAllClose(y_ref, y_val, atol=atol)\n\n  def _running_mean(self, old_mean, new_val, factor):\n    if factor == 1.0:\n      return new_val\n    else:\n      return (1.0 - factor) * old_mean + factor * new_val\n\n  def _training_ref(self, x, scale, offset, old_mean, old_var,\n                    exponential_avg_factor, epsilon, data_format):\n    if data_format not in ['NHWC', 'NCHW', 'NDHWC', 'NCDHW']:\n      raise ValueError('data_format must be NCHW or NHWC for 4D tensors or'\n                       'NCDHW or NDHWC for 5D tensors, got %s.' % data_format)\n    use_4d_tensor = (x.shape.ndims == 4)\n    if data_format == 'NCHW':\n      x = array_ops.transpose(x, [0, 2, 3, 1])\n    elif data_format == 'NCDHW':\n      x = array_ops.transpose(x, [0, 2, 3, 4, 1])\n\n    mean_axis = [0, 1, 2] if use_4d_tensor else [0, 1, 2, 3]\n    batch_mean, batch_var = nn_impl.moments(\n        math_ops.cast(x, scale.dtype), mean_axis, keep_dims=False)\n\n    y = self._batch_norm(x, batch_mean, batch_var, offset, scale, epsilon)\n    if data_format == 'NCHW':\n      y = array_ops.transpose(y, [0, 3, 1, 2])\n    elif data_format == 'NCDHW':\n      y = array_ops.transpose(y, [0, 4, 1, 2, 3])\n\n    # This is for Bessel's correction. tf.nn.moments uses n, instead of n-1, as\n    # the denominator in the formula to calculate variance, while\n    # tf.compat.v1.nn.fused_batch_norm has Bessel's correction built in.\n    sample_size = math_ops.cast(\n        array_ops.size(x) / array_ops.size(scale), scale.dtype)\n    batch_var_corrected = batch_var * sample_size / (\n        math_ops.maximum(sample_size - 1.0, 1.0))\n\n    mean = self._running_mean(old_mean, batch_mean, exponential_avg_factor)\n    var = self._running_mean(old_var, batch_var_corrected,\n                             exponential_avg_factor)\n    return self.evaluate(y), self.evaluate(mean), self.evaluate(var)\n\n  def _test_training(self,\n                     x_shape,\n                     x_dtype,\n                     scale_shape,\n                     scale_dtype,\n                     use_gpu=True,\n                     exponential_avg_factor=1.0,\n                     data_format='NHWC'):\n    np.random.seed(1)\n    x_val = np.random.random_sample(x_shape).astype(x_dtype)\n    scale_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n    offset_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n    if exponential_avg_factor == 1.0:\n      old_mean_val = None\n      old_var_val = None\n    else:\n      old_mean_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n      old_var_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n\n    with self.cached_session(use_gpu=use_gpu) as sess:\n      x = constant_op.constant(x_val, name='x')\n      scale = constant_op.constant(scale_val, name='scale')\n      offset = constant_op.constant(offset_val, name='offset')\n      epsilon = 0.001\n      y, mean, var = nn_impl.fused_batch_norm(\n          x,\n          scale,\n          offset,\n          mean=old_mean_val,\n          variance=old_var_val,\n          epsilon=epsilon,\n          exponential_avg_factor=exponential_avg_factor,\n          data_format=data_format,\n          is_training=True)\n      y_val, mean_val, var_val = self.evaluate([y, mean, var])\n      y_ref, mean_ref, var_ref = self._training_ref(x, scale, offset,\n                                                    old_mean_val, old_var_val,\n                                                    exponential_avg_factor,\n                                                    epsilon, data_format)\n    y_atol = 2e-3 if x_dtype == np.float16 else 1e-3\n    self.assertAllClose(y_ref, y_val, atol=y_atol)\n    self.assertAllClose(mean_ref, mean_val, atol=1e-3)\n    self.assertAllClose(var_ref, var_val, atol=1e-3)\n\n  def _compute_gradient_error_float16(self, x, x32, x_shape, y, y32, y_shape):\n    \"\"\"Computes the gradient error for float16 inputs and/or outputs.\n\n    This returns the same value as gradient_checker.compute_gradient_error. The\n    difference is that gradient_checker.compute_gradient_error does not\n    numerically compute the gradients in a numerically stable way for float16\n    tensors. To fix this, this function requires float32 versions of x and y to\n    numerically compute the gradients, to compare with the float16 symbolically\n    computed gradients.\n\n    Args:\n      x: The input tensor.\n      x32: A float32 version of x.\n      x_shape: The shape of x.\n      y: The output tensor.\n      y32: A float32 version of y. Must be calculated based on x32, not x.\n      y_shape: The shape of y.\n\n    Returns:\n      The maximum error in between the two Jacobians, as in\n      gradient_checker.compute_gradient_error.\n    \"\"\"\n    x_init_val = np.random.random_sample(x_shape).astype(np.float16)\n    x32_init_val = x_init_val.astype(np.float32)\n\n    # TODO(reedwm): Do not perform the unnecessary computations in\n    # compute_gradient, since they double the computation time of this function.\n    theoretical_grad, _ = gradient_checker.compute_gradient(\n        x, x_shape, y, y_shape, delta=1e-3, x_init_value=x_init_val)\n    _, numerical_grad = gradient_checker.compute_gradient(\n        x32, x_shape, y32, y_shape, delta=1e-3, x_init_value=x32_init_val)\n\n    # If grad is empty, no error.\n    if theoretical_grad.size == 0 and numerical_grad.size == 0:\n      return 0\n    return np.fabs(theoretical_grad - numerical_grad).max()\n\n  def _test_gradient(self,\n                     x_shape,\n                     x_dtype,\n                     scale_shape,\n                     scale_dtype,\n                     use_gpu=True,\n                     exponential_avg_factor=1.0,\n                     data_format='NHWC',\n                     is_training=True):\n    np.random.seed(1)\n    x_val = np.random.random_sample(x_shape).astype(x_dtype)\n    scale_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n    offset_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n\n    with self.cached_session(use_gpu=use_gpu):\n      x = constant_op.constant(x_val, name='x')\n      scale = constant_op.constant(scale_val, name='scale')\n      offset = constant_op.constant(offset_val, name='offset')\n      if is_training and exponential_avg_factor == 1.0:\n        pop_mean = None\n        pop_var = None\n      else:\n        pop_mean = np.random.random_sample(scale_shape).astype(scale_dtype)\n        pop_var = np.random.random_sample(scale_shape).astype(scale_dtype)\n      y, _, _ = nn_impl.fused_batch_norm(\n          x,\n          scale,\n          offset,\n          mean=pop_mean,\n          variance=pop_var,\n          exponential_avg_factor=exponential_avg_factor,\n          data_format=data_format,\n          is_training=is_training)\n      if x_dtype != np.float16:\n        err_x = gradient_checker.compute_gradient_error(x, x_shape, y, x_shape)\n        err_scale = gradient_checker.compute_gradient_error(\n            scale, scale_shape, y, x_shape)\n        err_offset = gradient_checker.compute_gradient_error(\n            offset, scale_shape, y, x_shape)\n      else:\n        x32 = constant_op.constant(x_val, name='x32', dtype=dtypes.float32)\n        y32, _, _ = nn_impl.fused_batch_norm(\n            x32,\n            scale,\n            offset,\n            mean=pop_mean,\n            variance=pop_var,\n            data_format=data_format,\n            exponential_avg_factor=exponential_avg_factor,\n            is_training=is_training)\n        err_x = self._compute_gradient_error_float16(x, x32, x_shape, y, y32,\n                                                     x_shape)\n        err_scale = self._compute_gradient_error_float16(\n            scale, scale, scale_shape, y, y32, x_shape)\n        err_offset = self._compute_gradient_error_float16(\n            offset, offset, scale_shape, y, y32, x_shape)\n\n    x_err_tolerance = 2e-3 if x_dtype == np.float16 else 1e-3\n    scale_err_tolerance = 1e-3\n    self.assertLess(err_x, x_err_tolerance)\n    self.assertLess(err_scale, scale_err_tolerance)\n    self.assertLess(err_offset, scale_err_tolerance)\n\n  def _test_grad_grad(self,\n                      x_shape,\n                      x_dtype,\n                      scale_shape,\n                      scale_dtype,\n                      use_gpu=True,\n                      exponential_avg_factor=1.0,\n                      data_format='NHWC',\n                      is_training=True,\n                      err_tolerance=1e-3):\n    np.random.seed(1)\n    x_val = np.random.random_sample(x_shape).astype(x_dtype)\n    grad_y_val = np.random.random_sample(x_shape).astype(x_dtype)\n    scale_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n    offset_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n\n    with self.cached_session(use_gpu=use_gpu) as sess:\n      x = constant_op.constant(x_val, name='x')\n      grad_y = constant_op.constant(grad_y_val, name='grad_y')\n      scale = constant_op.constant(scale_val, name='scale')\n      offset = constant_op.constant(offset_val, name='offset')\n      if is_training and exponential_avg_factor == 1.0:\n        pop_mean = None\n        pop_var = None\n      else:\n        pop_mean = np.random.random_sample(scale_shape).astype(scale_dtype)\n        pop_var = np.random.random_sample(scale_shape).astype(scale_dtype)\n      y, _, _ = nn_impl.fused_batch_norm(\n          x,\n          scale,\n          offset,\n          mean=pop_mean,\n          variance=pop_var,\n          exponential_avg_factor=exponential_avg_factor,\n          data_format=data_format,\n          is_training=is_training)\n      grad_x, grad_scale, grad_offset = gradients_impl.gradients(\n          y, [x, scale, offset], grad_y)\n\n      if is_training:\n        epsilon = y.op.get_attr('epsilon')\n        data_format = y.op.get_attr('data_format')\n        grad_vals = self.evaluate([grad_x, grad_scale, grad_offset])\n        grad_internal = nn_grad._BatchNormGrad(grad_y, x, scale, pop_mean,\n                                               pop_var, epsilon, data_format)\n        grad_internal_vals = self.evaluate(list(grad_internal))\n        for grad_val, grad_internal_val in zip(grad_vals, grad_internal_vals):\n          self.assertAllClose(grad_val, grad_internal_val, atol=err_tolerance)\n\n      if x_dtype != np.float16:\n        err_grad_grad_y_1 = gradient_checker.compute_gradient_error(\n            grad_y, x_shape, grad_x, x_shape)\n        err_grad_grad_y_2 = gradient_checker.compute_gradient_error(\n            grad_y, x_shape, grad_scale, scale_shape)\n        err_grad_grad_y_3 = gradient_checker.compute_gradient_error(\n            grad_y, x_shape, grad_offset, scale_shape)\n        # In freeze mode, grad_x is not a function of x.\n        if is_training:\n          err_grad_x_1 = gradient_checker.compute_gradient_error(\n              x, x_shape, grad_x, x_shape)\n        err_grad_x_2 = gradient_checker.compute_gradient_error(\n            x, x_shape, grad_scale, scale_shape)\n\n        err_grad_scale = gradient_checker.compute_gradient_error(\n            scale, scale_shape, grad_x, x_shape)\n      else:\n        x32 = constant_op.constant(x_val, dtype=dtypes.float32, name='x32')\n        grad_y32 = constant_op.constant(\n            grad_y_val, dtype=dtypes.float32, name='grad_y32')\n        y32, _, _ = nn_impl.fused_batch_norm(\n            x32,\n            scale,\n            offset,\n            mean=pop_mean,\n            variance=pop_var,\n            exponential_avg_factor=exponential_avg_factor,\n            data_format=data_format,\n            is_training=is_training)\n        grad_x32, grad_scale32, grad_offset32 = gradients_impl.gradients(\n            y32, [x32, scale, offset], grad_y32)\n        err_grad_grad_y_1 = self._compute_gradient_error_float16(\n            grad_y, grad_y32, x_shape, grad_x, grad_x32, x_shape)\n        err_grad_grad_y_2 = self._compute_gradient_error_float16(\n            grad_y, grad_y32, x_shape, grad_scale, grad_scale32, scale_shape)\n        err_grad_grad_y_3 = self._compute_gradient_error_float16(\n            grad_y, grad_y32, x_shape, grad_offset, grad_offset32, scale_shape)\n        # In freeze mode, grad_x is not a function of x.\n        if is_training:\n          err_grad_x_1 = self._compute_gradient_error_float16(\n              x, x32, x_shape, grad_x, grad_x32, x_shape)\n        err_grad_x_2 = self._compute_gradient_error_float16(\n            x, x32, x_shape, grad_scale, grad_scale32, scale_shape)\n\n        err_grad_scale = self._compute_gradient_error_float16(\n            scale, scale, scale_shape, grad_x, grad_x32, x_shape)\n\n    self.assertLess(err_grad_grad_y_1, err_tolerance)\n    self.assertLess(err_grad_grad_y_2, err_tolerance)\n    self.assertLess(err_grad_grad_y_3, err_tolerance)\n    if is_training:\n      self.assertLess(err_grad_x_1, err_tolerance)\n    self.assertLess(err_grad_x_2, err_tolerance)\n    self.assertLess(err_grad_scale, err_tolerance)\n\n  def _runtests(self, x_shape, is_training, gradient_test=False,\n                cpu_only=False):\n    if len(x_shape) == 4:\n      data_format_list = ['NHWC', 'NCHW']\n    else:\n      data_format_list = ['NCDHW', 'NDHWC']\n    use_gpu_vals = [False]\n    if test.is_gpu_available(cuda_only=True) and not cpu_only:\n      use_gpu_vals += [True]\n    factors = [1.0, 0.6]\n    for dtype in [np.float16, np.float32]:\n      for use_gpu in use_gpu_vals:\n        for data_format in data_format_list:\n          if data_format == 'NHWC' or data_format == 'NDHWC':\n            scale_shape = x_shape[-1:]\n          else:\n            scale_shape = x_shape[1:2]\n          for exponential_avg_factor in factors:\n            if gradient_test:\n              self._test_gradient(\n                  x_shape,\n                  dtype,\n                  scale_shape,\n                  np.float32,\n                  use_gpu=use_gpu,\n                  data_format=data_format,\n                  is_training=is_training,\n                  exponential_avg_factor=exponential_avg_factor)\n            else:\n              if is_training:\n                self._test_training(\n                    x_shape,\n                    dtype,\n                    scale_shape,\n                    np.float32,\n                    use_gpu=use_gpu,\n                    data_format=data_format,\n                    exponential_avg_factor=exponential_avg_factor)\n              else:\n                self._test_inference(\n                    x_shape,\n                    dtype,\n                    scale_shape,\n                    np.float32,\n                    use_gpu=use_gpu,\n                    data_format=data_format,\n                    exponential_avg_factor=exponential_avg_factor)\n\n  def testInferenceShape1(self):\n    x_shape = [1, 1, 6, 1]\n    self._runtests(x_shape, False)\n\n  def testInferenceShape2(self):\n    x_shape = [1, 1, 6, 2]\n    self._runtests(x_shape, False)\n\n  def testInferenceShape3(self):\n    x_shape = [1, 2, 1, 6]\n    self._runtests(x_shape, False)\n\n  def testInferenceShape4(self):\n    x_shape = [27, 131, 127, 6]\n    self._runtests(x_shape, False)\n\n  def testInferenceShape5(self):\n    x_shape = [0, 131, 127, 6]\n    self._runtests(x_shape, False)\n\n  def testInferenceShape6(self):\n    x_shape = [1, 1, 1, 1]\n    # GPU kernel doesn't properly handle case where non-channel dimensions are 1\n    self._runtests(x_shape, False, cpu_only=True)\n\n  def testInferenceShape7(self):\n    x_shape = [1, 2, 6, 1, 3]\n    self._runtests(x_shape, False)\n\n  def testTrainingShape1(self):\n    x_shape = [1, 1, 6, 1]\n    self._runtests(x_shape, True)\n\n  def testTrainingShape2(self):\n    x_shape = [1, 1, 6, 2]\n    self._runtests(x_shape, True)\n\n  def testTrainingShape3(self):\n    x_shape = [1, 2, 1, 6]\n    self._runtests(x_shape, True)\n\n  def testTrainingShape4(self):\n    x_shape = [27, 131, 127, 6]\n    self._runtests(x_shape, True)\n\n  @test_util.disable_xla('b/141236973: Empty inputs wrong on CPU.')\n  def testTrainingShape5(self):\n    x_shape = [0, 131, 127, 6]\n    self._runtests(x_shape, True)\n\n  @test_util.run_deprecated_v1\n  def testTrainingShape6(self):\n    x_shape = [1, 1, 1, 1]\n    # GPU kernel doesn't properly handle case where non-channel dimensions are 1\n    self._runtests(x_shape, True, cpu_only=True)\n\n  def testTrainingShape7(self):\n    x_shape = [1, 2, 6, 1, 3]\n    self._runtests(x_shape, True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradInferenceShape1(self):\n    x_shape = [1, 1, 6, 1]\n    self._runtests(x_shape, is_training=False, gradient_test=True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradInferenceShape2(self):\n    x_shape = [1, 1, 6, 2]\n    self._runtests(x_shape, is_training=False, gradient_test=True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradInferenceShape3(self):\n    x_shape = [1, 2, 1, 6]\n    self._runtests(x_shape, is_training=False, gradient_test=True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradInferenceShape4(self):\n    x_shape = [5, 7, 11, 4]\n    self._runtests(x_shape, is_training=False, gradient_test=True)\n\n  @test_util.run_deprecated_v1\n  @test_util.disable_xla('This test never passed for XLA')\n  def testBatchNormGradInferenceShape5(self):\n    x_shape = [0, 7, 11, 4]\n    self._runtests(x_shape, is_training=False, gradient_test=True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradInferenceShape6(self):\n    x_shape = [1, 1, 1, 1]\n    # GPU kernel doesn't properly handle case where non-channel dimensions are 1\n    self._runtests(x_shape, is_training=False, gradient_test=True,\n                   cpu_only=True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradInferenceShape7(self):\n    x_shape = [1, 2, 6, 1, 3]\n    self._runtests(x_shape, is_training=False, gradient_test=True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradTrainingShape1(self):\n    x_shape = [1, 1, 6, 1]\n    self._runtests(x_shape, is_training=True, gradient_test=True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradTrainingShape2(self):\n    x_shape = [1, 1, 6, 2]\n    self._runtests(x_shape, is_training=True, gradient_test=True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradTrainingShape3(self):\n    x_shape = [1, 2, 1, 6]\n    self._runtests(x_shape, is_training=True, gradient_test=True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradTrainingShape4(self):\n    x_shape = [5, 7, 11, 4]\n    self._runtests(x_shape, is_training=True, gradient_test=True)\n\n  @test_util.run_deprecated_v1\n  @test_util.disable_xla('This test never passed for XLA')\n  def testBatchNormGradTrainingShape5(self):\n    x_shape = [0, 7, 11, 4]\n    self._runtests(x_shape, is_training=True, gradient_test=True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradTrainingShape6(self):\n    x_shape = [1, 1, 1, 1]\n    # GPU kernel doesn't properly handle case where non-channel dimensions are 1\n    self._runtests(x_shape, is_training=True, gradient_test=True, cpu_only=True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradTrainingShape7(self):\n    x_shape = [1, 2, 6, 1, 3]\n    self._runtests(x_shape, is_training=True, gradient_test=True)\n\n  def _testBatchNormGradGrad(self, config):\n    shape = config['shape']\n    err_tolerance = config['err_tolerance']\n    dtype = config['dtype']\n    rank = len(shape)\n    if rank == 4:\n      data_format_nhwc, features_nhwc = 'NHWC', shape[3]\n      data_format_nchw, features_nchw = 'NCHW', shape[1]\n    else:\n      data_format_nhwc, features_nhwc = 'NDHWC', shape[4]\n      data_format_nchw, features_nchw = 'NCDHW', shape[1]\n    for is_training in [True, False]:\n      if test.is_gpu_available(cuda_only=True):\n        self._test_grad_grad(\n            shape,\n            dtype, [features_nhwc],\n            np.float32,\n            use_gpu=True,\n            data_format=data_format_nhwc,\n            is_training=is_training,\n            err_tolerance=err_tolerance)\n        self._test_grad_grad(\n            shape,\n            dtype, [features_nchw],\n            np.float32,\n            use_gpu=True,\n            data_format=data_format_nchw,\n            is_training=is_training,\n            err_tolerance=err_tolerance)\n      self._test_grad_grad(\n          shape,\n          dtype, [features_nhwc],\n          np.float32,\n          use_gpu=False,\n          data_format=data_format_nhwc,\n          is_training=is_training,\n          err_tolerance=err_tolerance)\n      self._test_grad_grad(\n          shape,\n          dtype, [features_nchw],\n          np.float32,\n          use_gpu=False,\n          data_format=data_format_nchw,\n          is_training=is_training,\n          err_tolerance=err_tolerance)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradGradConfig1(self):\n    config = {\n        'shape': [2, 3, 4, 5],\n        'err_tolerance': 1e-2,\n        'dtype': np.float32,\n    }\n    self._testBatchNormGradGrad(config)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradGradConfig2(self):\n    config = {\n        'shape': [2, 3, 2, 2],\n        'err_tolerance': 1e-3,\n        'dtype': np.float32,\n    }\n    self._testBatchNormGradGrad(config)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradGradConfig3(self):\n    config = {\n        'shape': [2, 3, 4, 5],\n        'err_tolerance': 2e-2,\n        'dtype': np.float16,\n    }\n    self._testBatchNormGradGrad(config)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradGradConfig4(self):\n    config = {\n        'shape': [2, 3, 2, 2],\n        'err_tolerance': 2e-3,\n        'dtype': np.float16,\n    }\n    self._testBatchNormGradGrad(config)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradGradConfig5(self):\n    config = {\n        'shape': [2, 3, 2, 2, 2],\n        'err_tolerance': 2e-3,\n        'dtype': np.float32,\n    }\n    self._testBatchNormGradGrad(config)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradGradConfig6(self):\n    config = {\n        'shape': [2, 3, 2, 2, 2],\n        'err_tolerance': 3e-3,\n        'dtype': np.float16,\n    }\n    self._testBatchNormGradGrad(config)\n\n  def test5dBatchNormFollowedByRelu(self):\n    # The remapper grappler pass previously did not properly handle a 5D\n    # inference FusedBatchNorm followed by Relu. This asserts that this case is\n    # correctly handled.\n    np.random.seed(1)\n    x = np.random.random_sample((2, 3, 2, 2, 3)).astype(np.float32)\n    scale = np.random.random_sample((3,)).astype(np.float32)\n    offset = np.random.random_sample((3,)).astype(np.float32)\n    mean = np.random.random_sample((3,)).astype(np.float32)\n    var = np.random.random_sample((3,)).astype(np.float32)\n\n    epsilon = 0.001\n    y, _, _ = nn_impl.fused_batch_norm(\n        x,\n        scale,\n        offset,\n        mean=mean,\n        variance=var,\n        epsilon=epsilon,\n        data_format='NCDHW',\n        is_training=False)\n    y = nn_ops.relu(y)\n    y_val = self.evaluate(y)\n    y_ref = self._inference_ref(x, scale, offset, mean, var, epsilon,\n                                'NCDHW')\n    y_ref = np.maximum(y_ref, 0.)\n    self.assertAllClose(y_ref, y_val, atol=1e-3)\n\n\nif __name__ == '__main__':\n  test.main()"