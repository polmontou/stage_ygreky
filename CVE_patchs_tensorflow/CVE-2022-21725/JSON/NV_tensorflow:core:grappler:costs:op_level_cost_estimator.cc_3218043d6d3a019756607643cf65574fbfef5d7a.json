"\n/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/core/grappler/costs/op_level_cost_estimator.h\"\n\n#include \"absl/strings/match.h\"\n#include \"third_party/eigen3/Eigen/Core\"\n#include \"tensorflow/core/framework/attr_value.pb.h\"\n#include \"tensorflow/core/framework/attr_value_util.h\"\n#include \"tensorflow/core/framework/tensor.pb.h\"\n#include \"tensorflow/core/framework/tensor_shape.pb.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/grappler/clusters/utils.h\"\n#include \"tensorflow/core/grappler/costs/op_context.h\"\n#include \"tensorflow/core/grappler/costs/utils.h\"\n#include \"tensorflow/core/platform/errors.h\"\n#include \"tensorflow/core/util/overflow.h\"\n\nnamespace tensorflow {\nnamespace grappler {\n\n// TODO(dyoon): update op to Predict method map for TF ops with V2 or V3 suffix.\nconstexpr int kOpsPerMac = 2;\nconstexpr char kGuaranteeConst[] = \"GuaranteeConst\";\nconstexpr char kAddN[] = \"AddN\";\nconstexpr char kBitCast[] = \"BitCast\";\nconstexpr char kConcatV2[] = \"ConcatV2\";\nconstexpr char kConv2d[] = \"Conv2D\";\nconstexpr char kConv2dBackpropFilter[] = \"Conv2DBackpropFilter\";\nconstexpr char kConv2dBackpropInput[] = \"Conv2DBackpropInput\";\nconstexpr char kFusedConv2dBiasActivation[] = \"FusedConv2DBiasActivation\";\nconstexpr char kDataFormatVecPermute[] = \"DataFormatVecPermute\";\nconstexpr char kDepthToSpace[] = \"DepthToSpace\";\nconstexpr char kDepthwiseConv2dNative[] = \"DepthwiseConv2dNative\";\nconstexpr char kDepthwiseConv2dNativeBackpropFilter[] =\n    \"DepthwiseConv2dNativeBackpropFilter\";\nconstexpr char kDepthwiseConv2dNativeBackpropInput[] =\n    \"DepthwiseConv2dNativeBackpropInput\";\nconstexpr char kMatMul[] = \"MatMul\";\nconstexpr char kXlaEinsum[] = \"XlaEinsum\";\nconstexpr char kEinsum[] = \"Einsum\";\nconstexpr char kExpandDims[] = \"ExpandDims\";\nconstexpr char kFill[] = \"Fill\";\nconstexpr char kSparseMatMul[] = \"SparseMatMul\";\nconstexpr char kSparseTensorDenseMatMul[] = \"SparseTensorDenseMatMul\";\nconstexpr char kPlaceholder[] = \"Placeholder\";\nconstexpr char kIdentity[] = \"Identity\";\nconstexpr char kIdentityN[] = \"IdentityN\";\nconstexpr char kRefIdentity[] = \"RefIdentity\";\nconstexpr char kNoOp[] = \"NoOp\";\nconstexpr char kReshape[] = \"Reshape\";\nconstexpr char kSplit[] = \"Split\";\nconstexpr char kSqueeze[] = \"Squeeze\";\nconstexpr char kRecv[] = \"_Recv\";\nconstexpr char kSend[] = \"_Send\";\nconstexpr char kBatchMatMul[] = \"BatchMatMul\";\nconstexpr char kBatchMatMulV2[] = \"BatchMatMulV2\";\nconstexpr char kOneHot[] = \"OneHot\";\nconstexpr char kPack[] = \"Pack\";\nconstexpr char kRank[] = \"Rank\";\nconstexpr char kRange[] = \"Range\";\nconstexpr char kShape[] = \"Shape\";\nconstexpr char kShapeN[] = \"ShapeN\";\nconstexpr char kSize[] = \"Size\";\nconstexpr char kStopGradient[] = \"StopGradient\";\nconstexpr char kPreventGradient[] = \"PreventGradient\";\nconstexpr char kGather[] = \"Gather\";\nconstexpr char kGatherNd[] = \"GatherNd\";\nconstexpr char kGatherV2[] = \"GatherV2\";\nconstexpr char kScatterAdd[] = \"ScatterAdd\";\nconstexpr char kScatterDiv[] = \"ScatterDiv\";\nconstexpr char kScatterMax[] = \"ScatterMax\";\nconstexpr char kScatterMin[] = \"ScatterMin\";\nconstexpr char kScatterMul[] = \"ScatterMul\";\nconstexpr char kScatterSub[] = \"ScatterSub\";\nconstexpr char kScatterUpdate[] = \"ScatterUpdate\";\nconstexpr char kSlice[] = \"Slice\";\nconstexpr char kStridedSlice[] = \"StridedSlice\";\nconstexpr char kSpaceToDepth[] = \"SpaceToDepth\";\nconstexpr char kTranspose[] = \"Transpose\";\nconstexpr char kTile[] = \"Tile\";\nconstexpr char kMaxPool[] = \"MaxPool\";\nconstexpr char kMaxPoolGrad[] = \"MaxPoolGrad\";\nconstexpr char kAvgPool[] = \"AvgPool\";\nconstexpr char kAvgPoolGrad[] = \"AvgPoolGrad\";\nconstexpr char kFusedBatchNorm[] = \"FusedBatchNorm\";\nconstexpr char kFusedBatchNormGrad[] = \"FusedBatchNormGrad\";\nconstexpr char kQuantizedMatMul[] = \"QuantizedMatMul\";\nconstexpr char kQuantizedMatMulV2[] = \"QuantizedMatMulV2\";\nconstexpr char kUnpack[] = \"Unpack\";\nconstexpr char kSoftmax[] = \"Softmax\";\nconstexpr char kResizeBilinear[] = \"ResizeBilinear\";\nconstexpr char kCropAndResize[] = \"CropAndResize\";\n// Dynamic control flow ops.\nconstexpr char kSwitch[] = \"Switch\";\nconstexpr char kMerge[] = \"Merge\";\nconstexpr char kEnter[] = \"Enter\";\nconstexpr char kExit[] = \"Exit\";\nconstexpr char kNextIteration[] = \"NextIteration\";\n// Persistent ops.\nconstexpr char kConst[] = \"Const\";\nconstexpr char kVariable[] = \"Variable\";\nconstexpr char kVariableV2[] = \"VariableV2\";\nconstexpr char kAutoReloadVariable[] = \"AutoReloadVariable\";\nconstexpr char kVarHandleOp[] = \"VarHandleOp\";\nconstexpr char kVarHandlesOp[] = \"_VarHandlesOp\";\nconstexpr char kReadVariableOp[] = \"ReadVariableOp\";\nconstexpr char kReadVariablesOp[] = \"_ReadVariablesOp\";\nconstexpr char kAssignVariableOp[] = \"AssignVariableOp\";\nconstexpr char kAssignAddVariableOp[] = \"AssignAddVariableOp\";\nconstexpr char kAssignSubVariableOp[] = \"AssignSubVariableOp\";\n\nstatic const Costs::Duration kMinComputeTime(1);\nstatic const int64_t kMinComputeOp = 1;\n\nnamespace {\n\nstd::string GetDataFormat(const OpInfo& op_info) {\n  std::string data_format = \"NHWC\";  // Default format.\n  if (op_info.attr().find(\"data_format\") != op_info.attr().end()) {\n    data_format = op_info.attr().at(\"data_format\").s();\n  }\n  return data_format;\n}\n\nstd::string GetFilterFormat(const OpInfo& op_info) {\n  std::string filter_format = \"HWIO\";  // Default format.\n  if (op_info.attr().find(\"filter_format\") != op_info.attr().end()) {\n    filter_format = op_info.attr().at(\"filter_format\").s();\n  }\n  return filter_format;\n}\n\nPadding GetPadding(const OpInfo& op_info) {\n  if (op_info.attr().find(\"padding\") != op_info.attr().end() &&\n      op_info.attr().at(\"padding\").s() == \"VALID\") {\n    return Padding::VALID;\n  }\n  return Padding::SAME;  // Default padding.\n}\n\nbool IsTraining(const OpInfo& op_info) {\n  if (op_info.attr().find(\"is_training\") != op_info.attr().end() &&\n      op_info.attr().at(\"is_training\").b()) {\n    return true;\n  }\n  return false;\n}\n\n// TODO(dyoon): support non-4D tensors in the cost functions of convolution\n// related ops (Conv, Pool, BatchNorm, and their backprops) and the related\n// helper functions.\nstd::vector<int64_t> GetStrides(const OpInfo& op_info) {\n  if (op_info.attr().find(\"strides\") != op_info.attr().end()) {\n    const auto strides = op_info.attr().at(\"strides\").list().i();\n    DCHECK(strides.size() == 4)\n        << \"Attr strides is not a length-4 vector: \" << op_info.DebugString();\n    if (strides.size() != 4) return {1, 1, 1, 1};\n    return {strides[0], strides[1], strides[2], strides[3]};\n  }\n  return {1, 1, 1, 1};\n}\n\nstd::vector<int64_t> GetKernelSize(const OpInfo& op_info) {\n  if (op_info.attr().find(\"ksize\") != op_info.attr().end()) {\n    const auto ksize = op_info.attr().at(\"ksize\").list().i();\n    DCHECK(ksize.size() == 4)\n        << \"Attr ksize is not a length-4 vector: \" << op_info.DebugString();\n    if (ksize.size() != 4) return {1, 1, 1, 1};\n    return {ksize[0], ksize[1], ksize[2], ksize[3]};\n  }\n  // Note that FusedBatchNorm doesn't have ksize attr, but GetKernelSize returns\n  // {1, 1, 1, 1} in that case.\n  return {1, 1, 1, 1};\n}\n\nint64_t GetOutputSize(const int64_t input, const int64_t filter,\n                      const int64_t stride, const Padding& padding) {\n  // Logic for calculating output shape is from GetWindowedOutputSizeVerbose()\n  // function in third_party/tensorflow/core/framework/common_shape_fns.cc.\n  if (padding == Padding::VALID) {\n    return (input - filter + stride) / stride;\n  } else {  // SAME.\n    return (input + stride - 1) / stride;\n  }\n}\n\n// Return the output element count of a multi-input element-wise op considering\n// broadcasting.\nint64_t CwiseOutputElementCount(const OpInfo& op_info) {\n  int max_rank = 1;\n  for (const OpInfo::TensorProperties& input_properties : op_info.inputs()) {\n    max_rank = std::max(max_rank, input_properties.shape().dim_size());\n  }\n\n  TensorShapeProto output_shape;\n  output_shape.mutable_dim()->Reserve(max_rank);\n  for (int i = 0; i < max_rank; ++i) {\n    output_shape.add_dim();\n  }\n\n  // Expand the shape of the output to follow the numpy-style broadcast rule\n  // which matches each input starting with the trailing dimensions and working\n  // its way forward. To do this, iterate through each input shape's dimensions\n  // in reverse order, and potentially increase the corresponding output\n  // dimension.\n  for (const OpInfo::TensorProperties& input_properties : op_info.inputs()) {\n    const TensorShapeProto& input_shape = input_properties.shape();\n    for (int i = input_shape.dim_size() - 1; i >= 0; --i) {\n      int output_shape_dim_index =\n          i + output_shape.dim_size() - input_shape.dim_size();\n      output_shape.mutable_dim(output_shape_dim_index)\n          ->set_size(std::max(output_shape.dim(output_shape_dim_index).size(),\n                              input_shape.dim(i).size()));\n    }\n  }\n\n  int64_t count = 1;\n  for (int i = 0; i < output_shape.dim_size(); i++) {\n    count *= output_shape.dim(i).size();\n  }\n  return count;\n}\n\n// Helper function for determining whether there are repeated indices in the\n// input Einsum equation.\nbool CheckRepeatedDimensions(const absl::string_view dim_str) {\n  int str_size = dim_str.size();\n  for (int idx = 0; idx < str_size - 1; idx++) {\n    if (dim_str.find(dim_str[idx], idx + 1) != std::string::npos) {\n      return true;\n    }\n  }\n  return false;\n}\n\n// Auxiliary function for determining whether OpLevelCostEstimator is compatible\n// with a given Einsum.\nbool IsEinsumCorrectlyFormed(const OpContext& einsum_context) {\n  const auto& op_info = einsum_context.op_info;\n\n  auto it = op_info.attr().find(\"equation\");\n  if (it == op_info.attr().end()) return false;\n  const absl::string_view equation = it->second.s();\n  std::vector<std::string> equation_split = absl::StrSplit(equation, \"->\");\n\n  if (equation_split.empty()) {\n    LOG(WARNING) << \"Einsum with malformed equation\";\n    return false;\n  }\n  std::vector<absl::string_view> input_split =\n      absl::StrSplit(equation_split[0], ',');\n\n  // The current model covers Einsum operations with two operands and a RHS\n  if (op_info.inputs_size() != 2 || equation_split.size() != 2) {\n    VLOG(1) << \"Missing accurate estimator for op: \" << op_info.op();\n    return false;\n  }\n  const auto& a_input = op_info.inputs(0);\n  const auto& b_input = op_info.inputs(1);\n  absl::string_view rhs_str = equation_split[1];\n  absl::string_view a_input_str = input_split[0];\n  absl::string_view b_input_str = input_split[1];\n\n  // Ellipsis are not currently supported\n  if (absl::StrContains(a_input_str, \"...\") ||\n      absl::StrContains(b_input_str, \"...\")) {\n    VLOG(1) << \"Missing accurate estimator for op: \" << op_info.op()\n            << \", ellipsis not supported\";\n    return false;\n  }\n\n  constexpr int kMatrixRank = 2;\n\n  bool a_input_shape_unknown = false;\n  bool b_input_shape_unknown = false;\n\n  TensorShapeProto a_input_shape = MaybeGetMinimumShape(\n      a_input.shape(), std::max(kMatrixRank, a_input.shape().dim_size()),\n      &a_input_shape_unknown);\n  TensorShapeProto b_input_shape = MaybeGetMinimumShape(\n      b_input.shape(), std::max(kMatrixRank, b_input.shape().dim_size()),\n      &b_input_shape_unknown);\n\n  if (a_input_str.size() != static_cast<size_t>(a_input_shape.dim_size()) ||\n      b_input_str.size() != static_cast<size_t>(b_input_shape.dim_size())) {\n    VLOG(1) << \"Missing accurate estimator for op: \" << op_info.op()\n            << \", equation subscripts don't match tensor rank.\";\n    return false;\n  }\n\n  // Subscripts where axis appears more than once for a single input are not yet\n  // supported\n  if (CheckRepeatedDimensions(a_input_str) ||\n      CheckRepeatedDimensions(b_input_str) ||\n      CheckRepeatedDimensions(rhs_str)) {\n    VLOG(1) << \"Missing accurate estimator for op: \" << op_info.op()\n            << \", Subscripts where axis appears more than once for a single \"\n               \"input are not yet supported\";\n    return false;\n  }\n\n  return true;\n}\n\n}  // namespace\n\n// Return a minimum shape if the shape is unknown. If known, return the original\n// shape.\nTensorShapeProto MaybeGetMinimumShape(const TensorShapeProto& original_shape,\n                                      int rank, bool* found_unknown_shapes) {\n  auto shape = original_shape;\n  bool is_scalar = !shape.unknown_rank() && shape.dim_size() == 0;\n\n  if (shape.unknown_rank() || (!is_scalar && shape.dim_size() < rank)) {\n    *found_unknown_shapes = true;\n    VLOG(2) << \"Use minimum shape because the rank is unknown.\";\n    // The size of each dimension is at least 1, if unknown.\n    for (int i = shape.dim_size(); i < rank; i++) {\n      shape.add_dim()->set_size(1);\n    }\n  } else if (is_scalar) {\n    for (int i = 0; i < rank; i++) {\n      shape.add_dim()->set_size(1);\n    }\n  } else if (shape.dim_size() > rank) {\n    *found_unknown_shapes = true;\n    shape.clear_dim();\n    for (int i = 0; i < rank; i++) {\n      shape.add_dim()->set_size(original_shape.dim(i).size());\n    }\n  } else {\n    for (int i = 0; i < shape.dim_size(); i++) {\n      if (shape.dim(i).size() < 0) {\n        *found_unknown_shapes = true;\n        VLOG(2) << \"Use minimum dim size 1 because the shape is unknown.\";\n        // The size of each dimension is at least 1, if unknown.\n        shape.mutable_dim(i)->set_size(1);\n      }\n    }\n  }\n  return shape;\n}\n\nOpLevelCostEstimator::OpLevelCostEstimator() {\n  // Syntactic sugar to build and return a lambda that takes an OpInfo and\n  // returns a cost.\n  typedef Status (OpLevelCostEstimator::*CostImpl)(const OpContext& op_context,\n                                                   NodeCosts*) const;\n  auto wrap = [this](CostImpl impl)\n      -> std::function<Status(const OpContext&, NodeCosts*)> {\n    return [this, impl](const OpContext& op_context, NodeCosts* node_costs) {\n      return (this->*impl)(op_context, node_costs);\n    };\n  };\n\n  device_cost_impl_.emplace(kConv2d,\n                            wrap(&OpLevelCostEstimator::PredictConv2D));\n  device_cost_impl_.emplace(\n      kConv2dBackpropFilter,\n      wrap(&OpLevelCostEstimator::PredictConv2DBackpropFilter));\n  device_cost_impl_.emplace(\n      kConv2dBackpropInput,\n      wrap(&OpLevelCostEstimator::PredictConv2DBackpropInput));\n  device_cost_impl_.emplace(\n      kFusedConv2dBiasActivation,\n      wrap(&OpLevelCostEstimator::PredictFusedConv2DBiasActivation));\n  // reuse Conv2D for DepthwiseConv2dNative because the calculation is the\n  // same although the actual meaning of the parameters are different. See\n  // comments in PredictConv2D and related functions\n  device_cost_impl_.emplace(kDepthwiseConv2dNative,\n                            wrap(&OpLevelCostEstimator::PredictConv2D));\n  device_cost_impl_.emplace(\n      kDepthwiseConv2dNativeBackpropFilter,\n      wrap(&OpLevelCostEstimator::PredictConv2DBackpropFilter));\n  device_cost_impl_.emplace(\n      kDepthwiseConv2dNativeBackpropInput,\n      wrap(&OpLevelCostEstimator::PredictConv2DBackpropInput));\n  device_cost_impl_.emplace(kMatMul,\n                            wrap(&OpLevelCostEstimator::PredictMatMul));\n  device_cost_impl_.emplace(kSparseMatMul,\n                            wrap(&OpLevelCostEstimator::PredictMatMul));\n  device_cost_impl_.emplace(\n      kSparseTensorDenseMatMul,\n      wrap(&OpLevelCostEstimator::PredictSparseTensorDenseMatMul));\n  device_cost_impl_.emplace(kBatchMatMul,\n                            wrap(&OpLevelCostEstimator::PredictBatchMatMul));\n  device_cost_impl_.emplace(kBatchMatMulV2,\n                            wrap(&OpLevelCostEstimator::PredictBatchMatMul));\n  device_cost_impl_.emplace(kQuantizedMatMul,\n                            wrap(&OpLevelCostEstimator::PredictMatMul));\n  device_cost_impl_.emplace(kQuantizedMatMulV2,\n                            wrap(&OpLevelCostEstimator::PredictMatMul));\n  device_cost_impl_.emplace(kXlaEinsum,\n                            wrap(&OpLevelCostEstimator::PredictEinsum));\n  device_cost_impl_.emplace(kEinsum,\n                            wrap(&OpLevelCostEstimator::PredictEinsum));\n\n  device_cost_impl_.emplace(kNoOp, wrap(&OpLevelCostEstimator::PredictNoOp));\n  device_cost_impl_.emplace(kGuaranteeConst,\n                            wrap(&OpLevelCostEstimator::PredictNoOp));\n\n  device_cost_impl_.emplace(kGather,\n                            wrap(&OpLevelCostEstimator::PredictGatherOrSlice));\n  device_cost_impl_.emplace(kGatherNd,\n                            wrap(&OpLevelCostEstimator::PredictGatherOrSlice));\n  device_cost_impl_.emplace(kGatherV2,\n                            wrap(&OpLevelCostEstimator::PredictGatherOrSlice));\n  device_cost_impl_.emplace(kScatterAdd,\n                            wrap(&OpLevelCostEstimator::PredictScatter));\n  device_cost_impl_.emplace(kScatterDiv,\n                            wrap(&OpLevelCostEstimator::PredictScatter));\n  device_cost_impl_.emplace(kScatterMax,\n                            wrap(&OpLevelCostEstimator::PredictScatter));\n  device_cost_impl_.emplace(kScatterMin,\n                            wrap(&OpLevelCostEstimator::PredictScatter));\n  device_cost_impl_.emplace(kScatterMul,\n                            wrap(&OpLevelCostEstimator::PredictScatter));\n  device_cost_impl_.emplace(kScatterSub,\n                            wrap(&OpLevelCostEstimator::PredictScatter));\n  device_cost_impl_.emplace(kScatterUpdate,\n                            wrap(&OpLevelCostEstimator::PredictScatter));\n\n  device_cost_impl_.emplace(kSlice,\n                            wrap(&OpLevelCostEstimator::PredictGatherOrSlice));\n  device_cost_impl_.emplace(kStridedSlice,\n                            wrap(&OpLevelCostEstimator::PredictGatherOrSlice));\n\n  device_cost_impl_.emplace(kPlaceholder,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kIdentity,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kIdentityN,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kRefIdentity,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kStopGradient,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kPreventGradient,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kReshape,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kRecv,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kSend,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kSwitch,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kMerge,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kEnter,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kExit,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kNextIteration,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n  device_cost_impl_.emplace(kBitCast,\n                            wrap(&OpLevelCostEstimator::PredictIdentity));\n\n  device_cost_impl_.emplace(kConcatV2,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kDataFormatVecPermute,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kDepthToSpace,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kExpandDims,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kFill,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kOneHot,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kPack,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kRange,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kSpaceToDepth,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kSplit,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kSqueeze,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kTranspose,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kTile,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n  device_cost_impl_.emplace(kUnpack,\n                            wrap(&OpLevelCostEstimator::PredictPureMemoryOp));\n\n  device_cost_impl_.emplace(kRank,\n                            wrap(&OpLevelCostEstimator::PredictMetadata));\n  device_cost_impl_.emplace(kShape,\n                            wrap(&OpLevelCostEstimator::PredictMetadata));\n  device_cost_impl_.emplace(kShapeN,\n                            wrap(&OpLevelCostEstimator::PredictMetadata));\n  device_cost_impl_.emplace(kSize,\n                            wrap(&OpLevelCostEstimator::PredictMetadata));\n  device_cost_impl_.emplace(kMaxPool,\n                            wrap(&OpLevelCostEstimator::PredictMaxPool));\n  device_cost_impl_.emplace(kMaxPoolGrad,\n                            wrap(&OpLevelCostEstimator::PredictMaxPoolGrad));\n  device_cost_impl_.emplace(kAvgPool,\n                            wrap(&OpLevelCostEstimator::PredictAvgPool));\n  device_cost_impl_.emplace(kAvgPoolGrad,\n                            wrap(&OpLevelCostEstimator::PredictAvgPoolGrad));\n  device_cost_impl_.emplace(kFusedBatchNorm,\n                            wrap(&OpLevelCostEstimator::PredictFusedBatchNorm));\n  device_cost_impl_.emplace(\n      kFusedBatchNormGrad,\n      wrap(&OpLevelCostEstimator::PredictFusedBatchNormGrad));\n  device_cost_impl_.emplace(kSoftmax,\n                            wrap(&OpLevelCostEstimator::PredictSoftmax));\n  device_cost_impl_.emplace(kResizeBilinear,\n                            wrap(&OpLevelCostEstimator::PredictResizeBilinear));\n  device_cost_impl_.emplace(kCropAndResize,\n                            wrap(&OpLevelCostEstimator::PredictCropAndResize));\n  device_cost_impl_.emplace(\n      kAssignVariableOp, wrap(&OpLevelCostEstimator::PredictAssignVariableOps));\n  device_cost_impl_.emplace(\n      kAssignAddVariableOp,\n      wrap(&OpLevelCostEstimator::PredictAssignVariableOps));\n  device_cost_impl_.emplace(\n      kAssignSubVariableOp,\n      wrap(&OpLevelCostEstimator::PredictAssignVariableOps));\n  device_cost_impl_.emplace(kAddN, wrap(&OpLevelCostEstimator::PredictNaryOp));\n\n  persistent_ops_ = {\n      kConst,       kVariable,       kVariableV2,   kAutoReloadVariable,\n      kVarHandleOp, kReadVariableOp, kVarHandlesOp, kReadVariablesOp};\n\n#define EIGEN_COST(X) Eigen::internal::functor_traits<Eigen::internal::X>::Cost\n\n  // Quantize = apply min and max bounds, multiply by scale factor and round.\n  const int quantize_v2_cost =\n      EIGEN_COST(scalar_product_op<float>) + EIGEN_COST(scalar_max_op<float>) +\n      EIGEN_COST(scalar_min_op<float>) + EIGEN_COST(scalar_round_op<float>);\n  const int quantize_and_dequantize_v2_cost =\n      quantize_v2_cost + EIGEN_COST(scalar_product_op<float>);\n\n  // Unary ops alphabetically sorted\n  elementwise_ops_.emplace(\"Acos\", EIGEN_COST(scalar_acos_op<float>));\n  elementwise_ops_.emplace(\"All\", EIGEN_COST(scalar_boolean_and_op));\n  elementwise_ops_.emplace(\"ArgMax\", EIGEN_COST(scalar_max_op<float>));\n  elementwise_ops_.emplace(\"Asin\", EIGEN_COST(scalar_asin_op<float>));\n  elementwise_ops_.emplace(\"Atan\", EIGEN_COST(scalar_atan_op<float>));\n  elementwise_ops_.emplace(\"Atan2\", EIGEN_COST(scalar_quotient_op<float>) +\n                                        EIGEN_COST(scalar_atan_op<float>));\n  // For now, we use Eigen cost model for float to int16 cast as an example\n  // case; Eigen cost model is zero when src and dst types are identical,\n  // and it uses AddCost (1) when different. We may implement a separate\n  // cost functions for cast ops, using the actual input and output types.\n  elementwise_ops_.emplace(\n      \"Cast\", Eigen::internal::functor_traits<\n                  Eigen::internal::scalar_cast_op<float, int16>>::Cost);\n  elementwise_ops_.emplace(\"Ceil\", EIGEN_COST(scalar_ceil_op<float>));\n  elementwise_ops_.emplace(\"Cos\", EIGEN_COST(scalar_cos_op<float>));\n  elementwise_ops_.emplace(\"Dequantize\", EIGEN_COST(scalar_product_op<float>));\n  elementwise_ops_.emplace(\"Erf\", 1);\n  elementwise_ops_.emplace(\"Erfc\", 1);\n  elementwise_ops_.emplace(\"Exp\", EIGEN_COST(scalar_exp_op<float>));\n  elementwise_ops_.emplace(\"Expm1\", EIGEN_COST(scalar_expm1_op<float>));\n  elementwise_ops_.emplace(\"Floor\", EIGEN_COST(scalar_floor_op<float>));\n  elementwise_ops_.emplace(\"Inv\", EIGEN_COST(scalar_inverse_op<float>));\n  elementwise_ops_.emplace(\"InvGrad\", 1);\n  elementwise_ops_.emplace(\"Lgamma\", 1);\n  elementwise_ops_.emplace(\"Log\", EIGEN_COST(scalar_log_op<float>));\n  elementwise_ops_.emplace(\"Log1p\", EIGEN_COST(scalar_log1p_op<float>));\n  elementwise_ops_.emplace(\"Max\", EIGEN_COST(scalar_max_op<float>));\n  elementwise_ops_.emplace(\"Min\", EIGEN_COST(scalar_min_op<float>));\n  elementwise_ops_.emplace(\"Neg\", EIGEN_COST(scalar_opposite_op<float>));\n  elementwise_ops_.emplace(\"Prod\", EIGEN_COST(scalar_product_op<float>));\n  elementwise_ops_.emplace(\"QuantizeAndDequantizeV2\",\n                           quantize_and_dequantize_v2_cost);\n  elementwise_ops_.emplace(\"QuantizeAndDequantizeV4\",\n                           quantize_and_dequantize_v2_cost);\n  elementwise_ops_.emplace(\"QuantizedSigmoid\",\n                           EIGEN_COST(scalar_logistic_op<float>));\n  elementwise_ops_.emplace(\"QuantizeV2\", quantize_v2_cost);\n  elementwise_ops_.emplace(\"Reciprocal\", EIGEN_COST(scalar_inverse_op<float>));\n  elementwise_ops_.emplace(\"Relu\", EIGEN_COST(scalar_max_op<float>));\n  elementwise_ops_.emplace(\"Relu6\", EIGEN_COST(scalar_max_op<float>));\n  elementwise_ops_.emplace(\"Rint\", 1);\n  elementwise_ops_.emplace(\"Round\", EIGEN_COST(scalar_round_op<float>));\n  elementwise_ops_.emplace(\"Rsqrt\", EIGEN_COST(scalar_rsqrt_op<float>));\n  elementwise_ops_.emplace(\"Sigmoid\", EIGEN_COST(scalar_logistic_op<float>));\n  elementwise_ops_.emplace(\"Sign\", EIGEN_COST(scalar_sign_op<float>));\n  elementwise_ops_.emplace(\"Sin\", EIGEN_COST(scalar_sin_op<float>));\n  elementwise_ops_.emplace(\"Sqrt\", EIGEN_COST(scalar_sqrt_op<float>));\n  elementwise_ops_.emplace(\"Square\", EIGEN_COST(scalar_square_op<float>));\n  elementwise_ops_.emplace(\"Sum\", EIGEN_COST(scalar_sum_op<float>));\n  elementwise_ops_.emplace(\"Tan\", EIGEN_COST(scalar_tan_op<float>));\n  elementwise_ops_.emplace(\"Tanh\", EIGEN_COST(scalar_tanh_op<float>));\n  elementwise_ops_.emplace(\"TopKV2\", EIGEN_COST(scalar_max_op<float>));\n  // Binary ops alphabetically sorted\n  elementwise_ops_.emplace(\"Add\", EIGEN_COST(scalar_sum_op<float>));\n  elementwise_ops_.emplace(\"AddV2\", EIGEN_COST(scalar_sum_op<float>));\n  elementwise_ops_.emplace(\"ApproximateEqual\", 1);\n  elementwise_ops_.emplace(\"BiasAdd\", EIGEN_COST(scalar_sum_op<float>));\n  elementwise_ops_.emplace(\"QuantizedBiasAdd\",\n                           EIGEN_COST(scalar_sum_op<float>));\n  elementwise_ops_.emplace(\"Div\", EIGEN_COST(scalar_quotient_op<float>));\n  elementwise_ops_.emplace(\"Equal\", 1);\n  elementwise_ops_.emplace(\"FloorDiv\", EIGEN_COST(scalar_quotient_op<float>));\n  elementwise_ops_.emplace(\"FloorMod\", EIGEN_COST(scalar_mod_op<float>));\n  elementwise_ops_.emplace(\"Greater\", 1);\n  elementwise_ops_.emplace(\"GreaterEqual\", 1);\n  elementwise_ops_.emplace(\"Less\", 1);\n  elementwise_ops_.emplace(\"LessEqual\", 1);\n  elementwise_ops_.emplace(\"LogicalAnd\", EIGEN_COST(scalar_boolean_and_op));\n  elementwise_ops_.emplace(\"LogicalNot\", 1);\n  elementwise_ops_.emplace(\"LogicalOr\", EIGEN_COST(scalar_boolean_or_op));\n  elementwise_ops_.emplace(\"Maximum\", EIGEN_COST(scalar_max_op<float>));\n  elementwise_ops_.emplace(\"Minimum\", EIGEN_COST(scalar_min_op<float>));\n  elementwise_ops_.emplace(\"Mod\", EIGEN_COST(scalar_mod_op<float>));\n  elementwise_ops_.emplace(\"Mul\", EIGEN_COST(scalar_product_op<float>));\n  elementwise_ops_.emplace(\"NotEqual\", 1);\n  elementwise_ops_.emplace(\"QuantizedAdd\", EIGEN_COST(scalar_sum_op<float>));\n  elementwise_ops_.emplace(\"QuantizedMul\",\n                           EIGEN_COST(scalar_product_op<float>));\n  elementwise_ops_.emplace(\"RealDiv\", EIGEN_COST(scalar_quotient_op<float>));\n  elementwise_ops_.emplace(\"ReluGrad\", EIGEN_COST(scalar_max_op<float>));\n  elementwise_ops_.emplace(\"Select\", EIGEN_COST(scalar_boolean_or_op));\n  elementwise_ops_.emplace(\"SelectV2\", EIGEN_COST(scalar_boolean_or_op));\n  elementwise_ops_.emplace(\"SquaredDifference\",\n                           EIGEN_COST(scalar_square_op<float>) +\n                               EIGEN_COST(scalar_difference_op<float>));\n  elementwise_ops_.emplace(\"Sub\", EIGEN_COST(scalar_difference_op<float>));\n  elementwise_ops_.emplace(\"TruncateDiv\",\n                           EIGEN_COST(scalar_quotient_op<float>));\n  elementwise_ops_.emplace(\"TruncateMod\", EIGEN_COST(scalar_mod_op<float>));\n  elementwise_ops_.emplace(\"Where\", 1);\n\n#undef EIGEN_COST\n\n  // By default, use sum of memory_time and compute_time for execution_time.\n  compute_memory_overlap_ = false;\n}\n\nCosts OpLevelCostEstimator::PredictCosts(const OpContext& op_context) const {\n  Costs costs;\n  NodeCosts node_costs;\n  if (PredictNodeCosts(op_context, &node_costs).ok()) {\n    if (node_costs.has_costs) {\n      return node_costs.costs;\n    }\n    // Convert NodeCosts to Costs.\n    if (node_costs.minimum_cost_op) {\n      // Override to minimum cost; Note that some ops with minimum cost may have\n      // non-typical device (e.g., channel for _Send), which may fail with\n      // GetDeviceInfo(), called from PredictOpCountBasedCost(). Make sure we\n      // directly set minimum values to Costs here, not calling\n      // PredictOpCountBasedCost().\n      costs.compute_time = kMinComputeTime;\n      costs.execution_time = kMinComputeTime;\n      costs.memory_time = 0;\n      costs.intermediate_memory_time = 0;\n      costs.intermediate_memory_read_time = 0;\n      costs.intermediate_memory_write_time = 0;\n    } else {\n      // Convert NodeCosts to Costs.\n      costs = PredictOpCountBasedCost(\n          node_costs.num_compute_ops, node_costs.num_total_read_bytes(),\n          node_costs.num_total_write_bytes(), op_context.op_info);\n    }\n    VLOG(1) << \"Operation \" << op_context.op_info.op() << \" takes \"\n            << costs.execution_time.count() << \" ns.\";\n    // Copy additional stats from NodeCosts to Costs.\n    costs.max_memory = node_costs.max_memory;\n    costs.persistent_memory = node_costs.persistent_memory;\n    costs.temporary_memory = node_costs.temporary_memory;\n    costs.inaccurate = node_costs.inaccurate;\n    costs.num_ops_with_unknown_shapes =\n        node_costs.num_nodes_with_unknown_shapes;\n    costs.num_ops_total = node_costs.num_nodes;\n    return costs;\n  }\n  // Errors during node cost estimate.\n  LOG(WARNING) << \"Error in PredictCost() for the op: \"\n               << op_context.op_info.ShortDebugString();\n  costs = Costs::ZeroCosts(/*inaccurate=*/true);\n  costs.num_ops_with_unknown_shapes = node_costs.num_nodes_with_unknown_shapes;\n  return costs;\n}\n\nStatus OpLevelCostEstimator::PredictNodeCosts(const OpContext& op_context,\n                                              NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  auto it = device_cost_impl_.find(op_info.op());\n  if (it != device_cost_impl_.end()) {\n    std::function<Status(const OpContext&, NodeCosts*)> estimator = it->second;\n    return estimator(op_context, node_costs);\n  }\n\n  if (persistent_ops_.find(op_info.op()) != persistent_ops_.end()) {\n    return PredictVariable(op_context, node_costs);\n  }\n\n  if (elementwise_ops_.find(op_info.op()) != elementwise_ops_.end()) {\n    return PredictCwiseOp(op_context, node_costs);\n  }\n\n  VLOG(1) << \"Missing accurate estimator for op: \" << op_info.op();\n\n  node_costs->num_nodes_with_unknown_op_type = 1;\n  return PredictCostOfAnUnknownOp(op_context, node_costs);\n}\n\n// This method assumes a typical system composed of CPUs and GPUs, connected\n// through PCIe. To define device info more precisely, override this method.\nDeviceInfo OpLevelCostEstimator::GetDeviceInfo(\n    const DeviceProperties& device) const {\n  double gflops = -1;\n  double gb_per_sec = -1;\n\n  if (device.type() == \"CPU\") {\n    // Check if vector instructions are available, and refine performance\n    // prediction based on this.\n    // Frequencies are stored in MHz in the DeviceProperties.\n    gflops = device.num_cores() * device.frequency() * 1e-3;\n    if (gb_per_sec < 0) {\n      if (device.bandwidth() > 0) {\n        gb_per_sec = device.bandwidth() / 1e6;\n      } else {\n        gb_per_sec = 32;\n      }\n    }\n  } else if (device.type() == \"GPU\") {\n    const auto& device_env = device.environment();\n    auto it = device_env.find(\"architecture\");\n    if (it != device_env.end()) {\n      const std::string architecture = device_env.at(\"architecture\");\n      int cores_per_multiprocessor;\n      if (architecture < \"3\") {\n        // Fermi\n        cores_per_multiprocessor = 32;\n      } else if (architecture < \"4\") {\n        // Kepler\n        cores_per_multiprocessor = 192;\n      } else if (architecture < \"6\") {\n        // Maxwell\n        cores_per_multiprocessor = 128;\n      } else {\n        // Pascal (compute capability version 6) and Volta (compute capability\n        // version 7)\n        cores_per_multiprocessor = 64;\n      }\n      gflops = device.num_cores() * device.frequency() * 1e-3 *\n               cores_per_multiprocessor * kOpsPerMac;\n      if (device.bandwidth() > 0) {\n        gb_per_sec = device.bandwidth() / 1e6;\n      } else {\n        gb_per_sec = 100;\n      }\n    } else {\n      // Architecture is not available (ex: pluggable device), return default\n      // value.\n      gflops = 100;     // Dummy value;\n      gb_per_sec = 12;  // default PCIe x16 gen3.\n    }\n  } else {\n    LOG_EVERY_N(WARNING, 1000) << \"Unknown device type: \" << device.type()\n                               << \", assuming PCIe between CPU and GPU.\";\n    gflops = 1;  // Dummy value; data transfer ops would not have compute ops.\n    gb_per_sec = 12;  // default PCIe x16 gen3.\n  }\n  VLOG(1) << \"Device: \" << device.type() << \" gflops: \" << gflops\n          << \" gb_per_sec: \" << gb_per_sec;\n\n  return DeviceInfo(gflops, gb_per_sec);\n}\n\nStatus OpLevelCostEstimator::PredictCwiseOp(const OpContext& op_context,\n                                            NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  bool found_unknown_shapes = false;\n  // For element-wise operations, op count is the element count of any input. We\n  // use the count for the largest input here to be more robust in case that the\n  // shape is unknown or partially known for other input.\n  int64_t op_count = CalculateLargestInputCount(op_info, &found_unknown_shapes);\n  // If output shape is available, try to use the element count calculated from\n  // that.\n  if (op_info.outputs_size() > 0) {\n    op_count = std::max(\n        op_count,\n        CalculateTensorElementCount(op_info.outputs(0), &found_unknown_shapes));\n  }\n  // Calculate the output shape possibly resulting from broadcasting.\n  if (op_info.inputs_size() >= 2) {\n    op_count = std::max(op_count, CwiseOutputElementCount(op_info));\n  }\n\n  int op_cost = 1;\n  auto it = elementwise_ops_.find(op_info.op());\n  if (it != elementwise_ops_.end()) {\n    op_cost = it->second;\n  } else {\n    return errors::InvalidArgument(\"Not a cwise op: \", op_info.op());\n  }\n\n  return PredictDefaultNodeCosts(op_count * op_cost, op_context,\n                                 &found_unknown_shapes, node_costs);\n}\n\nStatus OpLevelCostEstimator::PredictCostOfAnUnknownOp(\n    const OpContext& op_context, NodeCosts* node_costs) const {\n  // Don't assume the operation is cwise, return cost based on input/output size\n  // and admit that it is inaccurate...\n  bool found_unknown_shapes = false;\n  node_costs->inaccurate = true;\n  return PredictDefaultNodeCosts(0, op_context, &found_unknown_shapes,\n                                 node_costs);\n}\n\nCosts OpLevelCostEstimator::PredictOpCountBasedCost(\n    double operations, const OpInfo& op_info) const {\n  bool unknown_shapes = false;\n  const double input_size = CalculateInputSize(op_info, &unknown_shapes);\n  const double output_size = CalculateOutputSize(op_info, &unknown_shapes);\n  Costs costs =\n      PredictOpCountBasedCost(operations, input_size, output_size, op_info);\n  costs.inaccurate = unknown_shapes;\n  costs.num_ops_with_unknown_shapes = unknown_shapes;\n  costs.max_memory = output_size;\n  return costs;\n}\n\nCosts OpLevelCostEstimator::PredictOpCountBasedCost(\n    double operations, double input_io_bytes, double output_io_bytes,\n    const OpInfo& op_info) const {\n  double total_io_bytes = input_io_bytes + output_io_bytes;\n  const DeviceInfo device_info = GetDeviceInfo(op_info.device());\n  if (device_info.gigaops <= 0 || device_info.gb_per_sec <= 0 ||\n      device_info.intermediate_read_gb_per_sec <= 0 ||\n      device_info.intermediate_write_gb_per_sec <= 0) {\n    VLOG(1) << \"BAD DEVICE. Op:\" << op_info.op()\n            << \" device type:\" << op_info.device().type()\n            << \" device model:\" << op_info.device().model();\n  }\n\n  Costs::NanoSeconds compute_cost(std::ceil(operations / device_info.gigaops));\n  VLOG(1) << \"Op:\" << op_info.op() << \" GOps:\" << operations / 1e9\n          << \" Compute Time (ns):\" << compute_cost.count();\n\n  Costs::NanoSeconds memory_cost(\n      std::ceil(total_io_bytes / device_info.gb_per_sec));\n  VLOG(1) << \"Op:\" << op_info.op() << \" Size (KB):\" << (total_io_bytes) / 1e3\n          << \" Memory Time (ns):\" << memory_cost.count();\n\n  // Check if bytes > 0.  If it's not and the bandwidth is set to infinity\n  // then the result would be undefined.\n  double intermediate_read_time =\n      (input_io_bytes > 0)\n          ? std::ceil(input_io_bytes / device_info.intermediate_read_gb_per_sec)\n          : 0;\n\n  double intermediate_write_time =\n      (output_io_bytes > 0)\n          ? std::ceil(output_io_bytes /\n                      device_info.intermediate_write_gb_per_sec)\n          : 0;\n\n  Costs::NanoSeconds intermediate_memory_cost =\n      compute_memory_overlap_\n          ? std::max(intermediate_read_time, intermediate_write_time)\n          : (intermediate_read_time + intermediate_write_time);\n  VLOG(1) << \"Op:\" << op_info.op() << \" Size (KB):\" << (total_io_bytes) / 1e3\n          << \" Intermediate Memory Time (ns):\"\n          << intermediate_memory_cost.count();\n\n  Costs costs = Costs::ZeroCosts();\n  costs.compute_time = compute_cost;\n  costs.memory_time = memory_cost;\n  costs.intermediate_memory_time = intermediate_memory_cost;\n  costs.intermediate_memory_read_time =\n      Costs::NanoSeconds(intermediate_read_time);\n  costs.intermediate_memory_write_time =\n      Costs::NanoSeconds(intermediate_write_time);\n  CombineCostsAndUpdateExecutionTime(compute_memory_overlap_, &costs);\n  return costs;\n}\n\nint64_t OpLevelCostEstimator::CountConv2DOperations(\n    const OpInfo& op_info, bool* found_unknown_shapes) {\n  return CountConv2DOperations(op_info, nullptr, found_unknown_shapes);\n}\n\n// Helper to translate the positional arguments into named fields.\n/* static */\nOpLevelCostEstimator::ConvolutionDimensions\nOpLevelCostEstimator::ConvolutionDimensionsFromInputs(\n    const TensorShapeProto& original_image_shape,\n    const TensorShapeProto& original_filter_shape, const OpInfo& op_info,\n    bool* found_unknown_shapes) {\n  VLOG(2) << \"op features: \" << op_info.DebugString();\n  VLOG(2) << \"Original image shape: \" << original_image_shape.DebugString();\n  VLOG(2) << \"Original filter shape: \" << original_filter_shape.DebugString();\n\n  int x_index, y_index, major_channel_index, minor_channel_index = -1;\n  const std::string& data_format = GetDataFormat(op_info);\n  if (data_format == \"NCHW\") {\n    major_channel_index = 1;\n    y_index = 2;\n    x_index = 3;\n  } else if (data_format == \"NCHW_VECT_C\") {\n    // Use NCHW_VECT_C\n    minor_channel_index = 1;\n    y_index = 2;\n    x_index = 3;\n    major_channel_index = 4;\n  } else {\n    // Use NHWC.\n    y_index = 1;\n    x_index = 2;\n    major_channel_index = 3;\n  }\n  const std::string& filter_format = GetFilterFormat(op_info);\n  int filter_x_index, filter_y_index, in_major_channel_index, out_channel_index,\n      in_minor_channel_index = -1;\n  if (filter_format == \"HWIO\") {\n    filter_y_index = 0;\n    filter_x_index = 1;\n    in_major_channel_index = 2;\n    out_channel_index = 3;\n  } else if (filter_format == \"OIHW_VECT_I\") {\n    out_channel_index = 0;\n    in_minor_channel_index = 1;\n    filter_y_index = 2;\n    filter_x_index = 3;\n    in_major_channel_index = 4;\n  } else {\n    // Use OIHW\n    out_channel_index = 0;\n    in_major_channel_index = 1;\n    filter_y_index = 2;\n    filter_x_index = 3;\n  }\n\n  auto image_shape = MaybeGetMinimumShape(original_image_shape,\n                                          minor_channel_index >= 0 ? 5 : 4,\n                                          found_unknown_shapes);\n  auto filter_shape = MaybeGetMinimumShape(original_filter_shape,\n                                           in_minor_channel_index >= 0 ? 5 : 4,\n                                           found_unknown_shapes);\n  VLOG(2) << \"Image shape: \" << image_shape.DebugString();\n  VLOG(2) << \"Filter shape: \" << filter_shape.DebugString();\n\n  int64_t batch = image_shape.dim(0).size();\n  int64_t ix = image_shape.dim(x_index).size();\n  int64_t iy = image_shape.dim(y_index).size();\n  int64_t iz = minor_channel_index >= 0\n                   ? image_shape.dim(minor_channel_index).size() *\n                         image_shape.dim(major_channel_index).size()\n                   : image_shape.dim(major_channel_index).size();\n  int64_t kx = filter_shape.dim(filter_x_index).size();\n  int64_t ky = filter_shape.dim(filter_y_index).size();\n  int64_t kz = in_minor_channel_index >= 0\n                   ? filter_shape.dim(in_major_channel_index).size() *\n                         filter_shape.dim(in_minor_channel_index).size()\n                   : filter_shape.dim(in_major_channel_index).size();\n  std::vector<int64_t> strides = GetStrides(op_info);\n  const auto padding = GetPadding(op_info);\n  int64_t sx = strides[x_index];\n  int64_t sy = strides[y_index];\n  int64_t ox = GetOutputSize(ix, kx, sx, padding);\n  int64_t oy = GetOutputSize(iy, ky, sy, padding);\n  int64_t oz = filter_shape.dim(out_channel_index).size();\n  // Only check equality when both sizes are known (in other words, when\n  // neither is set to a minimum dimension size of 1).\n  if (iz != 1 && kz != 1) {\n    DCHECK_EQ(iz % kz, 0) << \"Input channel \" << iz\n                          << \" is not a multiple of filter channel \" << kz\n                          << \".\";\n    if (iz % kz) {\n      *found_unknown_shapes = true;\n    }\n  } else {\n    iz = kz = std::max<int64_t>(iz, kz);\n  }\n  OpLevelCostEstimator::ConvolutionDimensions conv_dims = {\n      batch, ix, iy, iz, kx, ky, kz, oz, ox, oy, sx, sy, padding};\n\n  VLOG(1) << \"Batch Size:\" << batch;\n  VLOG(1) << \"Image Dims:\" << ix << \",\" << iy;\n  VLOG(1) << \"Input Depth:\" << iz;\n  VLOG(1) << \"Kernel Dims:\" << kx << \",\" << ky;\n  VLOG(1) << \"Kernel Depth:\" << kz;\n  VLOG(1) << \"Output Dims:\" << ox << \",\" << oy;\n  VLOG(1) << \"Output Depth:\" << oz;\n  VLOG(1) << \"Strides:\" << sx << \",\" << sy;\n  VLOG(1) << \"Padding:\" << (padding == Padding::VALID ? \"VALID\" : \"SAME\");\n  return conv_dims;\n}\n\nint64_t OpLevelCostEstimator::CountConv2DOperations(\n    const OpInfo& op_info, ConvolutionDimensions* conv_info,\n    bool* found_unknown_shapes) {\n  DCHECK(op_info.op() == kConv2d || op_info.op() == kDepthwiseConv2dNative)\n      << \"Invalid Operation: not Conv2D nor DepthwiseConv2dNative\";\n\n  if (op_info.inputs_size() < 2) {  // Unexpected inputs.\n    *found_unknown_shapes = true;\n    return 0;\n  }\n\n  ConvolutionDimensions conv_dims = ConvolutionDimensionsFromInputs(\n      op_info.inputs(0).shape(), op_info.inputs(1).shape(), op_info,\n      found_unknown_shapes);\n\n  //  in DepthwiseConv2dNative conv_dims.oz is actually the channel depth\n  //  multiplier; The effective output channel depth oz_effective is\n  //  conv_dims.iz * conv_dims.oz. thus # ops = N x H x W x oz_effective x 2RS.\n  //  Compare to Conv2D where # ops =  N x H x W x kz x oz x 2RS,\n  //  oz = oz_effective,  then Conv2D_ops / Depthwise_conv2d_native_ops = kz.\n  int64_t ops = conv_dims.batch;\n  ops *= conv_dims.ox * conv_dims.oy;\n  ops *= conv_dims.kx * conv_dims.ky;\n  if (op_info.op() == kConv2d) {\n    ops *= conv_dims.kz * conv_dims.oz;\n  } else {\n    // To ensure output tensor dims to be correct for DepthwiseConv2DNative,\n    // although ops are the same as Conv2D.\n    conv_dims.oz *= conv_dims.iz;\n    ops *= conv_dims.oz;\n  }\n  ops *= kOpsPerMac;\n\n  if (conv_info != nullptr) {\n    *conv_info = conv_dims;\n  }\n  return ops;\n}\n\nint64_t OpLevelCostEstimator::CountMatMulOperations(\n    const OpInfo& op_info, bool* found_unknown_shapes) {\n  return CountMatMulOperations(op_info, nullptr, found_unknown_shapes);\n}\n\n// TODO(nishantpatil): Create separate estimator for Sparse Matmul\nint64_t OpLevelCostEstimator::CountMatMulOperations(\n    const OpInfo& op_info, MatMulDimensions* mat_mul,\n    bool* found_unknown_shapes) {\n  double ops = 0;\n\n  if (op_info.inputs_size() < 2) {\n    LOG(ERROR) << \"Need 2 inputs but got \" << op_info.inputs_size();\n    // TODO(pcma): Try to separate invalid inputs from unknown shapes\n    *found_unknown_shapes = true;\n    return 0;\n  }\n\n  auto& a_matrix = op_info.inputs(0);\n  auto& b_matrix = op_info.inputs(1);\n\n  bool transpose_a = false;\n  bool transpose_b = false;\n\n  double m_dim, n_dim, k_dim, k_dim_b = 0;\n\n  for (const auto& item : op_info.attr()) {\n    VLOG(1) << \"Key:\" << item.first\n            << \" Value:\" << SummarizeAttrValue(item.second);\n    if (item.first == \"transpose_a\" && item.second.b() == true)\n      transpose_a = true;\n    if (item.first == \"transpose_b\" && item.second.b() == true)\n      transpose_b = true;\n  }\n  VLOG(1) << \"transpose_a:\" << transpose_a;\n  VLOG(1) << \"transpose_b:\" << transpose_b;\n  auto a_matrix_shape =\n      MaybeGetMinimumShape(a_matrix.shape(), 2, found_unknown_shapes);\n  auto b_matrix_shape =\n      MaybeGetMinimumShape(b_matrix.shape(), 2, found_unknown_shapes);\n  if (transpose_a) {\n    m_dim = a_matrix_shape.dim(1).size();\n    k_dim = a_matrix_shape.dim(0).size();\n  } else {\n    m_dim = a_matrix_shape.dim(0).size();\n    k_dim = a_matrix_shape.dim(1).size();\n  }\n  if (transpose_b) {\n    k_dim_b = b_matrix_shape.dim(1).size();\n    n_dim = b_matrix_shape.dim(0).size();\n  } else {\n    k_dim_b = b_matrix_shape.dim(0).size();\n    n_dim = b_matrix_shape.dim(1).size();\n  }\n\n  VLOG(1) << \"M, N, K: \" << m_dim << \",\" << n_dim << \",\" << k_dim;\n  // Only check equality when both sizes are known (in other words, when\n  // neither is set to a minimum dimension size of 1).\n  if (k_dim_b != 1 && k_dim != 1 && k_dim_b != k_dim) {\n    LOG(ERROR) << \"Incompatible Matrix dimensions\";\n    return ops;\n  } else {\n    // One of k_dim and k_dim_b might be 1 (minimum dimension size).\n    k_dim = std::max(k_dim, k_dim_b);\n  }\n\n  ops = m_dim * n_dim * k_dim * 2;\n  VLOG(1) << \"Operations for Matmul: \" << ops;\n\n  if (mat_mul != nullptr) {\n    mat_mul->m = m_dim;\n    mat_mul->n = n_dim;\n    mat_mul->k = k_dim;\n  }\n  return ops;\n}\n\nbool OpLevelCostEstimator::GenerateBatchMatmulContextFromEinsum(\n    const OpContext& einsum_context, OpContext* batch_matmul_context,\n    bool* found_unknown_shapes) const {\n  // This auxiliary function transforms an einsum OpContext into its equivalent\n  // Batch Matmul OpContext. The function returns a boolean, which determines\n  // whether it was successful in generating the output OpContext or not.\n\n  // Einsum computes a generalized contraction between tensors of arbitrary\n  // dimension as defined by the equation written in the Einstein summation\n  // convention. The number of tensors in the computation and the number of\n  // contractions can be arbitrarily long. The current model only contemplates\n  // Einsum equations, which can be translated into a single BatchMatMul\n  // operation. Einsum operations with more than two operands are not currently\n  // supported. Subscripts where an axis appears more than once for a single\n  // input and ellipsis are currently also excluded. See:\n  // https://www.tensorflow.org/api_docs/python/tf/einsum\n  // We distinguish four kinds of dimensions, depending on their placement in\n  // the equation:\n  // + B: Batch dimensions: Dimensions which appear in both operands and RHS.\n  // + K: Contracting dimensions: These appear in both inputs but not RHS.\n  // + M: Operand A dimensions: These appear in the first operand and the RHS.\n  // + N: Operand B dimensions: These appear in the second operand and the RHS.\n  // Then, the operation to estimate is BatchMatMul([B,M,K],[B,K,N])\n\n  if (batch_matmul_context == nullptr) {\n    VLOG(1) << \"Output context should not be a nullptr.\";\n    return false;\n  }\n  if (!IsEinsumCorrectlyFormed(einsum_context)) return false;\n  const auto& op_info = einsum_context.op_info;\n  std::vector<std::string> equation_split =\n      absl::StrSplit(op_info.attr().find(\"equation\")->second.s(), \"->\");\n  std::vector<absl::string_view> input_split =\n      absl::StrSplit(equation_split[0], ',');\n  const auto& a_input = op_info.inputs(0);\n  const auto& b_input = op_info.inputs(1);\n  absl::string_view rhs_str = equation_split[1];\n  absl::string_view a_input_str = input_split[0];\n  absl::string_view b_input_str = input_split[1];\n\n  constexpr int kMatrixRank = 2;\n\n  bool a_input_shape_unknown = false;\n  bool b_input_shape_unknown = false;\n\n  TensorShapeProto a_input_shape = MaybeGetMinimumShape(\n      a_input.shape(), std::max(kMatrixRank, a_input.shape().dim_size()),\n      &a_input_shape_unknown);\n  TensorShapeProto b_input_shape = MaybeGetMinimumShape(\n      b_input.shape(), std::max(kMatrixRank, b_input.shape().dim_size()),\n      &b_input_shape_unknown);\n\n  *found_unknown_shapes = a_input_shape_unknown || b_input_shape_unknown ||\n                          (a_input.shape().dim_size() < kMatrixRank) ||\n                          (b_input.shape().dim_size() < kMatrixRank);\n\n  OpInfo batch_matmul_op_info = op_info;\n  batch_matmul_op_info.mutable_inputs()->Clear();\n  batch_matmul_op_info.set_op(\"BatchMatMul\");\n\n  AttrValue transpose_attribute;\n  transpose_attribute.set_b(false);\n  (*batch_matmul_op_info.mutable_attr())[\"transpose_a\"] = transpose_attribute;\n  (*batch_matmul_op_info.mutable_attr())[\"transpose_b\"] = transpose_attribute;\n\n  OpInfo::TensorProperties* a_matrix = batch_matmul_op_info.add_inputs();\n  TensorShapeProto* a_matrix_shape = a_matrix->mutable_shape();\n  a_matrix->set_dtype(a_input.dtype());\n\n  OpInfo::TensorProperties* b_matrix = batch_matmul_op_info.add_inputs();\n  b_matrix->set_dtype(b_input.dtype());\n  TensorShapeProto* b_matrix_shape = b_matrix->mutable_shape();\n\n  TensorShapeProto_Dim m_dim;\n  TensorShapeProto_Dim n_dim;\n  TensorShapeProto_Dim k_dim;\n\n  m_dim.set_size(1);\n  n_dim.set_size(1);\n  k_dim.set_size(1);\n\n  for (int i_idx = 0, a_input_str_size = a_input_str.size();\n       i_idx < a_input_str_size; ++i_idx) {\n    if (b_input_str.find(a_input_str[i_idx]) == std::string::npos) {\n      if (rhs_str.find(a_input_str[i_idx]) == std::string::npos) {\n        VLOG(1) << \"Missing accurate estimator for op: \" << op_info.op();\n        return false;\n      }\n\n      m_dim.set_size(m_dim.size() * a_input_shape.dim(i_idx).size());\n      continue;\n    } else if (rhs_str.find(a_input_str[i_idx]) == std::string::npos) {\n      // The dimension does not appear in the RHS, therefore it is a contracting\n      // dimension.\n      k_dim.set_size(k_dim.size() * a_input_shape.dim(i_idx).size());\n      continue;\n    }\n    // It appears in both input operands, therefore we place it as an outer\n    // dimension for the Batch Matmul.\n    *(a_matrix_shape->add_dim()) = a_input_shape.dim(i_idx);\n    *(b_matrix_shape->add_dim()) = a_input_shape.dim(i_idx);\n  }\n  for (int i_idx = 0, b_input_str_size = b_input_str.size();\n       i_idx < b_input_str_size; ++i_idx) {\n    if (a_input_str.find(b_input_str[i_idx]) == std::string::npos) {\n      if (rhs_str.find(b_input_str[i_idx]) == std::string::npos) {\n        VLOG(1) << \"Missing accurate estimator for op: \" << op_info.op();\n        return false;\n      }\n      n_dim.set_size(n_dim.size() * b_input_shape.dim(i_idx).size());\n    }\n  }\n\n  // The two inner-most dimensions of the Batch Matmul are added.\n  *(a_matrix_shape->add_dim()) = m_dim;\n  *(a_matrix_shape->add_dim()) = k_dim;\n  *(b_matrix_shape->add_dim()) = k_dim;\n  *(b_matrix_shape->add_dim()) = n_dim;\n\n  *batch_matmul_context = einsum_context;\n  batch_matmul_context->op_info = batch_matmul_op_info;\n  return true;\n}\n\nint64_t OpLevelCostEstimator::CountBatchMatMulOperations(\n    const OpInfo& op_info, bool* found_unknown_shapes) {\n  return CountBatchMatMulOperations(op_info, nullptr, found_unknown_shapes);\n}\n\nint64_t OpLevelCostEstimator::CountBatchMatMulOperations(\n    const OpInfo& op_info, BatchMatMulDimensions* batch_mat_mul,\n    bool* found_unknown_shapes) {\n  if (op_info.op() != kBatchMatMul && op_info.op() != kBatchMatMulV2) {\n    LOG(ERROR) << \"Invalid Operation: \" << op_info.op();\n    // TODO(pcma): Try to separate invalid inputs from unknown shapes\n    *found_unknown_shapes = true;\n    return 0;\n  }\n  if (op_info.inputs_size() != 2) {\n    LOG(ERROR) << \"Expected 2 inputs but got \" << op_info.inputs_size();\n    // TODO(pcma): Try to separate invalid inputs from unknown shapes\n    *found_unknown_shapes = true;\n    return 0;\n  }\n\n  double ops = 0;\n  const auto& a_input = op_info.inputs(0);\n  const auto& b_input = op_info.inputs(1);\n\n  // BatchMatMul requires inputs of at least matrix shape (rank 2).\n  // The two most minor dimensions of each input are matrices that\n  // need to be multiplied together. The other dimensions determine\n  // the number of such MatMuls.  For example, if the BatchMatMul has\n  // inputs of shape:\n  //   a_input_shape = [2, 3, 4, 5]\n  //   b_input_shape = [2, 3, 5, 6]\n  // then there are 2*3 = 6 MatMuls of dimensions m = 4, k = 5, n = 6\n  // in this BatchMatMul.\n  const int matrix_rank = 2;\n\n  bool a_input_shape_unknown = false;\n  bool b_input_shape_unknown = false;\n\n  TensorShapeProto a_input_shape = MaybeGetMinimumShape(\n      a_input.shape(), std::max(matrix_rank, a_input.shape().dim_size()),\n      &a_input_shape_unknown);\n  TensorShapeProto b_input_shape = MaybeGetMinimumShape(\n      b_input.shape(), std::max(matrix_rank, b_input.shape().dim_size()),\n      &b_input_shape_unknown);\n\n  *found_unknown_shapes = a_input_shape_unknown || b_input_shape_unknown ||\n                          (a_input.shape().dim_size() < matrix_rank) ||\n                          (b_input.shape().dim_size() < matrix_rank);\n\n  // Compute the number of matmuls as the max indicated at each dimension\n  // by either input. Note that the shapes do not have to have\n  // the same rank due to incompleteness.\n  TensorShapeProto* bigger_rank_shape = &a_input_shape;\n  TensorShapeProto* smaller_rank_shape = &b_input_shape;\n  if (b_input_shape.dim_size() > a_input_shape.dim_size()) {\n    bigger_rank_shape = &b_input_shape;\n    smaller_rank_shape = &a_input_shape;\n  }\n  int num_matmuls = 1;\n  for (int b_i = 0,\n           s_i = smaller_rank_shape->dim_size() - bigger_rank_shape->dim_size();\n       b_i < bigger_rank_shape->dim_size() - matrix_rank; ++b_i, ++s_i) {\n    int b_dim = bigger_rank_shape->dim(b_i).size();\n    int s_dim = 1;\n    if (s_i >= 0) {\n      s_dim = smaller_rank_shape->dim(s_i).size();\n    }\n    if (batch_mat_mul != nullptr) {\n      batch_mat_mul->batch_dims.push_back(s_dim);\n    }\n    num_matmuls *= std::max(b_dim, s_dim);\n  }\n\n  // Build the MatMul. Note that values are ignored here since we are just\n  // counting ops (e.g. only shapes matter).\n  OpInfo matmul_op_info;\n  matmul_op_info.set_op(\"MatMul\");\n\n  AttrValue transpose_a;\n  transpose_a.set_b(false);\n  if (op_info.attr().find(\"adj_x\") != op_info.attr().end()) {\n    transpose_a.set_b(op_info.attr().at(\"adj_x\").b());\n  }\n  (*matmul_op_info.mutable_attr())[\"transpose_a\"] = transpose_a;\n\n  AttrValue transpose_b;\n  transpose_b.set_b(false);\n  if (op_info.attr().find(\"adj_y\") != op_info.attr().end()) {\n    transpose_b.set_b(op_info.attr().at(\"adj_y\").b());\n  }\n  (*matmul_op_info.mutable_attr())[\"transpose_b\"] = transpose_b;\n\n  OpInfo::TensorProperties* a_matrix = matmul_op_info.add_inputs();\n  a_matrix->set_dtype(a_input.dtype());\n  TensorShapeProto* a_matrix_shape = a_matrix->mutable_shape();\n  for (int i = std::max(0, a_input_shape.dim_size() - matrix_rank);\n       i < a_input_shape.dim_size(); ++i) {\n    *(a_matrix_shape->add_dim()) = a_input_shape.dim(i);\n  }\n\n  OpInfo::TensorProperties* b_matrix = matmul_op_info.add_inputs();\n  b_matrix->set_dtype(b_input.dtype());\n  TensorShapeProto* b_matrix_shape = b_matrix->mutable_shape();\n  for (int i = std::max(0, b_input_shape.dim_size() - matrix_rank);\n       i < b_input_shape.dim_size(); ++i) {\n    *(b_matrix_shape->add_dim()) = b_input_shape.dim(i);\n  }\n  if (batch_mat_mul != nullptr) {\n    batch_mat_mul->matmul_dims.m = (transpose_a.b())\n                                       ? a_matrix_shape->dim(1).size()\n                                       : a_matrix_shape->dim(0).size();\n    batch_mat_mul->matmul_dims.k = (transpose_a.b())\n                                       ? a_matrix_shape->dim(0).size()\n                                       : a_matrix_shape->dim(1).size();\n    batch_mat_mul->matmul_dims.n = (transpose_b.b())\n                                       ? b_matrix_shape->dim(0).size()\n                                       : b_matrix_shape->dim(1).size();\n  }\n\n  for (int i = 0; i < num_matmuls; ++i) {\n    bool matmul_unknown_shapes = false;\n    ops += CountMatMulOperations(matmul_op_info, &matmul_unknown_shapes);\n    *found_unknown_shapes |= matmul_unknown_shapes;\n  }\n  return ops;\n}\n\nbool GetTensorShapeProtoFromTensorProto(const TensorProto& tensor_proto,\n                                        TensorShapeProto* tensor_shape_proto) {\n  tensor_shape_proto->Clear();\n  // First convert TensorProto into Tensor class so that it correctly parses\n  // data values within TensorProto (whether it's in int_val, int64_val,\n  // tensor_content, or anything.\n  Tensor tensor(tensor_proto.dtype());\n  if (!tensor.FromProto(tensor_proto)) {\n    LOG(WARNING) << \"GetTensorShapeProtoFromTensorProto() -- \"\n                 << \"failed to parse TensorProto: \"\n                 << tensor_proto.DebugString();\n    return false;\n  }\n  if (tensor.dims() != 1) {\n    LOG(WARNING) << \"GetTensorShapeProtoFromTensorProto() -- \"\n                 << \"tensor is not 1D: \" << tensor.dims();\n    return false;\n  }\n  // Then, convert it back to TensorProto using AsProtoField, which makes sure\n  // the data is in int_val, int64_val, or such repeated data fields, not in\n  // tensor_content.\n  TensorProto temp_tensor;\n  tensor.AsProtoField(&temp_tensor);\n\n#define TENSOR_VALUES_TO_TENSOR_SHAPE_PROTO(type)        \\\n  do {                                                   \\\n    for (const auto& value : temp_tensor.type##_val()) { \\\n      tensor_shape_proto->add_dim()->set_size(value);    \\\n    }                                                    \\\n  } while (0)\n\n  if (tensor.dtype() == DT_INT32 || tensor.dtype() == DT_INT16 ||\n      tensor.dtype() == DT_INT8 || tensor.dtype() == DT_UINT8) {\n    TENSOR_VALUES_TO_TENSOR_SHAPE_PROTO(int);\n  } else if (tensor.dtype() == DT_INT64) {\n    TENSOR_VALUES_TO_TENSOR_SHAPE_PROTO(int64);\n  } else if (tensor.dtype() == DT_UINT32) {\n    TENSOR_VALUES_TO_TENSOR_SHAPE_PROTO(uint32);\n  } else if (tensor.dtype() == DT_UINT64) {\n    TENSOR_VALUES_TO_TENSOR_SHAPE_PROTO(uint64);\n  } else {\n    LOG(WARNING) << \"GetTensorShapeProtoFromTensorProto() -- \"\n                 << \"Unsupported dtype: \" << tensor.dtype();\n    return false;\n  }\n#undef TENSOR_VALUES_TO_TENSOR_SHAPE_PROTO\n\n  return true;\n}\n\n// TODO(cliffy): Dedup this method and CountConv2DBackpropFilterOperations.\nint64_t OpLevelCostEstimator::CountConv2DBackpropInputOperations(\n    const OpInfo& op_info, ConvolutionDimensions* returned_conv_dims,\n    bool* found_unknown_shapes) {\n  int64_t ops = 0;\n\n  DCHECK(op_info.op() == kConv2dBackpropInput ||\n         op_info.op() == kDepthwiseConv2dNativeBackpropInput)\n      << \"Invalid Operation: not kConv2dBackpropInput nor\"\n         \"kDepthwiseConv2dNativeBackpropInput\";\n\n  if (op_info.inputs_size() < 2) {\n    // TODO(pcma): Try to separate invalid inputs from unknown shapes\n    *found_unknown_shapes = true;\n    return ops;\n  }\n\n  TensorShapeProto input_shape;\n  bool shape_found = false;\n  if (op_info.inputs(0).has_value()) {\n    const TensorProto& value = op_info.inputs(0).value();\n    shape_found = GetTensorShapeProtoFromTensorProto(value, &input_shape);\n  }\n  if (!shape_found && op_info.outputs_size() == 1) {\n    input_shape = op_info.outputs(0).shape();\n    shape_found = true;\n  }\n  if (!shape_found) {\n    // Set the minimum filter size that's feasible.\n    input_shape.Clear();\n    for (int i = 0; i < 4; ++i) {\n      input_shape.add_dim()->set_size(1);\n    }\n    *found_unknown_shapes = true;\n  }\n\n  ConvolutionDimensions conv_dims = ConvolutionDimensionsFromInputs(\n      input_shape, op_info.inputs(1).shape(), op_info, found_unknown_shapes);\n\n  ops = conv_dims.batch;\n  ops *= conv_dims.ox * conv_dims.oy;\n  ops *= conv_dims.kx * conv_dims.ky;\n  if (op_info.op() == kConv2dBackpropInput) {\n    ops *= conv_dims.kz * conv_dims.oz;\n  } else {\n    // conv_dims always use forward path definition regardless\n    conv_dims.oz *= conv_dims.iz;\n    ops *= conv_dims.oz;\n  }\n  ops *= kOpsPerMac;\n\n  VLOG(1) << \"Operations for\" << op_info.op() << \"  \" << ops;\n\n  if (returned_conv_dims != nullptr) {\n    *returned_conv_dims = conv_dims;\n  }\n  return ops;\n}\n\nint64_t OpLevelCostEstimator::CountConv2DBackpropFilterOperations(\n    const OpInfo& op_info, ConvolutionDimensions* returned_conv_dims,\n    bool* found_unknown_shapes) {\n  int64_t ops = 0;\n\n  DCHECK(op_info.op() == kConv2dBackpropFilter ||\n         op_info.op() == kDepthwiseConv2dNativeBackpropFilter)\n      << \"Invalid Operation: not kConv2dBackpropFilter nor\"\n         \"kDepthwiseConv2dNativeBackpropFilter\";\n\n  TensorShapeProto filter_shape;\n  bool shape_found = false;\n  if (op_info.inputs_size() >= 2 && op_info.inputs(1).has_value()) {\n    const TensorProto& value = op_info.inputs(1).value();\n    shape_found = GetTensorShapeProtoFromTensorProto(value, &filter_shape);\n  }\n  if (!shape_found && op_info.outputs_size() == 1) {\n    filter_shape = op_info.outputs(0).shape();\n    shape_found = true;\n  }\n  if (!shape_found) {\n    // Set the minimum filter size that's feasible.\n    filter_shape.Clear();\n    for (int i = 0; i < 4; ++i) {\n      filter_shape.add_dim()->set_size(1);\n    }\n    *found_unknown_shapes = true;\n  }\n\n  if (op_info.inputs_size() < 1) {\n    // TODO(pcma): Try to separate invalid inputs from unknown shapes\n    *found_unknown_shapes = true;\n    return ops;\n  }\n  ConvolutionDimensions conv_dims = ConvolutionDimensionsFromInputs(\n      op_info.inputs(0).shape(), filter_shape, op_info, found_unknown_shapes);\n\n  ops = conv_dims.batch;\n  ops *= conv_dims.ox * conv_dims.oy;\n  ops *= conv_dims.kx * conv_dims.ky;\n  if (op_info.op() == kConv2dBackpropFilter) {\n    ops *= conv_dims.kz * conv_dims.oz;\n  } else {\n    // conv_dims always use forward path definition regardless\n    conv_dims.oz *= conv_dims.iz;\n    ops *= conv_dims.oz;\n  }\n  ops *= kOpsPerMac;\n  VLOG(1) << \"Operations for\" << op_info.op() << \"  \" << ops;\n\n  if (returned_conv_dims != nullptr) {\n    *returned_conv_dims = conv_dims;\n  }\n  return ops;\n}\n\nint64_t OpLevelCostEstimator::CalculateTensorElementCount(\n    const OpInfo::TensorProperties& tensor, bool* found_unknown_shapes) {\n  VLOG(2) << \"   with \" << DataTypeString(tensor.dtype()) << \" tensor of shape \"\n          << tensor.shape().DebugString();\n  int64_t tensor_size = 1;\n  int num_dims = std::max(1, tensor.shape().dim_size());\n  auto tensor_shape =\n      MaybeGetMinimumShape(tensor.shape(), num_dims, found_unknown_shapes);\n  for (const auto& dim : tensor_shape.dim()) {\n    int64_t new_tensor_size = MultiplyWithoutOverflow(tensor_size, dim.size());\n    if (new_tensor_size < 0) {\n      VLOG(1) << \"Overflow encountered when computing element count of a \"\n                 \"tensor, multiplying \"\n              << tensor_size << \" with \" << dim.size();\n      return -1;\n    }\n    tensor_size = new_tensor_size;\n  }\n  return tensor_size;\n}\n\nint64_t OpLevelCostEstimator::CalculateTensorSize(\n    const OpInfo::TensorProperties& tensor, bool* found_unknown_shapes) {\n  int64_t count = CalculateTensorElementCount(tensor, found_unknown_shapes);\n  int size = DataTypeSize(BaseType(tensor.dtype()));\n  VLOG(2) << \"Count: \" << count << \" DataTypeSize: \" << size;\n  int64_t tensor_size = MultiplyWithoutOverflow(count, size);\n  if (tensor_size < 0) {\n    VLOG(1) << \"Overflow encountered when computing tensor size, multiplying \"\n            << count << \" with \" << size;\n    return -1;\n  }\n  return tensor_size;\n}\n\nint64_t OpLevelCostEstimator::CalculateInputSize(const OpInfo& op_info,\n                                                 bool* found_unknown_shapes) {\n  int64_t total_input_size = 0;\n  for (auto& input : op_info.inputs()) {\n    int64_t input_size = CalculateTensorSize(input, found_unknown_shapes);\n    total_input_size += input_size;\n    VLOG(1) << \"Input Size: \" << input_size\n            << \" Total Input Size:\" << total_input_size;\n  }\n  return total_input_size;\n}\n\nstd::vector<int64_t> OpLevelCostEstimator::CalculateInputTensorSize(\n    const OpInfo& op_info, bool* found_unknown_shapes) {\n  std::vector<int64_t> input_tensor_size;\n  input_tensor_size.reserve(op_info.inputs().size());\n  for (auto& input : op_info.inputs()) {\n    input_tensor_size.push_back(\n        CalculateTensorSize(input, found_unknown_shapes));\n  }\n  return input_tensor_size;\n}\n\nint64_t OpLevelCostEstimator::CalculateLargestInputCount(\n    const OpInfo& op_info, bool* found_unknown_shapes) {\n  int64_t largest_input_count = 0;\n  for (auto& input : op_info.inputs()) {\n    int64_t input_count =\n        CalculateTensorElementCount(input, found_unknown_shapes);\n    if (input_count > largest_input_count) {\n      largest_input_count = input_count;\n    }\n    VLOG(1) << \"Input Count: \" << input_count\n            << \" Largest Input Count:\" << largest_input_count;\n  }\n  return largest_input_count;\n}\n\nint64_t OpLevelCostEstimator::CalculateOutputSize(const OpInfo& op_info,\n                                                  bool* found_unknown_shapes) {\n  int64_t total_output_size = 0;\n  // Use float as default for calculations.\n  for (const auto& output : op_info.outputs()) {\n    DataType dt = output.dtype();\n    const auto& original_output_shape = output.shape();\n    int64_t output_size = DataTypeSize(BaseType(dt));\n    int num_dims = std::max(1, original_output_shape.dim_size());\n    auto output_shape = MaybeGetMinimumShape(original_output_shape, num_dims,\n                                             found_unknown_shapes);\n    for (const auto& dim : output_shape.dim()) {\n      int64_t new_output_size =\n          MultiplyWithoutOverflow(output_size, dim.size());\n      if (new_output_size < 0) {\n        VLOG(1) << \"Overflow encountered when estimating cost, multiplying \"\n                << output_size << \" with \" << dim.size();\n        return -1;\n      }\n      output_size = new_output_size;\n    }\n    total_output_size += output_size;\n    VLOG(1) << \"Output Size: \" << output_size\n            << \" Total Output Size:\" << total_output_size;\n  }\n  return total_output_size;\n}\n\nstd::vector<int64_t> OpLevelCostEstimator::CalculateOutputTensorSize(\n    const OpInfo& op_info, bool* found_unknown_shapes) {\n  std::vector<int64_t> output_tensor_size;\n  output_tensor_size.reserve(op_info.outputs().size());\n  // Use float as default for calculations.\n  for (const auto& output : op_info.outputs()) {\n    DataType dt = output.dtype();\n    const auto& original_output_shape = output.shape();\n    int64_t output_size = DataTypeSize(BaseType(dt));\n    int num_dims = std::max(1, original_output_shape.dim_size());\n    auto output_shape = MaybeGetMinimumShape(original_output_shape, num_dims,\n                                             found_unknown_shapes);\n    for (const auto& dim : output_shape.dim()) {\n      output_size *= dim.size();\n    }\n    output_tensor_size.push_back(output_size);\n  }\n  return output_tensor_size;\n}\n\nStatus OpLevelCostEstimator::PredictDefaultNodeCosts(\n    const int64_t num_compute_ops, const OpContext& op_context,\n    bool* found_unknown_shapes, NodeCosts* node_costs) {\n  const auto& op_info = op_context.op_info;\n  node_costs->num_compute_ops = num_compute_ops;\n  node_costs->num_input_bytes_accessed =\n      CalculateInputTensorSize(op_info, found_unknown_shapes);\n  node_costs->num_output_bytes_accessed =\n      CalculateOutputTensorSize(op_info, found_unknown_shapes);\n  node_costs->max_memory = node_costs->num_total_output_bytes();\n  if (*found_unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nbool HasZeroDim(const OpInfo& op_info) {\n  for (int i = 0; i < op_info.inputs_size(); ++i) {\n    const auto& input = op_info.inputs(i);\n    for (int j = 0; j < input.shape().dim_size(); ++j) {\n      const auto& dim = input.shape().dim(j);\n      if (dim.size() == 0) {\n        VLOG(1) << \"Convolution config has zero dim \"\n                << op_info.ShortDebugString();\n        return true;\n      }\n    }\n  }\n  return false;\n}\n\nStatus OpLevelCostEstimator::PredictConv2D(const OpContext& op_context,\n                                           NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  if (HasZeroDim(op_info)) {\n    node_costs->num_nodes_with_unknown_shapes = 1;\n    return errors::InvalidArgument(\"Conv2D op includes zero dimension: \",\n                                   op_info.ShortDebugString());\n  }\n  bool found_unknown_shapes = false;\n  int64_t num_compute_ops =\n      CountConv2DOperations(op_info, &found_unknown_shapes);\n  return PredictDefaultNodeCosts(num_compute_ops, op_context,\n                                 &found_unknown_shapes, node_costs);\n}\n\nStatus OpLevelCostEstimator::PredictConv2DBackpropInput(\n    const OpContext& op_context, NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  if (HasZeroDim(op_info)) {\n    node_costs->num_nodes_with_unknown_shapes = 1;\n    return errors::InvalidArgument(\n        \"Conv2DBackpropInput op includes zero dimension\",\n        op_info.ShortDebugString());\n  }\n  bool found_unknown_shapes = false;\n  int64_t num_compute_ops = CountConv2DBackpropInputOperations(\n      op_info, nullptr, &found_unknown_shapes);\n  return PredictDefaultNodeCosts(num_compute_ops, op_context,\n                                 &found_unknown_shapes, node_costs);\n}\n\nStatus OpLevelCostEstimator::PredictConv2DBackpropFilter(\n    const OpContext& op_context, NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  if (HasZeroDim(op_info)) {\n    node_costs->num_nodes_with_unknown_shapes = 1;\n    return errors::InvalidArgument(\n        \"Conv2DBackpropFilter op includes zero dimension\",\n        op_info.ShortDebugString());\n  }\n  bool found_unknown_shapes = false;\n  int64_t num_compute_ops = CountConv2DBackpropFilterOperations(\n      op_info, nullptr, &found_unknown_shapes);\n  return PredictDefaultNodeCosts(num_compute_ops, op_context,\n                                 &found_unknown_shapes, node_costs);\n}\n\nStatus OpLevelCostEstimator::PredictFusedConv2DBiasActivation(\n    const OpContext& op_context, NodeCosts* node_costs) const {\n  // FusedConv2DBiasActivation computes a fused kernel which implements:\n  // 2D convolution, adds side input with separate scaling on convolution and\n  // side inputs, then adds bias, and finally applies the ReLU activation\n  // function to the result:\n  //\n  // Input -> Conv2D  ->  Add  -> BiasAdd  -> ReLU\n  //            ^          ^         ^\n  //          Filter   Side Input   Bias\n  //\n  // Note that when adding the side input, the operation multiplies the output\n  // of Conv2D by conv_input_scale, confusingly, and the side_input by\n  // side_input_scale.\n  //\n  // Note that in the special case that side_input_scale is 0, which we infer\n  // from side_input having dimensions [], we skip that addition operation.\n  //\n  // For more information, see\n  // contrib/fused_conv/kernels/fused_conv2d_bias_activation_op.cc\n\n  // TODO(yaozhang): Support NHWC_VECT_W.\n  std::string data_format = GetDataFormat(op_context.op_info);\n  if (data_format != \"NCHW\" && data_format != \"NHWC\" &&\n      data_format != \"NCHW_VECT_C\") {\n    return errors::InvalidArgument(\n        \"Unsupported data format (\", data_format,\n        \") for op: \", op_context.op_info.ShortDebugString());\n  }\n  std::string filter_format = GetFilterFormat(op_context.op_info);\n  if (filter_format != \"HWIO\" && filter_format != \"OIHW\" &&\n      filter_format != \"OIHW_VECT_I\") {\n    return errors::InvalidArgument(\n        \"Unsupported filter format (\", filter_format,\n        \") for op: \", op_context.op_info.ShortDebugString());\n  }\n\n  auto& conv_input = op_context.op_info.inputs(0);\n  auto& filter = op_context.op_info.inputs(1);\n  auto& side_input = op_context.op_info.inputs(3);\n  auto& conv_input_scale = op_context.op_info.inputs(4);\n  auto& side_input_scale = op_context.op_info.inputs(5);\n\n  // Manually compute our convolution dimensions.\n  bool found_unknown_shapes = false;\n  auto dims = ConvolutionDimensionsFromInputs(\n      conv_input.shape(), filter.shape(), op_context.op_info,\n      &found_unknown_shapes);\n  OpInfo::TensorProperties output;\n  if (data_format == \"NCHW\" || data_format == \"NCHW_VECT_C\") {\n    output = DescribeTensor(DT_FLOAT, {dims.batch, dims.oz, dims.oy, dims.ox});\n  } else if (data_format == \"NHWC\") {\n    output = DescribeTensor(DT_FLOAT, {dims.batch, dims.oy, dims.ox, dims.oz});\n  }\n\n  // Add the operations the fused op always computes.\n  std::vector<OpContext> component_ops = {\n      FusedChildContext(op_context, \"Conv2D\", output, {conv_input, filter}),\n      FusedChildContext(op_context, \"Mul\", output, {output, conv_input_scale}),\n      FusedChildContext(\n          op_context, \"BiasAdd\", output,\n          {output, output}),  // Note we're no longer using bias at all\n      FusedChildContext(op_context, \"Relu\", output, {output})};\n\n  // Add our side_input iff it's non-empty.\n  if (side_input.shape().dim_size() > 0) {\n    component_ops.push_back(FusedChildContext(op_context, \"Mul\", side_input,\n                                              {side_input, side_input_scale}));\n    component_ops.push_back(FusedChildContext(\n        op_context, \"Add\", output,\n        {output, output}));  // Note that we're not using side_input here\n  }\n\n  // Construct an op_context which definitely has our output shape.\n  auto op_context_with_output = op_context;\n  op_context_with_output.op_info.mutable_outputs()->Clear();\n  *op_context_with_output.op_info.mutable_outputs()->Add() = output;\n\n  // Construct component operations and run the cost computation.\n  if (found_unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return PredictFusedOp(op_context_with_output, component_ops, node_costs);\n}\n\nStatus OpLevelCostEstimator::PredictMatMul(const OpContext& op_context,\n                                           NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  bool found_unknown_shapes = false;\n  int64_t num_compute_ops =\n      CountMatMulOperations(op_info, &found_unknown_shapes);\n  return PredictDefaultNodeCosts(num_compute_ops, op_context,\n                                 &found_unknown_shapes, node_costs);\n}\n\nStatus OpLevelCostEstimator::PredictEinsum(const OpContext& op_context,\n                                           NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n\n  auto it = op_info.attr().find(\"equation\");\n  if (it == op_info.attr().end()) {\n    return errors::InvalidArgument(\"Einsum op doesn't have equation attr: \",\n                                   op_info.ShortDebugString());\n  }\n\n  OpContext batch_matmul_op_context;\n  bool found_unknown_shapes = false;\n  bool success = GenerateBatchMatmulContextFromEinsum(\n      op_context, &batch_matmul_op_context, &found_unknown_shapes);\n  if (found_unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  if (!success) {\n    return PredictCostOfAnUnknownOp(op_context, node_costs);\n  }\n  return PredictNodeCosts(batch_matmul_op_context, node_costs);\n}\n\nStatus OpLevelCostEstimator::PredictSparseTensorDenseMatMul(\n    const OpContext& op_context, NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  bool found_unknown_shapes = false;\n  // input[0]: indices in sparse matrix a\n  // input[1]: values in sparse matrix a\n  // input[2]: shape of matrix a\n  // input[3]: matrix b\n  // See\n  // https://github.com/tensorflow/tensorflow/blob/9a43dfeac5/tensorflow/core/ops/sparse_ops.cc#L85\n  int64_t num_elems_in_a =\n      CalculateTensorElementCount(op_info.inputs(1), &found_unknown_shapes);\n  auto b_matrix = op_info.inputs(3);\n  auto b_matrix_shape =\n      MaybeGetMinimumShape(b_matrix.shape(), 2, &found_unknown_shapes);\n  int64_t n_dim = b_matrix_shape.dim(1).size();\n\n  // Each element in A is multiplied and added with an element from each column\n  // in b.\n  const int64_t op_count = kOpsPerMac * num_elems_in_a * n_dim;\n\n  int64_t a_indices_input_size =\n      CalculateTensorSize(op_info.inputs(0), &found_unknown_shapes);\n  int64_t a_values_input_size =\n      CalculateTensorSize(op_info.inputs(1), &found_unknown_shapes);\n  int64_t a_shape_input_size =\n      CalculateTensorSize(op_info.inputs(2), &found_unknown_shapes);\n  int64_t b_input_size =\n      num_elems_in_a * n_dim * DataTypeSize(BaseType(b_matrix.dtype()));\n  int64_t output_size = CalculateOutputSize(op_info, &found_unknown_shapes);\n\n  node_costs->num_compute_ops = op_count;\n  node_costs->num_input_bytes_accessed = {a_indices_input_size,\n                                          a_values_input_size,\n                                          a_shape_input_size, b_input_size};\n  node_costs->num_output_bytes_accessed = {output_size};\n  if (found_unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictNoOp(const OpContext& op_context,\n                                         NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  VLOG(1) << \"Op:\" << op_info.op() << \" Execution Time 0 (ns)\";\n  // By default, NodeCosts is initialized to zero ops and bytes.\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictPureMemoryOp(const OpContext& op_context,\n                                                 NodeCosts* node_costs) const {\n  // Each output element is a copy of some element from input, with no required\n  // computation, so just compute memory costs.\n  bool found_unknown_shapes = false;\n  node_costs->num_nodes_with_pure_memory_op = 1;\n  return PredictDefaultNodeCosts(0, op_context, &found_unknown_shapes,\n                                 node_costs);\n}\n\nStatus OpLevelCostEstimator::PredictIdentity(const OpContext& op_context,\n                                             NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  VLOG(1) << \"Op:\" << op_info.op() << \" Minimum cost for Identity\";\n  node_costs->minimum_cost_op = true;\n  node_costs->num_compute_ops = kMinComputeOp;\n  // Identity op internally pass input tensor buffer's pointer to the output\n  // tensor buffer; no actual memory operation.\n  node_costs->num_input_bytes_accessed = {0};\n  node_costs->num_output_bytes_accessed = {0};\n  bool inaccurate = false;\n  node_costs->max_memory = CalculateOutputSize(op_info, &inaccurate);\n  if (inaccurate) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictVariable(const OpContext& op_context,\n                                             NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  VLOG(1) << \"Op:\" << op_info.op() << \" Minimum cost for Variable\";\n  node_costs->minimum_cost_op = true;\n  node_costs->num_compute_ops = kMinComputeOp;\n  // Variables are persistent ops; initialized before step; hence, no memory\n  // cost.\n  node_costs->num_input_bytes_accessed = {0};\n  node_costs->num_output_bytes_accessed = {0};\n  bool inaccurate = false;\n  node_costs->persistent_memory = CalculateOutputSize(op_info, &inaccurate);\n  if (inaccurate) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictBatchMatMul(const OpContext& op_context,\n                                                NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  bool found_unknown_shapes = false;\n  int64_t num_compute_ops =\n      CountBatchMatMulOperations(op_info, &found_unknown_shapes);\n  return PredictDefaultNodeCosts(num_compute_ops, op_context,\n                                 &found_unknown_shapes, node_costs);\n}\n\nStatus OpLevelCostEstimator::PredictMetadata(const OpContext& op_context,\n                                             NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  node_costs->minimum_cost_op = true;\n  node_costs->num_compute_ops = kMinComputeOp;\n  node_costs->num_input_bytes_accessed = {0};\n  node_costs->num_output_bytes_accessed = {0};\n  bool inaccurate = false;\n  node_costs->max_memory = CalculateOutputSize(op_info, &inaccurate);\n  if (inaccurate) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictGatherOrSlice(const OpContext& op_context,\n                                                  NodeCosts* node_costs) const {\n  // Gather & Slice ops can have a very large input, but only access a small\n  // part of it. For these op the size of the output determines the memory cost.\n  const auto& op_info = op_context.op_info;\n\n  const int inputs_needed = op_info.op() == \"Slice\" ? 3 : 2;\n  if (op_info.outputs_size() == 0 || op_info.inputs_size() < inputs_needed) {\n    return errors::InvalidArgument(\n        op_info.op(),\n        \" Op doesn't have valid input / output: \", op_info.ShortDebugString());\n  }\n\n  bool unknown_shapes = false;\n\n  // Each output element is a copy of some element from input.\n  // For roofline estimate we assume each copy has a unit cost.\n  const int64_t op_count =\n      CalculateTensorElementCount(op_info.outputs(0), &unknown_shapes);\n  node_costs->num_compute_ops = op_count;\n\n  const int64_t output_size = CalculateOutputSize(op_info, &unknown_shapes);\n  node_costs->num_output_bytes_accessed = {output_size};\n\n  node_costs->num_input_bytes_accessed.reserve(op_info.inputs().size());\n  int64_t input_size = output_size;\n  // Note that input(0) byte accessed is not equal to input(0) tensor size.\n  // It's equal to the output size; though, input access is indexed gather or\n  // slice (ignore duplicate indices).\n  node_costs->num_input_bytes_accessed.push_back(input_size);\n  int begin_input_index = 1;\n  int end_input_index;\n  if (op_info.op() == \"Slice\") {\n    // Slice: 'input' (omitted), 'begin', 'size'\n    end_input_index = 3;\n  } else if (op_info.op() == \"StridedSlice\") {\n    // StridedSlice: 'input' (omitted), 'begin', 'end', 'strides'\n    end_input_index = 4;\n  } else {\n    // Gather, GatherV2, GatherNd: 'params' (omitted), 'indices'\n    end_input_index = 2;\n  }\n  for (int i = begin_input_index; i < end_input_index; ++i) {\n    node_costs->num_input_bytes_accessed.push_back(\n        CalculateTensorElementCount(op_info.inputs(i), &unknown_shapes));\n  }\n  if (unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictScatter(const OpContext& op_context,\n                                            NodeCosts* node_costs) const {\n  // Scatter ops sparsely access a reference input and output tensor.\n  const auto& op_info = op_context.op_info;\n  bool found_unknown_shapes = false;\n\n  // input[0]: ref tensor that will be sparsely accessed\n  // input[1]: indices - A tensor of indices into the first dimension of ref.\n  // input[2]: updates where updates.shape = indices.shape + ref.shape[1:]\n  // See\n  // https://www.tensorflow.org/api_docs/python/tf/scatter_add and\n  // https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/state_ops.cc#L146\n\n  const int64_t num_indices =\n      CalculateTensorElementCount(op_info.inputs(1), &found_unknown_shapes);\n\n  int64_t num_elems_in_ref_per_index = 1;\n  auto ref_tensor_shape = MaybeGetMinimumShape(\n      op_info.inputs(0).shape(), op_info.inputs(0).shape().dim_size(),\n      &found_unknown_shapes);\n  for (int i = 1; i < ref_tensor_shape.dim().size(); ++i) {\n    num_elems_in_ref_per_index *= ref_tensor_shape.dim(i).size();\n  }\n  const int64_t op_count = num_indices * num_elems_in_ref_per_index;\n  node_costs->num_compute_ops = op_count;\n\n  // Sparsely access ref so input size depends on the number of operations\n  int64_t ref_input_size =\n      op_count * DataTypeSize(BaseType(op_info.inputs(0).dtype()));\n  int64_t indices_input_size =\n      CalculateTensorSize(op_info.inputs(1), &found_unknown_shapes);\n  int64_t updates_input_size =\n      CalculateTensorSize(op_info.inputs(2), &found_unknown_shapes);\n  node_costs->num_input_bytes_accessed = {ref_input_size, indices_input_size,\n                                          updates_input_size};\n\n  // Sparsely access ref so output size depends on the number of operations\n  int64_t output_size =\n      op_count * DataTypeSize(BaseType(op_info.outputs(0).dtype()));\n  node_costs->num_output_bytes_accessed = {output_size};\n\n  if (found_unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictFusedOp(\n    const OpContext& op_context,\n    const std::vector<OpContext>& fused_op_contexts,\n    NodeCosts* node_costs) const {\n  // Note that PredictDefaultNodeCosts will get the correct memory costs from\n  // the node's inputs and outputs; but we don't want to have to re-implement\n  // the logic for computing the operation count of each of our component\n  // operations here; so we simply add the compute times of each component\n  // operation, then update the cost.\n  bool found_unknown_shapes = false;\n  Status s =\n      PredictDefaultNodeCosts(0, op_context, &found_unknown_shapes, node_costs);\n\n  for (auto& fused_op : fused_op_contexts) {\n    NodeCosts fused_node_costs;\n    s.Update(PredictNodeCosts(fused_op, &fused_node_costs));\n    node_costs->num_compute_ops += fused_node_costs.num_compute_ops;\n    node_costs->inaccurate |= fused_node_costs.inaccurate;\n    // Set, not increment. Note that we are predicting the cost of one fused\n    // node, not a function node composed of many nodes.\n    node_costs->num_nodes_with_unknown_shapes |=\n        fused_node_costs.num_nodes_with_unknown_shapes;\n    node_costs->num_nodes_with_unknown_op_type |=\n        fused_node_costs.num_nodes_with_unknown_op_type;\n    node_costs->num_nodes_with_pure_memory_op |=\n        fused_node_costs.num_nodes_with_pure_memory_op;\n  }\n\n  return Status::OK();\n}\n\n/* static */\nOpContext OpLevelCostEstimator::FusedChildContext(\n    const OpContext& parent, const std::string& op_name,\n    const OpInfo::TensorProperties& output,\n    const std::vector<OpInfo::TensorProperties>& inputs) {\n  // Setup the base parameters of our new context.\n  OpContext new_context;\n  new_context.name = op_name;\n  new_context.device_name = parent.device_name;\n  new_context.op_info = parent.op_info;\n  new_context.op_info.set_op(op_name);\n\n  // Setup the inputs of our new context.\n  new_context.op_info.mutable_inputs()->Clear();\n  for (const auto& input : inputs) {\n    *new_context.op_info.mutable_inputs()->Add() = input;\n  }\n\n  // Setup the output of our new context.\n  new_context.op_info.mutable_outputs()->Clear();\n  *new_context.op_info.mutable_outputs()->Add() = output;\n\n  return new_context;\n}\n\n/* static */\nOpInfo::TensorProperties OpLevelCostEstimator::DescribeTensor(\n    DataType type, const std::vector<int64_t>& dims) {\n  OpInfo::TensorProperties ret;\n  ret.set_dtype(type);\n\n  auto shape = ret.mutable_shape();\n  for (const int dim : dims) {\n    shape->add_dim()->set_size(dim);\n  }\n\n  return ret;\n}\n\n/* static */\nStatusOr<OpLevelCostEstimator::ConvolutionDimensions>\nOpLevelCostEstimator::OpDimensionsFromInputs(\n    const TensorShapeProto& original_image_shape, const OpInfo& op_info,\n    bool* found_unknown_shapes) {\n  VLOG(2) << \"op features: \" << op_info.DebugString();\n  VLOG(2) << \"Original image shape: \" << original_image_shape.DebugString();\n  auto image_shape =\n      MaybeGetMinimumShape(original_image_shape, 4, found_unknown_shapes);\n  VLOG(2) << \"Image shape: \" << image_shape.DebugString();\n\n  int x_index, y_index, channel_index;\n  const std::string& data_format = GetDataFormat(op_info);\n  if (data_format == \"NCHW\") {\n    channel_index = 1;\n    y_index = 2;\n    x_index = 3;\n  } else {\n    y_index = 1;\n    x_index = 2;\n    channel_index = 3;\n  }\n  int64_t batch = image_shape.dim(0).size();\n  int64_t ix = image_shape.dim(x_index).size();\n  int64_t iy = image_shape.dim(y_index).size();\n  int64_t iz = image_shape.dim(channel_index).size();\n\n  // Note that FusedBatchNorm doesn't have ksize attr, but GetKernelSize returns\n  // {1, 1, 1, 1} in that case.\n  std::vector<int64_t> ksize = GetKernelSize(op_info);\n  int64_t kx = ksize[x_index];\n  int64_t ky = ksize[y_index];\n  // These ops don't support groupwise operation, therefore kz == iz.\n  int64_t kz = iz;\n\n  std::vector<int64_t> strides = GetStrides(op_info);\n  int64_t sx = strides[x_index];\n  int64_t sy = strides[y_index];\n  if (sx == 0 || sy == 0) {\n    return errors::InvalidArgument(\n        \"Stride must be > 0 for Height and Width, but got (\", sy, \", \", sx,\n        \")\");\n  }\n  const auto padding = GetPadding(op_info);\n\n  int64_t ox = GetOutputSize(ix, kx, sx, padding);\n  int64_t oy = GetOutputSize(iy, ky, sy, padding);\n  int64_t oz = iz;\n\n  OpLevelCostEstimator::ConvolutionDimensions conv_dims = {\n      batch, ix, iy, iz, kx, ky, kz, oz, ox, oy, sx, sy, padding};\n  return conv_dims;\n}\n\nStatus OpLevelCostEstimator::PredictMaxPool(const OpContext& op_context,\n                                            NodeCosts* node_costs) const {\n  bool found_unknown_shapes = false;\n  const auto& op_info = op_context.op_info;\n  // x: op_info.inputs(0)\n  TF_ASSIGN_OR_RETURN(ConvolutionDimensions dims,\n                      OpDimensionsFromInputs(op_info.inputs(0).shape(), op_info,\n                                             &found_unknown_shapes));\n  // kx * ky - 1 comparisons per output (kx * xy > 1)\n  // or 1 copy per output (kx * k1 = 1).\n  int per_output_ops = dims.kx * dims.ky == 1 ? 1 : dims.kx * dims.ky - 1;\n  int64_t ops = dims.batch * dims.ox * dims.oy * dims.oz * per_output_ops;\n  node_costs->num_compute_ops = ops;\n\n  int64_t input_size = 0;\n  if (dims.ky >= dims.sy) {\n    input_size = CalculateTensorSize(op_info.inputs(0), &found_unknown_shapes);\n  } else {  // dims.ky < dims.sy\n    // Vertical stride is larger than vertical kernel; assuming row-major\n    // format, skip unnecessary rows (or read every kx rows per sy rows, as the\n    // others are not used for output).\n    const auto data_size = DataTypeSize(BaseType(op_info.inputs(0).dtype()));\n    input_size = data_size * dims.batch * dims.ix * dims.ky * dims.oy * dims.iz;\n  }\n  node_costs->num_input_bytes_accessed = {input_size};\n  const int64_t output_size =\n      CalculateOutputSize(op_info, &found_unknown_shapes);\n  node_costs->num_output_bytes_accessed = {output_size};\n  node_costs->max_memory = output_size;\n  if (found_unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictMaxPoolGrad(const OpContext& op_context,\n                                                NodeCosts* node_costs) const {\n  bool found_unknown_shapes = false;\n  const auto& op_info = op_context.op_info;\n  // x: op_info.inputs(0)\n  // y: op_info.inputs(1)\n  // y_grad: op_info.inputs(2)\n  if (op_info.inputs_size() < 3) {\n    return errors::InvalidArgument(\"MaxPoolGrad op has invalid inputs: \",\n                                   op_info.ShortDebugString());\n  }\n\n  TF_ASSIGN_OR_RETURN(ConvolutionDimensions dims,\n                      OpDimensionsFromInputs(op_info.inputs(0).shape(), op_info,\n                                             &found_unknown_shapes));\n\n  int64_t ops = 0;\n  if (dims.kx == 1 && dims.ky == 1) {\n    // 1x1 window. No need to know which input was max.\n    ops = dims.batch * dims.ix * dims.iy * dims.iz;\n  } else if (dims.kx <= dims.sx && dims.ky <= dims.sy) {\n    // Non-overlapping window: re-run maxpool, then assign zero or y_grad.\n    ops = dims.batch * dims.iz *\n          (dims.ox * dims.oy * (dims.kx * dims.ky - 1) + dims.ix * dims.iy);\n  } else {\n    // Overlapping window: initialize with zeros, re-run maxpool, then\n    // accumulate y_gad to proper x_grad locations.\n    ops = dims.batch * dims.iz *\n          (dims.ox * dims.oy * (dims.kx * dims.ky - 1) + dims.ix * dims.iy * 2);\n  }\n  node_costs->num_compute_ops = ops;\n\n  // Just read x and y_grad; no need to read y as we assume MaxPoolGrad re-run\n  // MaxPool internally.\n  const int64_t input0_size =\n      CalculateTensorSize(op_info.inputs(0), &found_unknown_shapes);\n  const int64_t input2_size =\n      CalculateTensorSize(op_info.inputs(2), &found_unknown_shapes);\n  node_costs->num_input_bytes_accessed = {input0_size, 0, input2_size};\n  // Write x_grad; size equal to x.\n  const int64_t output_size =\n      CalculateTensorSize(op_info.inputs(0), &found_unknown_shapes);\n  node_costs->num_output_bytes_accessed = {output_size};\n  node_costs->max_memory = output_size;\n\n  if (found_unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\n/* This predict function handles three types of tensorflow ops\n * AssignVariableOp/AssignAddVariableOp/AssignSubVariableOp, broadcasting\n * was not possible for these ops, therefore the input tensor's shapes is\n * enough to compute the cost */\nStatus OpLevelCostEstimator::PredictAssignVariableOps(\n    const OpContext& op_context, NodeCosts* node_costs) const {\n  bool found_unknown_shapes = false;\n  const auto& op_info = op_context.op_info;\n  /* First input of these ops are reference to the assignee. */\n  if (op_info.inputs_size() != 2) {\n    return errors::InvalidArgument(\"AssignVariable op has invalid input: \",\n                                   op_info.ShortDebugString());\n  }\n\n  const int64_t ops = op_info.op() == kAssignVariableOp\n                          ? 0\n                          : CalculateTensorElementCount(op_info.inputs(1),\n                                                        &found_unknown_shapes);\n  node_costs->num_compute_ops = ops;\n  const int64_t input_size = CalculateInputSize(op_info, &found_unknown_shapes);\n  node_costs->num_input_bytes_accessed = {input_size};\n  // TODO(dyoon): check these ops' behavior whether it writes data;\n  // Op itself doesn't have output tensor, but it may modify the input (ref or\n  // resource). Maybe use node_costs->internal_write_bytes.\n  node_costs->num_output_bytes_accessed = {0};\n  if (found_unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictAvgPool(const OpContext& op_context,\n                                            NodeCosts* node_costs) const {\n  bool found_unknown_shapes = false;\n  const auto& op_info = op_context.op_info;\n  // x: op_info.inputs(0)\n  TF_ASSIGN_OR_RETURN(ConvolutionDimensions dims,\n                      OpDimensionsFromInputs(op_info.inputs(0).shape(), op_info,\n                                             &found_unknown_shapes));\n\n  // kx * ky - 1 additions and 1 multiplication per output.\n  int64_t ops = dims.batch * dims.ox * dims.oy * dims.oz * dims.kx * dims.ky;\n  node_costs->num_compute_ops = ops;\n\n  int64_t input_size;\n  if (dims.ky >= dims.sy) {\n    input_size = CalculateTensorSize(op_info.inputs(0), &found_unknown_shapes);\n  } else {  // dims.ky < dims.sy\n    // vertical stride is larger than vertical kernel; assuming row-major\n    // format, skip unnecessary rows (or read every kx rows per sy rows, as the\n    // others are not used for output).\n    const auto data_size = DataTypeSize(BaseType(op_info.inputs(0).dtype()));\n    input_size = data_size * dims.batch * dims.ix * dims.ky * dims.oy * dims.iz;\n  }\n  node_costs->num_input_bytes_accessed = {input_size};\n\n  const int64_t output_size =\n      CalculateOutputSize(op_info, &found_unknown_shapes);\n  node_costs->num_output_bytes_accessed = {output_size};\n  node_costs->max_memory = output_size;\n\n  if (found_unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictAvgPoolGrad(const OpContext& op_context,\n                                                NodeCosts* node_costs) const {\n  bool found_unknown_shapes = false;\n  const auto& op_info = op_context.op_info;\n  // x's shape: op_info.inputs(0)\n  // y_grad: op_info.inputs(1)\n\n  // Extract x_shape from op_info.inputs(0).value() or op_info.outputs(0).\n  bool shape_found = false;\n  TensorShapeProto x_shape;\n  if (op_info.inputs_size() >= 1 && op_info.inputs(0).has_value()) {\n    const TensorProto& value = op_info.inputs(0).value();\n    shape_found = GetTensorShapeProtoFromTensorProto(value, &x_shape);\n  }\n  if (!shape_found && op_info.outputs_size() > 0) {\n    x_shape = op_info.outputs(0).shape();\n    shape_found = true;\n  }\n  if (!shape_found) {\n    // Set the minimum shape that's feasible.\n    x_shape.Clear();\n    for (int i = 0; i < 4; ++i) {\n      x_shape.add_dim()->set_size(1);\n    }\n    found_unknown_shapes = true;\n  }\n\n  TF_ASSIGN_OR_RETURN(\n      ConvolutionDimensions dims,\n      OpDimensionsFromInputs(x_shape, op_info, &found_unknown_shapes));\n\n  int64_t ops = 0;\n  if (dims.kx <= dims.sx && dims.ky <= dims.sy) {\n    // Non-overlapping window.\n    ops = dims.batch * dims.iz * (dims.ix * dims.iy + dims.ox * dims.oy);\n  } else {\n    // Overlapping window.\n    ops = dims.batch * dims.iz *\n          (dims.ix * dims.iy + dims.ox * dims.oy * (dims.kx * dims.ky + 1));\n  }\n  auto s = PredictDefaultNodeCosts(ops, op_context, &found_unknown_shapes,\n                                   node_costs);\n  node_costs->max_memory = node_costs->num_total_output_bytes();\n  return s;\n}\n\nStatus OpLevelCostEstimator::PredictFusedBatchNorm(\n    const OpContext& op_context, NodeCosts* node_costs) const {\n  bool found_unknown_shapes = false;\n  const auto& op_info = op_context.op_info;\n  // x: op_info.inputs(0)\n  // scale: op_info.inputs(1)\n  // offset: op_info.inputs(2)\n  // mean: op_info.inputs(3)  --> only for inference\n  // variance: op_info.inputs(4) --> only for inference\n  TF_ASSIGN_OR_RETURN(ConvolutionDimensions dims,\n                      OpDimensionsFromInputs(op_info.inputs(0).shape(), op_info,\n                                             &found_unknown_shapes));\n  const bool is_training = IsTraining(op_info);\n\n  int64_t ops = 0;\n  const auto rsqrt_cost = Eigen::internal::functor_traits<\n      Eigen::internal::scalar_rsqrt_op<float>>::Cost;\n  if (is_training) {\n    ops = dims.iz * (dims.batch * dims.ix * dims.iy * 4 + 6 + rsqrt_cost);\n  } else {\n    ops = dims.batch * dims.ix * dims.iy * dims.iz * 2;\n  }\n  node_costs->num_compute_ops = ops;\n\n  const int64_t size_nhwc =\n      CalculateTensorSize(op_info.inputs(0), &found_unknown_shapes);\n  const int64_t size_c =\n      CalculateTensorSize(op_info.inputs(1), &found_unknown_shapes);\n  if (is_training) {\n    node_costs->num_input_bytes_accessed = {size_nhwc, size_c, size_c};\n    node_costs->num_output_bytes_accessed = {size_nhwc, size_c, size_c, size_c,\n                                             size_c};\n    // FusedBatchNorm in training mode internally re-reads the input tensor:\n    // one for mean/variance, and the 2nd internal read forthe actual scaling.\n    // Assume small intermediate data such as mean / variance (size_c) can be\n    // cached on-chip.\n    node_costs->internal_read_bytes = size_nhwc;\n  } else {\n    node_costs->num_input_bytes_accessed = {size_nhwc, size_c, size_c, size_c,\n                                            size_c};\n    node_costs->num_output_bytes_accessed = {size_nhwc};\n  }\n  node_costs->max_memory = node_costs->num_total_output_bytes();\n\n  if (found_unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictFusedBatchNormGrad(\n    const OpContext& op_context, NodeCosts* node_costs) const {\n  bool found_unknown_shapes = false;\n  const auto& op_info = op_context.op_info;\n  // y_backprop: op_info.inputs(0)\n  // x: op_info.inputs(1)\n  // scale: op_info.inputs(2)\n  // mean: op_info.inputs(3)\n  // variance or inverse of variance: op_info.inputs(4)\n  TF_ASSIGN_OR_RETURN(ConvolutionDimensions dims,\n                      OpDimensionsFromInputs(op_info.inputs(1).shape(), op_info,\n                                             &found_unknown_shapes));\n\n  int64_t ops = 0;\n  const auto rsqrt_cost = Eigen::internal::functor_traits<\n      Eigen::internal::scalar_rsqrt_op<float>>::Cost;\n  ops = dims.iz * (dims.batch * dims.ix * dims.iy * 11 + 5 + rsqrt_cost);\n  node_costs->num_compute_ops = ops;\n\n  const int64_t size_nhwc =\n      CalculateTensorSize(op_info.inputs(1), &found_unknown_shapes);\n  const int64_t size_c =\n      CalculateTensorSize(op_info.inputs(2), &found_unknown_shapes);\n  // TODO(dyoon): fix missing memory cost for variance input (size_c) and\n  // yet another read of y_backprop (size_nhwc) internally.\n  node_costs->num_input_bytes_accessed = {size_nhwc, size_nhwc, size_c, size_c};\n  node_costs->num_output_bytes_accessed = {size_nhwc, size_c, size_c};\n  // FusedBatchNormGrad has to read y_backprop internally.\n  node_costs->internal_read_bytes = size_nhwc;\n  node_costs->max_memory = node_costs->num_total_output_bytes();\n\n  if (found_unknown_shapes) {\n    node_costs->inaccurate = true;\n    node_costs->num_nodes_with_unknown_shapes = 1;\n  }\n  return Status::OK();\n}\n\nStatus OpLevelCostEstimator::PredictNaryOp(const OpContext& op_context,\n                                           NodeCosts* node_costs) const {\n  const auto& op_info = op_context.op_info;\n  bool found_unknown_shapes = false;\n  // Calculate the largest known tensor size across all inputs and output.\n  int64_t op_count = CalculateLargestInputCount(op_info, &found_unknown_shapes);\n  // If output shape is available, try to use the element count calculated from\n  // that.\n  if (op_info.outputs_size() > 0) {\n    op_count = std::max(\n        op_count,\n        CalculateTensorElementCount(op_info.outputs(0), &found_unknown_shapes));\n  }\n  // Also calculate the output shape possibly resulting from broadcasting.\n  // Note that the some Nary ops (such as AddN) do not support broadcasting,\n  // but we're including this here for completeness.\n  if (op_info.inputs_size() >= 2) {\n    op_count = std::max(op_count, CwiseOutputElementCount(op_info));\n  }\n\n  // Nary ops perform one operation for every element in every input tensor.\n  op_count *= op_info.inputs_size() - 1;\n\n  const auto sum_cost = Eigen::internal::functor_traits<\n      Eigen::internal::scalar_sum_op<float>>::Cost;\n  return PredictDefaultNodeCosts(op_count * sum_cost, op_context,\n                                 &found_unknown_shapes, node_costs);\n}\n\n// softmax[i, j] = exp(logits[i, j]) / sum_j(exp(logits[i, j]))\nStatus OpLevelCostEstimator::PredictSoftmax(const OpContext& op_context,\n                                            NodeCosts* node_costs) const {\n  bool found_unknown_shapes = false;\n  const int64_t logits_size = CalculateTensorElementCount(\n      op_context.op_info.inputs(0), &found_unknown_shapes);\n  // Softmax input rank should be >=1.\n  TensorShapeProto logits_shape = op_context.op_info.inputs(0).shape();\n  if (logits_shape.unknown_rank() || logits_shape.dim_size() == 0) {\n    return errors::InvalidArgument(\"Softmax op has invalid input: \",\n                                   op_context.op_info.ShortDebugString());\n  }\n\n#define EIGEN_COST(X) Eigen::internal::functor_traits<Eigen::internal::X>::Cost\n\n  // Every element of <logits> will be exponentiated, have that result included\n  // in a sum across j, and also have that result multiplied by the reciprocal\n  // of the sum_j. In addition, we'll compute 1/sum_j for every i.\n  auto ops =\n      (EIGEN_COST(scalar_exp_op<float>) + EIGEN_COST(scalar_sum_op<float>) +\n       EIGEN_COST(scalar_product_op<float>)) *\n          logits_size +\n      EIGEN_COST(scalar_inverse_op<float>) * logits_shape.dim(0).size();\n\n#undef EIGEN_COST\n  return PredictDefaultNodeCosts(ops, op_context, &found_unknown_shapes,\n                                 node_costs);\n}\n\nStatus OpLevelCostEstimator::PredictResizeBilinear(\n    const OpContext& op_context, NodeCosts* node_costs) const {\n  bool found_unknown_shapes = false;\n\n  if (op_context.op_info.outputs().empty() ||\n      op_context.op_info.inputs().empty()) {\n    return errors::InvalidArgument(\n        \"ResizeBilinear op has invalid input / output \",\n        op_context.op_info.ShortDebugString());\n  }\n\n  const int64_t output_elements = CalculateTensorElementCount(\n      op_context.op_info.outputs(0), &found_unknown_shapes);\n\n  const auto half_pixel_centers =\n      op_context.op_info.attr().find(\"half_pixel_centers\");\n  bool use_half_pixel_centers = false;\n  if (half_pixel_centers == op_context.op_info.attr().end()) {\n    LOG(WARNING) << \"half_pixel_centers attr not set for ResizeBilinear.\";\n    return PredictCostOfAnUnknownOp(op_context, node_costs);\n  } else {\n    use_half_pixel_centers = half_pixel_centers->second.b();\n  }\n\n  // Compose cost of bilinear interpolation.\n  int64_t ops = 0;\n\n#define EIGEN_COST(X) Eigen::internal::functor_traits<Eigen::internal::X>::Cost\n  const auto sub_cost_float = EIGEN_COST(scalar_difference_op<float>);\n  const auto sub_cost_int = EIGEN_COST(scalar_difference_op<int64_t>);\n  const auto add_cost = EIGEN_COST(scalar_sum_op<float>);\n  const auto mul_cost = EIGEN_COST(scalar_product_op<float>);\n  const auto floor_cost = EIGEN_COST(scalar_floor_op<float>);\n  const auto max_cost = EIGEN_COST(scalar_max_op<int64_t>);\n  const auto min_cost = EIGEN_COST(scalar_min_op<int64_t>);\n  const auto cast_to_int_cost = Eigen::internal::functor_traits<\n      Eigen::internal::scalar_cast_op<float, int64_t>>::Cost;\n  const auto cast_to_float_cost = Eigen::internal::functor_traits<\n      Eigen::internal::scalar_cast_op<int64_t, float>>::Cost;\n  const auto ceil_cost = EIGEN_COST(scalar_ceil_op<float>);\n#undef EIGEN_COST\n\n  // Ops calculated from tensorflow/core/kernels/image/resize_bilinear_op.cc.\n\n  // Op counts taken from resize_bilinear implementation on 07/21/2020.\n  // Computed op counts may become inaccurate if resize_bilinear implementation\n  // changes.\n\n  // resize_bilinear has an optimization where the interpolation weights are\n  // precomputed and cached. Given input tensors of size [B,H1,W1,C] and output\n  // tensors of size [B,H2,W2,C], the last dimension C that needs to be accessed\n  // in the input for interpolation are identical at every point in the output.\n  // These values are cached in the compute_interpolation_weights function. For\n  // a particular y in [0...H2-1], the rows to be accessed in the input are the\n  // same. Likewise, for a particular x in [0...H2-1], the columns to be accsed\n  // are the same. So the precomputation only needs to be done for H2 + W2\n  // values.\n  const auto output_shape = MaybeGetMinimumShape(\n      op_context.op_info.outputs(0).shape(), 4, &found_unknown_shapes);\n  // Assume H is dim 1 and W is dim 2 to match logic in resize_bilinear, which\n  // also makes this assumption.\n  const int64_t output_height = output_shape.dim(1).size();\n  const int64_t output_width = output_shape.dim(2).size();\n  // Add the ops done outside of the scaler function in\n  // compute_interpolation_weights.\n  int64_t interp_weight_cost = floor_cost + max_cost + min_cost +\n                               sub_cost_float + sub_cost_int + ceil_cost +\n                               cast_to_int_cost * 2;\n  // There are two options for computing the weight of each pixel in the\n  // interpolation. Algorithm can use pixel centers, or corners, for the\n  // weight. Ops depend on the scaler function passed into\n  // compute_interpolation_weights.\n  if (use_half_pixel_centers) {\n    // Ops for HalfPixelScalaer.\n    interp_weight_cost +=\n        add_cost + mul_cost + sub_cost_float + cast_to_float_cost;\n  } else {\n    // Ops for LegacyScaler.\n    interp_weight_cost += cast_to_float_cost + mul_cost;\n  }\n  // Cost for the interpolation is multiplied by (H2 + w2), as mentioned above.\n  ops += interp_weight_cost * (output_height + output_width);\n\n  // Ops for computing the new values, done for every element. Logic is from\n  // compute_lerp in the inner loop of resize_image which consists of:\n  //   const float top = top_left + (top_right - top_left) * x_lerp;\n  //   const float bottom = bottom_left + (bottom_right - bottom_left) * x_lerp;\n  //   return top + (bottom - top) * y_lerp;\n  ops += (add_cost * 3 + sub_cost_float * 3 + mul_cost * 3) * output_elements;\n\n  return PredictDefaultNodeCosts(ops, op_context, &found_unknown_shapes,\n                                 node_costs);\n}\n\nStatus OpLevelCostEstimator::PredictCropAndResize(const OpContext& op_context,\n                                                  NodeCosts* node_costs) const {\n  bool found_unknown_shapes = false;\n\n  const auto method = op_context.op_info.attr().find(\"method\");\n  bool use_bilinear_interp;\n  if (method == op_context.op_info.attr().end() ||\n      method->second.s() == \"bilinear\") {\n    use_bilinear_interp = true;\n  } else if (method->second.s() == \"nearest\") {\n    use_bilinear_interp = false;\n  } else {\n    LOG(WARNING) << \"method attr in CropAndResize invalid; expected bilinear \"\n                    \"or nearest.\";\n    return PredictCostOfAnUnknownOp(op_context, node_costs);\n  }\n\n  const int64_t num_boxes = op_context.op_info.inputs(1).shape().dim(0).size();\n  const auto crop_shape = MaybeGetMinimumShape(\n      op_context.op_info.outputs(0).shape(), 4, &found_unknown_shapes);\n  const int64_t crop_height = crop_shape.dim(1).size();\n  const int64_t crop_width = crop_shape.dim(2).size();\n  const int64_t output_elements = CalculateTensorElementCount(\n      op_context.op_info.outputs(0), &found_unknown_shapes);\n\n#define EIGEN_COST(X) Eigen::internal::functor_traits<Eigen::internal::X>::Cost\n  const auto sub_cost = EIGEN_COST(scalar_difference_op<float>);\n  const auto add_cost = EIGEN_COST(scalar_sum_op<float>);\n  const auto mul_cost = EIGEN_COST(scalar_product_op<float>);\n  auto div_cost = EIGEN_COST(scalar_div_cost<float>);\n  const auto floor_cost = EIGEN_COST(scalar_floor_op<float>);\n  const auto ceil_cost = EIGEN_COST(scalar_ceil_op<float>);\n  auto round_cost = EIGEN_COST(scalar_round_op<float>);\n  const auto cast_to_float_cost = Eigen::internal::functor_traits<\n      Eigen::internal::scalar_cast_op<int64_t, float>>::Cost;\n#undef EIGEN_COST\n\n  // Computing ops following\n  // tensorflow/core/kernels/image/crop_and_resize_op.cc at 08/25/2020. Op\n  // calculation differs from rough estimate in implementation, as it separates\n  // out cost per box from cost per pixel and cost per element.\n\n  // Since crop arguments are user controlled, check for overflow.\n  int64_t crop_area = MultiplyWithoutOverflow(crop_height, crop_width);\n  if (crop_area < 0)\n    return errors::InvalidArgument(\"Cannot estimate cost, multiplying \",\n                                   crop_height, \" with \", crop_width,\n                                   \" would overflow\");\n  int64_t crop_volume = MultiplyWithoutOverflow(crop_area, num_boxes);\n  if (crop_volume < 0)\n    return errors::InvalidArgument(\"Cannot estimate cost, multiplying \",\n                                   crop_area, \" with \", num_boxes,\n                                   \" would overflow\");\n  int64_t crop_depth = MultiplyWithoutOverflow(crop_height, num_boxes);\n  if (crop_depth < 0)\n    return errors::InvalidArgument(\"Cannot estimate cost, multiplying \",\n                                   crop_height, \" with \", num_boxes,\n                                   \" would overflow\");\n\n  // Ops for variables height_scale and width_scale.\n  int64_t ops = (sub_cost * 6 + mul_cost * 2 + div_cost * 2) * num_boxes;\n  // Ops for variable in_y.\n  ops += (mul_cost * 2 + sub_cost + add_cost) * crop_depth;\n  // Ops for variable in_x (same computation across both branches).\n  ops += (mul_cost * 2 + sub_cost + add_cost) * crop_volume;\n  // Specify op_cost based on the method.\n  if (use_bilinear_interp) {\n    // Ops for variables top_y_index, bottom_y_index, y_lerp.\n    ops += (floor_cost + ceil_cost + sub_cost) * crop_depth;\n    // Ops for variables left_x, right_x, x_lerp;\n    ops += (floor_cost + ceil_cost + sub_cost) * crop_volume;\n    // Ops for innermost loop across depth.\n    ops +=\n        (cast_to_float_cost * 4 + add_cost * 3 + sub_cost * 3 + mul_cost * 3) *\n        output_elements;\n  } else /* method == \"nearest\" */ {\n    // Ops for variables closest_x_index and closest_y_index.\n    ops += round_cost * 2 * crop_volume;\n    // Ops for innermost loop across depth.\n    ops += cast_to_float_cost * output_elements;\n  }\n  return PredictDefaultNodeCosts(ops, op_context, &found_unknown_shapes,\n                                 node_costs);\n}\n\n}  // end namespace grappler\n}  // end namespace tensorflow"