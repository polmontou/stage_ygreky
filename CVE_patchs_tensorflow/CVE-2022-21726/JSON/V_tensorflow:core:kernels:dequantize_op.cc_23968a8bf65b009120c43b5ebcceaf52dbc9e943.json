"/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/math_ops.cc.\n\n#define EIGEN_USE_THREADS\n\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/type_traits.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/kernels/meta_support.h\"\n#include \"tensorflow/core/kernels/quantization_utils.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/platform/bfloat16.h\"\n\nnamespace {\nenum {\n  QUANTIZE_MODE_MIN_COMBINED,\n  QUANTIZE_MODE_MIN_FIRST,\n  QUANTIZE_MODE_SCALED,\n};\n}  // namespace\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n\ntemplate <typename T>\nT Cast(float v) {\n  return v;\n}\n\ntemplate <>\nbfloat16 Cast<bfloat16>(float v) {\n  return bfloat16(v);\n}\n\ntemplate <typename Device, typename T, typename S>\nclass DequantizeOp : public OpKernel {\n public:\n  explicit DequantizeOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    string mode_string;\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"mode\", &mode_string));\n    OP_REQUIRES(\n        ctx,\n        (ctx->output_type(0) == DT_FLOAT || ctx->output_type(0) == DT_BFLOAT16),\n        errors::InvalidArgument(\"Output type must be bfloat16 or float,\"\n                                \" is '\" +\n                                DataTypeString(ctx->output_type(0)) + \"'\"));\n\n    need_cast_ = true;\n    if (ctx->output_type(0) == DT_FLOAT) {\n      need_cast_ = false;\n      OP_REQUIRES(ctx,\n                  (mode_string == \"MIN_COMBINED\" ||\n                   mode_string == \"MIN_FIRST\" || mode_string == \"SCALED\"),\n                  errors::InvalidArgument(\"Mode string must be 'MIN_COMBINED',\"\n                                          \" 'MIN_FIRST', or 'SCALED', is '\" +\n                                          mode_string + \"'\"));\n    } else {\n      OP_REQUIRES(\n          ctx, (mode_string == \"MIN_COMBINED\"),\n          errors::InvalidArgument(\"When output type is bfloat16, Mode\"\n                                  \" string must be 'MIN_COMBINED', is '\" +\n                                  mode_string + \"'\"));\n    }\n\n    if (mode_string == \"MIN_COMBINED\") {\n      mode_ = QUANTIZE_MODE_MIN_COMBINED;\n    } else if (mode_string == \"MIN_FIRST\") {\n      mode_ = QUANTIZE_MODE_MIN_FIRST;\n    } else if (mode_string == \"SCALED\") {\n      mode_ = QUANTIZE_MODE_SCALED;\n    }\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"narrow_range\", &narrow_range_));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"axis\", &axis_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(0);\n    const Tensor& input_min_tensor = ctx->input(1);\n    const Tensor& input_max_tensor = ctx->input(2);\n\n    int num_slices = 1;\n    if (axis_ > -1) {\n      num_slices = input.dim_size(axis_);\n    }\n    OP_REQUIRES(ctx, input_min_tensor.NumElements() == num_slices,\n                errors::InvalidArgument(\n                    \"input_min_tensor must have as many elements as input on \"\n                    \"the dequantization axis (\",\n                    axis_, \"), got \", input_min_tensor.NumElements(),\n                    \", expected \", num_slices));\n    OP_REQUIRES(ctx, input_max_tensor.NumElements() == num_slices,\n                errors::InvalidArgument(\n                    \"input_max_tensor must have as many elements as input on \"\n                    \"the dequantization axis (\",\n                    axis_, \"), got \", input_max_tensor.NumElements(),\n                    \", expected \", num_slices));\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));\n    Tensor float_output =\n        need_cast_ ? tensorflow::Tensor(DT_FLOAT, input.shape()) : *output;\n    if (num_slices == 1) {\n      const float min_range = input_min_tensor.flat<float>()(0);\n      const float max_range = input_max_tensor.flat<float>()(0);\n      DequantizeTensor(ctx, input, min_range, max_range, &float_output);\n    } else {\n      OP_REQUIRES(ctx, mode_ != QUANTIZE_MODE_MIN_FIRST,\n                  errors::Unimplemented(\"MIN_FIRST mode is not implemented for \"\n                                        \"Dequantize with axis != -1.\"));\n\n      int64_t pre_dim = 1, post_dim = 1;\n      for (int i = 0; i < axis_; ++i) {\n        pre_dim *= float_output.dim_size(i);\n      }\n      for (int i = axis_ + 1; i < float_output.dims(); ++i) {\n        post_dim *= float_output.dim_size(i);\n      }\n      auto input_tensor = input.template bit_casted_shaped<T, 3>(\n          {pre_dim, num_slices, post_dim});\n      auto output_tensor =\n          float_output.flat_inner_outer_dims<float, 3>(axis_ - 1);\n      auto min_ranges = input_min_tensor.vec<float>();\n      auto max_ranges = input_max_tensor.vec<float>();\n      for (int i = 0; i < num_slices; ++i) {\n        DequantizeSlice(ctx->eigen_device<Device>(), ctx,\n                        input_tensor.template chip<1>(i), min_ranges(i),\n                        max_ranges(i), output_tensor.template chip<1>(i));\n      }\n    }\n    if (need_cast_) {\n      S* out_ptr = output->flat<S>().data();\n      float* in_ptr = float_output.flat<float>().data();\n      for (int64_t i = 0; i < float_output.NumElements(); ++i) {\n        out_ptr[i] = static_cast<S>(in_ptr[i]);\n      }\n    }\n  }\n\n  void DequantizeTensor(OpKernelContext* ctx, const Tensor& input,\n                        const float min_range, const float max_range,\n                        Tensor* output) {\n    const float half_range =\n        !std::is_signed<T>::value\n            ? 0.0f\n            : (static_cast<float>(std::numeric_limits<T>::max()) -\n               std::numeric_limits<T>::min() + 1) /\n                  2.0f;\n\n    if (mode_ == QUANTIZE_MODE_MIN_COMBINED) {\n      const float scale_factor =\n          (max_range - min_range) /\n          (static_cast<float>(std::numeric_limits<T>::max()) -\n           std::numeric_limits<T>::min());\n\n      const auto& input_tensor = input.flat<T>();\n      output->flat<float>() =\n          ((input_tensor.template cast<float>() + half_range) * scale_factor) +\n          min_range;\n\n    } else if (mode_ == QUANTIZE_MODE_MIN_FIRST) {\n      if (meta::IsSupportedAndEnabled() && std::is_same<T, quint8>()) {\n        auto input_ui8_array = input.flat<quint8>();\n        meta::Dequantize(ctx, input_ui8_array.data(), input_ui8_array.size(),\n                         min_range, max_range, output->flat<float>().data());\n      } else {\n        QuantizedTensorToFloatInPlaceUsingEigen<T>(\n            ctx->template eigen_device<Device>(), input, min_range, max_range,\n            output);\n      }\n    } else if (mode_ == QUANTIZE_MODE_SCALED) {\n      const int min_output_value =\n          std::numeric_limits<T>::min() + (narrow_range_ ? 1 : 0);\n      const float scale_factor =\n          std::numeric_limits<T>::min() == 0\n              ? (max_range / std::numeric_limits<T>::max())\n              : std::max(min_range / min_output_value,\n                         max_range / std::numeric_limits<T>::max());\n      const auto& input_tensor = input.flat<T>();\n      output->flat<float>() =\n          input_tensor.template cast<int>().template cast<float>() *\n          scale_factor;\n    }\n  }\n\n  template <typename ConstVec, typename Vec>\n  void DequantizeSlice(const Device& d, OpKernelContext* ctx,\n                       const ConstVec& input, float min_range, float max_range,\n                       Vec output) {\n    // TODO(pauldonnelly): Factor out the similar calculations in quantize,\n    //   dequantize and quantize_and_dequantize ops.\n    const float half_range =\n        !std::is_signed<T>::value\n            ? 0.0f\n            : (static_cast<float>(std::numeric_limits<T>::max()) -\n               std::numeric_limits<T>::min() + 1) /\n                  2.0f;\n\n    if (mode_ == QUANTIZE_MODE_MIN_COMBINED) {\n      const float scale_factor =\n          (max_range - min_range) /\n          (static_cast<float>(std::numeric_limits<T>::max()) -\n           std::numeric_limits<T>::min());\n\n      output.device(d) =\n          ((input.template cast<float>() + half_range) * scale_factor) +\n          min_range;\n    } else if (mode_ == QUANTIZE_MODE_SCALED) {\n      const int min_output_value =\n          std::numeric_limits<T>::min() + (narrow_range_ ? 1 : 0);\n      const float scale_factor =\n          std::numeric_limits<T>::min() == 0\n              ? (max_range / std::numeric_limits<T>::max())\n              : std::max(min_range / min_output_value,\n                         max_range / std::numeric_limits<T>::max());\n      output.device(d) = input.template cast<float>() * scale_factor;\n    }\n  }\n\n private:\n  int mode_;\n  int axis_;\n  bool narrow_range_;\n  bool need_cast_;\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<quint8>(\"T\")\n                            .TypeConstraint<float>(\"dtype\"),\n                        DequantizeOp<CPUDevice, quint8, float>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<qint8>(\"T\")\n                            .TypeConstraint<float>(\"dtype\"),\n                        DequantizeOp<CPUDevice, qint8, float>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<quint16>(\"T\")\n                            .TypeConstraint<float>(\"dtype\"),\n                        DequantizeOp<CPUDevice, quint16, float>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<qint16>(\"T\")\n                            .TypeConstraint<float>(\"dtype\"),\n                        DequantizeOp<CPUDevice, qint16, float>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<qint32>(\"T\")\n                            .TypeConstraint<float>(\"dtype\"),\n                        DequantizeOp<CPUDevice, qint32, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<quint8>(\"T\")\n                            .TypeConstraint<bfloat16>(\"dtype\"),\n                        DequantizeOp<CPUDevice, quint8, bfloat16>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<qint8>(\"T\")\n                            .TypeConstraint<bfloat16>(\"dtype\"),\n                        DequantizeOp<CPUDevice, qint8, bfloat16>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<quint16>(\"T\")\n                            .TypeConstraint<bfloat16>(\"dtype\"),\n                        DequantizeOp<CPUDevice, quint16, bfloat16>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<qint16>(\"T\")\n                            .TypeConstraint<bfloat16>(\"dtype\"),\n                        DequantizeOp<CPUDevice, qint16, bfloat16>);\nREGISTER_KERNEL_BUILDER(Name(\"Dequantize\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<qint32>(\"T\")\n                            .TypeConstraint<bfloat16>(\"dtype\"),\n                        DequantizeOp<CPUDevice, qint32, bfloat16>);\n}  // namespace tensorflow"