"# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for array_ops.\"\"\"\nimport re\nimport time\nimport unittest\n\nfrom absl.testing import parameterized\nimport numpy as np\n\nfrom tensorflow.python.client import session\nfrom tensorflow.python.eager import backprop\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import config\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_spec\nfrom tensorflow.python.framework import test_ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gen_array_ops\nfrom tensorflow.python.ops import gradient_checker_v2\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import list_ops\nfrom tensorflow.python.ops import map_fn\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import resource_variable_ops\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.ops.ragged.ragged_tensor import RaggedTensor\nfrom tensorflow.python.platform import test as test_lib\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass BatchMatrixTransposeTest(test_util.TensorFlowTestCase):\n\n  def testNonBatchMatrix(self):\n    matrix = [[1, 2, 3], [4, 5, 6]]  # Shape (2, 3)\n    expected_transposed = [[1, 4], [2, 5], [3, 6]]  # Shape (3, 2)\n    transposed = array_ops.matrix_transpose(matrix)\n    self.assertEqual((3, 2), transposed.get_shape())\n    self.assertAllEqual(expected_transposed, transposed)\n\n  def testConjugate(self):\n    m = [[1 + 1j, 2 + 2j, 3 + 3j], [4 + 4j, 5 + 5j, 6 + 6j]]\n    expected_transposed = [[1 - 1j, 4 - 4j], [2 - 2j, 5 - 5j], [3 - 3j, 6 - 6j]]\n    matrix = ops.convert_to_tensor(m)\n    transposed = array_ops.matrix_transpose(matrix, conjugate=True)\n    self.assertEqual((3, 2), transposed.get_shape())\n    self.assertAllEqual(expected_transposed, transposed)\n\n  def testBatchMatrix(self):\n    matrix_0 = [[1, 2, 3], [4, 5, 6]]\n    matrix_0_t = [[1, 4], [2, 5], [3, 6]]\n    matrix_1 = [[11, 22, 33], [44, 55, 66]]\n    matrix_1_t = [[11, 44], [22, 55], [33, 66]]\n    batch_matrix = [matrix_0, matrix_1]  # Shape (2, 2, 3)\n    expected_transposed = [matrix_0_t, matrix_1_t]  # Shape (2, 3, 2)\n    transposed = array_ops.matrix_transpose(batch_matrix)\n    self.assertEqual((2, 3, 2), transposed.get_shape())\n    self.assertAllEqual(expected_transposed, transposed)\n\n  def testNonBatchMatrixDynamicallyDefined(self):\n    # needs explicit `constant` because lists are not automatically\n    # converted to sensors when applying `transpose` below\n    matrix = constant_op.constant([[1, 2, 3], [4, 5, 6]])  # Shape (2, 3)\n    expected_transposed = [[1, 4], [2, 5], [3, 6]]  # Shape (3, 2)\n\n    @def_function.function(input_signature=[\n        tensor_spec.TensorSpec(shape=None, dtype=dtypes.int32)\n    ])\n    def transpose(matrix):\n      self.assertIs(matrix.shape.ndims, None)\n      return array_ops.matrix_transpose(matrix)\n\n    self.assertAllEqual(expected_transposed, transpose(matrix))\n\n  def testBatchMatrixDynamicallyDefined(self):\n    matrix_0 = [[1, 2, 3], [4, 5, 6]]\n    matrix_0_t = [[1, 4], [2, 5], [3, 6]]\n    matrix_1 = [[11, 22, 33], [44, 55, 66]]\n    matrix_1_t = [[11, 44], [22, 55], [33, 66]]\n    # needs explicit `constant` because lists are not automatically\n    # converted to sensors when applying `transpose` below\n    batch_matrix = constant_op.constant([matrix_0, matrix_1])  # Shape (2, 2, 3)\n    expected_transposed = [matrix_0_t, matrix_1_t]  # Shape (2, 3, 2)\n\n    @def_function.function(input_signature=[\n        tensor_spec.TensorSpec(shape=None, dtype=dtypes.int32)\n    ])\n    def transpose(matrix):\n      self.assertIs(matrix.shape.ndims, None)\n      return array_ops.matrix_transpose(matrix)\n\n    self.assertAllEqual(expected_transposed, transpose(batch_matrix))\n\n  def testTensorWithStaticRankLessThanTwoRaisesBecauseNotAMatrix(self):\n    vector = [1, 2, 3]\n    with self.assertRaisesRegex(ValueError, \"should be a \"):\n      array_ops.matrix_transpose(vector)\n\n  def testNarrowMatrixConjugateTranspose(self):\n    for dtype in (dtypes.float32, dtypes.float64):\n      for conjugate in (True, False):\n        with self.subTest(complex_type=dtype, conjugate=conjugate):\n          vector = math_ops.complex(\n              constant_op.constant(0, dtype=dtype),\n              math_ops.range(96, dtype=dtype))\n          column_vector = array_ops.expand_dims(vector, axis=-1)\n          row_vector = array_ops.expand_dims(vector, axis=0)\n          narrow_matrix = array_ops.tile(column_vector, [1, 2])  # [96, 2]\n          expected_transposed = array_ops.tile(row_vector, [2, 1])  # [2, 96]\n          if conjugate:\n            expected_transposed = -expected_transposed\n\n          transposed = array_ops.matrix_transpose(\n              narrow_matrix, conjugate=conjugate)\n\n          self.assertEqual((2, 96), transposed.get_shape())\n          self.assertAllEqual(expected_transposed, transposed)\n\n\nclass BooleanMaskTest(test_util.TensorFlowTestCase):\n\n  def setUp(self):\n    self.rng = np.random.RandomState(42)\n\n  def CheckVersusNumpy(self, ndims_mask, arr_shape, make_mask=None, axis=None):\n    \"\"\"Check equivalence between boolean_mask and numpy masking.\"\"\"\n    if make_mask is None:\n      make_mask = lambda shape: self.rng.randint(0, 2, size=shape).astype(bool)\n    arr = np.random.rand(*arr_shape)\n    mask = make_mask(arr_shape[:ndims_mask])\n    if axis is not None:\n      mask = make_mask(arr_shape[axis:ndims_mask + axis])\n    if axis is None or axis == 0:\n      masked_arr = arr[mask]\n    elif axis == 1:\n      masked_arr = arr[:, mask]\n    elif axis == 2:\n      masked_arr = arr[:, :, mask]\n    masked_tensor = array_ops.boolean_mask(arr, mask, axis=axis)\n\n    # Leading dimension size of masked_tensor is always unknown until runtime\n    # since we don't how many elements will be kept.\n    leading = 1 if axis is None else axis + 1\n    self.assertAllEqual(masked_tensor.get_shape()[leading:],\n                        masked_arr.shape[leading:])\n\n    self.assertAllClose(masked_arr, masked_tensor)\n\n  def testMaskDim1ArrDim2Axis1(self):\n    ndims_mask = 1\n    for arr_shape in [(1, 1), (2, 2), (2, 5)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape, axis=1)\n\n  def testMaskDim2ArrDim2Axis1(self):\n    ndims_mask = 2\n    for arr_shape in [(1, 1), (2, 2), (2, 5)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape, axis=1)\n\n  def testMaskDim1ArrDim1(self):\n    ndims_mask = 1\n    for arr_shape in [(1,), (2,), (3,), (10,)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape)\n\n  def testMaskDim1ArrDim2(self):\n    ndims_mask = 1\n    for arr_shape in [(1, 1), (2, 2), (2, 5)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape)\n\n  def testMaskDim2ArrDim2(self):\n    ndims_mask = 2\n    for arr_shape in [(1, 1), (2, 2), (2, 5)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape)\n\n  def testMaskDim2ArrDim3(self):\n    ndims_mask = 2\n    for arr_shape in [(1, 1, 1), (1, 2, 2), (2, 2, 1)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape)\n\n  def testEmptyInput2D(self):\n    mask = np.array([True, False])\n    arr = np.array([[], []]).astype(np.float32)\n    numpy_result = arr[mask]\n    tf_result = array_ops.boolean_mask(arr, mask)\n    self.assertAllEqual(numpy_result.shape[1:], tf_result.get_shape()[1:])\n    with self.cached_session():\n      self.assertAllClose(numpy_result, tf_result)\n\n  def testEmptyInput1D(self):\n    mask = np.array([]).astype(bool)\n    arr = np.array([]).astype(np.float32)\n    numpy_result = arr[mask]\n    tf_result = array_ops.boolean_mask(arr, mask)\n    self.assertAllEqual(numpy_result.shape[1:], tf_result.get_shape()[1:])\n    with self.cached_session():\n      self.assertAllClose(numpy_result, tf_result)\n\n  def testEmptyOutput(self):\n    make_mask = lambda shape: np.zeros(shape, dtype=bool)\n    for ndims_mask in range(1, 4):\n      for ndims_arr in range(ndims_mask, ndims_mask + 3):\n        for _ in range(3):\n          with self.subTest(ndims_mask=ndims_mask, ndims_arr=ndims_arr, _=_):\n            arr_shape = np.random.randint(1, 5, size=ndims_arr)\n            self.CheckVersusNumpy(ndims_mask, arr_shape, make_mask=make_mask)\n\n  def testWorksWithDimensionsEqualToNoneDuringGraphBuild(self):\n    # The rank of the mask tensor must be specified. This is explained\n    # in the docstring as well.\n    @def_function.function\n    def func(ph_tensor, ph_mask):\n      return array_ops.boolean_mask(ph_tensor, ph_mask)\n\n    f = func.get_concrete_function(\n        tensor_spec.TensorSpec(None, dtypes.int32),\n        tensor_spec.TensorSpec([None], dtypes.bool))\n    arr = np.array([[1, 2], [3, 4]], np.int32)\n    mask = np.array([False, True])\n    masked_tensor = f(arr, mask)\n    self.assertAllEqual(masked_tensor, arr[mask])\n\n  def testMaskDimensionsSetToNoneRaises(self):\n    # The rank of the mask tensor must be specified. This is explained\n    # in the docstring as well.\n    @def_function.function\n    def func(tensor, mask):\n      return array_ops.boolean_mask(tensor, mask)\n\n    with self.assertRaisesRegex(ValueError, \"dimensions must be specified\"):\n      _ = func.get_concrete_function(\n          tensor_spec.TensorSpec([None, 2], dtypes.int32),\n          tensor_spec.TensorSpec(None, dtypes.bool))\n\n  def testMaskHasMoreDimsThanTensorRaises(self):\n    mask = [[True, True], [False, False]]\n    tensor = [1, 2, 3, 4]\n    with self.cached_session():\n      with self.assertRaisesRegex(ValueError, \"incompatible\"):\n        self.evaluate(array_ops.boolean_mask(tensor, mask))\n\n  def testMaskIsScalarRaises(self):\n    mask = True\n    tensor = 1\n    with self.cached_session():\n      with self.assertRaisesRegex(ValueError, \"mask.*scalar\"):\n        self.evaluate(array_ops.boolean_mask(tensor, mask))\n\n  def testMaskShapeDifferentThanFirstPartOfTensorShapeRaises(self):\n    mask = [True, True, True]\n    tensor = [[1, 2], [3, 4]]\n    with self.cached_session():\n      with self.assertRaisesRegex(ValueError, \"incompatible\"):\n        self.evaluate(array_ops.boolean_mask(tensor, mask))\n\n  def testStringMask(self):\n    # Reproduces b/111171330, where the optimized boolean_mask graph would\n    # be incorrectly placed on GPU.\n    config.set_optimizer_experimental_options({\"shape_optimization\": True})\n\n    @def_function.function\n    def func(tile_input):\n      string_tensor = array_ops.tile([[\"hello\"]], tile_input)\n      bool_tensor = array_ops.tile([[True]], tile_input)\n      masked_tensor = array_ops.boolean_mask(string_tensor, bool_tensor)\n      return masked_tensor\n\n    result = func([2, 2])\n    self.assertAllEqual([b\"hello\", b\"hello\", b\"hello\", b\"hello\"], result)\n\n  def testMaskWithAxisTensor(self):\n\n    @def_function.function(autograph=False)\n    def f():\n      return array_ops.boolean_mask([1, 2, 3], [True, False, True],\n                                    axis=constant_op.constant(\n                                        0, dtype=dtypes.int32))\n\n    self.assertAllEqual(self.evaluate(f()), [1, 3])\n\n  def testMaskWithAxisNonConstTensor(self):\n\n    @def_function.function(\n        autograph=False,\n        input_signature=[\n            tensor_spec.TensorSpec(shape=None, dtype=dtypes.int32)\n        ])\n    def f(axis):\n      return array_ops.boolean_mask([1, 2, 3], [True, False, True], axis=axis)\n\n    self.assertAllEqual(\n        self.evaluate(f(constant_op.constant(0, dtype=dtypes.int32))), [1, 3])\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass OperatorShapeTest(test_util.TensorFlowTestCase):\n\n  def testExpandScalar(self):\n    scalar = \"hello\"\n    scalar_expanded = array_ops.expand_dims(scalar, [0])\n    self.assertEqual(scalar_expanded.get_shape(), (1,))\n\n  def testSqueezeScalar(self):\n    scalar = \"hello\"\n    scalar_squeezed = array_ops.squeeze(scalar, ())\n    self.assertEqual(scalar_squeezed.get_shape(), ())\n\n  def testSqueezeMatrix(self):\n    matrix = [[1, 2, 3]]\n    matrix_squeezed = array_ops.squeeze(matrix, [0])\n    self.assertEqual(matrix_squeezed.get_shape(), (3))\n\n    with self.assertRaisesRegex(\n        Exception, \"Can not squeeze dim.1., expected a dimension of 1, got 3\"):\n      matrix_squeezed = array_ops.squeeze(matrix, [1])\n\n  def testSqueezeScalarDim(self):\n    matrix = [[1, 2, 3]]\n    matrix_squeezed = array_ops.squeeze(matrix, 0)\n    self.assertEqual(matrix_squeezed.get_shape(), (3))\n\n  def testExpandDimsWithNonScalarDim(self):\n    with self.assertRaisesRegex(Exception,\n                                \"must be a tensor with a single value\"):\n      array_ops.expand_dims(1, axis=[0, 1])\n\n\n@test_util.with_eager_op_as_function\nclass ReverseV2Test(test_util.TensorFlowTestCase):\n\n  def testReverse0DimAuto(self):\n    x_np = 4\n    for use_gpu in [False, True]:\n      with self.subTest(use_gpu=use_gpu):\n        with self.cached_session(use_gpu=use_gpu):\n          x_tf = self.evaluate(array_ops.reverse_v2(x_np, []))\n          self.assertAllEqual(x_tf, x_np)\n\n  def _reverse1DimAuto(self, np_dtype):\n    x_np = np.array([1, 200, 3, 40, 5], dtype=np_dtype)\n\n    for use_gpu in [False, True]:\n      for axis_dtype in [dtypes.int32, dtypes.int64]:\n        with self.subTest(use_gpu=use_gpu, axis_dtype=axis_dtype):\n          x_tf = self.evaluate(\n              array_ops.reverse_v2(x_np,\n                                   constant_op.constant([0], dtype=axis_dtype)))\n          self.assertAllEqual(x_tf, np.asarray(x_np)[::-1])\n\n  def _reverse2DimAuto(self, np_dtype):\n    x_np = np.array([[1, 200, 3], [4, 5, 60]], dtype=np_dtype)\n\n    for reverse_f in [array_ops.reverse_v2, array_ops.reverse]:\n      for use_gpu in [False, True]:\n        for axis_dtype in [dtypes.int32, dtypes.int64]:\n          with self.subTest(\n              reverse_f=reverse_f, use_gpu=use_gpu, axis_dtype=axis_dtype):\n            x_tf_1 = self.evaluate(\n                reverse_f(x_np, constant_op.constant([0], dtype=axis_dtype)))\n            x_tf_2 = self.evaluate(\n                reverse_f(x_np, constant_op.constant([-2], dtype=axis_dtype)))\n            x_tf_3 = self.evaluate(\n                reverse_f(x_np, constant_op.constant([1], dtype=axis_dtype)))\n            x_tf_4 = self.evaluate(\n                reverse_f(x_np, constant_op.constant([-1], dtype=axis_dtype)))\n            x_tf_5 = self.evaluate(\n                reverse_f(x_np, constant_op.constant([1, 0], dtype=axis_dtype)))\n            self.assertAllEqual(x_tf_1, np.asarray(x_np)[::-1, :])\n            self.assertAllEqual(x_tf_2, np.asarray(x_np)[::-1, :])\n            self.assertAllEqual(x_tf_3, np.asarray(x_np)[:, ::-1])\n            self.assertAllEqual(x_tf_4, np.asarray(x_np)[:, ::-1])\n            self.assertAllEqual(x_tf_5, np.asarray(x_np)[::-1, ::-1])\n\n  # This test covers the axis validation in the shape function\n  # (no eval())\n  def testInvalidAxis(self):\n    x_np = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32)\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"is out of.* range\"):\n      array_ops.reverse_v2(x_np, [-30])\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"is out of.* range\"):\n      array_ops.reverse_v2(x_np, [2])\n    with self.assertRaisesRegex(\n        (ValueError, errors.InvalidArgumentError),\n        r\"axis 0 specified more than once|axis 0 was repeated\"):\n      array_ops.reverse_v2(x_np, [0, -2])\n\n  # This is the version of reverse that uses axis indices rather than\n  # bool tensors\n  # TODO(b/32254538): Change this test to use array_ops.reverse\n  #\n  # Note: this test passes placeholder as constant axis is validated\n  # in shape function (see testInvalidAxis)\n  def testInvalid(self):\n    x_np = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32)\n\n    @def_function.function\n    def func(ax):\n      return array_ops.reverse_v2(x_np, ax)\n\n    with self.assertRaisesRegex((ValueError, errors_impl.InvalidArgumentError),\n                                \"is out of.*range\"):\n      func([-30])\n    with self.assertRaisesRegex((ValueError, errors_impl.InvalidArgumentError),\n                                \"is out of.*range\"):\n      func([2])\n    with self.assertRaisesRegex(\n        (ValueError, errors_impl.InvalidArgumentError),\n        \"(axis 0 specified more than once|canonicalized axis 0 was repeated.)\"):\n      func([0, -2])\n\n  def testReverse1DimAuto(self):\n    for dtype in [\n        np.uint8, np.int8, np.uint16, np.int16, np.uint32, np.int32, np.uint64,\n        np.int64, np.bool_, np.float16, np.float32, np.float64, np.complex64,\n        np.complex128,\n        np.array(b\"\").dtype.type\n    ]:\n      self._reverse1DimAuto(dtype)\n\n  def testReverse2DimAuto(self):\n    for dtype in [\n        np.uint8, np.int8, np.uint16, np.int16, np.uint32, np.int32, np.uint64,\n        np.int64, np.bool_, np.float16, np.float32, np.float64, np.complex64,\n        np.complex128,\n        np.array(b\"\").dtype.type\n    ]:\n      self._reverse2DimAuto(dtype)\n\n  def testReverseRowsOf3Channels(self):\n    \"\"\"Tests optimized code for reversing rows with last dim size = 3.\"\"\"\n    for reverse_f in [array_ops.reverse_v2, array_ops.reverse]:\n      for outer_size in (1, 2):\n        for middle_size in list(range(50)) + [100000]:\n          with self.subTest(\n              reverse_f=reverse_f,\n              outer_size=outer_size,\n              middle_size=middle_size,\n              use_gpu=True):\n            x_np = np.reshape(\n                np.arange(outer_size * middle_size * 3, dtype=np.float32),\n                newshape=(outer_size, middle_size, 3))\n            x_tf = self.evaluate(reverse_f(x_np, [1]))\n            np_answer = x_np[:, ::-1, :]\n            self.assertAllEqual(x_tf, np_answer)\n\n  def testReverseRowsOf4Channels(self):\n    for reverse_f in [array_ops.reverse_v2, array_ops.reverse]:\n      for outer_size in (1, 2):\n        for middle_size in list(range(50)) + [100000]:\n          with self.subTest(\n              reverse_f=reverse_f,\n              outer_size=outer_size,\n              middle_size=middle_size,\n              use_gpu=True):\n            x_np = np.reshape(\n                np.arange(outer_size * middle_size * 4, dtype=np.float32),\n                newshape=(outer_size, middle_size, 4))\n            x_tf = self.evaluate(reverse_f(x_np, [1]))\n            np_answer = x_np[:, ::-1, :]\n            self.assertAllEqual(x_tf, np_answer)\n\n  def testReverseColumnsOf3Channels(self):\n    for reverse_f in [array_ops.reverse_v2, array_ops.reverse]:\n      for outer_size in list(range(50)) + [100000]:\n        for middle_size in (1, 2):\n          with self.subTest(\n              reverse_f=reverse_f,\n              outer_size=outer_size,\n              middle_size=middle_size,\n              use_gpu=True):\n            x_np = np.reshape(\n                np.arange(outer_size * middle_size * 3, dtype=np.float32),\n                newshape=(outer_size, middle_size, 3))\n            x_tf = self.evaluate(reverse_f(x_np, [0]))\n            np_answer = x_np[::-1, :, :]\n            self.assertAllEqual(x_tf, np_answer)\n\n  def testReverseInvalidShape(self):\n    x = np.ndarray(shape=[0, 1, 1])\n    v = array_ops.reverse_v2(x, axis=[1])\n    self.assertAllEqual(self.evaluate(v), v)\n\n\nclass MeshgridTest(test_util.TensorFlowTestCase):\n\n  def _compareDiff(self, x, y, use_gpu):\n    for index in (\"ij\", \"xy\"):\n      numpy_out = np.meshgrid(x, y, indexing=index)\n      tf_out = array_ops.meshgrid(x, y, indexing=index)\n      with self.cached_session(use_gpu=use_gpu):\n        for xx, yy in zip(numpy_out, tf_out):\n          self.assertAllEqual(xx, yy)\n\n  def _compareDiffType(self, n, np_dtype, use_gpu):\n    inputs = []\n    for index in (\"ij\", \"xy\"):\n      for _ in range(n):\n        x = np.linspace(-10, 10, 5).astype(np_dtype)\n        if np_dtype in (np.complex64, np.complex128):\n          x += 1j\n        inputs.append(x)\n      numpy_out = np.meshgrid(*inputs, indexing=index)\n      with test_util.device(use_gpu=use_gpu):\n        tf_out = array_ops.meshgrid(*inputs, indexing=index)\n        for x_np, x_tf in zip(numpy_out, tf_out):\n          self.assertAllEqual(x_np, x_tf)\n\n  def testCompare(self):\n    for t in (np.float16, np.float32, np.float64, np.int32, np.int64,\n              np.complex64, np.complex128):\n      with self.subTest(t=t):\n        self._compareDiffType(2, t, False)\n        self._compareDiffType(3, t, False)\n\n        x = [1, 2, 3]\n        y = [4, 5]\n\n        a = [[1, 1], [1, 1]]\n\n        self._compareDiff(x, y, False)\n        self._compareDiff(x, a, False)\n\n\nclass StridedSliceChecker(object):\n  \"\"\"Check a given tensor against the numpy result.\"\"\"\n\n  REF_TENSOR = np.arange(1, 19, dtype=np.float32).reshape(3, 2, 3)\n  REF_TENSOR_ALIGNED = np.arange(1, 97, dtype=np.float32).reshape(3, 4, 8)\n\n  def __init__(self, test, x, tensor_type=dtypes.int32, check_type_infer=True):\n    self.x_np = np.array(x).astype(tensor_type.as_numpy_dtype)\n    if tensor_type.is_bool:\n      self.x_np = np.array(x % 3).astype(np.bool_)\n    # Give the value a non-zero imaginary component for complex types.\n    if tensor_type.is_complex:\n      self.x_np -= 1j * self.x_np\n    self.test = test\n    self.x = constant_op.constant(self.x_np, dtype=tensor_type)\n    self.check_type_infer = check_type_infer\n\n  def __getitem__(self, spec):\n    op = self.x.__getitem__(spec)\n\n    def eval_if_tensor(x):\n      try:\n        return self.test.evaluate(x)\n      except (AttributeError, TypeError, ValueError):\n        return x\n\n    if isinstance(spec, bool) or \\\n      (isinstance(spec, ops.Tensor) and spec.dtype == dtypes.bool) or \\\n      (isinstance(spec, np.ndarray) and spec.dtype == bool) or \\\n      (isinstance(spec, (list, tuple)) and np.asarray(spec).dtype == bool):\n      tensor = self.test.evaluate(op)\n      np_spec = eval_if_tensor(spec)\n      self.test.assertAllEqual(self.x_np[np_spec], tensor)\n      return tensor\n\n    if not isinstance(spec, (list, tuple)):\n      spec = [spec]\n\n    tensor = self.test.evaluate(op)\n\n    # Make a numpy spec that pre-evals the tensors\n    np_specs = []\n\n    for s in spec:\n      if isinstance(s, slice):\n        start = eval_if_tensor(s.start)\n        stop = eval_if_tensor(s.stop)\n        step = eval_if_tensor(s.step)\n        np_specs.append(slice(start, stop, step))\n      else:\n        np_specs.append(eval_if_tensor(s))\n\n    self.test.assertAllEqual(self.x_np[tuple(np_specs)], tensor)\n    if self.check_type_infer:\n      self.test.assertAllEqual(tensor.shape, op.get_shape())\n    return tensor\n\n\nSTRIDED_SLICE_TYPES = [\n    dtypes.int32, dtypes.int64, dtypes.int16, dtypes.int8, dtypes.uint8,\n    dtypes.float32, dtypes.float64, dtypes.complex64, dtypes.complex128,\n    dtypes.bool\n]\n\n\nclass StridedSliceTest(test_util.TensorFlowTestCase):\n  \"\"\"Test the strided slice operation with variants of slices.\"\"\"\n\n  def test_basic_slice(self):\n    for tensor_type in STRIDED_SLICE_TYPES:\n      with self.subTest(tensor_type=tensor_type, use_gpu=True):\n        checker = StridedSliceChecker(\n            self, StridedSliceChecker.REF_TENSOR, tensor_type=tensor_type)\n        _ = checker[:, :, :]\n        # Various ways of representing identity slice\n        _ = checker[:, :, :]\n        _ = checker[::, ::, ::]\n        _ = checker[::1, ::1, ::1]\n        # Not zero slice\n        _ = checker[::1, ::5, ::2]\n        # Reverse in each dimension independently\n        _ = checker[::-1, :, :]\n        _ = checker[:, ::-1, :]\n        _ = checker[:, :, ::-1]\n        ## negative index tests i.e. n-2 in first component\n        _ = checker[-2::-1, :, ::1]\n        # negative index tests i.e. n-2 in first component, non-unit stride\n        _ = checker[-2::-1, :, ::2]\n\n        # Check rank-0 examples\n        checker2 = StridedSliceChecker(self, 5, tensor_type=tensor_type)\n        _ = checker2[None]\n        _ = checker2[...]\n        _ = checker2[tuple()]\n\n  def testInt64GPU(self):\n    if not test_util.is_gpu_available():\n      self.skipTest(\"No GPU available\")\n\n    with test_util.force_gpu():\n      x = constant_op.constant([1., 2., 3.])\n      begin = constant_op.constant([2], dtype=dtypes.int64)\n      end = constant_op.constant([3], dtype=dtypes.int64)\n      strides = constant_op.constant([1], dtype=dtypes.int64)\n      s = array_ops.strided_slice(x, begin, end, strides)\n      self.assertAllEqual([3.], self.evaluate(s))\n\n  @test_util.assert_no_new_pyobjects_executing_eagerly\n  @test_util.assert_no_garbage_created\n  def testTensorSliceEagerMemory(self):\n    with context.eager_mode():\n      inputs = constant_op.constant([[[1], [2], [3], [4]]],\n                                    dtype=dtypes.float32)\n      # Tests that slicing an EagerTensor doesn't leak memory\n      inputs[0]  # pylint: disable=pointless-statement\n\n  @test_util.assert_no_new_pyobjects_executing_eagerly\n  @test_util.assert_no_garbage_created\n  def testVariableSliceEagerMemory(self):\n    with context.eager_mode():\n      v = variables.Variable([1., 2.])\n      v[0]  # pylint: disable=pointless-statement\n\n  def testDegenerateSlices(self):\n    with test_util.device(use_gpu=True):\n      checker = StridedSliceChecker(self, StridedSliceChecker.REF_TENSOR)\n      # degenerate by offering a forward interval with a negative stride\n      _ = checker[0:-1:-1, :, :]\n      # degenerate with a reverse interval with a positive stride\n      _ = checker[-1:0, :, :]\n      # empty interval in every dimension\n      _ = checker[-1:0, 2:2, 2:3:-1]\n      # empty first dimension only (used to break for aligned tensors).\n      checker = StridedSliceChecker(self,\n                                    StridedSliceChecker.REF_TENSOR_ALIGNED)\n      _ = checker[1:0]\n\n  def testSliceWithUndefinedDimension(self):\n    t = constant_op.constant([1, 2, 3])\n    d = tensor_shape.Dimension(None)\n    self.assertAllEqual(t[d:d:d], t)\n\n  def testEllipsis(self):\n    with test_util.device(use_gpu=True):\n      raw = [[[[[1, 2], [3, 4], [5, 6]]], [[[7, 8], [9, 10], [11, 12]]]]]\n      checker = StridedSliceChecker(self, raw)\n\n      _ = checker[0:]\n      # implicit ellipsis\n      _ = checker[0:, ...]\n      # ellipsis alone\n      _ = checker[...]\n      # ellipsis at end\n      _ = checker[0:1, ...]\n      # ellipsis at begin\n      _ = checker[..., 0:1]\n      # ellipsis at middle\n      _ = checker[0:1, ..., 0:1]\n      # multiple ellipses not allowed\n      with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                  \"Multiple ellipses\"):\n        _ = checker[..., :, ...].eval()\n\n  def testShrink(self):\n    with test_util.device(use_gpu=True):\n      raw = [[[[[1, 2, 4, 5], [5, 6, 7, 8], [9, 10, 11, 12]]],\n              [[[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]]]]\n      checker = StridedSliceChecker(self, raw)\n      _ = checker[:, :, :, :, 3]\n      _ = checker[..., 3]\n      _ = checker[:, 0]\n      _ = checker[:, :, 0]\n\n  def testBothNewAxisAndShrink(self):\n    with test_util.device(use_gpu=True):\n\n      @def_function.function\n      def func(inp):\n        return inp[array_ops.newaxis, :, 0]\n\n      f = func.get_concrete_function(\n          tensor_spec.TensorSpec([2, 2], dtypes.int16))\n\n      # TODO(b/190416665): Allow the constant to be eagerly copied/created on\n      # the GPU.\n      with ops.device(\"CPU\"):\n        ones = constant_op.constant([[1, 1], [1, 1]], dtypes.int16)\n      self.assertAllEqual([[1, 1]], self.evaluate(f(ones)))\n\n  def testTensorIndexing(self):\n    with test_util.device(use_gpu=True):\n      raw = [[[[[1, 2, 4, 5], [5, 6, 7, 8], [9, 10, 11, 12]]],\n              [[[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]]]]\n      checker = StridedSliceChecker(self, raw, check_type_infer=False)\n      bar = constant_op.constant(2)\n      bar2 = constant_op.constant(3)\n      _ = checker[..., bar:bar2]\n      _ = checker[..., bar]\n      _ = checker[..., 3]\n      _ = checker[..., 2**64 // 2**63]  # Test longs in Python 2\n\n  def testTensorIndexingTypeError(self):\n    with self.session():\n      checker = StridedSliceChecker(self, StridedSliceChecker.REF_TENSOR)\n      expected = re.escape(array_ops._SLICE_TYPE_ERROR)\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[\"foo\"]\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[constant_op.constant(\"foo\")]\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[0.0]\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[constant_op.constant(0.0)]\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[constant_op.constant([1, 2, 3])]\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[[2.1, -0.7, 1.5]]\n\n  def testExpand(self):\n    with test_util.device(use_gpu=True):\n      raw = [[[[[1, 2, 4, 5], [5, 6, 7, 8], [9, 10, 11, 12]]],\n              [[[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]]]]\n      checker = StridedSliceChecker(self, raw)\n      # new axis (followed by implicit ellipsis)\n      _ = checker[np.newaxis]\n      # newaxis after ellipsis\n      _ = checker[..., np.newaxis]\n      # newaxis in between ellipsis and explicit range\n      _ = checker[..., np.newaxis, :]\n      _ = checker[:, ..., np.newaxis, :, :]\n      # Reverse final dimension with new axis\n      _ = checker[:, :, np.newaxis, :, 2::-1]\n      # Ellipsis in middle of two newaxis\n      _ = checker[np.newaxis, ..., np.newaxis]\n\n  def testExpandVariable(self):\n    with test_util.device(use_gpu=True):\n      x = variables.Variable(7, dtype=dtypes.int32)\n      self.evaluate(x.initializer)\n      y = self.evaluate(x[None])\n      self.assertEqual(y.shape, (1,))\n      self.assertAllEqual(y, (7,))\n\n  def testOptimizedCases(self):\n    with test_util.device(use_gpu=True):\n      checker = StridedSliceChecker(self,\n                                    StridedSliceChecker.REF_TENSOR_ALIGNED)\n      # Identity\n      _ = checker[:]\n      # Identity\n      _ = checker[...]\n      # Identity\n      _ = checker[np.newaxis, ..., np.newaxis]\n      # First axis slice\n      _ = checker[1:]\n      # First axis slice\n      _ = checker[np.newaxis, 1:]\n\n  def testMasks(self):\n    with test_util.device(use_gpu=True):\n      scalar = np.array(0)\n      # Test tensor type mask\n      checker = StridedSliceChecker(self, StridedSliceChecker.REF_TENSOR)\n      _ = checker[checker.x > 2]\n      _ = checker[checker.x <= 5]\n      _ = checker[ops.convert_to_tensor(scalar)]\n\n      # Test numpy array type mask\n      raw = np.array([[[[[1, 2, 4, 5], [5, 6, 7, 8], [9, 10, 11, 12]]],\n                       [[[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23,\n                                                              24]]]]])\n      checker1 = StridedSliceChecker(self, raw)\n      _ = checker1[raw >= 4]\n      _ = checker1[raw < 19]\n      _ = checker1[scalar]\n\n      # Test boolean and non boolean cases\n      mask = np.array([True, False, True])\n      raw1 = np.array([[1, 2, 4, 5], [5, 6, 7, 8], [9, 10, 11, 12]])\n      checker2 = StridedSliceChecker(self, raw1)\n      _ = checker2[mask]\n      _ = checker2[ops.convert_to_tensor(mask)]\n\n\nclass StridedSliceShapeTest(test_util.TensorFlowTestCase):\n  \"\"\"Test the shape inference of StridedSliceShapes.\"\"\"\n\n  def testUnknown(self):\n    with test_util.device(use_gpu=True):\n\n      @def_function.function\n      def f(x):\n        y = x[...]\n        self.assertAllEqual(y.get_shape().ndims, None)\n\n      _ = f.get_concrete_function(tensor_spec.TensorSpec(None, dtypes.float32))\n\n  def tensorShapeEqual(self, x, y):\n    self.assertTrue(x is not None and y is not None or x is None and y is None)\n    self.assertEqual(x.as_list(), y.as_list())\n\n  def testTensorShapeUncertain(self):\n    with test_util.device(use_gpu=True):\n\n      @def_function.function\n      def f1(x):\n        y = x[3:5]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([2, None, 7]))\n\n      _ = f1.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f2(x):\n        y = x[3:5, :, 4]\n        self.tensorShapeEqual(y.get_shape(), tensor_shape.TensorShape([2,\n                                                                       None]))\n\n      _ = f2.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f3(x):\n        y = x[3:5, 3:4, 4]\n        self.tensorShapeEqual(y.get_shape(), tensor_shape.TensorShape([2,\n                                                                       None]))\n\n      _ = f3.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f4(x):\n        y = x[3:5, :, 5:10]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([2, None, 2]))\n\n      _ = f4.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f5(x):\n        y = x[3:5, :, 50:3]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([2, None, 0]))\n\n      _ = f5.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f6(x):\n        y = x[3:5, :, array_ops.newaxis, 50:3,]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([2, None, 1, 0]))\n\n      _ = f6.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f7(x):\n        y = x[1:5:2, :, array_ops.newaxis, 50:3,]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([2, None, 1, 0]))\n\n      _ = f7.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f8(x):\n        y = x[:5:3, :, array_ops.newaxis, 50:3,]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([2, None, 1, 0]))\n\n      _ = f8.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f9(x):\n        y = x[:2:3, :, array_ops.newaxis, 50:3,]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([1, None, 1, 0]))\n\n      _ = f9.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f10(x):\n        y = x[::-1, :, array_ops.newaxis, ::-2]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([5, None, 1, 4]))\n\n      _ = f10.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n  def testTensorValuedIndexShape(self):\n    with self.session():\n\n      @def_function.function\n      def f1(x, y):\n        z = x[y]\n        self.tensorShapeEqual(z.get_shape(), tensor_shape.TensorShape([3, 7]))\n\n      _ = f1.get_concrete_function(\n          tensor_spec.TensorSpec((5, 3, 7)),\n          tensor_spec.TensorSpec((), dtypes.int32))\n\n      @def_function.function\n      def f2(x, y):\n        z = x[y, ::-1]\n        self.tensorShapeEqual(z.get_shape(), tensor_shape.TensorShape([3, 7]))\n\n      _ = f2.get_concrete_function(\n          tensor_spec.TensorSpec((5, 3, 7)),\n          tensor_spec.TensorSpec((), dtypes.int32))\n\n      @def_function.function\n      def f3(x, y):\n        z = x[y, ::-2]\n        self.tensorShapeEqual(z.get_shape(), tensor_shape.TensorShape([2, 7]))\n\n      _ = f3.get_concrete_function(\n          tensor_spec.TensorSpec((5, 3, 7)),\n          tensor_spec.TensorSpec((), dtypes.int32))\n\n      @def_function.function\n      def f4(x, y, s):\n        z = x[y, s:2]\n        self.tensorShapeEqual(z.get_shape(), tensor_shape.TensorShape([None,\n                                                                       7]))\n\n      _ = f4.get_concrete_function(\n          tensor_spec.TensorSpec((5, 3, 7)),\n          tensor_spec.TensorSpec((), dtypes.int32),\n          tensor_spec.TensorSpec((), dtypes.int32))\n\n\nclass GradSliceChecker(object):\n  \"\"\"Tests that we can compute a gradient for var^2.\"\"\"\n\n  def __init__(self, test, var, varnp, use_tape):\n    self.test = test\n    self.var = var\n    self.varnp = varnp\n    self.use_tape = use_tape\n\n  def __getitem__(self, spec):\n    with test_util.AbstractGradientTape(\n        use_tape=self.use_tape, persistent=True) as tape:\n      tape.watch(self.var)\n      val = self.var * self.var\n      slice_var = self.var[spec]\n      slice_val = val[spec]\n\n      # compute analytic 2nd derivative\n      analytic_grad2 = 2 * slice_val\n\n      dy = variables.Variable(\n          array_ops.ones_like(slice_var, dtype=dtypes.float32))\n      assign = dy.assign(slice_var)\n\n      slice_val_grad = tape.gradient(slice_val, self.var, [dy])\n      slice_val_grad2 = tape.gradient(slice_val_grad, dy, [self.var])\n    self.test.evaluate(assign)\n    slice_val_grad_evaled, slice_val_grad2_evaled = (\n        self.test.evaluate([slice_val_grad, slice_val_grad2]))\n    analytic_grad2_evaled = self.test.evaluate(analytic_grad2)\n    self.test.assertAllEqual(slice_val_grad2_evaled, analytic_grad2_evaled)\n\n    # compute analytic gradient for slice\n    np_val_grad = (2 * self.varnp * self.varnp)\n    np_sliceval_grad = np.zeros(self.var.get_shape())\n    if isinstance(spec, ops.Tensor):\n      spec = self.test.evaluate([spec])\n    np_sliceval_grad[spec] = np_val_grad[spec]\n    # verify gradient\n    self.test.assertAllEqual(slice_val_grad_evaled, np_sliceval_grad)\n\n\nclass StridedSliceGradTest(test_util.TensorFlowTestCase,\n                           parameterized.TestCase):\n  \"\"\"Test that strided slice's custom gradient produces correct gradients.\"\"\"\n\n  @parameterized.parameters(set((True, context.executing_eagerly())))\n  def testGradient(self, use_tape):\n    with test_util.device(use_gpu=True):\n      var = variables.Variable(\n          array_ops.reshape(\n              math_ops.range(1, 97, 1, dtype=dtypes.float32), shape=(6, 4, 4)))\n      self.evaluate(var.initializer)\n\n      raw = np.array(range(1, 97, 1)).reshape((6, 4, 4))\n      grad = GradSliceChecker(self, var, raw, use_tape)\n      _ = grad[2:6:2, 1:3, 1:3]\n      _ = grad[3:0:-2, 1:3, 1:3]\n      _ = grad[3:0:-2, array_ops.newaxis, 1:3, 2, array_ops.newaxis]\n      _ = grad[3:0:-2, 1:3, 2]\n      _ = grad[:, -1, :]\n      _ = grad[:, -2, :]\n      with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                  \"out of bounds\"):\n        _ = grad[:, -200, :]\n      with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                  \"out of bounds\"):\n        _ = grad[:, 200, :]\n\n      # Test numpy array type mask\n      _ = grad[raw > 51]\n      # Test tensor type mask\n      _ = grad[ops.convert_to_tensor(raw) <= 76]\n\n  @parameterized.parameters(set((True, context.executing_eagerly())))\n  def testGradientZero(self, use_tape):\n    with test_util.device(use_gpu=True):\n      var = variables.Variable(8.)\n      self.evaluate(var.initializer)\n      grad = GradSliceChecker(self, var, np.array(8), use_tape)\n      _ = grad[tuple()]\n\n  @parameterized.parameters(set((True, context.executing_eagerly())))\n  def testInt64Indices(self, use_tape):\n    with test_util.AbstractGradientTape(use_tape=use_tape) as tape:\n      a = math_ops.range(3, dtype=dtypes.float32)\n      tape.watch(a)\n      index = constant_op.constant(1, dtype=dtypes.int64)\n      b = 2. * a[index]\n    grad = tape.gradient(b, a)\n    self.assertAllEqual(self.evaluate(grad), [0., 2., 0.])\n\n\nclass StridedSliceGradTypeTest(test_util.TensorFlowTestCase):\n  \"\"\"Test varied index types and host located memory.\"\"\"\n\n  def testHostVsDevice(self):\n    var2 = variables.Variable(\n        array_ops.reshape(\n            math_ops.cast(math_ops.range(1, 5, 1), dtypes.float32),\n            shape=(4, 1, 1)))\n    varshape = variables.Variable([6, 4, 4], dtype=dtypes.int32)\n    begin = constant_op.constant([0, 0, 0])\n    end = constant_op.constant([4, 1, 1])\n    strides = constant_op.constant([1, 1, 1])\n    foo = array_ops.strided_slice_grad(varshape, begin, end, strides, var2)\n    self.evaluate(var2.initializer)\n    self.evaluate(varshape.initializer)\n    self.evaluate(foo)\n\n  def testInt64Shape(self):\n    original_dy = array_ops.reshape(\n        math_ops.cast(math_ops.range(1, 5, 1), dtypes.float32), shape=(4, 1, 1))\n    original_shape = constant_op.constant([6, 4, 4], dtype=dtypes.int64)\n    begin = constant_op.constant([0, 0, 0], dtype=dtypes.int64)\n    end = constant_op.constant([4, 1, 1], dtype=dtypes.int64)\n    strides = constant_op.constant([1, 1, 1], dtype=dtypes.int64)\n    dx = array_ops.strided_slice_grad(original_shape, begin, end, strides,\n                                      original_dy)\n    self.evaluate(dx)\n\n  def testMixedIndexTypes(self):\n    original_dy = array_ops.reshape(\n        math_ops.cast(math_ops.range(1, 5, 1), dtypes.float32), shape=(4, 1, 1))\n    original_shape = constant_op.constant([6, 4, 4], dtype=dtypes.int64)\n    begin = constant_op.constant([0, 0, 0], dtype=dtypes.int32)\n    end = constant_op.constant([4, 1, 1], dtype=dtypes.int64)\n    strides = constant_op.constant([1, 1, 1], dtype=dtypes.int64)\n    with self.assertRaises((TypeError, errors_impl.InvalidArgumentError)):\n      dx = array_ops.strided_slice_grad(original_shape, begin, end, strides,\n                                        original_dy)\n      self.evaluate(dx)\n\n\nclass BenchmarkSlice(object):\n\n  def __init__(self, tensor):\n    self.tensor = tensor\n\n  def __getitem__(self, x):\n    return self.tensor[x]\n\n\nclass StridedSliceBenchmark(test_lib.Benchmark):\n  \"\"\"Benchmark new strided slice operation on non-trivial case.\"\"\"\n\n  def run_and_time(self, slice_op):\n    self.evaluate(variables.global_variables_initializer())\n    for _ in range(10):\n      _ = self.evaluate(slice_op)\n    iters = 1000\n    t0 = time.time()\n    for _ in range(iters):\n      self.evaluate(slice_op)\n    t1 = time.time()\n    self.report_benchmark(iters=iters, wall_time=(t1 - t0) / 1000.0)\n\n  def make_variable(self):\n    n = 256\n    shape = (n, n, n)\n    items = n**3\n    var = variables.Variable(\n        array_ops.reshape(math_ops.linspace(1., float(items), items), shape),\n        dtype=dtypes.float32)\n    return var\n\n  def benchmark_strided_slice_skip(self):\n    with session.Session():\n      var = self.make_variable()\n      helper = BenchmarkSlice(var)\n      slice_op = helper[::2, ::1, ::2]\n      self.run_and_time(slice_op)\n\n  def benchmark_strided_slice_easy(self):\n    with session.Session():\n      var = self.make_variable()\n      helper = BenchmarkSlice(var)\n      slice_op = helper[3::1, 3::1, 3::1]\n      self.run_and_time(slice_op)\n\n  def benchmark_slice_easy(self):\n    with session.Session():\n      var = self.make_variable()\n      slice_op = var[3::1, 3::1, 3::1]\n      self.run_and_time(slice_op)\n\n\nclass StridedSliceAssignChecker(object):\n\n  def __init__(self, test, x, tensor_type=dtypes.float32, use_resource=False):\n    self.tensor_type = tensor_type\n    self.test = test\n    self._use_resource = use_resource\n\n    self.x_np = np.array(x).astype(tensor_type.as_numpy_dtype)\n    # Give the value a non-zero imaginary component for complex types.\n    if tensor_type.is_complex:\n      self.x_np -= 1j * self.x_np\n    self.x = constant_op.constant(self.x_np, dtype=tensor_type)\n\n  def __setitem__(self, index, value):\n    value = np.array(value).astype(self.tensor_type.as_numpy_dtype)\n    # Give the value a non-zero imaginary component for complex types.\n    if self.tensor_type.is_complex:\n      value -= 1j * value\n\n    with test_util.device(use_gpu=True):\n      if self._use_resource:\n        var = resource_variable_ops.ResourceVariable(self.x)\n      else:\n        var = variables.Variable(self.x)\n      self.test.evaluate(var.initializer)\n      val = self.test.evaluate(var[index].assign(value))\n      # val_copy is used to check that tf.compat.v1.assign works equivalently\n      # to the assign method above.\n      val_copy = self.test.evaluate(state_ops.assign(var[index], value))\n      valnp = np.copy(self.x_np)\n      valnp[index] = np.array(value)\n      self.test.assertAllEqual(val, valnp)\n      self.test.assertAllEqual(val_copy, valnp)\n\n\nclass SliceAssignTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n\n  def testInvalidSlice(self):\n    foo = constant_op.constant([1, 2, 3])\n    with self.assertRaisesRegex(AttributeError, \"no attribute 'assign'\"):\n      bar = foo[:2].assign(constant_op.constant([1, 2]))\n      self.evaluate(bar)\n\n  def doTestSliceAssign(self, use_resource):\n    for dtype in STRIDED_SLICE_TYPES:\n      with self.subTest(dtype=dtype):\n        checker = StridedSliceAssignChecker(\n            self, [[1, 2, 3], [4, 5, 6]],\n            use_resource=use_resource,\n            tensor_type=dtype)\n        # Check if equal\n        checker[:] = [[10, 20, 30], [40, 50, 60]]\n        # Check trivial (1,1) shape tensor\n        checker[1:2, 1:2] = [[66]]\n        # shrinks shape changes\n        checker[1:2, 1] = [66]\n        checker[1, 1:2] = [66]\n        checker[1, 1] = 66\n        # newaxis shape changes\n        checker[:, None, :] = [[[10, 20, 30]], [[40, 50, 50]]]\n        # shrink and newaxis\n        checker[None, None, 0, 0:1] = [[[99]]]\n        # Non unit strides\n        checker[::1, ::-2] = [[3, 33], [4, 44]]\n        # degenerate interval\n        checker[8:10, 0] = []\n        checker[8:10, 8:10] = [[]]\n    # Assign vector to scalar (rank-0) using newaxis\n    checker2 = StridedSliceAssignChecker(self, 222)\n    checker2[()] = 6  # no indices\n    checker2[...] = 6  # ellipsis\n    checker2[None] = [6]  # new axis\n\n  @test_util.disable_xla(\"b/123559667\")\n  def testSliceAssign(self):\n    self.doTestSliceAssign(use_resource=False)\n\n  @test_util.disable_xla(\"b/123559667\")\n  def testSliceAssignResource(self):\n    self.doTestSliceAssign(use_resource=True)\n\n  def testTypeError(self):\n    init_val = constant_op.constant([1, 2], dtype=dtypes.int32)\n    too_small_val = constant_op.constant([3, 4], dtype=dtypes.int8)\n    too_large_val = constant_op.constant([3, 4], dtype=dtypes.int64)\n    v = variables.VariableV1(init_val)\n    with self.assertRaises((ValueError, TypeError)):\n      self.evaluate(v[:].assign(too_small_val))\n    with self.assertRaises((ValueError, TypeError)):\n      self.evaluate(v[:].assign(too_large_val))\n\n  def testTypeErrorResource(self):\n    init_val = constant_op.constant([1, 2], dtype=dtypes.int32)\n    too_small_val = constant_op.constant([3, 4], dtype=dtypes.int8)\n    too_large_val = constant_op.constant([3, 4], dtype=dtypes.int64)\n    v = resource_variable_ops.ResourceVariable(init_val)\n    self.evaluate(v.initializer)\n    with self.assertRaises(ValueError):\n      self.evaluate(v[:].assign(too_large_val))\n    with self.assertRaises(ValueError):\n      self.evaluate(v[:].assign(too_small_val))\n\n  @test_util.disable_xla(\"b/123559667\")\n  @test_util.run_in_graph_and_eager_modes\n  def testTensorStridedSliceUpdateWithInputForward(self):\n    \"\"\"Tests tensor_strided_slice_update with input-forwarding taking effect.\"\"\"\n    @def_function.function\n    def assign(x):\n      y = x + 1\n      return gen_array_ops.tensor_strided_slice_update(y, [0], [1], [1], [0])\n    self.assertAllEqual([0, 1], self.evaluate(assign(array_ops.zeros([2]))))\n\n  @test_util.disable_xla(\"b/123559667\")\n  @test_util.run_in_graph_and_eager_modes\n  def testTensorStridedSliceUpdateNoInputForward(self):\n    \"\"\"Tests tensor_strided_slice_update with no input-forwarding.\"\"\"\n    x = constant_op.constant([0.2, 0.3])\n    y = x + 1\n    # y's buffer won't be forwarded to z because y and z will be alive at the\n    # same time later.\n    z = gen_array_ops.tensor_strided_slice_update(y, [0], [1], [1], [0.4])\n    ans = y + z\n    self.assertAllClose([1.6, 2.6], self.evaluate(ans))\n\n  @test_util.disable_xla(\"b/123559667\")\n  def testTensorStridedSliceUpdateGradSimple(self):\n    original = constant_op.constant([0.2, 0.3])\n    updates = constant_op.constant([0.4])\n    with backprop.GradientTape() as tape:\n      tape.watch([original, updates])\n      updated = gen_array_ops.tensor_strided_slice_update(\n          original, [0], [1], [1], updates)\n    d1, d2 = tape.gradient(updated, [original, updates],\n                           output_gradients=constant_op.constant([2.0, 3.0]))\n    self.assertAllClose([0.0, 3.0], d1)\n    self.assertAllClose([2.0], d2)\n\n  @parameterized.named_parameters(\n      (\"_%s\" % i, *args) for i, args in enumerate([  # pylint:disable=g-complex-comprehension\n          ([2, 5], [0, 1], [1, 0], [1, 2], [2], 0, 2, 0, 0, 1),\n          ([4], [5], [3], [1], [3], 1, 0, 0, 0, 0),\n          ([2, 2, 3, 2], [0, 0, 1], [1, 0, 2], [1, 0, 1], [2, 3], 0, 0, 2, 0, 5)\n      ]))\n  @test_util.disable_xla(\"b/123559667\")\n  def testTensorStridedSliceUpdateGrad(\n      self, shape, begin, end, strides, updates_shape, *args):\n    with self.cached_session():\n      def f(a, b):\n        return gen_array_ops.tensor_strided_slice_update(\n            a, begin, end, strides, b, *args)\n      theoretical, numerical = gradient_checker_v2.compute_gradient(\n          f, [array_ops.zeros(shape), array_ops.ones(updates_shape)], delta=1.0)\n      self.assertAllClose(theoretical, numerical)\n\n\nclass ShapeSizeRankTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def testDenseShape(self):\n    t_value = [[0, 42], [24, 0]]\n    self.assertAllEqual((2, 2), self.evaluate(array_ops.shape(t_value)))\n    self.assertEqual(4, self.evaluate(array_ops.size(t_value)))\n    self.assertEqual(2, self.evaluate(array_ops.rank(t_value)))\n\n    t = constant_op.constant(t_value)\n    self.assertAllEqual((2, 2), self.evaluate(array_ops.shape(t)))\n    self.assertEqual(4, self.evaluate(array_ops.size(t)))\n    self.assertEqual(2, self.evaluate(array_ops.rank(t)))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSparseShape(self):\n    sp_value = sparse_tensor.SparseTensorValue(\n        indices=((0, 1), (1, 0)), values=(42, 24), dense_shape=(2, 2))\n    self.assertAllEqual((2, 2), self.evaluate(array_ops.shape(sp_value)))\n    self.assertEqual(4, self.evaluate(array_ops.size(sp_value)))\n    self.assertEqual(2, self.evaluate(array_ops.rank(sp_value)))\n\n    sp = sparse_tensor.SparseTensor.from_value(sp_value)\n    self.assertAllEqual((2, 2), self.evaluate(array_ops.shape(sp)))\n    self.assertEqual(4, self.evaluate(array_ops.size(sp)))\n    self.assertEqual(2, self.evaluate(array_ops.rank(sp)))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSizeDtype(self):\n    tensor = [1]\n    self.assertEqual(dtypes.int32, self.evaluate(array_ops.size(tensor)).dtype)\n    self.assertEqual(\n        dtypes.int64,\n        self.evaluate(array_ops.size(tensor, out_type=dtypes.int64)).dtype)\n\n\nclass SequenceMaskTest(test_util.TensorFlowTestCase):\n\n  def testExceptions(self):\n    with self.cached_session():\n      with self.assertRaisesRegex(ValueError, \"`maxlen` must be scalar\"):\n        array_ops.sequence_mask([10, 20], [10, 20])\n\n  def testOneDimensionalWithMaxlen(self):\n    res = array_ops.sequence_mask(constant_op.constant([1, 3, 2]), 5)\n    self.assertAllEqual(res.get_shape(), [3, 5])\n    self.assertAllEqual(\n        res,\n        [[True, False, False, False, False], [True, True, True, False, False],\n         [True, True, False, False, False]])\n\n  def testOneDimensionalDtypeWithoutMaxlen(self):\n    # test dtype and default maxlen:\n    res = array_ops.sequence_mask(\n        constant_op.constant([0, 1, 4]), dtype=dtypes.float32)\n    self.assertAllEqual(res.get_shape().as_list(), [3, 4])\n    self.assertAllEqual(\n        res, [[0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0]])\n\n  def testOneDimensionalWithoutMaxlen(self):\n    res = array_ops.sequence_mask(constant_op.constant([0, 1, 4]))\n    self.assertAllEqual(res.get_shape().as_list(), [3, 4])\n    self.assertAllEqual(res,\n                        [[False, False, False, False],\n                         [True, False, False, False], [True, True, True, True]])\n\n  def testTwoDimensional(self):\n    res = array_ops.sequence_mask(constant_op.constant([[1, 3, 2]]), 5)\n    self.assertAllEqual(res.get_shape(), [1, 3, 5])\n    self.assertAllEqual(\n        res,\n        [[[True, False, False, False, False], [True, True, True, False, False],\n          [True, True, False, False, False]]])\n\n    # test dtype and default maxlen:\n    res = array_ops.sequence_mask(\n        constant_op.constant([[0, 1, 4], [1, 2, 3]]), dtype=dtypes.float32)\n    self.assertAllEqual(res.get_shape().as_list(), [2, 3, 4])\n    self.assertAllEqual(\n        res,\n        [[[0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0]],\n         [[1.0, 0.0, 0.0, 0.0], [1.0, 1.0, 0.0, 0.0], [1.0, 1.0, 1.0, 0.0]]])\n\n  def testDtypes(self):\n\n    def check_dtypes(lengths_dtype, maxlen_dtype):\n      res = array_ops.sequence_mask(\n          constant_op.constant([1, 3, 2], dtype=lengths_dtype),\n          constant_op.constant(5, dtype=maxlen_dtype))\n      self.assertAllEqual(res.get_shape(), [3, 5])\n      self.assertAllEqual(\n          res,\n          [[True, False, False, False, False], [True, True, True, False, False],\n           [True, True, False, False, False]])\n\n    check_dtypes(dtypes.int32, dtypes.int32)\n    check_dtypes(dtypes.int32, dtypes.int64)\n    check_dtypes(dtypes.int64, dtypes.int32)\n    check_dtypes(dtypes.int64, dtypes.int64)\n\n  def testOutputDtype(self):\n\n    def check_output_dtype(output_dtype):\n      res = self.evaluate(\n          array_ops.sequence_mask(\n              constant_op.constant([1, 3, 2], dtype=dtypes.int32),\n              constant_op.constant(5, dtype=dtypes.int32),\n              dtype=output_dtype))\n      self.assertAllEqual(\n          res,\n          self.evaluate(\n              math_ops.cast([[True, False, False, False, False],\n                             [True, True, True, False, False],\n                             [True, True, False, False, False]], output_dtype)))\n\n    check_output_dtype(dtypes.bool)\n    check_output_dtype(\"bool\")\n    check_output_dtype(np.bool_)\n    check_output_dtype(dtypes.int32)\n    check_output_dtype(\"int32\")\n    check_output_dtype(np.int32)\n    check_output_dtype(dtypes.float32)\n    check_output_dtype(\"float32\")\n    check_output_dtype(np.float32)\n    check_output_dtype(dtypes.int64)\n    check_output_dtype(\"float64\")\n    check_output_dtype(np.float64)\n\n\nclass ConcatSliceResourceTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConcatSlice(self):\n    r1 = test_ops.stub_resource_handle_op(container=\"a\", shared_name=\"b\")\n    r2 = test_ops.stub_resource_handle_op(container=\"a\", shared_name=\"c\")\n    c = array_ops.stack([r1, r2])\n    s = array_ops.strided_slice(c, [1], [2])\n    self.evaluate(test_ops.resource_create_op(s))\n    with self.assertRaises(errors.AlreadyExistsError):\n      self.evaluate(test_ops.resource_create_op(r2))\n\n\nclass IdentityTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_gpu_only\n  def testEagerIdentity(self):\n    with context.eager_mode():\n\n      def _test(x, y, device):\n        self.assertAllEqual(x.numpy(), y.numpy())\n        self.assertTrue(device in y.device.lower())\n\n      with test_util.force_gpu():\n        a = constant_op.constant([[2], [3]], dtype=dtypes.float32)\n      with test_util.force_gpu():\n        b = array_ops.identity(a)\n        _test(a, b, \"gpu\")\n      with test_util.force_cpu():\n        c = array_ops.identity(b)\n        _test(b, c, \"cpu\")\n      with test_util.force_cpu():\n        d = array_ops.identity(c)\n        _test(c, d, \"cpu\")\n      with test_util.force_gpu():\n        e = array_ops.identity(d)\n        _test(d, e, \"gpu\")\n\n\nclass PadTest(test_util.TensorFlowTestCase):\n\n  def testEager(self):\n    with context.eager_mode():\n      t = constant_op.constant([[1, 2, 3], [4, 5, 6]])\n      paddings = constant_op.constant([[\n          1,\n          1,\n      ], [2, 2]])\n      padded = array_ops.pad(t, paddings, \"CONSTANT\")\n      self.assertAllEqual(padded.numpy(),\n                          [[0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 2, 3, 0, 0],\n                           [0, 0, 4, 5, 6, 0, 0], [0, 0, 0, 0, 0, 0, 0]])\n\n  def testSymmetricMirrorPadGrad(self):\n    t = np.broadcast_to(np.arange(0, 7), (3, 2, 1, 7))\n    paddings = constant_op.constant([\n        [1, 1],\n        [0, 0],\n        [0, 0],\n        [2, 2],\n    ])\n    expected = np.broadcast_to(np.array([9, 27, 27]), (1, 2, 1, 3))\n    result = gen_array_ops.mirror_pad_grad(t, paddings, \"SYMMETRIC\")\n    self.assertAllEqual(result, expected)\n\n  def testReflectMirrorPadGrad(self):\n    t = np.broadcast_to(np.reshape(np.arange(0, 7), (7, 1)), (1, 4, 7, 1))\n    paddings = constant_op.constant([\n        [0, 0],\n        [1, 1],\n        [2, 2],\n        [0, 0],\n    ])\n    expected = np.broadcast_to(\n        np.reshape(np.array([16, 18, 8]), (3, 1)), (1, 2, 3, 1))\n    result = gen_array_ops.mirror_pad_grad(t, paddings, \"REFLECT\")\n    self.assertAllEqual(result, expected)\n\n\nclass InvertPermutationTest(test_util.TensorFlowTestCase):\n\n  def testInvertPermutation(self):\n    for dtype in [dtypes.int32, dtypes.int64]:\n      with self.subTest(dtype=dtype, use_gpu=True):\n        x = constant_op.constant([3, 4, 0, 2, 1], dtype=dtype)\n        y = array_ops.invert_permutation(x)\n        self.assertAllEqual(y.get_shape(), [5])\n        self.assertAllEqual(y, [2, 4, 3, 0, 1])\n\n\nclass UnravelIndexTest(test_util.TensorFlowTestCase):\n\n  # TODO(b/73086570): Reenable test.\n  @unittest.skip(\"Test does not pass internally.\")\n  def testUnravelIndex(self):\n    with self.cached_session():\n      for dtype in [dtypes.int32, dtypes.int64]:\n        with self.subTest(dtype=dtype):\n          indices_1 = constant_op.constant(1621, dtype=dtype)\n          dims_1 = constant_op.constant([6, 7, 8, 9], dtype=dtype)\n          out_1 = array_ops.unravel_index(indices_1, dims_1)\n          self.assertAllEqual(out_1, [3, 1, 4, 1])\n\n          indices_2 = constant_op.constant([1621], dtype=dtype)\n          dims_2 = constant_op.constant([6, 7, 8, 9], dtype=dtype)\n          out_2 = array_ops.unravel_index(indices_2, dims_2)\n          self.assertAllEqual(out_2, [[3], [1], [4], [1]])\n\n          indices_3 = constant_op.constant([22, 41, 37], dtype=dtype)\n          dims_3 = constant_op.constant([7, 6], dtype=dtype)\n          out_3 = array_ops.unravel_index(indices_3, dims_3)\n          self.assertAllEqual(out_3, [[3, 6, 6], [4, 5, 1]])\n\n  # Test case for GitHub issue 40204.\n  def testUnravelIndexZeroDim(self):\n    with self.cached_session():\n      for dtype in [dtypes.int32, dtypes.int64]:\n        with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                    \"dims cannot contain a dim of zero\"):\n          indices = constant_op.constant([2, 5, 7], dtype=dtype)\n          dims = constant_op.constant([3, 0], dtype=dtype)\n          self.evaluate(array_ops.unravel_index(indices=indices, dims=dims))\n\n\nclass GuaranteeConstOpTest(test_util.TensorFlowTestCase):\n\n  def testSimple(self):\n    a = array_ops.constant(10)\n    guarantee_a = array_ops.guarantee_const(a)\n    self.assertEqual(10, self.evaluate(guarantee_a))\n\n  def testVariables(self):\n    for use_resource in [False, True]:\n      with self.subTest(use_resource=use_resource):\n        a = variable_scope.get_variable(\n            \"var_{}\".format(use_resource), [],\n            initializer=init_ops.constant_initializer(10.0),\n            use_resource=use_resource)\n        guarantee_a = array_ops.guarantee_const(a)\n        self.evaluate(a.initializer)\n        self.assertEqual(10.0, self.evaluate(guarantee_a))\n\n  def testResourceRejection(self):\n    with ops.device(\"/cpu:0\"):\n      a = variable_scope.get_variable(\n          \"resource_var\", [],\n          initializer=init_ops.constant_initializer(10.0),\n          use_resource=True)\n    with self.assertRaisesWithPredicateMatch(errors.InvalidArgumentError,\n                                             \"cannot be a resource variable\"):\n      guarantee_a = array_ops.guarantee_const(a.handle)\n      self.evaluate(a.initializer)\n      self.evaluate(guarantee_a)\n\n\nclass SnapshotOpTest(test_util.TensorFlowTestCase):\n\n  def testInvertPermutation(self):\n    for dtype in [dtypes.int32, dtypes.int64, dtypes.float32, dtypes.float64]:\n      with self.subTest(dtype=dtype, use_gpu=True):\n        x = constant_op.constant([0, 1, 2, 3], dtype=dtype)\n        y = gen_array_ops.snapshot(x)\n        self.assertAllEqual(y, [0, 1, 2, 3])\n\n\n@test_util.with_eager_op_as_function\n@test_util.run_all_in_graph_and_eager_modes\nclass QuantizeAndDequantizeTest(test_util.TensorFlowTestCase):\n\n  # Generates a tensor of the specified `shape` using values from `values`\n  # scaled by (slice_idx + 1) along `axis` dimension.\n  def _scale_per_slice(self, shape, axis, values):\n    # Note: repeats the values if the shape is larger than values.\n    out = np.take(values, np.remainder(np.arange(np.prod(shape)),\n                                       len(values))).reshape(shape)\n    if axis is not None:\n      scale_shape = [1] * len(shape)\n      scale_shape[axis] = shape[axis]\n      out *= np.arange(1, shape[axis] + 1).reshape(scale_shape)\n    return out\n\n  def testAxis(self):\n    shape = np.array([2, 3, 4, 5])\n    values = np.array([-1, -0.5, 0, 0.3, 0.8, 0.555, 0.5], dtype=np.float32)\n    quant_values = np.array(\n        [-1, -0.5, 0, 38.0 / 128, 102.0 / 128, 71.0 / 128, 0.5],\n        dtype=np.float32)\n    for axis in [None, 0, 1, 2, 3]:\n      with self.subTest(axis=axis):\n        inputs = constant_op.constant(\n            self._scale_per_slice(shape, axis, values))\n        expected = self._scale_per_slice(shape, axis, quant_values)\n        unused_minmax_value = 0 if axis is None else [0] * shape[axis]\n        fake_quantized = self.evaluate(\n            array_ops.quantize_and_dequantize_v2(\n                inputs,\n                unused_minmax_value,\n                unused_minmax_value,\n                range_given=False,\n                round_mode=\"HALF_UP\",\n                axis=axis))\n        self.assertAllEqual(fake_quantized, expected)\n        if axis is not None:\n          fake_quantized = self.evaluate(\n              array_ops.quantize_and_dequantize_v2(\n                  inputs,\n                  unused_minmax_value,\n                  unused_minmax_value,\n                  range_given=False,\n                  axis=(axis - 4)))\n          self.assertAllClose(fake_quantized, expected)\n\n  def testBadAxis(self):\n    input_tensor = [2.5, 2.5]\n    input_min = [0, 0]\n    input_max = [1, 1]\n    # When eager_op_as_function mode is enabled XLA auto-clustering kicks in.\n    # XLA raises an UnimplementedError on invalid axis.\n    error_message_pattern = (r\"Shape must be at least rank 11 but is rank \"\n                             r\"1|invalid axis\")\n    # TODO(b/171260356): Eager mode and graph mode throw different error types\n    error = (errors.InvalidArgumentError, ValueError, errors.UnimplementedError)\n    with self.assertRaisesRegex(error, error_message_pattern):\n      self.evaluate(\n          array_ops.quantize_and_dequantize_v2(\n              input=input_tensor,\n              input_min=input_min,\n              input_max=input_max,\n              axis=10))\n\n  def testQuantizeDequantizeGrad(self):\n    shape = (2, 2)\n    max_threshold = 0\n    min_threshold = -10\n    input_value = np.random.rand(2, 2) * 40.0 - 20.0\n    input_tensor = constant_op.constant(input_value, shape=shape,\n                                        name=\"input_tensor\")\n    with self.cached_session():\n      def f(a):\n        return array_ops.quantize_and_dequantize_v2(\n            a,\n            input_min=min_threshold,\n            input_max=max_threshold,\n            range_given=True)\n      output_grad = gradient_checker_v2.compute_gradient(f, [input_tensor])\n      self.assertAllClose(output_grad[0], np.zeros([1, 4, 4]))\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass SortedSearchTest(test_util.TensorFlowTestCase):\n\n  def testUpperBoundFloatHandCoded(self):\n    cdf = np.array([0, .2, .5, .6, .8, 1.], dtype=np.float32)\n    arr = np.array([.04, .99, .53, .58, .31, .01, .79, .8, .21],\n                   dtype=np.float32)\n    result = np.searchsorted(cdf, arr, side=\"right\")\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n    self.assertAllEqual(result, tf_result)\n\n  def testUpperBoundFloatRandomNd(self):\n    dim_size = 7\n    for d in range(1, 5):\n      shape = [dim_size] * d\n      cdf = np.cumsum(\n          np.random.uniform(size=shape).astype(np.float32), axis=(d - 1))\n      arr = np.random.uniform(size=shape).astype(np.float32) * dim_size\n\n      tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n\n      cdf = cdf.reshape([-1, dim_size])\n      arr = arr.reshape([-1, dim_size])\n      result = np.zeros(arr.shape, dtype=np.int32)\n      for i in range(dim_size**(d - 1)):\n        result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"right\")\n\n      result = result.reshape(shape)\n\n      self.assertAllEqual(result, tf_result)\n\n  def testUpperBoundFloatUneven(self):\n    batch_size = 7\n    size_search_array = 1000\n    size_values = 47\n    cdf = np.cumsum(\n        np.random.uniform(size=[batch_size, size_search_array]).astype(\n            np.float32),\n        axis=1)\n    arr = np.random.uniform(size=[batch_size, size_values]).astype(\n        np.float32) * size_search_array\n\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n\n    result = np.zeros(arr.shape, dtype=np.int32)\n    for i in range(batch_size):\n      result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"right\")\n\n    self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundFloatHandCoded(self):\n    cdf = np.array([0, .2, .5, .6, .8, 1.], dtype=np.float32)\n    arr = np.array([.04, .99, .53, .58, .31, .01, .79, .8, .21],\n                   dtype=np.float32)\n    result = np.searchsorted(cdf, arr, side=\"left\")\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n    self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundFloatRandomNd(self):\n    dim_size = 7\n    for d in range(1, 5):\n      shape = [dim_size] * d\n      cdf = np.cumsum(\n          np.random.uniform(size=shape).astype(np.float32), axis=(d - 1))\n      arr = np.random.uniform(size=shape).astype(np.float32) * dim_size\n\n      tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n\n      cdf = cdf.reshape([-1, dim_size])\n      arr = arr.reshape([-1, dim_size])\n      result = np.zeros(arr.shape, dtype=np.int32)\n      for i in range(dim_size**(d - 1)):\n        result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"left\")\n\n      result = result.reshape(shape)\n\n      self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundFloatUneven(self):\n    batch_size = 7\n    size_search_array = 1000\n    size_values = 47\n    cdf = np.cumsum(\n        np.random.uniform(size=[batch_size, size_search_array]).astype(\n            np.float32),\n        axis=1)\n    arr = np.random.uniform(size=[batch_size, size_values]).astype(\n        np.float32) * size_search_array\n\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n\n    result = np.zeros(arr.shape, dtype=np.int32)\n    for i in range(batch_size):\n      result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"left\")\n\n    self.assertAllEqual(result, tf_result)\n\n  def testUpperBoundIntHandCoded(self):\n    cdf = np.array([0, 20, 50, 60, 80, 100], dtype=np.int64)\n    arr = np.array([4, 99, 53, 58, 31, 1, 79, 8, 21], dtype=np.int64)\n    result = np.searchsorted(cdf, arr, side=\"right\")\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n    self.assertAllEqual(result, tf_result)\n\n  def testUpperBoundIntRandomNd(self):\n    dim_size = 7\n    for d in range(1, 5):\n      shape = [dim_size] * d\n      cdf = np.cumsum(\n          np.random.randint(low=0, high=10, size=shape).astype(np.int64),\n          axis=(d - 1))\n      arr = np.random.randint(\n          low=0, high=10 * dim_size, size=shape).astype(np.int64)\n\n      tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n\n      cdf = cdf.reshape([-1, dim_size])\n      arr = arr.reshape([-1, dim_size])\n      result = np.zeros(arr.shape, dtype=np.int32)\n      for i in range(dim_size**(d - 1)):\n        result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"right\")\n\n      result = result.reshape(shape)\n\n      self.assertAllEqual(result, tf_result)\n\n  def testUpperBoundIntUneven(self):\n    batch_size = 7\n    size_search_array = 1000\n    size_values = 47\n    cdf = np.cumsum(\n        np.random.randint(low=0, high=10,\n                          size=[batch_size,\n                                size_search_array]).astype(np.int64),\n        axis=1)\n    arr = np.random.randint(\n        low=0, high=10 * size_search_array, size=[batch_size,\n                                                  size_values]).astype(np.int64)\n\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n\n    result = np.zeros(arr.shape, dtype=np.int32)\n    for i in range(batch_size):\n      result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"right\")\n\n    self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundIntHandCoded(self):\n    cdf = np.array([0, 20, 50, 60, 80, 100], dtype=np.int64)\n    arr = np.array([4, 99, 53, 58, 31, 1, 79, 8, 21], dtype=np.int64)\n    result = np.searchsorted(cdf, arr, side=\"left\")\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n    self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundIntRandomNd(self):\n    dim_size = 7\n    for d in range(1, 5):\n      shape = [dim_size] * d\n      cdf = np.cumsum(\n          np.random.randint(low=0, high=10, size=shape).astype(np.int64),\n          axis=(d - 1))\n      arr = np.random.randint(\n          low=0, high=10 * dim_size, size=shape).astype(np.int64)\n\n      tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n\n      cdf = cdf.reshape([-1, dim_size])\n      arr = arr.reshape([-1, dim_size])\n      result = np.zeros(arr.shape, dtype=np.int32)\n      for i in range(dim_size**(d - 1)):\n        result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"left\")\n\n      result = result.reshape(shape)\n\n      self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundIntUneven(self):\n    batch_size = 7\n    size_search_array = 1000\n    size_values = 47\n    cdf = np.cumsum(\n        np.random.randint(low=0, high=10,\n                          size=[batch_size,\n                                size_search_array]).astype(np.int64),\n        axis=1)\n    arr = np.random.randint(\n        low=0, high=10 * size_search_array, size=[batch_size,\n                                                  size_values]).astype(np.int64)\n\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n\n    result = np.zeros(arr.shape, dtype=np.int32)\n    for i in range(batch_size):\n      result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"left\")\n\n    self.assertAllEqual(result, tf_result)\n\n  def testZeroSequenceSize(self):\n    dtype = dtypes.int32\n    for side in (\"left\", \"right\"):\n      with self.subTest(side=side):\n        self.assertAllEqual(\n            array_ops.searchsorted(\n                array_ops.ones([2, 0]),\n                array_ops.ones([2, 3]),\n                side=side,\n                out_type=dtype), array_ops.zeros([2, 3], dtype))\n\n  def testZeroValueSize(self):\n    dtype = dtypes.int32\n    for side in (\"left\", \"right\"):\n      with self.subTest(side=side):\n        self.assertAllEqual(\n            array_ops.searchsorted(\n                array_ops.ones([2, 3]),\n                array_ops.ones([2, 0]),\n                side=side,\n                out_type=dtype), array_ops.zeros([2, 0], dtype))\n\n  def testInt64(self):\n\n    @def_function.function\n    def g():\n      x = random_ops.random_normal(shape=[int(1e10)])\n      y = array_ops.ones(shape=[int(1e10)])\n      return array_ops.searchsorted(x, y, out_type=dtypes.int64)\n\n    _ = g.get_concrete_function()\n\n  def testInt64UnspecifiedOutType(self):\n\n    @def_function.function\n    def g():\n      x = random_ops.random_normal(shape=[int(1e10)])\n      y = array_ops.ones(shape=[int(1e10)])\n      return array_ops.searchsorted(x, y)\n\n    _ = g.get_concrete_function()\n\n\nclass BatchGatherNdTest(test_util.TensorFlowTestCase):\n\n  def testShapesMatch(self):\n    \"\"\"Tests for various different shape combinations.\"\"\"\n    shapes = []\n    # params_shape, indices_shape, batch_dims\n    shapes.append(((2, 2, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 2), (2, 3), 0),)\n    shapes.append(((2, 2, 2), (3,), 0),)\n    shapes.append(((2, 2, 2), (1,), 0),)\n    shapes.append(((2, 2, 3, 2), (2, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 3, 1), 1),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 2),)\n\n    for params_shape, indices_shape, batch_dims in shapes:\n      with self.subTest(\n          params_shape=params_shape,\n          indices_shape=indices_shape,\n          batch_dims=batch_dims):\n        params = constant_op.constant(1.0, shape=(params_shape))\n        indices = constant_op.constant(\n            1, shape=(indices_shape), dtype=dtypes.int32)\n        out = array_ops.batch_gather_nd(\n            params=params, indices=indices, batch_dims=batch_dims)\n        ndims_params = len(params_shape) - batch_dims\n        ndims_rows = ndims_params - indices_shape[-1]\n        expected_out_shape = indices_shape[:-1]\n        if ndims_rows > 0:\n          expected_out_shape += params_shape[-ndims_rows:]\n        self.assertSequenceEqual(out.shape, expected_out_shape)\n\n  def testReducesToGatherNDWhenBatchDimIsZero(self):\n    \"\"\"Confirms setting batch_dims to zero reduces to tf.gather_nd.\"\"\"\n    params = constant_op.constant(np.random.uniform(0.0, 1.0, size=(7, 8, 9)))\n    indices_shapes = []\n    indices_shapes.append((1,))\n    indices_shapes.append((3, 1))\n    indices_shapes.append((3, 3, 1))\n    indices_shapes.append((2,))\n    indices_shapes.append((3, 2))\n    indices_shapes.append((3, 3, 2))\n    indices_shapes.append((3,))\n    indices_shapes.append((3, 3))\n    indices_shapes.append((3, 3, 3))\n\n    for indices_shape in indices_shapes:\n      with self.subTest(indices_shape=indices_shape):\n        indices = np.random.randint(0, 7, size=indices_shape)\n        gather_nd_result = gen_array_ops.gather_nd(params, indices)\n        batch_gather_nd_result = array_ops.batch_gather_nd(\n            params=params, indices=indices, batch_dims=0)\n        self.assertAllEqual(gather_nd_result, batch_gather_nd_result)\n\n  def testSameResultAsMapFn(self):\n    \"\"\"Compares results with gather_nd called on every element with map_fn.\"\"\"\n    shapes = []\n    # params_shape, indices_shape, batch_dims\n    shapes.append(((2, 2, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 3, 1), 1),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 2),)\n\n    for params_shape, indices_shape, batch_dims in shapes:\n      with self.subTest(\n          params_shape=params_shape,\n          indices_shape=indices_shape,\n          batch_dims=batch_dims):\n        params = constant_op.constant(\n            np.random.uniform(0.0, 1.0, size=(params_shape)))\n        indices = np.random.randint(0, 2, size=indices_shape)\n        batch_gather_nd_result = array_ops.batch_gather_nd(\n            params=params, indices=indices, batch_dims=batch_dims)\n\n        if batch_dims > 1:\n          params = array_ops.reshape(\n              params, shape=[-1] + list(params_shape[batch_dims:]))\n          indices = array_ops.reshape(\n              indices, shape=[-1] + list(indices_shape[batch_dims:]))\n\n        map_fn_gather_nd_result = map_fn.map_fn(\n            fn=self._map_fn_body, elems=(params, indices), dtype=dtypes.float64)\n\n        if batch_dims > 1:\n          out_shape = map_fn_gather_nd_result.shape.as_list()\n          out_shape = list(params_shape[:batch_dims]) + out_shape[1:]\n          map_fn_gather_nd_result = array_ops.reshape(\n              map_fn_gather_nd_result, shape=out_shape)\n\n        self.assertAllEqual(map_fn_gather_nd_result, batch_gather_nd_result)\n\n  def _map_fn_body(self, elems):\n    return gen_array_ops.gather_nd(elems[0], elems[1])\n\n  def testBatchDimsAsTensor(self):\n    \"\"\"Tests Tensor batch_dims as input works as intended.\"\"\"\n    shapes = []\n    # params_shape, indices_shape, batch_dims\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 0),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 1),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 2),)\n\n    for params_shape, indices_shape, batch_dims in shapes:\n      with self.subTest(\n          params_shape=params_shape,\n          indices_shape=indices_shape,\n          batch_dims=batch_dims):\n        params = constant_op.constant(\n            np.random.uniform(0.0, 1.0, size=(params_shape)))\n        indices = np.random.randint(0, 2, size=indices_shape)\n        batch_gather_nd_result = array_ops.gather_nd(\n            params=params, indices=indices, batch_dims=batch_dims)\n        batch_dims_tensor = constant_op.constant([batch_dims])\n        batch_gather_nd_tensor_batch_dims_result = array_ops.gather_nd(\n            params=params, indices=indices, batch_dims=batch_dims_tensor)\n\n        self.assertAllEqual(batch_gather_nd_tensor_batch_dims_result,\n                            batch_gather_nd_result)\n\n  def testInvalidBatchDimsRaisesException(self):\n    \"\"\"Tests whether invalid batch_dims raise expected exceptions.\"\"\"\n    params = constant_op.constant(\n        np.random.uniform(0.0, 1.0, size=(3, 2, 2, 3, 4)))\n    indices = np.random.randint(0, 2, size=(3, 2, 3))\n\n    with self.assertRaises(TypeError):\n      array_ops.batch_gather_nd(\n          params=params,\n          indices=indices,\n          batch_dims=constant_op.constant((0, 1)))\n\n    with self.assertRaises(ValueError):\n      array_ops.batch_gather_nd(params=params, indices=indices, batch_dims=-1)\n\n    with self.assertRaises(ValueError):\n      array_ops.batch_gather_nd(params=params, indices=indices, batch_dims=4)\n\n  def testNoneBatchDimensions(self):\n    \"\"\"Tests gather_nd works with None dimensions.\"\"\"\n    shapes = []\n    # params_shape, indices_shape, batch_dims\n    shapes.append(((2, 2, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 3, 1), 1),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 2),)\n\n    for params_shape, indices_shape, batch_dims in shapes:\n      params_ph_shape = list(params_shape)\n      indices_ph_shape = list(indices_shape)\n      for i in range(batch_dims):\n        params_ph_shape[i] = None\n        indices_ph_shape[i] = None\n\n      @def_function.function\n      def func(params, indices):\n        return array_ops.batch_gather_nd(\n            params=params, indices=indices, batch_dims=batch_dims)  # pylint: disable=cell-var-from-loop\n\n      f = func.get_concrete_function(\n          tensor_spec.TensorSpec(params_ph_shape, dtypes.float32),\n          tensor_spec.TensorSpec(indices_ph_shape, dtypes.int32))\n\n      params_val = np.ones(dtype=np.float32, shape=params_shape)\n      indices_val = np.ones(dtype=np.int32, shape=indices_shape)\n      res = f(params_val, indices_val)\n      row_ndims = len(params_shape) - batch_dims - indices_shape[-1]\n      expected_out_shape = indices_shape[:-1]\n      if row_ndims > 0:\n        expected_out_shape += params_shape[-row_ndims:]\n\n      self.assertSequenceEqual(res.shape, expected_out_shape)\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass RepeatTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n\n  @parameterized.parameters(\n      (3, 4, None),\n      ([[1, 2], [3, 4]], 2, None),\n      ([[1, 2], [3, 4]], [1, 2], 0),\n      ([[1, 2], [3, 4]], [1, 2], 1),\n      ([[1, 2], [3, 4]], 3, 1),\n      ([[1, 2], [3, 4]], [1, 2, 3, 4], None),\n      (np.ones([0, 4]), 0, 1),\n      (np.ones([1, 2]), [2], None),\n  )\n  def testRepeat(self, array, repeats, axis):\n    array = np.array(array)\n\n    @def_function.function(\n        input_signature=[tensor_spec.TensorSpec(None, dtypes.int32)] * 2)\n    def repeat_fn(array, repeats):\n      return array_ops.repeat(array, repeats, axis)\n\n    v_tf = array_ops.repeat(constant_op.constant(array), repeats, axis)\n    v_tf_fn = repeat_fn(\n        constant_op.constant(array, dtype=dtypes.int32), repeats)\n    v_np = np.repeat(array, repeats, axis)\n    self.assertAllEqual(v_tf, v_np)\n    self.assertAllEqual(v_tf_fn, v_np)\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass TileVariantTest(test_util.TensorFlowTestCase):\n\n  def test_tile_tensor_list(self):\n    t = constant_op.constant(np.random.uniform(size=[2, 3, 4]))\n    handle = list_ops.tensor_list_from_tensor(t, element_shape=None)\n    with ops.device(\"CPU:0\"):\n      tiled_handles = array_ops.tile(array_ops.reshape(handle, [1]), [2])\n    tiled_tensor_0 = list_ops.tensor_list_stack(tiled_handles[0], t.dtype, 2,\n                                                [3, 4])\n    tiled_tensor_1 = list_ops.tensor_list_stack(tiled_handles[1], t.dtype, 2,\n                                                [3, 4])\n    self.assertAllEqual(t, tiled_tensor_0)\n    self.assertAllEqual(t, tiled_tensor_1)\n    # Now mutate some of the lists and make sure the changes are not reflected\n    # in the tiled handles.\n    with ops.control_dependencies([\n        list_ops.tensor_list_scatter([t[0] + 1], [0], input_handle=handle),\n        list_ops.tensor_list_set_item(tiled_handles[0], 0, t[0] + 2)]):\n      tiled_tensor_0 = list_ops.tensor_list_stack(tiled_handles[0], t.dtype, 2,\n                                                  [3, 4])\n      tiled_tensor_1 = list_ops.tensor_list_stack(tiled_handles[1], t.dtype, 2,\n                                                  [3, 4])\n    self.assertAllEqual(t, tiled_tensor_0)\n    self.assertAllEqual(t, tiled_tensor_1)\n\n\nclass StopGradientTest(test_util.TensorFlowTestCase):\n\n  def testStopGradient(self):\n    x = array_ops.zeros(3)\n    y = array_ops.stop_gradient(x)\n    self.assertAllEqual(x, y)\n\n  def testStopGradientRaggedTensor(self):\n    x = RaggedTensor.from_row_splits(values=[1, 2, 3], row_splits=[0, 1, 1, 3])\n    y = array_ops.stop_gradient(x)\n    self.assertAllEqual(x, y)\n\n  def testStopGradientGradientTape(self):\n    x = array_ops.zeros(3)\n    with backprop.GradientTape() as tape:\n      y = array_ops.stop_gradient(x)\n\n    self.assertIsNone(tape.gradient(y, x))\n\n  def testStopGradientGradientTapeRaggedTensor(self):\n    x = RaggedTensor.from_row_splits(values=[1, 2, 3], row_splits=[0, 1, 1, 3])\n    with backprop.GradientTape() as tape:\n      y = array_ops.stop_gradient(x)\n\n    # TODO(b/202162002): Once GradientTape supports composiste tensors, use\n    # tape.gradient(y, x).\n    self.assertIsNone(tape.gradient(y.values, x.values))\n\n\nif __name__ == \"__main__\":\n  test_lib.main()"