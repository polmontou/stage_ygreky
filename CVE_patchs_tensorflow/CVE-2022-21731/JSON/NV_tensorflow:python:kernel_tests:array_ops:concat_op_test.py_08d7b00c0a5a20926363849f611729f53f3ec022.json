"# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Functional tests for Concat Op.\"\"\"\n\nimport numpy as np\n\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gen_array_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.platform import test\n\n\nclass ConcatOpTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testHStack(self):\n    with self.session():\n      p1 = array_ops.placeholder(dtypes.float32, shape=[4, 4])\n      p2 = array_ops.placeholder(dtypes.float32, shape=[4, 4])\n      c = array_ops.concat([p1, p2], 0)\n      params = {\n          p1: np.random.rand(4, 4).astype(\"f\"),\n          p2: np.random.rand(4, 4).astype(\"f\")\n      }\n      result = c.eval(feed_dict=params)\n\n    self.assertEqual(result.shape, c.get_shape())\n    self.assertAllEqual(result[:4, :], params[p1])\n    self.assertAllEqual(result[4:, :], params[p2])\n\n  @test_util.run_deprecated_v1\n  def testVStack(self):\n    with self.session():\n      p1 = array_ops.placeholder(dtypes.float32, shape=[4, 4])\n      p2 = array_ops.placeholder(dtypes.float32, shape=[4, 4])\n      c = array_ops.concat([p1, p2], 1)\n      params = {\n          p1: np.random.rand(4, 4).astype(\"f\"),\n          p2: np.random.rand(4, 4).astype(\"f\")\n      }\n      result = c.eval(feed_dict=params)\n\n    self.assertEqual(result.shape, c.get_shape())\n    self.assertAllEqual(result[:, :4], params[p1])\n    self.assertAllEqual(result[:, 4:], params[p2])\n\n  @test_util.run_deprecated_v1\n  def test4DStack(self):\n    with self.session():\n      p1 = array_ops.placeholder(dtypes.float32, shape=[2, 3, 1, 1])\n      p2 = array_ops.placeholder(dtypes.float32, shape=[2, 3, 4, 1])\n      c = array_ops.concat([p1, p2], 2)\n      params = {\n          p1: np.random.rand(2, 3, 1, 1).astype(\"f\"),\n          p2: np.random.rand(2, 3, 4, 1).astype(\"f\")\n      }\n      result = c.eval(feed_dict=params)\n\n    self.assertEqual(result.shape, c.get_shape())\n    self.assertAllEqual(result[:, :, :1, :], params[p1])\n    self.assertAllEqual(result[:, :, 1:, :], params[p2])\n\n  def testInt32GPU(self):\n    with test_util.use_gpu():\n      p1 = np.random.rand(2, 3).astype(\"i\")\n      p2 = np.random.rand(2, 3).astype(\"i\")\n      x1 = constant_op.constant(p1)\n      x2 = constant_op.constant(p2)\n      c = array_ops.concat([x1, x2], 0)\n      result = self.evaluate(c)\n    self.assertAllEqual(result[:2, :], p1)\n    self.assertAllEqual(result[2:, :], p2)\n\n  def testRefType(self):\n    with test_util.use_gpu():\n      p1 = np.random.rand(4, 4).astype(\"f\")\n      p2 = np.random.rand(4, 4).astype(\"f\")\n      v1 = variables.Variable(p1)\n      v2 = variables.Variable(p2)\n      c = array_ops.concat([v1, v2], 0)\n      self.evaluate(variables.global_variables_initializer())\n      result = self.evaluate(c)\n\n    self.assertEqual(result.shape, c.get_shape())\n    self.assertAllEqual(result[:4, :], p1)\n    self.assertAllEqual(result[4:, :], p2)\n\n  def _testRandom(self, dtype):\n    # Random dims of rank 5\n    shape = np.random.randint(1, 5, size=5)\n    # Random number of tensors, but always > 1.\n    num_tensors = np.random.randint(2, 10)\n    # Random dim to concat on\n    concat_dim = np.random.randint(5)\n    params = {}\n    if dtype == dtypes.bfloat16:\n      dtype_feed = dtypes.float32\n    else:\n      dtype_feed = dtype\n    with self.session():\n      p = []\n      for i in np.arange(num_tensors):\n        input_shape = shape\n        input_shape[concat_dim] = np.random.randint(1, 5)\n        placeholder = array_ops.placeholder(dtype_feed, shape=input_shape)\n        p.append(placeholder)\n\n        t = dtype_feed.as_numpy_dtype\n        params[placeholder] = np.random.rand(*input_shape).astype(t)\n\n      if dtype != dtype_feed:\n        concat_inputs = [math_ops.cast(p_i, dtype) for p_i in p]\n      else:\n        concat_inputs = p\n      c = array_ops.concat(concat_inputs, concat_dim)\n      if dtype != dtype_feed:\n        c = math_ops.cast(c, dtype_feed)\n      result = c.eval(feed_dict=params)\n\n    self.assertEqual(result.shape, c.get_shape())\n    cur_offset = 0\n\n    for i in np.arange(num_tensors):\n      # The index into the result is the ':' along all dimensions\n      # except the concat_dim. slice(0, size) is used for ':', and\n      # a list of slices is used to index into result.\n      ind = [slice(0, params[p[i]].shape[j]) for j in np.arange(5)]\n      ind[concat_dim] = slice(cur_offset,\n                              cur_offset + params[p[i]].shape[concat_dim])\n      cur_offset += params[p[i]].shape[concat_dim]\n      if dtype == dtype_feed:\n        self.assertAllEqual(result[ind], params[p[i]])\n      else:\n        self.assertAllClose(result[ind], params[p[i]], 0.01)\n\n  @test_util.run_deprecated_v1\n  def testRandom(self):\n    self._testRandom(dtypes.bool)\n    self._testRandom(dtypes.float32)\n    self._testRandom(dtypes.int16)\n    self._testRandom(dtypes.int32)\n    self._testRandom(dtypes.int64)\n    self._testRandom(dtypes.bfloat16)\n    self._testRandom(dtypes.complex64)\n    self._testRandom(dtypes.complex128)\n\n  @test_util.run_deprecated_v1\n  def testInvalidConcatDimTypeAndShape(self):\n    a = variables.Variable(constant_op.constant(1.0, shape=[1]))\n    b = variables.Variable(constant_op.constant(2.0, shape=[1]))\n    with self.assertRaises(ValueError):\n      array_ops.concat(b, a)\n    with self.assertRaises(TypeError):\n      array_ops.concat(1, 4.2)\n    with self.assertRaises(ValueError):\n      array_ops.concat(1, a)\n    with self.assertRaises(TypeError):\n      array_ops.concat([a, b], a)\n    with self.assertRaises(ValueError):\n      array_ops.concat([a, b], [3])\n    with self.assertRaises(ValueError):\n      array_ops.concat([], 0)\n    # An integer tensor for shape dim should throw no error.\n    array_ops.concat(1, constant_op.constant(0, shape=[]))\n    # A non-scalar tensor for shape should throw ValueError.\n    with self.assertRaises(ValueError):\n      array_ops.concat(1, constant_op.constant(0, shape=[1]))\n\n  def _testGradientsSimple(self, dtype):\n    # Test both positive and negative concat axis.\n    # -2 and 1 correspond to the same axis for 3-dimensional tensors.\n    for axis in [-2, 1]:\n      with test_util.use_gpu():\n        inp = []\n        inp_tensors = []\n        for x in [1, 2, 6]:\n          shape = [10, x, 2]\n          t = np.random.rand(*shape).astype(dtype.as_numpy_dtype)\n          if dtype.is_complex:\n            t += -1j * t\n          inp.append(t)\n          inp_tensors.append(\n              constant_op.constant(\n                  t.flatten(),\n                  shape=shape,\n                  dtype=dtype))\n        c = array_ops.concat(inp_tensors, axis)\n        output_shape = [10, 9, 2]\n        grad_inp = np.random.rand(*output_shape).astype(dtype.as_numpy_dtype)\n        if dtype.is_complex:\n          grad_inp += -1j * grad_inp\n        grad_tensor = constant_op.constant(\n            grad_inp.flatten(), shape=output_shape)\n        grad = gradients_impl.gradients([c], inp_tensors, [grad_tensor])\n        concated_grad = array_ops.concat(grad, axis)\n        result = self.evaluate(concated_grad)\n    self.assertAllEqual(result, grad_inp)\n\n  @test_util.run_deprecated_v1\n  def testGradientsSimple(self):\n    self._testGradientsSimple(dtypes.float32)\n    self._testGradientsSimple(dtypes.complex64)\n\n  @test_util.run_deprecated_v1\n  def testGradientsFirstDim(self):\n    with test_util.use_gpu():\n      inp = []\n      inp_tensors = []\n      for x in [1, 2, 6]:\n        shape = [x, 10, 2]\n        t = np.random.rand(*shape).astype(\"f\")\n        inp.append(t)\n        inp_tensors.append(\n            constant_op.constant(\n                t.flatten(),\n                shape=shape,\n                dtype=dtypes.float32))\n      c = array_ops.concat(inp_tensors, 0)\n      output_shape = [9, 10, 2]\n      grad_inp = np.random.rand(*output_shape).astype(\"f\")\n      grad_tensor = constant_op.constant(\n          grad_inp.flatten(), shape=output_shape)\n      grad = gradients_impl.gradients([c], inp_tensors, [grad_tensor])\n      concated_grad = array_ops.concat(grad, 0)\n      result = self.evaluate(concated_grad)\n\n    self.assertAllEqual(result, grad_inp)\n\n  @test_util.run_deprecated_v1\n  def testGradientsLastDim(self):\n    # Test both positive and negative concat axis.\n    # -1 and 2 correspond to the same axis for 3-dimensional tensors.\n    for axis in [-1, 2]:\n      with test_util.use_gpu():\n        inp = []\n        inp_tensors = []\n        for x in [1, 2, 6]:\n          shape = [10, 2, x]\n          t = np.random.rand(*shape).astype(\"f\")\n          inp.append(t)\n          inp_tensors.append(\n              constant_op.constant(\n                  t.flatten(),\n                  shape=shape,\n                  dtype=dtypes.float32))\n        c = array_ops.concat(inp_tensors, 2)\n        output_shape = [10, 2, 9]\n        grad_inp = np.random.rand(*output_shape).astype(\"f\")\n        grad_tensor = constant_op.constant(\n            grad_inp.flatten(), shape=output_shape)\n        grad = gradients_impl.gradients([c], inp_tensors, [grad_tensor])\n        concated_grad = array_ops.concat(grad, axis)\n        result = self.evaluate(concated_grad)\n\n    self.assertAllEqual(result, grad_inp)\n\n  def _RunAndVerifyGradientsRandom(self):\n    # Random dims of rank 5\n    input_shape = np.random.randint(1, 5, size=5)\n    # Random number of tensors\n    num_tensors = np.random.randint(12, 20)\n    # Random dim to concat on\n    concat_dim = np.random.randint(5)\n    concat_dim_sizes = np.random.randint(1, 5, size=num_tensors)\n    with test_util.use_gpu():\n      inp = []\n      inp_tensors = []\n      for x in concat_dim_sizes:\n        shape = input_shape\n        shape[concat_dim] = x\n        t = np.random.rand(*shape).astype(\"f\")\n        inp.append(t)\n        inp_tensors.append(\n            constant_op.constant(t.flatten(), shape=shape,\n                                 dtype=dtypes.float32))\n      c = array_ops.concat(inp_tensors, concat_dim)\n      output_shape = input_shape\n      output_shape[concat_dim] = concat_dim_sizes.sum()\n      grad_inp = np.random.rand(*output_shape).astype(\"f\")\n      grad_tensor = constant_op.constant(grad_inp.flatten(), shape=output_shape)\n      grad = gradients_impl.gradients([c], inp_tensors, [grad_tensor])\n      concated_grad = array_ops.concat(grad, concat_dim)\n      result = self.evaluate(concated_grad)\n\n    self.assertAllEqual(result, grad_inp)\n\n  @test_util.run_deprecated_v1\n  def testGradientsRandom(self):\n    for _ in range(5):\n      self._RunAndVerifyGradientsRandom()\n\n  @test_util.run_deprecated_v1\n  def testGradientWithUnknownInputDim(self):\n    with self.session():\n      x = array_ops.placeholder(dtypes.float32)\n      y = array_ops.placeholder(dtypes.float32)\n      c = array_ops.concat([x, y], 2)\n\n      output_shape = [10, 2, 9]\n      grad_inp = np.random.rand(*output_shape).astype(\"f\")\n      grad_tensor = constant_op.constant(\n          grad_inp.flatten(), shape=output_shape)\n\n      grad = gradients_impl.gradients([c], [x, y], [grad_tensor])\n      concated_grad = array_ops.concat(grad, 2)\n      params = {\n          x: np.random.rand(10, 2, 3).astype(\"f\"),\n          y: np.random.rand(10, 2, 6).astype(\"f\")\n      }\n      result = concated_grad.eval(feed_dict=params)\n\n      self.assertAllEqual(result, grad_inp)\n\n  @test_util.run_deprecated_v1\n  def testShapeError(self):\n    # Rank doesn't match.\n    with self.assertRaises(ValueError):\n      array_ops.concat(\n          [constant_op.constant(10.0, shape=[4, 4, 4, 4]),\n           constant_op.constant(20.0, shape=[4, 4, 4])\n          ], 1)\n\n    # Dimensions don't match in a non-concat dim.\n    with self.assertRaises(ValueError):\n      array_ops.concat(\n          [constant_op.constant(10.0, shape=[1, 2, 1]),\n           constant_op.constant(20.0, shape=[3, 2, 1])\n          ], 1)\n\n    # concat_dim out of range.\n    with self.assertRaises(ValueError):\n      array_ops.concat(\n          [constant_op.constant(10.0, shape=[4, 4, 4]),\n           constant_op.constant(20.0, shape=[4, 4, 4])\n          ], 3)\n\n    # concat_dim out of range\n    with self.assertRaises(ValueError):\n      array_ops.concat(\n          [constant_op.constant(10.0, shape=[4, 4, 4]),\n           constant_op.constant(20.0, shape=[4, 4, 4])\n          ], -4)\n\n  @test_util.run_deprecated_v1\n  def testShapeWithUnknownConcatDim(self):\n    p1 = array_ops.placeholder(dtypes.float32)\n    c1 = constant_op.constant(10.0, shape=[4, 4, 4, 4])\n    p2 = array_ops.placeholder(dtypes.float32)\n    c2 = constant_op.constant(20.0, shape=[4, 4, 4, 4])\n    dim = array_ops.placeholder(dtypes.int32)\n    concat = array_ops.concat([p1, c1, p2, c2], dim)\n    self.assertEqual(4, concat.get_shape().ndims)\n\n    # All dimensions unknown.\n    concat2 = array_ops.concat([p1, p2], dim)\n    self.assertEqual(None, concat2.get_shape())\n\n    # Rank doesn't match.\n    c3 = constant_op.constant(30.0, shape=[4, 4, 4])\n    with self.assertRaises(ValueError):\n      array_ops.concat([p1, c1, p2, c3], dim)\n\n  @test_util.run_deprecated_v1\n  def testZeroSize(self):\n    # Verify that concat doesn't crash and burn for zero size inputs\n    np.random.seed(7)\n    with test_util.use_gpu():\n      for shape0 in (), (2,):\n        axis = len(shape0)\n        for shape1 in (), (3,):\n          for n0 in 0, 1, 2:\n            for n1 in 0, 1, 2:\n              x0 = np.random.randn(*(shape0 + (n0,) + shape1))\n              x1 = np.random.randn(*(shape0 + (n1,) + shape1))\n              correct = np.concatenate([x0, x1], axis=axis)\n              # TODO(irving): Make tf.concat handle map, then drop list().\n              xs = list(map(constant_op.constant, [x0, x1]))\n              c = array_ops.concat(xs, axis)\n              self.assertAllEqual(self.evaluate(c), correct)\n              # Check gradients\n              dc = np.random.randn(*c.get_shape().as_list())\n              dxs = self.evaluate(gradients_impl.gradients(c, xs, dc))\n              self.assertAllEqual(dc, np.concatenate(dxs, axis=axis))\n\n  @test_util.run_deprecated_v1\n  def testTensorConcatDim0Grad(self):\n    x_shapes = [[20, 7, 3], [10, 7, 3], [14, 7, 3]]\n    output_shape = [44, 7, 3]\n    x_vals = [\n        np.random.random_sample(x_shape).astype(np.float64)\n        for x_shape in x_shapes\n    ]\n    with self.cached_session():\n      xs = [constant_op.constant(x_val) for x_val in x_vals]\n      output = array_ops.concat(xs, 0)\n      err = gradient_checker.compute_gradient_error(xs, x_shapes, output,\n                                                    output_shape)\n    self.assertLess(err, 1e-11)\n\n  @test_util.run_deprecated_v1\n  def testTensorConcatDim1Grad(self):\n    x_shapes = [[20, 7, 3], [20, 3, 3], [20, 1, 3]]\n    output_shape = [20, 11, 3]\n    x_vals = [\n        np.random.random_sample(x_shape).astype(np.float64)\n        for x_shape in x_shapes\n    ]\n    with self.cached_session():\n      xs = [constant_op.constant(x_val) for x_val in x_vals]\n      output = array_ops.concat(xs, 1)\n      err = gradient_checker.compute_gradient_error(xs, x_shapes, output,\n                                                    output_shape)\n    self.assertLess(err, 1e-11)\n\n  @test_util.run_deprecated_v1\n  def testIndexedSlicesConcatDim0Grad(self):\n    x_shapes = [[20, 7, 3], [10, 7, 3], [14, 7, 3]]\n    output_shape = [4, 7, 3]\n    x_vals = [\n        np.random.random_sample(x_shape).astype(np.float64)\n        for x_shape in x_shapes\n    ]\n    with self.cached_session():\n      xs = [constant_op.constant(x_val) for x_val in x_vals]\n      x_concat = array_ops.concat(xs, 0)\n      output = array_ops.gather(x_concat, [1, 2, 0, 5])\n      err = gradient_checker.compute_gradient_error(xs, x_shapes, output,\n                                                    output_shape)\n    self.assertLess(err, 1e-11)\n\n  @test_util.run_deprecated_v1\n  def testIndexedSlicesConcatDim1Grad(self):\n    x_shapes = [[20, 7, 3], [20, 3, 3], [20, 1, 3]]\n    output_shape = [4, 11, 3]\n    x_vals = [\n        np.random.random_sample(x_shape).astype(np.float64)\n        for x_shape in x_shapes\n    ]\n    with self.cached_session():\n      xs = [constant_op.constant(x_val) for x_val in x_vals]\n      x_concat = array_ops.concat(xs, 1)\n      output = array_ops.gather(x_concat, [1, 2, 0, 5])\n      err = gradient_checker.compute_gradient_error(xs, x_shapes, output,\n                                                    output_shape)\n    self.assertLess(err, 1e-11)\n\n  @test_util.run_deprecated_v1\n  def testIndexedSlicesConcatDim2Grad(self):\n    x_shapes = [[20, 7, 3], [20, 7, 1], [20, 7, 2]]\n    output_shape = [4, 7, 6]\n    x_vals = [\n        np.random.random_sample(x_shape).astype(np.float64)\n        for x_shape in x_shapes\n    ]\n    with self.cached_session():\n      xs = [constant_op.constant(x_val) for x_val in x_vals]\n      x_concat = array_ops.concat(xs, 2)\n      output = array_ops.gather(x_concat, [1, 2, 0, 5])\n      err = gradient_checker.compute_gradient_error(xs, x_shapes, output,\n                                                    output_shape)\n    self.assertLess(err, 1e-11)\n\n  @test_util.run_deprecated_v1\n  def testIndexedSlicesConcatDim1Grad_UnknownInputDim(self):\n    x_shapes = [[20, 7, 3], [20, 3, 3], [20, 1, 3]]\n    output_shape = [4, 11, 3]\n    with self.cached_session():\n      x_1 = array_ops.placeholder(dtypes.float64)\n      x_2 = array_ops.placeholder(dtypes.float64)\n      x_3 = array_ops.placeholder(dtypes.float64)\n      xs = [x_1, x_2, x_3]\n\n      x_concat = array_ops.concat(xs, 1)\n      output = array_ops.gather(x_concat, [1, 2, 0, 5])\n      params = {\n          x_1: np.random.random_sample(x_shapes[0]).astype(np.float64),\n          x_2: np.random.random_sample(x_shapes[1]).astype(np.float64),\n          x_3: np.random.random_sample(x_shapes[2]).astype(np.float64)\n      }\n      err = gradient_checker.compute_gradient_error(xs, x_shapes, output,\n                                                    output_shape,\n                                                    extra_feed_dict=params)\n    self.assertLess(err, 1e-11)\n\n  def testConcatTuple(self):\n    c1 = np.random.rand(4, 4)\n    c2 = np.random.rand(4, 4)\n    concat_list_t = array_ops.concat([c1, c2], 0)\n    concat_tuple_t = array_ops.concat((c1, c2), 0)\n    self.assertAllEqual(\n        self.evaluate(concat_list_t), self.evaluate(concat_tuple_t))\n\n  @test_util.run_deprecated_v1\n  def testConcatNoScalars(self):\n    scalar = constant_op.constant(7)\n    dim = array_ops.placeholder(dtypes.int32)\n    with self.assertRaisesRegex(\n        ValueError, r\"Can't concatenate scalars \\(use tf\\.stack instead\\)\"):\n      array_ops.concat([scalar, scalar, scalar], dim)\n\n  # important as gpu implementation could fail if\n  # shared memory is not large for all the inputs\n  @test_util.run_deprecated_v1\n  def testConcatLargeNumberOfTensors(self):\n    with self.session():\n      for concat_dim in range(2):\n        params = {}\n        p = []\n        shape = np.array([7, 13])\n        if test.is_gpu_available():\n          num_tensors = 5000\n        else:\n          num_tensors = 500\n        for i in np.arange(num_tensors):\n          input_shape = shape\n          placeholder = array_ops.placeholder(dtypes.float32, shape=input_shape)\n          p.append(placeholder)\n\n          params[placeholder] = np.random.rand(*input_shape).astype(np.float32)\n\n        concat_inputs = p\n        c = array_ops.concat(concat_inputs, concat_dim)\n        result = c.eval(feed_dict=params)\n\n        self.assertEqual(result.shape, c.get_shape())\n        cur_offset = 0\n\n        for i in np.arange(num_tensors):\n          # The index into the result is the ':' along all dimensions\n          # except the concat_dim. slice(0, size) is used for ':', and\n          # a list of slices is used to index into result.\n          index = [slice(0, params[p[i]].shape[j]) for j in np.arange(2)]\n          index[concat_dim] = slice(cur_offset,\n                                    cur_offset + params[p[i]].shape[concat_dim])\n          cur_offset += params[p[i]].shape[concat_dim]\n          self.assertAllEqual(result[index], params[p[i]])\n\n  def testConcatEmpty(self):\n    with test_util.use_gpu():\n      t1 = []\n      t2 = []\n      output = gen_array_ops.concat_v2([t1, t2], 0)\n      self.assertFalse(self.evaluate(output))  # Checks that output is empty\n\n  @test_util.run_deprecated_v1\n  def testConcatInvalidAxis(self):\n    with self.assertRaises(ValueError):\n      with test_util.use_gpu():\n        t1 = [1]\n        t2 = [2]\n        gen_array_ops.concat_v2([t1, t2], 1).eval()\n\n  def testConcatInvalidAxisInTfFunction(self):\n\n    @def_function.function\n    def concat_wrapper():\n      y = gen_array_ops.concat_v2(\n          values=[[1, 2, 3], [4, 5, 6]], axis=0xb500005b)\n      return y\n\n    with self.assertRaises(ValueError):\n      concat_wrapper()\n\n  def testConcatNegativeAxis(self):\n    with test_util.use_gpu():\n      t1 = [[1, 2, 3], [4, 5, 6]]\n      t2 = [[7, 8, 9], [10, 11, 12]]\n\n      c = gen_array_ops.concat_v2([t1, t2], -2)\n      self.assertEqual([4, 3], c.get_shape().as_list())\n      output = self.evaluate(c)\n      self.assertAllEqual([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]],\n                          output)\n\n      c = gen_array_ops.concat_v2([t1, t2], -1)\n      self.assertEqual([2, 6], c.get_shape().as_list())\n      output = self.evaluate(c)\n      self.assertAllEqual([[1, 2, 3, 7, 8, 9], [4, 5, 6, 10, 11, 12]], output)\n\n  def _testGradientsForAxis(\n      self, inp_tensors, axis, output_shape, feed_dict=None):\n    with self.cached_session():\n      c = array_ops.concat(inp_tensors, axis)\n      grad_inp = np.random.rand(*output_shape).astype(\"f\")\n      grad_tensor = constant_op.constant(\n          grad_inp.flatten(), shape=output_shape)\n      grad = gradients_impl.gradients([c], inp_tensors, [grad_tensor])\n      concated_grad = array_ops.concat(grad, axis)\n      result = concated_grad.eval(feed_dict=feed_dict)\n      self.assertAllEqual(result, grad_inp)\n\n  def _testIndexedSlicesGradientsForAxis(\n      self, inp_tensors, axis, output_shape, gather_indexes, feed_dict=None):\n    with self.cached_session():\n      c = array_ops.gather(\n          array_ops.concat(inp_tensors, axis), gather_indexes)\n      grad_inp = np.random.rand(*output_shape).astype(\"f\")\n      grad_tensor = constant_op.constant(\n          grad_inp.flatten(), shape=output_shape)\n      grad = gradients_impl.gradients([c], inp_tensors, [grad_tensor])\n      concated_grad = array_ops.gather(\n          array_ops.concat(grad, axis), gather_indexes)\n      result = concated_grad.eval(feed_dict=feed_dict)\n      self.assertAllEqual(result, grad_inp)\n\n  @test_util.run_deprecated_v1\n  def testGradientsNegativeAxis(self):\n    x1 = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]\n    x2 = [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]]\n    inp_tensors = [constant_op.constant(x1, shape=(2, 3), dtype=dtypes.float32),\n                   constant_op.constant(x2, shape=(2, 3), dtype=dtypes.float32)]\n\n    # Test concat gradient with axis == -2\n    self._testGradientsForAxis(inp_tensors, -2, output_shape=[4, 3])\n\n    # Test concat gradient with unknown-shape tensors.\n    x1_placeholder = array_ops.placeholder(dtypes.float32)\n    x2_placeholder = array_ops.placeholder(dtypes.float32)\n    inp_tensors_placeholders = [x1_placeholder, x2_placeholder]\n    feed_dict = {x1_placeholder: x1, x2_placeholder: x2}\n    self._testGradientsForAxis(\n        inp_tensors_placeholders, -1, output_shape=[2, 6], feed_dict=feed_dict)\n\n    # Test IndexedSlices concat gradient.\n    self._testIndexedSlicesGradientsForAxis(\n        inp_tensors, -2, output_shape=[2, 3], gather_indexes=[2, 0])\n\n    # We don't support calculating IndexedSlices concat gradient for\n    # negative indexes when rank is not known.\n    with self.assertRaises(ValueError):\n      self._testIndexedSlicesGradientsForAxis(\n          inp_tensors_placeholders, -2, output_shape=[2, 3],\n          gather_indexes=[2, 0], feed_dict=feed_dict)\n\n  def testConcatDtype(self):\n    for dtype in [dtypes.int32, dtypes.int64, dtypes.uint32, dtypes.uint64]:\n      with test_util.use_gpu():\n        t1 = constant_op.constant([[1, 2, 3], [4, 5, 6]], dtype=dtype)\n        t2 = constant_op.constant([[7, 8, 9], [10, 11, 12]], dtype=dtype)\n\n        c = gen_array_ops.concat_v2([t1, t2], 1)\n        self.assertEqual([2, 6], c.get_shape().as_list())\n        output = self.evaluate(c)\n        self.assertAllEqual([[1, 2, 3, 7, 8, 9], [4, 5, 6, 10, 11, 12]], output)\n\n  def testConcatAxisType(self):\n    for dtype in [dtypes.int32, dtypes.int64]:\n      with test_util.use_gpu():\n        t1 = [[1, 2, 3], [4, 5, 6]]\n        t2 = [[7, 8, 9], [10, 11, 12]]\n\n        c = gen_array_ops.concat_v2([t1, t2],\n                                    constant_op.constant(1, dtype=dtype))\n        self.assertEqual([2, 6], c.get_shape().as_list())\n        output = self.evaluate(c)\n        self.assertAllEqual([[1, 2, 3, 7, 8, 9], [4, 5, 6, 10, 11, 12]], output)\n\n\nclass ConcatOffsetTest(test.TestCase):\n\n  def testBasic(self):\n    with test_util.use_gpu():\n      cdim = constant_op.constant(1, dtypes.int32)\n      s0 = constant_op.constant([2, 3, 5], dtypes.int32)\n      s1 = constant_op.constant([2, 7, 5], dtypes.int32)\n      s2 = constant_op.constant([2, 20, 5], dtypes.int32)\n      off = gen_array_ops.concat_offset(cdim, [s0, s1, s2])\n      ans = self.evaluate(off)\n      self.assertAllEqual(ans, [[0, 0, 0], [0, 3, 0], [0, 10, 0]])\n\n  @test_util.run_deprecated_v1\n  def testNotVector(self):\n    cdim = constant_op.constant(1, dtypes.int32)\n    s0 = constant_op.constant([[2, 3, 5]], dtypes.int32)\n    s1 = constant_op.constant([[2, 7, 5]], dtypes.int32)\n    off = gen_array_ops.concat_offset(cdim, [s0, s1])\n    with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n                                r\"should be a vector\"):\n      self.evaluate(off)\n\n  @test_util.run_deprecated_v1\n  def testConcatDimOutOfRange(self):\n    cdim = constant_op.constant(4, dtypes.int32)\n    s0 = constant_op.constant([2, 3, 5], dtypes.int32)\n    s1 = constant_op.constant([2, 7, 5], dtypes.int32)\n    off = gen_array_ops.concat_offset(cdim, [s0, s1])\n    with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n                                r\"Concat dim is out of range: 4 vs. 3\"):\n      self.evaluate(off)\n\n  @test_util.run_deprecated_v1\n  def testDimMismatch(self):\n    cdim = constant_op.constant(1, dtypes.int32)\n    s0 = constant_op.constant([2, 3, 5], dtypes.int32)\n    s1 = constant_op.constant([2, 7, 5, 10], dtypes.int32)\n    off = gen_array_ops.concat_offset(cdim, [s0, s1])\n    with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n                                r\"should contain 3 elem\"):\n      self.evaluate(off)\n\n  @test_util.run_deprecated_v1\n  def testSizeMismatch(self):\n    cdim = constant_op.constant(1, dtypes.int32)\n    s0 = constant_op.constant([2, 3, 5], dtypes.int32)\n    s1 = constant_op.constant([2, 7, 10], dtypes.int32)\n    off = gen_array_ops.concat_offset(cdim, [s0, s1])\n    with self.assertRaisesRegex(\n        errors_impl.InvalidArgumentError,\n        r\"All dimensions except 1 must match. Input 1 has shape \\[2 7 10\\] \"\n        r\"and doesn't match input 0 with shape \\[2 3 5\\].\"):\n      self.evaluate(off)\n\n  def testNegativeDim(self):\n    with test_util.use_gpu():\n      cdim = constant_op.constant(-2, dtypes.int32)\n      s0 = constant_op.constant([2, 3, 5], dtypes.int32)\n      s1 = constant_op.constant([2, 7, 5], dtypes.int32)\n      s2 = constant_op.constant([2, 20, 5], dtypes.int32)\n      off = gen_array_ops.concat_offset(cdim, [s0, s1, s2])\n      ans = self.evaluate(off)\n      self.assertAllEqual(ans, [[0, 0, 0], [0, 3, 0], [0, 10, 0]])\n\n      cdim = constant_op.constant(-3, dtypes.int32)\n      s0 = constant_op.constant([2, 3, 5], dtypes.int32)\n      s1 = constant_op.constant([1, 3, 5], dtypes.int32)\n      s2 = constant_op.constant([3, 3, 5], dtypes.int32)\n      off = gen_array_ops.concat_offset(cdim, [s0, s1, s2])\n      ans = self.evaluate(off)\n      self.assertAllEqual(ans, [[0, 0, 0], [2, 0, 0], [3, 0, 0]])\n\n  def testCreateMemDecBlockedFormat(self):\n    \"\"\"Try to create the mkl concat operation\n\n    when one of the input's memory descriptor is in blocked format\n    \"\"\"\n    if test_util.IsMklEnabled():\n      s0 = np.ones((1, 8188, 4092, 1), dtype=np.uint8).astype(np.float32)\n      s1 = array_ops.strided_slice(\n          s0, [0, 1, 1, 0], [0, -1, -1, 0], [1, 1, 1, 1],\n          begin_mask=9,\n          end_mask=9)\n      s2 = array_ops.slice(s1, [0, 0, 0, 0], [-1, -1, -1, 1])\n      s3_1 = array_ops.slice(s2, [0, 4, 4, 0], [-1, 8178, 4082, 1])\n      s3_2 = array_ops.slice(s2, [0, 4, 4, 0], [-1, 8178, 4082, 1])\n      filter4_1 = constant_op.constant([[[[1.18, -0.51]]]])\n      s4_1 = nn_ops.conv2d(\n          s3_1, filter4_1, strides=[1, 1, 1, 1], padding=\"VALID\")\n      filter4_2 = constant_op.constant([[[[1.38, -0.11]]]])\n      s4_2 = nn_ops.conv2d(\n          s3_2, filter4_2, strides=[1, 1, 1, 1], padding=\"VALID\")\n      s5_1 = array_ops.slice(s4_1, [0, 6, 6, 0], [-1, 1, 1, -1])\n      s5_2 = array_ops.slice(s4_2, [0, 6, 6, 0], [-1, 1, 1, -1])\n      x_concat = array_ops.concat([s5_1, s5_2], 3)\n      self.evaluate(\n          x_concat\n      )  # This test is only meant to check the creation is not crashed\n\n\nif __name__ == \"__main__\":\n  test.main()"