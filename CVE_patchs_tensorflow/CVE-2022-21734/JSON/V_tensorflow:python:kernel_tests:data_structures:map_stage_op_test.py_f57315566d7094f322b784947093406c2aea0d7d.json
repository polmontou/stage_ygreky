"# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import data_flow_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.platform import test\n\nTIMEOUT = 1\n\n\nclass MapStageTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testSimple(self):\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.float32)\n        pi = array_ops.placeholder(dtypes.int64)\n        gi = array_ops.placeholder(dtypes.int64)\n        v = 2. * (array_ops.zeros([128, 128]) + x)\n      with ops.device(test.gpu_device_name()):\n        stager = data_flow_ops.MapStagingArea([dtypes.float32])\n        stage = stager.put(pi, [v], [0])\n        k, y = stager.get(gi)\n        y = math_ops.reduce_max(math_ops.matmul(y, y))\n\n    G.finalize()\n\n    with self.session(graph=G) as sess:\n      sess.run(stage, feed_dict={x: -1, pi: 0})\n      for i in range(10):\n        _, yval = sess.run([stage, y], feed_dict={x: i, pi: i + 1, gi: i})\n        self.assertAllClose(4 * (i - 1) * (i - 1) * 128, yval, rtol=1e-4)\n\n  @test_util.run_deprecated_v1\n  def testMultiple(self):\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.float32)\n        pi = array_ops.placeholder(dtypes.int64)\n        gi = array_ops.placeholder(dtypes.int64)\n        v = 2. * (array_ops.zeros([128, 128]) + x)\n      with ops.device(test.gpu_device_name()):\n        stager = data_flow_ops.MapStagingArea([dtypes.float32, dtypes.float32])\n        stage = stager.put(pi, [x, v], [0, 1])\n        k, (z, y) = stager.get(gi)\n        y = math_ops.reduce_max(z * math_ops.matmul(y, y))\n\n    G.finalize()\n\n    with self.session(graph=G) as sess:\n      sess.run(stage, feed_dict={x: -1, pi: 0})\n      for i in range(10):\n        _, yval = sess.run([stage, y], feed_dict={x: i, pi: i + 1, gi: i})\n        self.assertAllClose(\n            4 * (i - 1) * (i - 1) * (i - 1) * 128, yval, rtol=1e-4)\n\n  @test_util.run_deprecated_v1\n  def testDictionary(self):\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.float32)\n        pi = array_ops.placeholder(dtypes.int64)\n        gi = array_ops.placeholder(dtypes.int64)\n        v = 2. * (array_ops.zeros([128, 128]) + x)\n      with ops.device(test.gpu_device_name()):\n        stager = data_flow_ops.MapStagingArea(\n            [dtypes.float32, dtypes.float32],\n            shapes=[[], [128, 128]],\n            names=['x', 'v'])\n        stage = stager.put(pi, {'x': x, 'v': v})\n        key, ret = stager.get(gi)\n        z = ret['x']\n        y = ret['v']\n        y = math_ops.reduce_max(z * math_ops.matmul(y, y))\n\n    G.finalize()\n\n    with self.session(graph=G) as sess:\n      sess.run(stage, feed_dict={x: -1, pi: 0})\n      for i in range(10):\n        _, yval = sess.run([stage, y], feed_dict={x: i, pi: i + 1, gi: i})\n        self.assertAllClose(\n            4 * (i - 1) * (i - 1) * (i - 1) * 128, yval, rtol=1e-4)\n\n  def testColocation(self):\n    gpu_dev = test.gpu_device_name()\n\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.float32)\n        v = 2. * (array_ops.zeros([128, 128]) + x)\n      with ops.device(gpu_dev):\n        stager = data_flow_ops.MapStagingArea([dtypes.float32])\n        y = stager.put(1, [v], [0])\n        expected_name = gpu_dev if 'gpu' not in gpu_dev else '/device:GPU:0'\n        self.assertEqual(y.device, expected_name)\n      with ops.device('/cpu:0'):\n        _, x = stager.get(1)\n        y = stager.peek(1)[0]\n        _, z = stager.get()\n        self.assertEqual(x[0].device, '/device:CPU:0')\n        self.assertEqual(y.device, '/device:CPU:0')\n        self.assertEqual(z[0].device, '/device:CPU:0')\n\n    G.finalize()\n\n  @test_util.run_deprecated_v1\n  def testPeek(self):\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.int32, name='x')\n        pi = array_ops.placeholder(dtypes.int64)\n        gi = array_ops.placeholder(dtypes.int64)\n        p = array_ops.placeholder(dtypes.int32, name='p')\n      with ops.device(test.gpu_device_name()):\n        stager = data_flow_ops.MapStagingArea(\n            [\n                dtypes.int32,\n            ], shapes=[[]])\n        stage = stager.put(pi, [x], [0])\n        peek = stager.peek(gi)\n        size = stager.size()\n\n    G.finalize()\n\n    n = 10\n\n    with self.session(graph=G) as sess:\n      for i in range(n):\n        sess.run(stage, feed_dict={x: i, pi: i})\n\n      for i in range(n):\n        self.assertTrue(sess.run(peek, feed_dict={gi: i})[0] == i)\n\n      self.assertTrue(sess.run(size) == 10)\n\n  @test_util.run_deprecated_v1\n  def testSizeAndClear(self):\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.float32, name='x')\n        pi = array_ops.placeholder(dtypes.int64)\n        gi = array_ops.placeholder(dtypes.int64)\n        v = 2. * (array_ops.zeros([128, 128]) + x)\n      with ops.device(test.gpu_device_name()):\n        stager = data_flow_ops.MapStagingArea(\n            [dtypes.float32, dtypes.float32],\n            shapes=[[], [128, 128]],\n            names=['x', 'v'])\n        stage = stager.put(pi, {'x': x, 'v': v})\n        size = stager.size()\n        clear = stager.clear()\n\n    G.finalize()\n\n    with self.session(graph=G) as sess:\n      sess.run(stage, feed_dict={x: -1, pi: 3})\n      self.assertEqual(sess.run(size), 1)\n      sess.run(stage, feed_dict={x: -1, pi: 1})\n      self.assertEqual(sess.run(size), 2)\n      sess.run(clear)\n      self.assertEqual(sess.run(size), 0)\n\n  @test_util.run_deprecated_v1\n  def testCapacity(self):\n    capacity = 3\n\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.int32, name='x')\n        pi = array_ops.placeholder(dtypes.int64, name='pi')\n        gi = array_ops.placeholder(dtypes.int64, name='gi')\n      with ops.device(test.gpu_device_name()):\n        stager = data_flow_ops.MapStagingArea(\n            [\n                dtypes.int32,\n            ], capacity=capacity, shapes=[[]])\n\n      stage = stager.put(pi, [x], [0])\n      get = stager.get()\n      size = stager.size()\n\n    G.finalize()\n\n    from six.moves import queue as Queue\n    import threading\n\n    queue = Queue.Queue()\n    n = 8\n\n    with self.session(graph=G) as sess:\n      # Stage data in a separate thread which will block\n      # when it hits the staging area's capacity and thus\n      # not fill the queue with n tokens\n      def thread_run():\n        for i in range(n):\n          sess.run(stage, feed_dict={x: i, pi: i})\n          queue.put(0)\n\n      t = threading.Thread(target=thread_run)\n      t.daemon = True\n      t.start()\n\n      # Get tokens from the queue until a timeout occurs\n      try:\n        for i in range(n):\n          queue.get(timeout=TIMEOUT)\n      except Queue.Empty:\n        pass\n\n      # Should've timed out on the iteration 'capacity'\n      if not i == capacity:\n        self.fail(\"Expected to timeout on iteration '{}' \"\n                  \"but instead timed out on iteration '{}' \"\n                  \"Staging Area size is '{}' and configured \"\n                  \"capacity is '{}'.\".format(capacity, i, sess.run(size),\n                                             capacity))\n\n      # Should have capacity elements in the staging area\n      self.assertTrue(sess.run(size) == capacity)\n\n      # Clear the staging area completely\n      for i in range(n):\n        sess.run(get)\n\n      self.assertTrue(sess.run(size) == 0)\n\n  @test_util.run_deprecated_v1\n  def testMemoryLimit(self):\n    memory_limit = 512 * 1024  # 512K\n    chunk = 200 * 1024  # 256K\n    capacity = memory_limit // chunk\n\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.uint8, name='x')\n        pi = array_ops.placeholder(dtypes.int64, name='pi')\n        gi = array_ops.placeholder(dtypes.int64, name='gi')\n      with ops.device(test.gpu_device_name()):\n        stager = data_flow_ops.MapStagingArea(\n            [dtypes.uint8], memory_limit=memory_limit, shapes=[[]])\n        stage = stager.put(pi, [x], [0])\n        get = stager.get()\n        size = stager.size()\n\n    G.finalize()\n\n    from six.moves import queue as Queue\n    import threading\n    import numpy as np\n\n    queue = Queue.Queue()\n    n = 8\n\n    with self.session(graph=G) as sess:\n      # Stage data in a separate thread which will block\n      # when it hits the staging area's capacity and thus\n      # not fill the queue with n tokens\n      def thread_run():\n        for i in range(n):\n          data = np.full(chunk, i, dtype=np.uint8)\n          sess.run(stage, feed_dict={x: data, pi: i})\n          queue.put(0)\n\n      t = threading.Thread(target=thread_run)\n      t.daemon = True\n      t.start()\n\n      # Get tokens from the queue until a timeout occurs\n      try:\n        for i in range(n):\n          queue.get(timeout=TIMEOUT)\n      except Queue.Empty:\n        pass\n\n      # Should've timed out on the iteration 'capacity'\n      if not i == capacity:\n        self.fail(\"Expected to timeout on iteration '{}' \"\n                  \"but instead timed out on iteration '{}' \"\n                  \"Staging Area size is '{}' and configured \"\n                  \"capacity is '{}'.\".format(capacity, i, sess.run(size),\n                                             capacity))\n\n      # Should have capacity elements in the staging area\n      self.assertTrue(sess.run(size) == capacity)\n\n      # Clear the staging area completely\n      for i in range(n):\n        sess.run(get)\n\n      self.assertTrue(sess.run(size) == 0)\n\n  @test_util.run_deprecated_v1\n  def testOrdering(self):\n    import six\n    import random\n\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.int32, name='x')\n        pi = array_ops.placeholder(dtypes.int64, name='pi')\n        gi = array_ops.placeholder(dtypes.int64, name='gi')\n      with ops.device(test.gpu_device_name()):\n        stager = data_flow_ops.MapStagingArea(\n            [\n                dtypes.int32,\n            ], shapes=[[]], ordered=True)\n        stage = stager.put(pi, [x], [0])\n        get = stager.get()\n        size = stager.size()\n\n    G.finalize()\n\n    n = 10\n\n    with self.session(graph=G) as sess:\n      # Keys n-1..0\n      keys = list(reversed(six.moves.range(n)))\n\n      for i in keys:\n        sess.run(stage, feed_dict={pi: i, x: i})\n\n      self.assertTrue(sess.run(size) == n)\n\n      # Check that key, values come out in ascending order\n      for i, k in enumerate(reversed(keys)):\n        get_key, values = sess.run(get)\n        self.assertTrue(i == k == get_key == values)\n\n      self.assertTrue(sess.run(size) == 0)\n\n  @test_util.run_deprecated_v1\n  def testPartialDictInsert(self):\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.float32)\n        f = array_ops.placeholder(dtypes.float32)\n        v = array_ops.placeholder(dtypes.float32)\n        pi = array_ops.placeholder(dtypes.int64)\n        gi = array_ops.placeholder(dtypes.int64)\n      with ops.device(test.gpu_device_name()):\n        # Test barrier with dictionary\n        stager = data_flow_ops.MapStagingArea(\n            [dtypes.float32, dtypes.float32, dtypes.float32],\n            names=['x', 'v', 'f'])\n        stage_xf = stager.put(pi, {'x': x, 'f': f})\n        stage_v = stager.put(pi, {'v': v})\n        key, ret = stager.get(gi)\n        size = stager.size()\n        isize = stager.incomplete_size()\n\n    G.finalize()\n\n    with self.session(graph=G) as sess:\n      # 0 complete and incomplete entries\n      self.assertTrue(sess.run([size, isize]) == [0, 0])\n      # Stage key 0, x and f tuple entries\n      sess.run(stage_xf, feed_dict={pi: 0, x: 1, f: 2})\n      self.assertTrue(sess.run([size, isize]) == [0, 1])\n      # Stage key 1, x and f tuple entries\n      sess.run(stage_xf, feed_dict={pi: 1, x: 1, f: 2})\n      self.assertTrue(sess.run([size, isize]) == [0, 2])\n\n      # Now complete key 0 with tuple entry v\n      sess.run(stage_v, feed_dict={pi: 0, v: 1})\n      # 1 complete and 1 incomplete entry\n      self.assertTrue(sess.run([size, isize]) == [1, 1])\n      # We can now obtain tuple associated with key 0\n      self.assertTrue(\n          sess.run([key, ret], feed_dict={\n              gi: 0\n          }) == [0, {\n              'x': 1,\n              'f': 2,\n              'v': 1\n          }])\n\n      # 0 complete and 1 incomplete entry\n      self.assertTrue(sess.run([size, isize]) == [0, 1])\n      # Now complete key 1 with tuple entry v\n      sess.run(stage_v, feed_dict={pi: 1, v: 3})\n      # We can now obtain tuple associated with key 1\n      self.assertTrue(\n          sess.run([key, ret], feed_dict={\n              gi: 1\n          }) == [1, {\n              'x': 1,\n              'f': 2,\n              'v': 3\n          }])\n\n  @test_util.run_deprecated_v1\n  def testPartialIndexInsert(self):\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.float32)\n        f = array_ops.placeholder(dtypes.float32)\n        v = array_ops.placeholder(dtypes.float32)\n        pi = array_ops.placeholder(dtypes.int64)\n        gi = array_ops.placeholder(dtypes.int64)\n      with ops.device(test.gpu_device_name()):\n        stager = data_flow_ops.MapStagingArea(\n            [dtypes.float32, dtypes.float32, dtypes.float32])\n        stage_xf = stager.put(pi, [x, f], [0, 2])\n        stage_v = stager.put(pi, [v], [1])\n        key, ret = stager.get(gi)\n        size = stager.size()\n        isize = stager.incomplete_size()\n\n    G.finalize()\n\n    with self.session(graph=G) as sess:\n      # 0 complete and incomplete entries\n      self.assertTrue(sess.run([size, isize]) == [0, 0])\n      # Stage key 0, x and f tuple entries\n      sess.run(stage_xf, feed_dict={pi: 0, x: 1, f: 2})\n      self.assertTrue(sess.run([size, isize]) == [0, 1])\n      # Stage key 1, x and f tuple entries\n      sess.run(stage_xf, feed_dict={pi: 1, x: 1, f: 2})\n      self.assertTrue(sess.run([size, isize]) == [0, 2])\n\n      # Now complete key 0 with tuple entry v\n      sess.run(stage_v, feed_dict={pi: 0, v: 1})\n      # 1 complete and 1 incomplete entry\n      self.assertTrue(sess.run([size, isize]) == [1, 1])\n      # We can now obtain tuple associated with key 0\n      self.assertTrue(sess.run([key, ret], feed_dict={gi: 0}) == [0, [1, 1, 2]])\n\n      # 0 complete and 1 incomplete entry\n      self.assertTrue(sess.run([size, isize]) == [0, 1])\n      # Now complete key 1 with tuple entry v\n      sess.run(stage_v, feed_dict={pi: 1, v: 3})\n      # We can now obtain tuple associated with key 1\n      self.assertTrue(sess.run([key, ret], feed_dict={gi: 1}) == [1, [1, 3, 2]])\n\n  @test_util.run_deprecated_v1\n  def testPartialDictGetsAndPeeks(self):\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.float32)\n        f = array_ops.placeholder(dtypes.float32)\n        v = array_ops.placeholder(dtypes.float32)\n        pi = array_ops.placeholder(dtypes.int64)\n        pei = array_ops.placeholder(dtypes.int64)\n        gi = array_ops.placeholder(dtypes.int64)\n      with ops.device(test.gpu_device_name()):\n        # Test barrier with dictionary\n        stager = data_flow_ops.MapStagingArea(\n            [dtypes.float32, dtypes.float32, dtypes.float32],\n            names=['x', 'v', 'f'])\n        stage_xf = stager.put(pi, {'x': x, 'f': f})\n        stage_v = stager.put(pi, {'v': v})\n        peek_xf = stager.peek(pei, ['x', 'f'])\n        peek_v = stager.peek(pei, ['v'])\n        key_xf, get_xf = stager.get(gi, ['x', 'f'])\n        key_v, get_v = stager.get(gi, ['v'])\n        pop_key_xf, pop_xf = stager.get(indices=['x', 'f'])\n        pop_key_v, pop_v = stager.get(pi, ['v'])\n        size = stager.size()\n        isize = stager.incomplete_size()\n\n    G.finalize()\n\n    with self.session(graph=G) as sess:\n      # 0 complete and incomplete entries\n      self.assertTrue(sess.run([size, isize]) == [0, 0])\n      # Stage key 0, x and f tuple entries\n      sess.run(stage_xf, feed_dict={pi: 0, x: 1, f: 2})\n      self.assertTrue(sess.run([size, isize]) == [0, 1])\n      # Stage key 1, x and f tuple entries\n      sess.run(stage_xf, feed_dict={pi: 1, x: 1, f: 2})\n      self.assertTrue(sess.run([size, isize]) == [0, 2])\n\n      # Now complete key 0 with tuple entry v\n      sess.run(stage_v, feed_dict={pi: 0, v: 1})\n      # 1 complete and 1 incomplete entry\n      self.assertTrue(sess.run([size, isize]) == [1, 1])\n\n      # We can now peek at 'x' and 'f' values associated with key 0\n      self.assertTrue(sess.run(peek_xf, feed_dict={pei: 0}) == {'x': 1, 'f': 2})\n      # Peek at 'v' value associated with key 0\n      self.assertTrue(sess.run(peek_v, feed_dict={pei: 0}) == {'v': 1})\n      # 1 complete and 1 incomplete entry\n      self.assertTrue(sess.run([size, isize]) == [1, 1])\n\n      # We can now obtain 'x' and 'f' values associated with key 0\n      self.assertTrue(\n          sess.run([key_xf, get_xf], feed_dict={\n              gi: 0\n          }) == [0, {\n              'x': 1,\n              'f': 2\n          }])\n      # Still have 1 complete and 1 incomplete entry\n      self.assertTrue(sess.run([size, isize]) == [1, 1])\n\n      # We can no longer get 'x' and 'f' from key 0\n      with self.assertRaises(errors.InvalidArgumentError) as cm:\n        sess.run([key_xf, get_xf], feed_dict={gi: 0})\n\n      exc_str = (\"Tensor at index '0' for key '0' \" 'has already been removed.')\n\n      self.assertTrue(exc_str in cm.exception.message)\n\n      # Obtain 'v' value associated with key 0\n      self.assertTrue(\n          sess.run([key_v, get_v], feed_dict={\n              gi: 0\n          }) == [0, {\n              'v': 1\n          }])\n      # 0 complete and 1 incomplete entry\n      self.assertTrue(sess.run([size, isize]) == [0, 1])\n\n      # Now complete key 1 with tuple entry v\n      sess.run(stage_v, feed_dict={pi: 1, v: 1})\n      # 1 complete and 1 incomplete entry\n      self.assertTrue(sess.run([size, isize]) == [1, 0])\n\n      # Pop without key to obtain 'x' and 'f' values associated with key 1\n      self.assertTrue(sess.run([pop_key_xf, pop_xf]) == [1, {'x': 1, 'f': 2}])\n      # still 1 complete and 1 incomplete entry\n      self.assertTrue(sess.run([size, isize]) == [1, 0])\n      # We can now obtain 'x' and 'f' values associated with key 1\n      self.assertTrue(\n          sess.run([pop_key_v, pop_v], feed_dict={\n              pi: 1\n          }) == [1, {\n              'v': 1\n          }])\n      # Nothing is left\n      self.assertTrue(sess.run([size, isize]) == [0, 0])\n\n  @test_util.run_deprecated_v1\n  def testPartialIndexGets(self):\n    with ops.Graph().as_default() as G:\n      with ops.device('/cpu:0'):\n        x = array_ops.placeholder(dtypes.float32)\n        f = array_ops.placeholder(dtypes.float32)\n        v = array_ops.placeholder(dtypes.float32)\n        pi = array_ops.placeholder(dtypes.int64)\n        pei = array_ops.placeholder(dtypes.int64)\n        gi = array_ops.placeholder(dtypes.int64)\n      with ops.device(test.gpu_device_name()):\n        # Test again with partial index gets\n        stager = data_flow_ops.MapStagingArea(\n            [dtypes.float32, dtypes.float32, dtypes.float32])\n        stage_xvf = stager.put(pi, [x, v, f], [0, 1, 2])\n        key_xf, get_xf = stager.get(gi, [0, 2])\n        key_v, get_v = stager.get(gi, [1])\n        size = stager.size()\n        isize = stager.incomplete_size()\n\n    G.finalize()\n\n    with self.session(graph=G) as sess:\n      # Stage complete tuple\n      sess.run(stage_xvf, feed_dict={pi: 0, x: 1, f: 2, v: 3})\n\n      self.assertTrue(sess.run([size, isize]) == [1, 0])\n\n      # Partial get using indices\n      self.assertTrue(\n          sess.run([key_xf, get_xf], feed_dict={\n              gi: 0\n          }) == [0, [1, 2]])\n\n      # Still some of key 0 left\n      self.assertTrue(sess.run([size, isize]) == [1, 0])\n\n      # Partial get of remaining index\n      self.assertTrue(sess.run([key_v, get_v], feed_dict={gi: 0}) == [0, [3]])\n\n      # All gone\n      self.assertTrue(sess.run([size, isize]) == [0, 0])\n\n\nif __name__ == '__main__':\n  test.main()"