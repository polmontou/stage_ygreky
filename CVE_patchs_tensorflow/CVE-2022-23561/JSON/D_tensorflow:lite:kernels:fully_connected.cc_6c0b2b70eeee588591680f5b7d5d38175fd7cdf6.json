"diff --git a/tensorflow/lite/kernels/fully_connected.cc b/tensorflow/lite/kernels/fully_connected.cc\nindex 8d59a2cbd5a..8b9f74f623e 100644\n--- a/tensorflow/lite/kernels/fully_connected.cc\n+++ b/tensorflow/lite/kernels/fully_connected.cc\n@@ -928,6 +928,36 @@ TfLiteStatus EvalShuffledQuantized(TfLiteContext* context, TfLiteNode* node,\n   return kTfLiteOk;\n }\n \n+// Verifies that sparsity values are valid given input/weight/output.\n+bool VerifySparsity(const RuntimeShape& weights_shape,\n+                    const RuntimeShape& input_shape,\n+                    const RuntimeShape& output_shape,\n+                    const TfLiteSparsity* sparsity) {\n+  const int weights_dims_count = weights_shape.DimensionsCount();\n+  const int output_dims_count = output_shape.DimensionsCount();\n+  const int w0_size = sparsity->dim_metadata[0].dense_size;\n+  const int accum_depth = weights_shape.Dims(weights_dims_count - 1);\n+  const int output_elements = output_shape.FlatSize();\n+  const int input_elements = input_shape.FlatSize();\n+  const int batches = FlatSizeSkipDim(output_shape, output_dims_count - 1);\n+  const int output_depth = MatchingDim(weights_shape, weights_dims_count - 2,\n+                                       output_shape, output_dims_count - 1);\n+  const int max_batch_index = batches - 1;\n+  const int max_output = max_batch_index * output_depth + w0_size;\n+  const int max_batch_depth = accum_depth * max_batch_index;\n+\n+  // Verify output size is enough.\n+  if (output_elements < max_output) return false;\n+\n+  // Verify index from sparse in input is valid.\n+  for (int i = 0; i < sparsity->dim_metadata[1].array_indices->size; ++i) {\n+    if (input_elements <=\n+        max_batch_depth + sparsity->dim_metadata[1].array_indices->data[i])\n+      return false;\n+  }\n+  return true;\n+}\n+\n template <KernelType kernel_type>\n TfLiteStatus EvalFloat(TfLiteContext* context, TfLiteNode* node,\n                        TfLiteFullyConnectedParams* params, OpData* data,\n@@ -968,24 +998,32 @@ TfLiteStatus EvalFloat(TfLiteContext* context, TfLiteNode* node,\n                            \"Unsupported sparse fully-connected weight format.\");\n         return kTfLiteError;\n       }\n+      const auto& input_shape = GetTensorShape(input);\n+      const auto& filter_shape = GetTensorShape(filter);\n+      const auto& output_shape = GetTensorShape(output);\n+      const auto& bias_shape = GetTensorShape(bias);\n+      if (!VerifySparsity(filter_shape, input_shape, output_shape, &sparsity)) {\n+        TF_LITE_KERNEL_LOG(context, \"Invalid sparse fully-connected format.\");\n+        return kTfLiteError;\n+      }\n \n       if (sparsity.dim_metadata_size == kDimMetadataSizeRandomSparse) {\n         // Random sparse.\n         optimized_ops::FullyConnectedSparseWeight(\n-            sparsity, op_params, GetTensorShape(input),\n-            GetTensorData<float>(input), GetTensorShape(filter),\n-            GetTensorData<float>(filter), GetTensorShape(bias),\n-            GetTensorData<float>(bias), GetTensorShape(output),\n-            GetTensorData<float>(output));\n+            sparsity, op_params,                         // Disable formatting\n+            input_shape, GetTensorData<float>(input),    // Disable formatting\n+            filter_shape, GetTensorData<float>(filter),  // Disable formatting\n+            bias_shape, GetTensorData<float>(bias),      // Disable formatting\n+            output_shape, GetTensorData<float>(output));\n       } else if (sparsity.dim_metadata_size == kDimMetadataSizeBlockSparse &&\n                  sparsity.dim_metadata[2].dense_size == 4) {\n         // Block sparse with block size of 1x4.\n         optimized_ops::FullyConnectedSparseWeight1x4(\n-            sparsity, op_params, GetTensorShape(input),\n-            GetTensorData<float>(input), GetTensorShape(filter),\n-            GetTensorData<float>(filter), GetTensorShape(bias),\n-            GetTensorData<float>(bias), GetTensorShape(output),\n-            GetTensorData<float>(output),\n+            sparsity, op_params,                         // Disable formatting\n+            input_shape, GetTensorData<float>(input),    // Disable formatting\n+            filter_shape, GetTensorData<float>(filter),  // Disable formatting\n+            bias_shape, GetTensorData<float>(bias),      // Disable formatting\n+            output_shape, GetTensorData<float>(output),\n             CpuBackendContext::GetFromContext(context));\n       } else {\n         TF_LITE_KERNEL_LOG(context,"