"/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/core/graph/graph.h\"\n\n#include <memory>\n#include <vector>\n\n#include \"absl/container/flat_hash_map.h\"\n#include \"tensorflow/core/framework/full_type.pb.h\"\n#include \"tensorflow/core/framework/graph.pb.h\"\n#include \"tensorflow/core/framework/node_def.pb.h\"\n#include \"tensorflow/core/framework/node_properties.h\"\n#include \"tensorflow/core/framework/op_def_builder.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/versions.pb.h\"\n#include \"tensorflow/core/graph/graph_node_util.h\"\n#include \"tensorflow/core/graph/while_context.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/lib/gtl/map_util.h\"\n#include \"tensorflow/core/lib/hash/hash.h\"\n#include \"tensorflow/core/lib/strings/strcat.h\"\n#include \"tensorflow/core/lib/strings/stringprintf.h\"\n#include \"tensorflow/core/platform/errors.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/public/version.h\"\n\nnamespace tensorflow {\n\nconst int Graph::kControlSlot = -1;\n\n// Node\nNode::NodeClass Node::GetNodeClassForOp(const std::string& ts) {\n  static const absl::flat_hash_map<std::string, Node::NodeClass>*\n      kNodeClassTable =\n#define REF_CLASS(key, value) \\\n  {key, value}, { \"Ref\" key, value }\n          new absl::flat_hash_map<std::string, Node::NodeClass>({\n              // Keep in same order as NodeClass values\n              REF_CLASS(\"Switch\", NC_SWITCH),\n              REF_CLASS(\"_SwitchN\", NC_SWITCH),\n              REF_CLASS(\"Merge\", NC_MERGE),\n              REF_CLASS(\"Enter\", NC_ENTER),\n              REF_CLASS(\"Exit\", NC_EXIT),\n              REF_CLASS(\"NextIteration\", NC_NEXT_ITERATION),\n              {\"LoopCond\", NC_LOOP_COND},\n              {\"ControlTrigger\", NC_CONTROL_TRIGGER},\n              {\"_Send\", NC_SEND},\n              {\"_HostSend\", NC_HOST_SEND},\n              {\"_Recv\", NC_RECV},\n              {\"_HostRecv\", NC_HOST_RECV},\n              {\"Const\", NC_CONSTANT},\n              {\"HostConst\", NC_CONSTANT},\n              {\"Variable\", NC_VARIABLE},\n              {\"VariableV2\", NC_VARIABLE},\n              REF_CLASS(\"Identity\", NC_IDENTITY),\n              {\"GetSessionHandle\", NC_GET_SESSION_HANDLE},\n              {\"GetSessionHandleV2\", NC_GET_SESSION_HANDLE},\n              {\"GetSessionTensor\", NC_GET_SESSION_TENSOR},\n              {\"DeleteSessionTensor\", NC_DELETE_SESSION_TENSOR},\n              {\"Size\", NC_METADATA},\n              {\"Shape\", NC_METADATA},\n              {\"Rank\", NC_METADATA},\n              {\"_ScopedAllocator\", NC_SCOPED_ALLOCATOR},\n              {\"CollectiveReduce\", NC_COLLECTIVE},\n              {\"CollectiveBcastSend\", NC_COLLECTIVE},\n              {\"CollectiveBcastRecv\", NC_COLLECTIVE},\n              {\"CollectiveGather\", NC_COLLECTIVE},\n              {\"FakeParam\", NC_FAKE_PARAM},\n              {\"PartitionedCall\", NC_PARTITIONED_CALL},\n              {\"StatefulPartitionedCall\", NC_PARTITIONED_CALL},\n              {\"SymbolicGradient\", NC_SYMBOLIC_GRADIENT},\n              {\"If\", NC_IF},\n              {\"StatelessIf\", NC_IF},\n              {\"While\", NC_WHILE},\n              {\"StatelessWhile\", NC_WHILE},\n              {\"Case\", NC_CASE},\n              {\"StatelessCase\", NC_CASE},\n              // Not using the constants defined in FunctionLibraryDefinition\n              // for the\n              // 4 ops below because android inference library does not link\n              // tf.function related files.\n              {\"_Arg\", NC_ARG},\n              {\"_DeviceArg\", NC_ARG},\n              {\"_Retval\", NC_RETVAL},\n              {\"_DeviceRetval\", NC_RETVAL},\n              {\"_XlaMerge\", NC_MERGE},\n          });\n#undef REF_CLASS\n\n  auto it = kNodeClassTable->find(ts);\n  if (it != kNodeClassTable->end()) {\n    return it->second;\n  } else {\n    return NC_OTHER;\n  }\n}\n\nstd::string Node::DebugString() const {\n  std::string ret = strings::StrCat(\"{name:'\", name(), \"' id:\", id_);\n  if (IsSource()) {\n    strings::StrAppend(&ret, \" source}\");\n  } else if (IsSink()) {\n    strings::StrAppend(&ret, \" sink}\");\n  } else {\n    strings::StrAppend(&ret, \" op device:\", \"{requested: '\", requested_device(),\n                       \"', assigned: '\", assigned_device_name(), \"'}\", \" def:{\",\n                       SummarizeNode(*this), \"}}\");\n  }\n  return ret;\n}\n\nNode::Node()\n    : id_(-1),\n      cost_id_(-1),\n      class_(NC_UNINITIALIZED),\n      props_(nullptr),\n      assigned_device_name_index_(0),\n      while_ctx_(nullptr) {}\n\nvoid Node::Initialize(int id, int cost_id,\n                      std::shared_ptr<NodeProperties> props,\n                      Node::NodeClass node_class) {\n  DCHECK_EQ(id_, -1);\n  DCHECK(in_edges_.empty());\n  DCHECK(out_edges_.empty());\n  id_ = id;\n  cost_id_ = cost_id;\n\n  props_ = std::move(props);\n  class_ = node_class;\n}\n\nvoid Node::Clear() {\n  in_edges_.clear();\n  out_edges_.clear();\n  id_ = -1;\n  cost_id_ = -1;\n  class_ = NC_UNINITIALIZED;\n  props_.reset();\n  assigned_device_name_index_ = 0;\n}\n\nvoid Node::UpdateProperties() {\n  DataTypeVector inputs;\n  DataTypeVector outputs;\n  Status status =\n      InOutTypesForNode(props_->node_def, *(props_->op_def), &inputs, &outputs);\n  if (!status.ok()) {\n    LOG(ERROR) << \"Failed at updating node: \" << status;\n    return;\n  }\n  if (props_->input_types != inputs || props_->output_types != outputs) {\n    if (TF_PREDICT_TRUE(props_.use_count() == 1)) {\n      props_->input_types = inputs;\n      props_->input_types_slice = props_->input_types;\n      props_->output_types = outputs;\n      props_->output_types_slice = props_->output_types;\n    } else {\n      props_ = std::make_shared<NodeProperties>(\n          props_->op_def, std::move(props_->node_def), inputs, outputs);\n    }\n  }\n}\n\nvoid Node::ClearTypeInfo() {\n  if (props_->node_def.has_experimental_type()) {\n    MaybeCopyOnWrite();\n    props_->node_def.clear_experimental_type();\n  }\n}\n\nvoid Node::RunForwardTypeInference() {\n  VLOG(4) << \"Forward type inference: \" << props_->node_def.DebugString();\n\n  if (props_->fwd_type_fn == nullptr) {\n    return;\n  }\n\n  std::vector<Node*> input_nodes(props_->input_types.size(), nullptr);\n  std::vector<int> input_idx(props_->input_types.size(), 0);\n  for (const auto& edge : in_edges_) {\n    if (edge->IsControlEdge()) {\n      continue;\n    }\n    DCHECK(edge->dst_input() < input_nodes.size()) << DebugString();\n    int i = edge->dst_input();\n    input_nodes.at(i) = edge->src();\n    input_idx.at(i) = edge->src_output();\n  }\n\n  // Note: technically, we could use a very generic type when some of the inputs\n  // are unknown. But there is an expectation that a node will have complete\n  // inputs soon, so updating intermediate types is largely unnecessary.\n\n  for (const auto* node : input_nodes) {\n    if (node == nullptr) {\n      // Incomplete inputs, bail.\n      ClearTypeInfo();\n      return;\n    }\n  }\n\n  static FullTypeDef* no_type = new FullTypeDef();\n\n  std::vector<std::reference_wrapper<const FullTypeDef>> input_types;\n  for (int i = 0; i < input_nodes.size(); i++) {\n    const auto* node = input_nodes[i];\n    if (node->def().has_experimental_type()) {\n      const auto& node_t = node->def().experimental_type();\n      if (node_t.type_id() != TFT_UNSET) {\n        int ix = input_idx[i];\n        if (ix >= node_t.args_size()) {\n          LOG(WARNING) << name() << \" has bad type information: input \" << i\n                       << \" should have an output \" << ix\n                       << \" but instead only has \" << node_t.args_size()\n                       << \" outputs: \" << node_t.DebugString()\n                       << \"\\nThis indicates either \"\n                          \"a bug in op registration or a corrupted graph.\";\n          ClearTypeInfo();\n          return;\n        }\n        input_types.emplace_back(node_t.args(ix));\n      } else {\n        input_types.emplace_back(*no_type);\n      }\n    } else {\n      // Incomplete inputs, bail.\n      ClearTypeInfo();\n      return;\n    }\n  }\n\n  const auto infer_type = props_->fwd_type_fn(input_types);\n  const FullTypeDef infer_typedef = infer_type.ValueOrDie();\n  if (infer_typedef.type_id() != TFT_UNSET) {\n    MaybeCopyOnWrite();\n    *(props_->node_def.mutable_experimental_type()) = infer_typedef;\n  }\n}\n\nconst std::string& Node::name() const { return props_->node_def.name(); }\nconst std::string& Node::type_string() const { return props_->node_def.op(); }\nconst NodeDef& Node::def() const { return props_->node_def; }\nconst OpDef& Node::op_def() const { return *props_->op_def; }\n\nNodeDef* Node::mutable_def() { return &props_->node_def; }\n\nint32 Node::num_inputs() const { return props_->input_types.size(); }\nDataType Node::input_type(int32_t i) const { return props_->input_types[i]; }\nconst DataTypeVector& Node::input_types() const { return props_->input_types; }\n\nint32 Node::num_outputs() const { return props_->output_types.size(); }\nDataType Node::output_type(int32_t o) const { return props_->output_types[o]; }\nconst DataTypeVector& Node::output_types() const {\n  return props_->output_types;\n}\n\nAttrSlice Node::attrs() const { return AttrSlice(def()); }\n\nconst protobuf::RepeatedPtrField<std::string>& Node::requested_inputs() const {\n  return def().input();\n}\n\nconst std::string& Node::requested_device() const { return def().device(); }\n\ngtl::iterator_range<NeighborIter> Node::out_nodes() const {\n  return gtl::make_range(NeighborIter(out_edges_.begin(), false),\n                         NeighborIter(out_edges_.end(), false));\n}\n\ngtl::iterator_range<NeighborIter> Node::in_nodes() const {\n  return gtl::make_range(NeighborIter(in_edges_.begin(), true),\n                         NeighborIter(in_edges_.end(), true));\n}\n\nvoid Node::MaybeCopyOnWrite() {\n  // TODO(mdan): As nodes become more dynamic, this may not be worth the cost.\n  // NodeProperties may be shared between Nodes. Make a copy if so.\n  if (!props_.unique()) {\n    props_ = std::make_shared<NodeProperties>(*props_);\n  }\n}\n\nAttrValue* Node::AddAttrHelper(const std::string& name) {\n  MaybeCopyOnWrite();\n  return &((*props_->node_def.mutable_attr())[name]);\n}\n\nvoid Node::ClearAttr(const std::string& name) {\n  MaybeCopyOnWrite();\n  (*props_->node_def.mutable_attr()).erase(name);\n}\n\nvoid Node::set_name(std::string name) {\n  MaybeCopyOnWrite();\n  props_->node_def.set_name(std::move(name));\n}\n\nvoid Node::set_requested_device(const std::string& device) {\n  MaybeCopyOnWrite();\n  props_->node_def.set_device(device);\n}\n\nvoid Node::set_original_node_names(const std::vector<std::string>& names) {\n  MaybeCopyOnWrite();\n  props_->node_def.mutable_experimental_debug_info()\n      ->clear_original_node_names();\n  if (!names.empty()) {\n    *props_->node_def.mutable_experimental_debug_info()\n         ->mutable_original_node_names() = {names.begin(), names.end()};\n  }\n}\n\nvoid Node::set_original_func_names(const std::vector<std::string>& names) {\n  MaybeCopyOnWrite();\n  props_->node_def.mutable_experimental_debug_info()\n      ->clear_original_func_names();\n  if (!names.empty()) {\n    *props_->node_def.mutable_experimental_debug_info()\n         ->mutable_original_func_names() = {names.begin(), names.end()};\n  }\n}\n\nStatus Node::input_edge(int idx, const Edge** e) const {\n  if (idx < 0 || idx >= num_inputs()) {\n    return errors::InvalidArgument(\"Invalid input_edge index: \", idx, \", Node \",\n                                   name(), \" only has \", num_inputs(),\n                                   \" inputs.\");\n  }\n\n  // This does a linear search over the edges.  In the common case,\n  // the number of elements is small enough that this search isn't\n  // expensive.  Should it become a bottleneck, one can make an\n  // optimization where, if the number of edges is small, we use\n  // linear iteration, and if the number of edges is large, we perform\n  // an indexing step during construction that keeps an array of Edges\n  // indexed by pointer.  This would keep the size of each Node small\n  // in the common case but make this function faster when the number\n  // of edges is large.\n  for (const Edge* edge : in_edges()) {\n    if (edge->dst_input() == idx) {\n      *e = edge;\n      return Status::OK();\n    }\n  }\n\n  return errors::NotFound(\"Could not find input edge \", idx, \" for \", name());\n}\n\n// Returns a vector of the non-control input edges to a node, indexed by ID.\nStatus Node::input_edges(std::vector<const Edge*>* input_edges) const {\n  input_edges->clear();\n  input_edges->resize(num_inputs(), nullptr);\n\n  for (const Edge* edge : in_edges()) {\n    if (edge->IsControlEdge()) continue;\n    if (edge->dst_input() < 0 || edge->dst_input() >= num_inputs()) {\n      return errors::Internal(\"Invalid edge input number \", edge->dst_input());\n    }\n    if ((*input_edges)[edge->dst_input()] != nullptr) {\n      return errors::Internal(\"Duplicate edge input number: \",\n                              edge->dst_input());\n    }\n    (*input_edges)[edge->dst_input()] = edge;\n  }\n\n  for (int i = 0; i < num_inputs(); ++i) {\n    if ((*input_edges)[i] == nullptr) {\n      return errors::InvalidArgument(\"Missing edge input number: \", i);\n    }\n  }\n  return Status::OK();\n}\n\nStatus Node::input_node(int idx, Node** n) const {\n  const Edge* e;\n  TF_RETURN_IF_ERROR(input_edge(idx, &e));\n  if (e == nullptr) {\n    *n = nullptr;\n  } else {\n    *n = e->src();\n  }\n  return Status::OK();\n}\n\nStatus Node::input_node(int idx, const Node** const_n) const {\n  Node* n;\n  TF_RETURN_IF_ERROR(input_node(idx, &n));\n  *const_n = n;\n  return Status::OK();\n}\n\nStatus Node::input_tensor(int idx, OutputTensor* t) const {\n  const Edge* e;\n  TF_RETURN_IF_ERROR(input_edge(idx, &e));\n  DCHECK(e != nullptr);\n  *t = OutputTensor(e->src(), e->src_output());\n  return Status::OK();\n}\n\n// NodeDebugInfo\n\nNodeDebugInfo::NodeDebugInfo(const Node& n) : NodeDebugInfo(n.def()) {}\nNodeDebugInfo::NodeDebugInfo(const NodeDef& ndef)\n    : NodeDebugInfo(ndef.name(), ndef.has_experimental_debug_info(),\n                    ndef.experimental_debug_info()) {}\nNodeDebugInfo::NodeDebugInfo(\n    StringPiece node_name, bool has_experimental_debug_info,\n    const NodeDef_ExperimentalDebugInfo& experimental_debug_info)\n    : name(node_name) {\n  if (has_experimental_debug_info) {\n    const auto& node_names = experimental_debug_info.original_node_names();\n    original_node_names.assign(node_names.begin(), node_names.end());\n    const auto& func_names = experimental_debug_info.original_func_names();\n    original_func_names.assign(func_names.begin(), func_names.end());\n  }\n}\n// InputTensor\n\nbool InputTensor::operator==(const InputTensor& other) const {\n  return node == other.node && index == other.index;\n}\n\nuint64 InputTensor::Hash::operator()(InputTensor const& s) const {\n  return Hash64Combine(std::hash<const Node*>()(s.node),\n                       std::hash<int>()(s.index));\n}\n\n// OutputTensor\n\nbool OutputTensor::operator==(const OutputTensor& other) const {\n  return node == other.node && index == other.index;\n}\n\nuint64 OutputTensor::Hash::operator()(OutputTensor const& s) const {\n  return Hash64Combine(std::hash<const Node*>()(s.node),\n                       std::hash<int>()(s.index));\n}\n\n// Graph\n\nGraph::Graph(const OpRegistryInterface* ops)\n    : ops_(ops, FunctionDefLibrary()),\n      versions_(new VersionDef),\n      arena_(8 << 10 /* 8kB */) {\n  versions_->set_producer(TF_GRAPH_DEF_VERSION);\n  versions_->set_min_consumer(TF_GRAPH_DEF_VERSION_MIN_CONSUMER);\n\n  // Initialize the name interning table for assigned_device_name.\n  device_names_.push_back(\"\");\n  DCHECK_EQ(0, InternDeviceName(\"\"));\n\n  // Source and sink have no endpoints, just control edges.\n  NodeDef def;\n  def.set_name(\"_SOURCE\");\n  def.set_op(\"NoOp\");\n  Status status;\n  Node* source = AddNode(def, &status);\n  TF_CHECK_OK(status);\n  CHECK_EQ(source->id(), kSourceId);\n\n  def.set_name(\"_SINK\");\n  Node* sink = AddNode(def, &status);\n  TF_CHECK_OK(status);\n  CHECK_EQ(sink->id(), kSinkId);\n\n  AddControlEdge(source, sink);\n}\n\nGraph::Graph(const FunctionLibraryDefinition& flib_def)\n    : Graph(flib_def.default_registry()) {\n  // Need a new-enough consumer to support the functions we add to the graph.\n  if (flib_def.num_functions() > 0 && versions_->min_consumer() < 12) {\n    versions_->set_min_consumer(12);\n  }\n  Status s = ops_.AddLibrary(flib_def);\n  CHECK(s.ok()) << s.error_message();\n}\n\nGraph::~Graph() {\n  // Manually call the destructors for all the Nodes we constructed using\n  // placement new.\n  for (Node* node : nodes_) {\n    if (node != nullptr) {\n      node->~Node();\n    }\n  }\n  for (Node* node : free_nodes_) {\n    node->~Node();\n  }\n  // Edges have no destructor, and we arena-allocated them, so no need to\n  // destroy them.\n}\n\nstd::unique_ptr<Graph> Graph::Clone() {\n  std::unique_ptr<Graph> new_graph(new Graph(flib_def()));\n  new_graph->Copy(*this);\n  return new_graph;\n}\n\nconst VersionDef& Graph::versions() const { return *versions_; }\nvoid Graph::set_versions(const VersionDef& versions) { *versions_ = versions; }\n\nvoid Graph::Copy(const Graph& src) {\n  SetConstructionContext(src.GetConstructionContextInternal());\n  for (Node* n : nodes()) {\n    CHECK(n->IsSource() || n->IsSink()) << \"*dest must be empty\";\n  }\n\n  // Copy GraphDef versions\n  set_versions(src.versions());\n\n  // Copy the nodes.\n  // \"Node in src\" -> \"Node in *dest\"\n  gtl::FlatMap<const Node*, Node*> node_map;\n  node_map.reserve(src.num_nodes());\n  node_map[src.source_node()] = source_node();\n  node_map[src.sink_node()] = sink_node();\n  for (Node* n : src.op_nodes()) {\n    auto copy = CopyNode(n);\n    copy->in_edges_.reserve(n->in_edges().size());\n    copy->out_edges_.reserve(n->out_edges().size());\n    node_map[n] = copy;\n  }\n\n  // Copy the edges\n  edges_.reserve(src.num_edges());\n  for (const Edge* e : src.edges()) {\n    Node* src_copy = node_map[e->src()];\n    Node* dst_copy = node_map[e->dst()];\n    AddEdge(src_copy, e->src_output(), dst_copy, e->dst_input());\n  }\n}\n\nNode* Graph::AddNode(NodeDef node_def, Status* status) {\n  const OpRegistrationData* op_reg_data;\n  status->Update(ops_.LookUp(node_def.op(), &op_reg_data));\n  if (!status->ok()) return nullptr;\n\n  DataTypeVector inputs;\n  DataTypeVector outputs;\n  status->Update(\n      InOutTypesForNode(node_def, op_reg_data->op_def, &inputs, &outputs));\n  if (!status->ok()) {\n    *status = AttachDef(*status, node_def);\n    return nullptr;\n  }\n\n  Node::NodeClass node_class = op_reg_data->is_function_op\n                                   ? Node::NC_FUNCTION_OP\n                                   : Node::GetNodeClassForOp(node_def.op());\n\n  if (op_reg_data->type_ctor != nullptr) {\n    VLOG(3) << \"AddNode: found type constructor for \" << node_def.name();\n    const auto ctor_type =\n        full_type::SpecializeType(AttrSlice(node_def), op_reg_data->op_def);\n    if (!ctor_type.ok()) {\n      *status = errors::InvalidArgument(\"type error: \",\n                                        ctor_type.status().ToString());\n      return nullptr;\n    }\n    const FullTypeDef ctor_typedef = ctor_type.ValueOrDie();\n    if (ctor_typedef.type_id() != TFT_UNSET) {\n      *(node_def.mutable_experimental_type()) = ctor_typedef;\n    }\n  } else {\n    VLOG(3) << \"AddNode: no type constructor for \" << node_def.name();\n  }\n\n  Node* node = AllocateNode(std::make_shared<NodeProperties>(\n                                &op_reg_data->op_def, std::move(node_def),\n                                inputs, outputs, op_reg_data->fwd_type_fn),\n                            nullptr, node_class);\n  return node;\n}\n\nNode* Graph::CopyNode(const Node* node) {\n  DCHECK(!node->IsSource());\n  DCHECK(!node->IsSink());\n  Node* copy = AllocateNode(node->props_, node, node->class_);\n  copy->set_assigned_device_name(node->assigned_device_name());\n\n  // Since the OpDef of a function may be owned by the Graph that owns 'node',\n  // relookup the OpDef in the target graph. If it differs, then clone the\n  // node properties with the updated OpDef.\n  const OpDef* op_def;\n  TF_CHECK_OK(ops_.LookUpOpDef(node->type_string(), &op_def));\n  if (op_def != node->props_->op_def) {\n    copy->MaybeCopyOnWrite();\n    copy->props_->op_def = op_def;\n  }\n  copy->SetStackTrace(node->GetStackTrace());\n\n  return copy;\n}\n\nvoid Graph::RemoveNode(Node* node) {\n  TF_DCHECK_OK(IsValidNode(node)) << node->DebugString();\n  DCHECK(!node->IsSource());\n  DCHECK(!node->IsSink());\n\n  // Remove any edges involving this node.\n  for (const Edge* e : node->in_edges_) {\n    CHECK_EQ(e->src_->out_edges_.erase(e), size_t{1});\n    edges_[e->id_] = nullptr;\n    RecycleEdge(e);\n    --num_edges_;\n  }\n  node->in_edges_.clear();\n  for (const Edge* e : node->out_edges_) {\n    CHECK_EQ(e->dst_->in_edges_.erase(e), size_t{1});\n    edges_[e->id_] = nullptr;\n    RecycleEdge(e);\n    --num_edges_;\n  }\n  node->out_edges_.clear();\n  ReleaseNode(node);\n}\n\nconst Edge* Graph::AddEdge(Node* source, int x, Node* dest, int y) {\n  TF_DCHECK_OK(IsValidNode(source)) << source->DebugString();\n  TF_DCHECK_OK(IsValidNode(dest)) << dest->DebugString();\n\n  // source/sink must only be linked via control slots, and\n  // control slots must only be linked to control slots.\n  if (source == source_node() || dest == sink_node() || x == kControlSlot ||\n      y == kControlSlot) {\n    DCHECK_EQ(x, kControlSlot) << source->DebugString();\n    DCHECK_EQ(y, kControlSlot) << dest->DebugString();\n  }\n\n  Edge* e = nullptr;\n  if (free_edges_.empty()) {\n    e = new (arena_.Alloc(sizeof(Edge))) Edge;  // placement new\n  } else {\n    e = free_edges_.back();\n    free_edges_.pop_back();\n  }\n  e->id_ = edges_.size();\n  e->src_ = source;\n  e->dst_ = dest;\n  e->src_output_ = x;\n  e->dst_input_ = y;\n  CHECK(source->out_edges_.insert(e).second);\n  CHECK(dest->in_edges_.insert(e).second);\n  edges_.push_back(e);\n  ++num_edges_;\n\n  if (!e->IsControlEdge()) {\n    if (dest->in_edges_.size() >= dest->props_->input_types.size()) {\n      // Note: this only produces consistent results at graph construction,\n      // and only when all incoming edges are up-to-date.\n      // If the graph is subsequently modified, or if the node is added before\n      // any of its upstream nodes, this type information would change as well.\n      // In general, graph transformations should run shole-graph type inference\n      // when done, and should not rely on types being fully up to date\n      // after each AddNode.\n      // TODO(mdan): Should we even run type inference here any more?\n      dest->RunForwardTypeInference();\n    }\n  }\n\n  return e;\n}\n\nvoid Graph::RemoveEdge(const Edge* e) {\n  TF_DCHECK_OK(IsValidNode(e->src_)) << e->src_->DebugString();\n  TF_DCHECK_OK(IsValidNode(e->dst_)) << e->dst_->DebugString();\n  CHECK_EQ(e->src_->out_edges_.erase(e), size_t{1});\n  CHECK_EQ(e->dst_->in_edges_.erase(e), size_t{1});\n  CHECK_EQ(e, edges_[e->id_]);\n  CHECK_GT(num_edges_, 0);\n\n  edges_[e->id_] = nullptr;\n  RecycleEdge(e);\n  --num_edges_;\n\n  if (!e->IsControlEdge()) {\n    // This may clear the node type if enough edges are removed.\n    e->dst_->RunForwardTypeInference();\n  }\n}\n\nvoid Graph::RecycleEdge(const Edge* e) {\n  free_edges_.push_back(const_cast<Edge*>(e));\n}\n\nconst Edge* Graph::AddControlEdge(Node* source, Node* dest,\n                                  bool allow_duplicates) {\n  if (!allow_duplicates) {\n    for (const Edge* edge : dest->in_edges()) {\n      if (edge->IsControlEdge() && edge->src() == source) {\n        // The requested edge already exists.\n        return nullptr;\n      }\n    }\n  }\n  // Modify dest's NodeDef if necessary.\n  if (!source->IsSource() && !dest->IsSink() && !allow_duplicates) {\n    // Check if this input is already in dest's NodeDef.\n    const std::string new_input = strings::StrCat(\"^\", source->name());\n    bool input_exists = false;\n    for (const std::string& input : dest->props_->node_def.input()) {\n      if (input == new_input) {\n        input_exists = true;\n        break;\n      }\n    }\n    if (!input_exists) {\n      dest->MaybeCopyOnWrite();\n      dest->props_->node_def.add_input(new_input);\n    }\n  }\n  return AddEdge(source, kControlSlot, dest, kControlSlot);\n}\n\nvoid Graph::RemoveControlEdge(const Edge* e) {\n  if (!e->src_->IsSource() && !e->dst_->IsSink()) {\n    e->dst_->MaybeCopyOnWrite();\n    std::string e_src_name = strings::StrCat(\"^\", e->src_->name());\n    auto* inputs = e->dst_->props_->node_def.mutable_input();\n    for (auto it = inputs->begin(); it != inputs->end(); ++it) {\n      if (*it == e_src_name) {\n        inputs->erase(it);\n        break;\n      }\n    }\n  }\n  RemoveEdge(e);\n}\n\nnamespace {\nconst Edge* FindEdge(const Node* dst, int index) {\n  for (const Edge* e : dst->in_edges()) {\n    if (e->dst_input() == index) return e;\n  }\n  return nullptr;\n}\n}  // namespace\n\nStatus Graph::UpdateEdge(Node* new_src, int new_src_index, Node* dst,\n                         int dst_index) {\n  TF_RETURN_IF_ERROR(IsValidOutputTensor(new_src, new_src_index));\n  TF_RETURN_IF_ERROR(IsValidInputTensor(dst, dst_index));\n  const Edge* e = FindEdge(dst, dst_index);\n  if (e == nullptr) {\n    return errors::InvalidArgument(\"Couldn't find edge to \",\n                                   FormatNodeForError(*dst));\n  }\n  RemoveEdge(e);\n  AddEdge(new_src, new_src_index, dst, dst_index);\n  dst->MaybeCopyOnWrite();\n  (*dst->props_->node_def.mutable_input())[dst_index] =\n      strings::StrCat(new_src->name(), \":\", new_src_index);\n  return Status::OK();\n}\n\nStatus Graph::AddWhileInputHack(Node* new_src, int new_src_index, Node* dst) {\n  if (!dst->IsWhileNode()) {\n    return errors::Internal(\n        \"dst argument to AddWhileEdgeHack should be a While op, got: \",\n        dst->DebugString());\n  }\n  TF_RETURN_IF_ERROR(IsValidOutputTensor(new_src, new_src_index));\n  // Find the current number of data inputs. We'll add the new edge to the next\n  // missing data input.\n  int dst_index = 0;\n  for (const Edge* edge : dst->in_edges()) {\n    if (edge->IsControlEdge()) continue;\n    ++dst_index;\n  }\n  TF_RETURN_IF_ERROR(IsValidInputTensor(dst, dst_index));\n  AddEdge(new_src, new_src_index, dst, dst_index);\n  dst->MaybeCopyOnWrite();\n  dst->props_->node_def.add_input(\n      strings::StrCat(new_src->name(), \":\", new_src_index));\n  return Status::OK();\n}\n\nStatus Graph::AddFunctionLibrary(const FunctionDefLibrary& fdef_lib) {\n  // Need a new-enough consumer to support the functions we add to the graph.\n  if (fdef_lib.function_size() > 0 && versions_->min_consumer() < 12) {\n    versions_->set_min_consumer(12);\n  }\n  return ops_.AddLibrary(fdef_lib);\n}\n\nnamespace {\n\nvoid AddInput(NodeDef* dst, StringPiece src_name, int src_slot) {\n  if (src_slot == Graph::kControlSlot) {\n    dst->add_input(strings::StrCat(\"^\", src_name));\n  } else if (src_slot == 0) {\n    dst->add_input(src_name.data(), src_name.size());\n  } else {\n    dst->add_input(strings::StrCat(src_name, \":\", src_slot));\n  }\n}\n\n}  // namespace\n\nvoid Graph::ToGraphDef(GraphDef* graph_def) const {\n  ToGraphDefSubRange(graph_def, 0);\n}\n\nGraphDef Graph::ToGraphDefDebug() const {\n  GraphDef ret;\n  ToGraphDef(&ret);\n  return ret;\n}\n\nvoid Graph::ToGraphDefSubRange(GraphDef* graph_def, int from_node_id) const {\n  graph_def->Clear();\n  *graph_def->mutable_versions() = versions();\n  *graph_def->mutable_library() = ops_.ToProto();\n\n  graph_def->mutable_node()->Reserve(std::max(1, num_nodes() - from_node_id));\n\n  std::vector<const Edge*>\n      inputs;  // Construct this outside the loop for speed.\n  for (auto id = from_node_id; id < num_node_ids(); ++id) {\n    const Node* node = FindNodeId(id);\n    if (node == nullptr || !node->IsOp()) continue;\n    NodeDef* node_def = graph_def->add_node();\n    *node_def = node->def();\n\n    // Use the node's assigned device, if any, instead of the device requested\n    // in the NodeDef.\n    if (!node->assigned_device_name().empty()) {\n      node_def->set_device(node->assigned_device_name());\n    }\n\n    // Get the inputs for this Node.  We make sure control inputs are\n    // after data inputs, as required by GraphDef.\n    inputs.clear();\n    inputs.resize(node->num_inputs(), nullptr);\n    for (const Edge* edge : node->in_edges()) {\n      if (edge->IsControlEdge()) {\n        inputs.push_back(edge);\n      } else {\n        DCHECK(edge->dst_input() < inputs.size())\n            << \"Edge \" << edge->DebugString()\n            << \" is overflowing the expected number of inputs (\"\n            << node->num_inputs() << \") for node \" << node->DebugString();\n        CHECK(inputs[edge->dst_input()] == nullptr)\n            << \"Edge \" << edge->src()->name() << \"->\" << edge->dst()->name()\n            << \" conflicts with pre-existing input edge \"\n            << inputs[edge->dst_input()]->src()->name() << \"->\"\n            << inputs[edge->dst_input()]->dst()->name();\n\n        inputs[edge->dst_input()] = edge;\n      }\n    }\n    // Sort the control inputs for more predictable serialization.\n    std::sort(inputs.begin() + node->num_inputs(), inputs.end(),\n              [](const Edge* a, const Edge* b) -> bool {\n                return a->src()->name() < b->src()->name();\n              });\n    node_def->clear_input();\n    node_def->mutable_input()->Reserve(inputs.size());\n\n    for (size_t i = 0; i < inputs.size(); ++i) {\n      const Edge* edge = inputs[i];\n      if (edge == nullptr) {\n        if (i < node->requested_inputs().size()) {\n          node_def->add_input(node->requested_inputs()[i]);\n        } else {\n          node_def->add_input(\"\");\n        }\n      } else {\n        const Node* src = edge->src();\n        if (!src->IsOp()) continue;\n        AddInput(node_def, src->name(), edge->src_output());\n      }\n    }\n  }\n}\n\nstd::string Graph::NewName(StringPiece prefix) {\n  return strings::StrCat(prefix, \"/_\", name_counter_++);\n}\n\nStatus Graph::IsValidNode(const Node* node) const {\n  if (node == nullptr) {\n    return errors::InvalidArgument(\"Node is null\");\n  }\n  const int id = node->id();\n  if (id < 0) {\n    return errors::InvalidArgument(\"node id \", id, \" is less than zero\");\n  }\n  if (static_cast<size_t>(id) >= nodes_.size()) {\n    return errors::InvalidArgument(\n        \"node id \", id, \" is >= than number of nodes in graph \", nodes_.size());\n  }\n  if (nodes_[id] != node) {\n    return errors::InvalidArgument(\"Node with id \", id,\n                                   \" is different from the passed in node. \"\n                                   \"Does it belong to a different graph?\");\n  }\n  return Status::OK();\n}\n\nStatus Graph::IsValidOutputTensor(const Node* node, int idx) const {\n  TF_RETURN_IF_ERROR(IsValidNode(node));\n  if (idx >= node->num_outputs() || idx < 0) {\n    return errors::OutOfRange(\"Node '\", node->name(), \"' (type: '\",\n                              node->op_def().name(),\n                              \"', num of outputs: \", node->num_outputs(),\n                              \") does not have \", \"output \", idx);\n  }\n  return Status::OK();\n}\n\nStatus Graph::IsValidInputTensor(const Node* node, int idx) const {\n  TF_RETURN_IF_ERROR(IsValidNode(node));\n  if (idx >= node->num_inputs() || idx < 0) {\n    return errors::OutOfRange(\"Node '\", node->name(), \"' (type: '\",\n                              node->op_def().name(),\n                              \"', num of inputs: \", node->num_inputs(),\n                              \") does not have \", \"input \", idx);\n  }\n  return Status::OK();\n}\n\nNode* Graph::AllocateNode(std::shared_ptr<NodeProperties> props,\n                          const Node* cost_node, Node::NodeClass node_class) {\n  Node* node = nullptr;\n  if (free_nodes_.empty()) {\n    node = new (arena_.Alloc(sizeof(Node))) Node;  // placement new\n  } else {\n    node = free_nodes_.back();\n    free_nodes_.pop_back();\n  }\n  node->graph_ = this;\n  const int id = nodes_.size();\n  int cost_id = cost_node ? cost_node->cost_id() : id;\n  node->Initialize(id, cost_id, std::move(props), node_class);\n  nodes_.push_back(node);\n  ++num_nodes_;\n  return node;\n}\n\nvoid Graph::ReleaseNode(Node* node) {\n  TF_DCHECK_OK(IsValidNode(node)) << node->DebugString();\n  nodes_[node->id()] = nullptr;\n  free_nodes_.push_back(node);\n  --num_nodes_;\n  node->Clear();\n}\n\n// Ensures that 'device_name' is present in the device name table, and returns\n// the index of that device name. The index is stable, and can be used in\n// calls to Node::set_assigned_device_name_index().\nint Graph::InternDeviceName(const std::string& device_name) {\n  // Special case, very common.  Also, this allows us to use a single map\n  // lookup below, instead of two.  The 'if (index_cell > 0)' test below\n  // relies on this check.\n  if (device_name.empty()) {\n    return 0;\n  }\n\n  int& index_cell = device_names_map_[device_name];\n  if (index_cell > 0) {\n    return index_cell;\n  }\n\n  const int index = device_names_map_.size();\n  index_cell = index;\n  device_names_.push_back(device_name);\n  return index;\n}\n\nStatus Graph::AddWhileContext(StringPiece frame_name,\n                              std::vector<Node*> enter_nodes,\n                              std::vector<Node*> exit_nodes,\n                              OutputTensor cond_output,\n                              std::vector<OutputTensor> body_inputs,\n                              std::vector<OutputTensor> body_outputs,\n                              WhileContext** result) {\n  auto pair = while_ctxs_.insert(std::pair<std::string, WhileContext>(\n      std::string(frame_name),\n      WhileContext(frame_name, std::move(enter_nodes), std::move(exit_nodes),\n                   cond_output, std::move(body_inputs),\n                   std::move(body_outputs))));\n  if (!pair.second) {\n    *result = nullptr;\n    return errors::InvalidArgument(\"WhileContext with frame name '\", frame_name,\n                                   \"' already exists\");\n  }\n  *result = &pair.first->second;\n  return Status::OK();\n}\n\nstd::unordered_map<std::string, Node*> Graph::BuildNodeNameIndex() const {\n  std::unordered_map<std::string, Node*> result;\n  for (Node* n : nodes()) {\n    result[n->name()] = n;\n  }\n  return result;\n}\n\nstd::string Edge::DebugString() const {\n  return strings::Printf(\"[id=%d %s:%d -> %s:%d]\", id_, src_->name().c_str(),\n                         src_output_, dst_->name().c_str(), dst_input_);\n}\n\n}  // namespace tensorflow"