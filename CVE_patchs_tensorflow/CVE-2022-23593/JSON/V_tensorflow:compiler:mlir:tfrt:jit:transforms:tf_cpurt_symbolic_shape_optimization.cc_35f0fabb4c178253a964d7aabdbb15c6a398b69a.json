"/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include <sys/types.h>\n\n#include <string>\n\n#include \"mlir/Dialect/StandardOps/IR/Ops.h\"\n#include \"mlir/IR/AffineMap.h\"\n#include \"mlir/IR/BuiltinOps.h\"\n#include \"mlir/IR/BuiltinTypes.h\"\n#include \"mlir/IR/MLIRContext.h\"\n#include \"mlir/IR/Operation.h\"\n#include \"mlir/IR/OperationSupport.h\"\n#include \"mlir/IR/TypeRange.h\"\n#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n#include \"llvm/ADT/DenseMap.h\"\n#include \"llvm/ADT/DenseSet.h\"\n#include \"llvm/ADT/STLExtras.h\"\n#include \"llvm/ADT/StringExtras.h\"\n#include \"llvm/ADT/iterator_range.h\"\n#include \"llvm/Support/Alignment.h\"\n#include \"llvm/Support/Casting.h\"\n#include \"llvm/Support/ErrorOr.h\"\n#include \"tensorflow/compiler/mlir/hlo/include/mlir-hlo/Analysis/shape_component_analysis.h\"\n#include \"tensorflow/compiler/mlir/hlo/include/mlir-hlo/Dialect/mhlo/IR/hlo_ops.h\"\n#include \"tensorflow/compiler/mlir/tfrt/jit/transforms/tf_cpurt_passes.h\"\n\nnamespace tensorflow {\nnamespace {\n\nusing llvm::ArrayRef;\nusing llvm::SmallVector;\n\nusing mlir::AffineExpr;\nusing mlir::AffineMap;\nusing mlir::failure;\nusing mlir::FuncOp;\nusing mlir::FunctionPass;\nusing mlir::Location;\nusing mlir::LogicalResult;\nusing mlir::MLIRContext;\nusing mlir::OpBuilder;\nusing mlir::RankedTensorType;\nusing mlir::ShapeComponentAnalysis;\nusing mlir::success;\nusing mlir::TypeRange;\nusing mlir::Value;\nusing mlir::ValueRange;\nusing mlir::arith::ConstantIndexOp;\nusing mlir::arith::ConstantOp;\nusing mlir::arith::IndexCastOp;\n\nnamespace linalg = mlir::linalg;\nnamespace mhlo = mlir::mhlo;\nnamespace shape = mlir::shape;\nnamespace tensor = mlir::tensor;\n\n#define GEN_PASS_CLASSES\n#include \"tensorflow/compiler/mlir/tfrt/jit/transforms/tf_cpurt_passes.h.inc\"\n\n// -------------------------------------------------------------------------- //\n\n// Rewrite shape.cstr_broadcastable with constant witness if can prove that\n// shapes are broadcastable from the symbolic shapes.\n\nclass CstrBroadcastableOpLowering\n    : public mlir::OpRewritePattern<shape::CstrBroadcastableOp> {\n public:\n  using Base = OpRewritePattern<shape::CstrBroadcastableOp>;\n\n  explicit CstrBroadcastableOpLowering(MLIRContext* ctx);\n\n  LogicalResult matchAndRewrite(shape::CstrBroadcastableOp op,\n                                mlir::PatternRewriter& rewriter) const override;\n};\n\nCstrBroadcastableOpLowering::CstrBroadcastableOpLowering(MLIRContext* ctx)\n    : Base(ctx) {}\n\n// Returns true if all of bcasted_shapes can be broadcasted with output_shape.\nbool isKnownBroadcastable(ShapeComponentAnalysis& analysis,\n                          ValueRange bcasted_shapes, Value output_shape) {\n  auto output_shape_dims = analysis.GetValueInfo(output_shape);\n  if (!output_shape_dims) return false;\n  for (Value shape : bcasted_shapes) {\n    auto shape_dims = analysis.GetValueInfo(shape);\n    if (!shape_dims) return false;\n    // Iterate backwards over the smallest input shape.\n    for (auto zip : llvm::zip(llvm::reverse(*output_shape_dims),\n                              llvm::reverse(*shape_dims))) {\n      const auto& first = std::get<0>(zip);\n      const auto& second = std::get<1>(zip);\n      // TODO(ezhulenev): What to do with dimensions statically known to be\n      // zero?\n      // Numpy can only broadcast [0] with [1], however Tensorflow can broadcast\n      // [0] with any dimension size, and produces dimension of size [0].\n      // Currently we'll conservatively return failure and will not proceed with\n      // a rewrite.\n      if (first.isConstant(0) || second.isConstant(0)) return false;\n      // If either shape has a static one dimension the broadcast will always\n      // succeed.\n      if (first.isConstant(1) || second.isConstant(1)) continue;\n      // Otherwise dims have to be equal.\n      if (first != second) return false;\n    }\n  }\n  return true;\n}\n\nLogicalResult CstrBroadcastableOpLowering::matchAndRewrite(\n    shape::CstrBroadcastableOp op, mlir::PatternRewriter& rewriter) const {\n  ShapeComponentAnalysis shape_component_analysis;\n  if (!isKnownBroadcastable(shape_component_analysis, op.getShapes(),\n                            op.getShapes().front()))\n    return failure();\n\n  // Replace constraint with a true witness.\n  rewriter.replaceOpWithNewOp<shape::ConstWitnessOp>(op, true);\n\n  return success();\n}\n\n// Replace shape.broadcast with a shape if it's statically known.\nclass BroadcastOpLowering final\n    : public mlir::OpRewritePattern<shape::BroadcastOp> {\n public:\n  explicit BroadcastOpLowering(MLIRContext* ctx) : OpRewritePattern(ctx) {}\n\n  LogicalResult matchAndRewrite(shape::BroadcastOp op,\n                                mlir::PatternRewriter& rewriter) const override;\n};\n\n// Returns a shape tensor if the shapes can be broadcasted to a known shape.\n// Will either return one of the shapes or a generated mix of the shapes.\nllvm::Optional<Value> simplifyBroadcast(ShapeComponentAnalysis& analysis,\n                                        ValueRange shapes, Location loc,\n                                        OpBuilder* builder) {\n  // First find the input shape with the largest rank.\n  SmallVector<ArrayRef<ShapeComponentAnalysis::SymbolicExpr>> shapes_found;\n  size_t maxRank = 0;\n  for (const auto &shape : llvm::enumerate(shapes)) {\n    auto found_shape = analysis.GetValueInfo(shape.value());\n    if (!found_shape) return {};\n    shapes_found.push_back(*found_shape);\n    maxRank = std::max(maxRank, found_shape->size());\n  }\n\n  SmallVector<const ShapeComponentAnalysis::SymbolicExpr*> joined_dimensions(\n      maxRank);\n  SmallVector<std::pair<Value, int64_t>> shape_and_rank_for_dim(maxRank);\n  for (const auto &shape : llvm::enumerate(shapes_found)) {\n    for (const auto &dim : llvm::enumerate(llvm::reverse(shape.value()))) {\n      // 1 dimensions don't contribute to the final result.\n      if (dim.value().isConstant(1)) continue;\n      // If it's not a 1 dimension it will be present in the result. Remember\n      // where it came from.\n      auto index = maxRank - dim.index() - 1;\n      if (!joined_dimensions[index]) {\n        joined_dimensions[index] = &dim.value();\n        shape_and_rank_for_dim[index] =\n            std::make_pair(shapes[shape.index()], shape.value().size());\n        continue;\n      }\n      // Bail if the dimensions are neither equal nor 1.\n      if (*joined_dimensions[index] != dim.value()) return {};\n    }\n  }\n  // If the output is the same as one of the inputs just return that.\n  if (llvm::is_splat(shape_and_rank_for_dim) &&\n      shape_and_rank_for_dim[0].first) {\n    return shape_and_rank_for_dim[0].first;\n  }\n  // Otherwise rematerialize the shape from the pieces we have.\n  SmallVector<Value> elements;\n  for (int i = 0; i != maxRank; ++i) {\n    // 1 dimensions are filtered above, recreate the constant.\n    if (!shape_and_rank_for_dim[i].first) {\n      auto one = builder->getIntegerAttr(\n          shapes[0].getType().cast<RankedTensorType>().getElementType(), 1);\n      elements.push_back(builder->create<ConstantOp>(loc, one));\n      continue;\n    }\n    // Extract from one of the shapes, accounting for the reverse indexing\n    // performed by broadcast.\n    Value index = builder->create<ConstantIndexOp>(\n        loc, i - maxRank + shape_and_rank_for_dim[i].second);\n    elements.push_back(builder->create<tensor::ExtractOp>(\n        loc, shape_and_rank_for_dim[i].first, index));\n  }\n  return Value(builder->create<tensor::FromElementsOp>(loc, elements));\n}\n\nLogicalResult BroadcastOpLowering::matchAndRewrite(\n    shape::BroadcastOp op, mlir::PatternRewriter& rewriter) const {\n  ShapeComponentAnalysis shape_component_analysis;\n  auto new_broadcast = simplifyBroadcast(\n      shape_component_analysis, op.getShapes(), op.getLoc(), &rewriter);\n  if (!new_broadcast) return failure();\n  rewriter.replaceOp(op, {*new_broadcast});\n  return success();\n}\n\n// -------------------------------------------------------------------------- //\n\n// Rewrite mhlo.dynamic_broadcast_in_dim operation into linalg.generic operation\n// if can infer the indexing maps for the operand from the symbolic shapes.\nclass DynamicBroadcastInDimOpLowering\n    : public mlir::OpRewritePattern<mhlo::DynamicBroadcastInDimOp> {\n public:\n  using Base = OpRewritePattern<mhlo::DynamicBroadcastInDimOp>;\n\n  explicit DynamicBroadcastInDimOpLowering(MLIRContext* ctx);\n\n  LogicalResult matchAndRewrite(mhlo::DynamicBroadcastInDimOp op,\n                                mlir::PatternRewriter& rewriter) const override;\n};\n\nDynamicBroadcastInDimOpLowering::DynamicBroadcastInDimOpLowering(\n    MLIRContext* ctx)\n    : Base(ctx) {}\n\n// Check if broadcasting `from` to `to_shape` is statically known to only have\n// dimensions that never expand or always expand.\nllvm::Optional<AffineMap> isNonExpandingBroadcast(\n    ShapeComponentAnalysis& analysis, Value from, Value to_shape) {\n  auto in_shape = analysis.GetShapeInfo(from);\n  auto out_shape = analysis.GetValueInfo(to_shape);\n  if (!in_shape || !out_shape) return {};\n\n  SmallVector<AffineExpr> input_map_exprs;\n  size_t rank = out_shape->size();\n  MLIRContext* ctx = (*out_shape)[0].expr.getContext();\n  size_t d = 0;\n  auto affine_zero = getAffineConstantExpr(0, ctx);\n  for (auto zip :\n       llvm::zip(llvm::reverse(*in_shape), llvm::reverse(*out_shape))) {\n    const auto& in = std::get<0>(zip);\n    const auto& out = std::get<1>(zip);\n    bool extend = in.isConstant(1) && !out.isConstant(1);\n    input_map_exprs.push_back(extend ? affine_zero\n                                     : getAffineDimExpr(rank - d - 1, ctx));\n    ++d;\n\n    // Bail if this is neither a known expansion nor a known non-expansion.\n    if (!extend && in != out) return {};\n  }\n  // Any leading dimensions will be expanded.\n  input_map_exprs.resize(in_shape->size(), affine_zero);\n  std::reverse(input_map_exprs.begin(), input_map_exprs.end());\n  return AffineMap::get(/*dimCount=*/rank,\n                        /*symbolCount=*/0, input_map_exprs, ctx);\n}\n\nLogicalResult DynamicBroadcastInDimOpLowering::matchAndRewrite(\n    mhlo::DynamicBroadcastInDimOp op, mlir::PatternRewriter& rewriter) const {\n  MLIRContext* ctx = getContext();\n\n  auto in_type = op.operand().getType().dyn_cast<RankedTensorType>();\n  auto out_type = op.getResult().getType().dyn_cast<RankedTensorType>();\n  if (!in_type || !out_type) return failure();\n\n  // Check that broadcast is right-aligned (numpy style), so that operand\n  // dimensions broadcasted to match inner-most dimensions of the output.\n  auto bcast_dims = op.broadcast_dimensions().getValues<int64_t>();\n  auto expected_bcast_dims = llvm::seq<int64_t>(\n      out_type.getRank() - in_type.getRank(), out_type.getRank());\n  if (!llvm::equal(bcast_dims, expected_bcast_dims)) return failure();\n\n  ShapeComponentAnalysis shape_component_analysis;\n  auto input_map = isNonExpandingBroadcast(\n      shape_component_analysis, op.operand(), op.output_dimensions());\n  if (!input_map) return failure();\n\n  // Resolve dynamic output dimensions for the `linalg.init_tensor` operation.\n  SmallVector<Value> output_dyn_dimensions;\n  Location loc = op.getLoc();\n  int64_t rank = out_type.getRank();\n  for (size_t d = 0; d < rank; ++d) {\n    int64_t output_dim = out_type.getShape()[d];\n\n    // Skip static output dimensions, they will be resolved from the shape.\n    if (output_dim >= 0) continue;\n\n    // Resolve the dynamic size of the output dimension.\n    Value output_dyn_dim = rewriter.create<tensor::ExtractOp>(\n        loc, op.output_dimensions(),\n        ValueRange{rewriter.create<ConstantIndexOp>(loc, d)});\n\n    // Symbolic shape analysis might have given us an i32 or i64. Cast to index.\n    if (!output_dyn_dim.getType().isIndex())\n      output_dyn_dim = rewriter.create<IndexCastOp>(loc, output_dyn_dim,\n                                                    rewriter.getIndexType());\n\n    output_dyn_dimensions.push_back(output_dyn_dim);\n  }\n\n  // Create a linalg.tensor_init operation to initialize output.\n  Value init = rewriter.create<linalg::InitTensorOp>(loc, output_dyn_dimensions,\n                                                     out_type.getShape(),\n                                                     out_type.getElementType());\n\n  // Output indexing map is an identity with `rank` number of loops.\n  AffineMap output_map = AffineMap::getMultiDimIdentityMap(rank, ctx);\n\n  // All iterators are parallel.\n  SmallVector<llvm::StringRef> iterator_types(rank, \"parallel\");\n\n  rewriter.replaceOpWithNewOp<linalg::GenericOp>(\n      op, /*resultTensorTypes=*/TypeRange{init.getType()},\n      /*inputs=*/ValueRange{op.operand()},\n      /*outputs=*/ValueRange{init},\n      /*indexingMaps=*/llvm::makeArrayRef({*input_map, output_map}),\n      /*iteratorTypes=*/iterator_types,\n      [&](OpBuilder& nested_builder, Location nested_loc, ValueRange args) {\n        nested_builder.create<linalg::YieldOp>(nested_loc, args[0]);\n      });\n\n  return success();\n}\n\n// -------------------------------------------------------------------------- //\n// Optimize function based on the symbolic shape attributes.\n// -------------------------------------------------------------------------- //\n\nstruct SymbolicShapeOptimizationPass\n    : public SymbolicShapeOptimizationBase<SymbolicShapeOptimizationPass> {\n  SymbolicShapeOptimizationPass() = default;\n\n  explicit SymbolicShapeOptimizationPass(bool constraints_only) {\n    this->optimize_only_constraints = constraints_only;\n  }\n\n  void runOnFunction() override {\n    FuncOp func = getFunction();\n\n    MLIRContext* ctx = &getContext();\n    mlir::RewritePatternSet patterns(ctx);\n\n    // Rewrite constraints based on the symbolic shapes.\n    patterns.insert<CstrBroadcastableOpLowering>(ctx);\n    // Rewrite shape.broadcast based on the symbolic shapes.\n    patterns.insert<BroadcastOpLowering>(ctx);\n\n    // Rewrite broadcasts based on the symbolic shapes if enabled.\n    if (!optimize_only_constraints)\n      patterns.insert<DynamicBroadcastInDimOpLowering>(ctx);\n\n    // Add shape dialect canonicalization patterns to fold shape operations\n    // after constraints are replaced with constant witness.\n    for (auto op : ctx->getRegisteredOperations()) {\n      if (llvm::isa<shape::ShapeDialect>(op.getDialect()))\n        op.getCanonicalizationPatterns(patterns, ctx);\n    }\n\n    (void)mlir::applyPatternsAndFoldGreedily(func, std::move(patterns));\n  }\n};\n\n}  // namespace\n\nstd::unique_ptr<FunctionPass> CreateSymbolicShapeOptimizationPass(\n    bool constraints_only) {\n  return std::make_unique<SymbolicShapeOptimizationPass>(constraints_only);\n}\n\n}  // namespace tensorflow"