"# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Functional tests for SpaceToBatch and BatchToSpace ops.\"\"\"\n\nimport numpy as np\n\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_util\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gen_array_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.platform import test\n\n\ndef space_to_batch_direct(input_array, block_shape, paddings):\n  \"\"\"Direct Python implementation of space-to-batch conversion.\n\n  This is used for tests only.\n\n  Args:\n    input_array: N-D array\n    block_shape: 1-D array of shape [num_block_dims].\n    paddings: 2-D array of shape [num_block_dims, 2].\n\n  Returns:\n    Converted tensor.\n  \"\"\"\n  input_array = np.array(input_array)\n  block_shape = np.array(block_shape)\n  num_block_dims = len(block_shape)\n  paddings = np.array(paddings).reshape((len(block_shape), 2))\n\n  padded = np.pad(input_array,\n                  pad_width=([[0, 0]] + list(paddings) + [[0, 0]] *\n                             (input_array.ndim - 1 - num_block_dims)),\n                  mode=\"constant\")\n  reshaped_padded_shape = [input_array.shape[0]]\n  output_shape = [input_array.shape[0] * np.prod(block_shape)]\n  for block_dim, block_shape_value in enumerate(block_shape):\n    reduced_size = padded.shape[block_dim + 1] // block_shape_value\n    reshaped_padded_shape.append(reduced_size)\n    output_shape.append(reduced_size)\n    reshaped_padded_shape.append(block_shape_value)\n  reshaped_padded_shape.extend(input_array.shape[num_block_dims + 1:])\n  output_shape.extend(input_array.shape[num_block_dims + 1:])\n\n  reshaped_padded = padded.reshape(reshaped_padded_shape)\n  permuted_reshaped_padded = np.transpose(reshaped_padded, (\n      list(np.arange(num_block_dims) * 2 + 2) + [0] +\n      list(np.arange(num_block_dims) * 2 + 1) + list(\n          np.arange(input_array.ndim - num_block_dims - 1) + 1 + num_block_dims\n          * 2)))\n  return permuted_reshaped_padded.reshape(output_shape)\n\n\nclass PythonOpImpl(object):\n\n  @staticmethod\n  def space_to_batch(*args, **kwargs):\n    return array_ops.space_to_batch(*args, **kwargs)\n\n  @staticmethod\n  def batch_to_space(*args, **kwargs):\n    return array_ops.batch_to_space(*args, **kwargs)\n\n\nclass CppOpImpl(object):\n\n  @staticmethod\n  def space_to_batch(*args, **kwargs):\n    return gen_array_ops.space_to_batch(*args, **kwargs)\n\n  @staticmethod\n  def batch_to_space(*args, **kwargs):\n    return gen_array_ops.batch_to_space(*args, **kwargs)\n\n\nclass SpaceToBatchTest(test.TestCase, PythonOpImpl):\n  \"\"\"Tests input-output pairs for the SpaceToBatch and BatchToSpace ops.\n\n  This uses the Python compatibility wrapper that forwards to space_to_batch_nd.\n  \"\"\"\n\n  def _testPad(self, inputs, paddings, block_size, outputs):\n    with self.cached_session():\n      # outputs = space_to_batch(inputs)\n      x_tf = self.space_to_batch(\n          math_ops.cast(inputs, dtypes.float32),\n          paddings,\n          block_size=block_size)\n      self.assertAllEqual(x_tf, outputs)\n      # inputs = batch_to_space(outputs)\n      x_tf = self.batch_to_space(\n          math_ops.cast(outputs, dtypes.float32),\n          paddings,\n          block_size=block_size)\n      self.assertAllEqual(x_tf, inputs)\n\n  def _testOne(self, inputs, block_size, outputs):\n    paddings = np.zeros((2, 2), dtype=np.int32)\n    self._testPad(inputs, paddings, block_size, outputs)\n\n  # [1, 2, 2, 1] <-> [4, 1, 1, 1]\n  @test_util.run_deprecated_v1\n  def testSmallInput2x2(self):\n    x_np = [[[[1], [2]], [[3], [4]]]]\n    block_size = 2\n    x_out = [[[[1]]], [[[2]]], [[[3]]], [[[4]]]]\n    self._testOne(x_np, block_size, x_out)\n\n  # [1, 2, 2, 1] <-> [1, 3, 3, 1] (padding) <-> [9, 1, 1, 1]\n  @test_util.run_deprecated_v1\n  def testSmallInput2x2Pad1x0(self):\n    x_np = [[[[1], [2]], [[3], [4]]]]\n    paddings = np.array([[1, 0], [1, 0]], dtype=np.int32)\n    block_size = 3\n    x_out = [[[[0]]], [[[0]]], [[[0]]], [[[0]]], [[[1]]], [[[2]]], [[[0]]],\n             [[[3]]], [[[4]]]]\n    self._testPad(x_np, paddings, block_size, x_out)\n\n  # Test with depth larger than 1.\n  # [1, 2, 2, 3] <-> [4, 1, 1, 3]\n  @test_util.run_deprecated_v1\n  def testDepthInput2x2(self):\n    x_np = [[[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]]\n    block_size = 2\n    x_out = [[[[1, 2, 3]]], [[[4, 5, 6]]], [[[7, 8, 9]]], [[[10, 11, 12]]]]\n    self._testOne(x_np, block_size, x_out)\n\n  # Test for larger input dimensions.\n  # [1, 4, 4, 1] <-> [4, 2, 2, 1]\n  @test_util.run_deprecated_v1\n  def testLargerInput2x2(self):\n    x_np = [[[[1], [2], [3], [4]], [[5], [6], [7], [8]],\n             [[9], [10], [11], [12]], [[13], [14], [15], [16]]]]\n    block_size = 2\n    x_out = [[[[1], [3]], [[9], [11]]], [[[2], [4]], [[10], [12]]],\n             [[[5], [7]], [[13], [15]]], [[[6], [8]], [[14], [16]]]]\n    self._testOne(x_np, block_size, x_out)\n\n  # Test with batch larger than 1.\n  # [2, 2, 4, 1] <-> [8, 1, 2, 1]\n  @test_util.run_deprecated_v1\n  def testBatchInput2x2(self):\n    x_np = [[[[1], [2], [3], [4]], [[5], [6], [7], [8]]],\n            [[[9], [10], [11], [12]], [[13], [14], [15], [16]]]]\n    block_size = 2\n    x_out = [[[[1], [3]]], [[[9], [11]]], [[[2], [4]]], [[[10], [12]]],\n             [[[5], [7]]], [[[13], [15]]], [[[6], [8]]], [[[14], [16]]]]\n    self._testOne(x_np, block_size, x_out)\n\n  # Tests for larger input spatial dimensions AND batch larger than 1, to ensure\n  # that elements are correctly laid out spatially and properly interleaved\n  # along the batch dimension.\n  # [2, 4, 4, 1] <-> [8, 2, 2, 1]\n  @test_util.run_deprecated_v1\n  def testLargerInputBatch2x2(self):\n    x_np = [[[[1], [2], [3], [4]], [[5], [6], [7], [8]],\n             [[9], [10], [11], [12]], [[13], [14], [15], [16]]],\n            [[[17], [18], [19], [20]], [[21], [22], [23], [24]],\n             [[25], [26], [27], [28]], [[29], [30], [31], [32]]]]\n    x_out = [[[[1], [3]], [[9], [11]]], [[[17], [19]], [[25], [27]]],\n             [[[2], [4]], [[10], [12]]], [[[18], [20]], [[26], [28]]],\n             [[[5], [7]], [[13], [15]]], [[[21], [23]], [[29], [31]]],\n             [[[6], [8]], [[14], [16]]], [[[22], [24]], [[30], [32]]]]\n    block_size = 2\n    self._testOne(x_np, block_size, x_out)\n\n\nclass SpaceToBatchCppTest(SpaceToBatchTest, CppOpImpl):\n  \"\"\"Tests input-output pairs for the SpaceToBatch and BatchToSpace ops.\n\n  This uses the C++ ops.\n  \"\"\"\n  pass\n\n\nclass SpaceToBatchNDTest(test.TestCase):\n  \"\"\"Tests input-output pairs for the SpaceToBatchND and BatchToSpaceND ops.\"\"\"\n\n  def _testPad(self, inputs, block_shape, paddings, outputs):\n    block_shape = np.array(block_shape)\n    paddings = np.array(paddings).reshape((len(block_shape), 2))\n    for use_gpu in [False, True]:\n      with self.cached_session(use_gpu=use_gpu):\n        # outputs = space_to_batch(inputs)\n        x_tf = array_ops.space_to_batch_nd(\n            math_ops.cast(inputs, dtypes.float32), block_shape, paddings)\n        self.assertAllEqual(x_tf, outputs)\n        # inputs = batch_to_space(outputs)\n        x_tf = array_ops.batch_to_space_nd(\n            math_ops.cast(outputs, dtypes.float32), block_shape, paddings)\n        self.assertAllEqual(x_tf, inputs)\n\n  def _testDirect(self, input_shape, block_shape, paddings):\n    inputs = np.arange(np.prod(input_shape), dtype=np.float32)\n    inputs = inputs.reshape(input_shape)\n    self._testPad(inputs, block_shape, paddings,\n                  space_to_batch_direct(inputs, block_shape, paddings))\n\n  @test_util.run_deprecated_v1\n  def testZeroBlockDimsZeroRemainingDims(self):\n    self._testPad(\n        inputs=[1, 2],\n        block_shape=[],\n        paddings=[],\n        outputs=[1, 2],)\n\n  @test_util.run_deprecated_v1\n  def testZeroBlockDimsOneRemainingDim(self):\n    self._testPad(\n        inputs=[[1, 2], [3, 4]],\n        block_shape=[],\n        paddings=[],\n        outputs=[[1, 2], [3, 4]])\n\n    # Same thing, but with a no-op block dim.\n    self._testPad(\n        inputs=[[1, 2], [3, 4]],\n        block_shape=[1],\n        paddings=[[0, 0]],\n        outputs=[[1, 2], [3, 4]])\n\n  @test_util.run_deprecated_v1\n  def testZeroBlockDimsTwoRemainingDims(self):\n    self._testPad(\n        inputs=[[[1, 2], [3, 4]], [[5, 6], [7, 8]]],\n        block_shape=[],\n        paddings=[],\n        outputs=[[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n\n    # Same thing, but with a no-op block dim.\n    self._testPad(\n        inputs=[[[1, 2], [3, 4]], [[5, 6], [7, 8]]],\n        block_shape=[1],\n        paddings=[[0, 0]],\n        outputs=[[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n\n    # Same thing, but with two no-op block dims.\n    self._testPad(\n        inputs=[[[1, 2], [3, 4]], [[5, 6], [7, 8]]],\n        block_shape=[1, 1],\n        paddings=[[0, 0], [0, 0]],\n        outputs=[[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n\n  @test_util.run_deprecated_v1\n  def testOneBlockDimZeroRemainingDims(self):\n    self._testPad(\n        inputs=[[1, 2, 3], [4, 5, 6]],\n        block_shape=[2],\n        paddings=[1, 0],\n        outputs=[[0, 2], [0, 5], [1, 3], [4, 6]])\n\n  @test_util.run_deprecated_v1\n  def testOneBlockDimOneRemainingDim(self):\n    self._testPad(\n        inputs=[[[1, 11], [2, 21], [3, 31]], [[4, 41], [5, 51], [6, 61]]],\n        block_shape=[2],\n        paddings=[1, 0],\n        outputs=[[[0, 0], [2, 21]], [[0, 0], [5, 51]], [[1, 11], [3, 31]],\n                 [[4, 41], [6, 61]]])\n\n  @test_util.run_deprecated_v1\n  def testDirect(self):\n    # Test with zero-size remaining dimension.\n    self._testDirect(\n        input_shape=[3, 1, 2, 0], block_shape=[3], paddings=[[0, 2]])\n\n    # Test with zero-size blocked dimension.\n    self._testDirect(\n        input_shape=[3, 0, 2, 5], block_shape=[3], paddings=[[0, 0]])\n\n    # Test with padding up from zero size.\n    self._testDirect(\n        input_shape=[3, 0, 2, 5], block_shape=[3], paddings=[[1, 2]])\n\n    self._testDirect(\n        input_shape=[3, 3, 4, 5, 2],\n        block_shape=[3, 4, 2],\n        paddings=[[1, 2], [0, 0], [3, 0]])\n\n    self._testDirect(\n        input_shape=[3, 3, 4, 5, 2],\n        block_shape=[3, 4, 2, 2],\n        paddings=[[1, 2], [0, 0], [3, 0], [0, 0]])\n\n    self._testDirect(\n        input_shape=[3, 2, 2, 3, 4, 5, 2, 5],\n        block_shape=[1, 1, 3, 4, 2, 2],\n        paddings=[[0, 0], [0, 0], [1, 2], [0, 0], [3, 0], [0, 0]])\n\n    self._testDirect(\n        input_shape=[3, 2, 2, 3, 4, 5, 2, 5],\n        block_shape=[1, 1, 3, 4, 2, 2, 1],\n        paddings=[[0, 0], [0, 0], [1, 2], [0, 0], [3, 0], [0, 0], [0, 0]])\n\n\nclass SpaceToBatchSpaceToDepth(test.TestCase, PythonOpImpl):\n\n  # Verifies that: space_to_batch(x) = transpose(space_to_depth(transpose(x)))\n  @test_util.run_deprecated_v1\n  def testSpaceToDepthTranspose(self):\n    x = np.arange(5 * 10 * 16 * 7, dtype=np.float32).reshape([5, 10, 16, 7])\n    block_size = 2\n    paddings = np.zeros((2, 2), dtype=np.int32)\n    y1 = self.space_to_batch(x, paddings, block_size=block_size)\n    y2 = array_ops.transpose(\n        array_ops.space_to_depth(\n            array_ops.transpose(x, [3, 1, 2, 0]), block_size=block_size),\n        [3, 1, 2, 0])\n    with self.session():\n      self.assertAllEqual(y1, y2)\n\n\nclass SpaceToBatchSpaceToDepthCpp(SpaceToBatchSpaceToDepth, CppOpImpl):\n  pass\n\n\nclass SpaceToBatchErrorHandlingTest(test.TestCase, PythonOpImpl):\n\n  @test_util.run_deprecated_v1\n  def testInputWrongDimMissingBatch(self):\n    # The input is missing the first dimension (\"batch\")\n    x_np = [[[1], [2]], [[3], [4]]]\n    paddings = np.zeros((2, 2), dtype=np.int32)\n    block_size = 2\n    with self.assertRaises(ValueError):\n      _ = self.space_to_batch(x_np, paddings, block_size)\n\n  @test_util.run_deprecated_v1\n  def testBlockSize0(self):\n    # The block size is 0.\n    x_np = [[[[1], [2]], [[3], [4]]]]\n    paddings = np.zeros((2, 2), dtype=np.int32)\n    block_size = 0\n    with self.assertRaises(ValueError):\n      out_tf = self.space_to_batch(x_np, paddings, block_size)\n      out_tf.eval()\n\n  @test_util.run_deprecated_v1\n  def testBlockSizeOne(self):\n    # The block size is 1. The block size needs to be > 1.\n    x_np = [[[[1], [2]], [[3], [4]]]]\n    paddings = np.zeros((2, 2), dtype=np.int32)\n    block_size = 1\n    with self.assertRaises(ValueError):\n      out_tf = self.space_to_batch(x_np, paddings, block_size)\n      out_tf.eval()\n\n  @test_util.run_deprecated_v1\n  def testBlockSizeLarger(self):\n    # The block size is too large for this input.\n    x_np = [[[[1], [2]], [[3], [4]]]]\n    paddings = np.zeros((2, 2), dtype=np.int32)\n    block_size = 10\n    with self.assertRaises(ValueError):\n      out_tf = self.space_to_batch(x_np, paddings, block_size)\n      self.evaluate(out_tf)\n\n  @test_util.run_deprecated_v1\n  def testBlockSizeNotDivisibleWidth(self):\n    # The block size divides width but not height.\n    x_np = [[[[1], [2], [3]], [[3], [4], [7]]]]\n    paddings = np.zeros((2, 2), dtype=np.int32)\n    block_size = 3\n    with self.assertRaises(ValueError):\n      _ = self.space_to_batch(x_np, paddings, block_size)\n\n  @test_util.run_deprecated_v1\n  def testBlockSizeNotDivisibleHeight(self):\n    # The block size divides height but not width.\n    x_np = [[[[1], [2]], [[3], [4]], [[5], [6]]]]\n    paddings = np.zeros((2, 2), dtype=np.int32)\n    block_size = 3\n    with self.assertRaises(ValueError):\n      _ = self.space_to_batch(x_np, paddings, block_size)\n\n  @test_util.run_deprecated_v1\n  def testBlockSizeNotDivisibleBoth(self):\n    # The block size does not divide neither width or height.\n    x_np = [[[[1], [2]], [[3], [4]]]]\n    paddings = np.zeros((2, 2), dtype=np.int32)\n    block_size = 3\n    with self.assertRaises(ValueError):\n      _ = self.space_to_batch(x_np, paddings, block_size)\n\n  @test_util.run_deprecated_v1\n  def testUnknownShape(self):\n    t = self.space_to_batch(\n        array_ops.placeholder(dtypes.float32),\n        array_ops.placeholder(dtypes.int32),\n        block_size=4)\n    self.assertEqual(4, t.get_shape().ndims)\n\n\nclass SpaceToBatchErrorHandlingCppTest(SpaceToBatchErrorHandlingTest,\n                                       CppOpImpl):\n  pass\n\n\nclass SpaceToBatchNDErrorHandlingTest(test.TestCase):\n\n  def _testStaticShape(self, input_shape, block_shape, paddings, error):\n    block_shape = np.array(block_shape)\n    paddings = np.array(paddings)\n\n    # Try with sizes known at graph construction time.\n    with self.assertRaises(error):\n      _ = array_ops.space_to_batch_nd(\n          np.zeros(input_shape, np.float32), block_shape, paddings)\n\n  def _testDynamicShape(self, input_shape, block_shape, paddings):\n    block_shape = np.array(block_shape)\n    paddings = np.array(paddings)\n    # Try with sizes unknown at graph construction time.\n    input_placeholder = array_ops.placeholder(dtypes.float32)\n    block_shape_placeholder = array_ops.placeholder(\n        dtypes.int32, shape=block_shape.shape)\n    paddings_placeholder = array_ops.placeholder(dtypes.int32)\n    t = array_ops.space_to_batch_nd(input_placeholder, block_shape_placeholder,\n                                    paddings_placeholder)\n\n    with self.assertRaises(ValueError):\n      _ = t.eval({\n          input_placeholder: np.zeros(input_shape, np.float32),\n          block_shape_placeholder: block_shape,\n          paddings_placeholder: paddings\n      })\n\n  def _testShape(self, input_shape, block_shape, paddings, error):\n    self._testStaticShape(input_shape, block_shape, paddings, error)\n    self._testDynamicShape(input_shape, block_shape, paddings)\n\n  @test_util.run_deprecated_v1\n  def testBlockSize0(self):\n    # The block size is 0.\n    self._testShape([1, 2, 2], [0, 2], [[0, 0], [0, 0]], ValueError)\n\n  @test_util.run_deprecated_v1\n  def testBlockSizeNegative(self):\n    self._testShape([1, 2, 2], [-1, 2], [[0, 0], [0, 0]], ValueError)\n\n  @test_util.run_deprecated_v1\n  def testNegativePadding(self):\n    # The padding is negative.\n    self._testShape([1, 2, 2], [1, 1], [[0, -1], [0, 0]], ValueError)\n\n  @test_util.run_deprecated_v1\n  def testBlockSizeNotDivisible(self):\n    # The padded size is not divisible by the block size.\n    self._testShape([1, 2, 3, 1], [3, 3], [[0, 0], [0, 0]], ValueError)\n\n  @test_util.run_deprecated_v1\n  def testBlockDimsMismatch(self):\n    # Shape of block_shape does not match shape of paddings.\n    self._testStaticShape([1, 3, 3, 1], [3, 3], [[0, 0]], ValueError)\n\n  @test_util.run_deprecated_v1\n  def testUnknown(self):\n    # Verify that input shape and paddings shape can be unknown.\n    _ = array_ops.space_to_batch_nd(\n        array_ops.placeholder(dtypes.float32),\n        array_ops.placeholder(\n            dtypes.int32, shape=(2,)),\n        array_ops.placeholder(dtypes.int32))\n\n    # Only number of input dimensions is known.\n    t = array_ops.space_to_batch_nd(\n        array_ops.placeholder(\n            dtypes.float32, shape=(None, None, None, None)),\n        array_ops.placeholder(\n            dtypes.int32, shape=(2,)),\n        array_ops.placeholder(dtypes.int32))\n    self.assertEqual(4, t.get_shape().ndims)\n\n    # Dimensions are partially known.\n    t = array_ops.space_to_batch_nd(\n        array_ops.placeholder(\n            dtypes.float32, shape=(None, None, None, 2)),\n        array_ops.placeholder(\n            dtypes.int32, shape=(2,)),\n        array_ops.placeholder(dtypes.int32))\n    self.assertEqual([None, None, None, 2], t.get_shape().as_list())\n\n    # Dimensions are partially known.\n    t = array_ops.space_to_batch_nd(\n        array_ops.placeholder(\n            dtypes.float32, shape=(3, None, None, 2)), [2, 3],\n        array_ops.placeholder(dtypes.int32))\n    self.assertEqual([3 * 2 * 3, None, None, 2], t.get_shape().as_list())\n\n    # Dimensions are partially known.\n    t = array_ops.space_to_batch_nd(\n        array_ops.placeholder(\n            dtypes.float32, shape=(3, None, 2, 2)), [2, 3], [[1, 1], [0, 1]])\n    self.assertEqual([3 * 2 * 3, None, 1, 2], t.get_shape().as_list())\n\n    # Dimensions are fully known.\n    t = array_ops.space_to_batch_nd(\n        array_ops.placeholder(\n            dtypes.float32, shape=(3, 2, 3, 2)), [2, 3], [[1, 1], [0, 0]])\n    self.assertEqual([3 * 2 * 3, 2, 1, 2], t.get_shape().as_list())\n\n\nclass SpaceToBatchGradientTest(test.TestCase, PythonOpImpl):\n\n  # Check the gradients.\n  def _checkGrad(self, x, paddings, block_size):\n    assert 4 == x.ndim\n    with self.cached_session():\n      tf_x = ops.convert_to_tensor(x)\n      tf_y = self.space_to_batch(tf_x, paddings, block_size)\n      epsilon = 1e-5\n      ((x_jacob_t, x_jacob_n)) = gradient_checker.compute_gradient(\n          tf_x,\n          x.shape,\n          tf_y,\n          tf_y.get_shape().as_list(),\n          x_init_value=x,\n          delta=epsilon)\n\n    self.assertAllClose(x_jacob_t, x_jacob_n, rtol=1e-2, atol=epsilon)\n\n  # Tests a gradient for space_to_batch of x which is a four dimensional\n  # tensor of shape [b, h * block_size, w * block_size, d].\n  def _compare(self, b, h, w, d, block_size, pad_beg, pad_end):\n    block_size_sq = block_size * block_size\n    x = np.random.normal(0, 1, b * h * w * d *\n                         block_size_sq).astype(np.float32).reshape(\n                             [b, h * block_size, w * block_size, d])\n    paddings = np.array(\n        [[pad_beg, pad_end], [pad_beg, pad_end]], dtype=np.int32)\n\n    self._checkGrad(x, paddings, block_size)\n\n  # Don't use very large numbers as dimensions here as the result is tensor\n  # with cartesian product of the dimensions.\n  @test_util.run_deprecated_v1\n  def testSmall(self):\n    block_size = 2\n    pad_beg = 0\n    pad_end = 0\n    self._compare(1, 2, 3, 5, block_size, pad_beg, pad_end)\n\n  @test_util.run_deprecated_v1\n  def testSmall2(self):\n    block_size = 2\n    pad_beg = 0\n    pad_end = 0\n    self._compare(2, 4, 3, 2, block_size, pad_beg, pad_end)\n\n  @test_util.run_deprecated_v1\n  def testSmallPad1x1(self):\n    block_size = 2\n    pad_beg = 1\n    pad_end = 1\n    self._compare(1, 2, 3, 5, block_size, pad_beg, pad_end)\n\n\nclass SpaceToBatchGradientCppTest(SpaceToBatchGradientTest, CppOpImpl):\n  pass\n\n\nclass SpaceToBatchNDGradientTest(test.TestCase):\n\n  # Check the gradients.\n  def _checkGrad(self, x, block_shape, paddings):\n    block_shape = np.array(block_shape)\n    paddings = np.array(paddings).reshape((len(block_shape), 2))\n    with self.cached_session():\n      tf_x = ops.convert_to_tensor(x)\n      tf_y = array_ops.space_to_batch_nd(tf_x, block_shape, paddings)\n      epsilon = 1e-5\n      ((x_jacob_t, x_jacob_n)) = gradient_checker.compute_gradient(\n          tf_x,\n          x.shape,\n          tf_y,\n          tf_y.get_shape().as_list(),\n          x_init_value=x,\n          delta=epsilon)\n\n    self.assertAllClose(x_jacob_t, x_jacob_n, rtol=1e-2, atol=epsilon)\n\n  def _compare(self, input_shape, block_shape, paddings):\n    x = np.random.normal(\n        0, 1, np.prod(input_shape)).astype(np.float32).reshape(input_shape)\n    self._checkGrad(x, block_shape, paddings)\n\n  # Don't use very large numbers as dimensions here as the result is tensor\n  # with cartesian product of the dimensions.\n  @test_util.run_deprecated_v1\n  def testSmall(self):\n    self._compare([1, 4, 6, 5], [2, 2], [[0, 0], [0, 0]])\n\n  @test_util.run_deprecated_v1\n  def testSmall2(self):\n    self._compare([2, 8, 6, 2], [2, 2], [[0, 0], [0, 0]])\n\n  @test_util.run_deprecated_v1\n  def testSmallPad1(self):\n    self._compare([2, 4, 6, 2], [2, 2], [[1, 1], [1, 1]])\n\n  @test_util.run_deprecated_v1\n  def testSmallPadThreeBlockDims(self):\n    self._compare([2, 2, 4, 3, 2], [2, 2, 2], [[1, 1], [1, 1], [1, 0]])\n\n\nclass RequiredSpaceToBatchPaddingsTest(test.TestCase):\n\n  def _checkProperties(self, input_shape, block_shape, base_paddings, paddings,\n                       crops):\n    \"\"\"Checks that `paddings` and `crops` satisfy invariants.\"\"\"\n    num_block_dims = len(block_shape)\n    self.assertEqual(len(input_shape), num_block_dims)\n    if base_paddings is None:\n      base_paddings = np.zeros((num_block_dims, 2), np.int32)\n    self.assertEqual(base_paddings.shape, (num_block_dims, 2))\n    self.assertEqual(paddings.shape, (num_block_dims, 2))\n    self.assertEqual(crops.shape, (num_block_dims, 2))\n    for i in range(num_block_dims):\n      self.assertEqual(paddings[i, 0], base_paddings[i, 0])\n      self.assertLessEqual(0, paddings[i, 1] - base_paddings[i, 1])\n      self.assertLess(paddings[i, 1] - base_paddings[i, 1], block_shape[i])\n      self.assertEqual(\n          (input_shape[i] + paddings[i, 0] + paddings[i, 1]) % block_shape[i],\n          0)\n      self.assertEqual(crops[i, 0], 0)\n      self.assertEqual(crops[i, 1], paddings[i, 1] - base_paddings[i, 1])\n\n  def _test(self, input_shape, block_shape, base_paddings):\n    input_shape = np.array(input_shape)\n    block_shape = np.array(block_shape)\n    if base_paddings is not None:\n      base_paddings = np.array(base_paddings)\n    # Check with constants.\n    paddings, crops = array_ops.required_space_to_batch_paddings(input_shape,\n                                                                 block_shape,\n                                                                 base_paddings)\n    paddings_const = tensor_util.constant_value(paddings)\n    crops_const = tensor_util.constant_value(crops)\n    self.assertIsNotNone(paddings_const)\n    self.assertIsNotNone(crops_const)\n    self._checkProperties(input_shape, block_shape, base_paddings,\n                          paddings_const, crops_const)\n    # Check with non-constants.\n    assignments = {}\n    input_shape_placeholder = array_ops.placeholder(dtypes.int32)\n    assignments[input_shape_placeholder] = input_shape\n    block_shape_placeholder = array_ops.placeholder(dtypes.int32,\n                                                    [len(block_shape)])\n    assignments[block_shape_placeholder] = block_shape\n    if base_paddings is not None:\n      base_paddings_placeholder = array_ops.placeholder(dtypes.int32,\n                                                        [len(block_shape), 2])\n      assignments[base_paddings_placeholder] = base_paddings\n    else:\n      base_paddings_placeholder = None\n    t_paddings, t_crops = array_ops.required_space_to_batch_paddings(\n        input_shape_placeholder, block_shape_placeholder,\n        base_paddings_placeholder)\n    with self.cached_session():\n      paddings_result = t_paddings.eval(assignments)\n      crops_result = t_crops.eval(assignments)\n    self.assertAllEqual(paddings_result, paddings_const)\n    self.assertAllEqual(crops_result, crops_const)\n\n  @test_util.run_deprecated_v1\n  def testSimple(self):\n    self._test(\n        input_shape=np.zeros((0,), np.int32),\n        block_shape=np.zeros((0,), np.int32),\n        base_paddings=None)\n    self._test(\n        input_shape=np.zeros((0,), np.int32),\n        block_shape=np.zeros((0,), np.int32),\n        base_paddings=np.zeros((0, 2), np.int32))\n    self._test(input_shape=[1], block_shape=[2], base_paddings=None)\n    self._test(input_shape=[1], block_shape=[2], base_paddings=[[1, 0]])\n    self._test(input_shape=[3], block_shape=[1], base_paddings=[[1, 2]])\n    self._test(input_shape=[1], block_shape=[2], base_paddings=[[2, 3]])\n    self._test(input_shape=[4, 5], block_shape=[3, 2], base_paddings=None)\n    self._test(\n        input_shape=[4, 5], block_shape=[3, 2], base_paddings=[[0, 0], [0, 1]])\n\n\nif __name__ == \"__main__\":\n  test.main()"