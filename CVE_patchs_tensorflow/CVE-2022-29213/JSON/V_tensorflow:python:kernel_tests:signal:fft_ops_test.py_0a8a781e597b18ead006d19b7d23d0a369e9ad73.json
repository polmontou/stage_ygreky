"# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for fft operations.\"\"\"\n\nimport itertools\n\nfrom absl.testing import parameterized\nimport numpy as np\n\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gen_spectral_ops\nfrom tensorflow.python.ops import gradient_checker_v2\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops.signal import fft_ops\nfrom tensorflow.python.platform import test\n\nVALID_FFT_RANKS = (1, 2, 3)\n\n\n# TODO(rjryan): Investigate precision issues. We should be able to achieve\n# better tolerances, at least for the complex128 tests.\nclass BaseFFTOpsTest(test.TestCase):\n\n  def _compare(self, x, rank, fft_length=None, use_placeholder=False,\n               rtol=1e-4, atol=1e-4):\n    self._compare_forward(x, rank, fft_length, use_placeholder, rtol, atol)\n    self._compare_backward(x, rank, fft_length, use_placeholder, rtol, atol)\n\n  def _compare_forward(self, x, rank, fft_length=None, use_placeholder=False,\n                       rtol=1e-4, atol=1e-4):\n    x_np = self._np_fft(x, rank, fft_length)\n    if use_placeholder:\n      x_ph = array_ops.placeholder(dtype=dtypes.as_dtype(x.dtype))\n      x_tf = self._tf_fft(x_ph, rank, fft_length, feed_dict={x_ph: x})\n    else:\n      x_tf = self._tf_fft(x, rank, fft_length)\n\n    self.assertAllClose(x_np, x_tf, rtol=rtol, atol=atol)\n\n  def _compare_backward(self, x, rank, fft_length=None, use_placeholder=False,\n                        rtol=1e-4, atol=1e-4):\n    x_np = self._np_ifft(x, rank, fft_length)\n    if use_placeholder:\n      x_ph = array_ops.placeholder(dtype=dtypes.as_dtype(x.dtype))\n      x_tf = self._tf_ifft(x_ph, rank, fft_length, feed_dict={x_ph: x})\n    else:\n      x_tf = self._tf_ifft(x, rank, fft_length)\n\n    self.assertAllClose(x_np, x_tf, rtol=rtol, atol=atol)\n\n  def _check_memory_fail(self, x, rank):\n    config = config_pb2.ConfigProto()\n    config.gpu_options.per_process_gpu_memory_fraction = 1e-2\n    with self.cached_session(config=config, force_gpu=True):\n      self._tf_fft(x, rank, fft_length=None)\n\n  def _check_grad_complex(self, func, x, y, result_is_complex=True,\n                          rtol=1e-2, atol=1e-2):\n    with self.cached_session():\n\n      def f(inx, iny):\n        inx.set_shape(x.shape)\n        iny.set_shape(y.shape)\n        # func is a forward or inverse, real or complex, batched or unbatched\n        # FFT function with a complex input.\n        z = func(math_ops.complex(inx, iny))\n        # loss = sum(|z|^2)\n        loss = math_ops.reduce_sum(math_ops.real(z * math_ops.conj(z)))\n        return loss\n\n      ((x_jacob_t, y_jacob_t), (x_jacob_n, y_jacob_n)) = (\n          gradient_checker_v2.compute_gradient(f, [x, y], delta=1e-2))\n\n    self.assertAllClose(x_jacob_t, x_jacob_n, rtol=rtol, atol=atol)\n    self.assertAllClose(y_jacob_t, y_jacob_n, rtol=rtol, atol=atol)\n\n  def _check_grad_real(self, func, x, rtol=1e-2, atol=1e-2):\n    def f(inx):\n      inx.set_shape(x.shape)\n      # func is a forward RFFT function (batched or unbatched).\n      z = func(inx)\n      # loss = sum(|z|^2)\n      loss = math_ops.reduce_sum(math_ops.real(z * math_ops.conj(z)))\n      return loss\n\n    (x_jacob_t,), (x_jacob_n,) = gradient_checker_v2.compute_gradient(\n        f, [x], delta=1e-2)\n    self.assertAllClose(x_jacob_t, x_jacob_n, rtol=rtol, atol=atol)\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass FFTOpsTest(BaseFFTOpsTest, parameterized.TestCase):\n\n  def _tf_fft(self, x, rank, fft_length=None, feed_dict=None):\n    # fft_length unused for complex FFTs.\n    with self.cached_session() as sess:\n      return sess.run(self._tf_fft_for_rank(rank)(x), feed_dict=feed_dict)\n\n  def _tf_ifft(self, x, rank, fft_length=None, feed_dict=None):\n    # fft_length unused for complex FFTs.\n    with self.cached_session() as sess:\n      return sess.run(self._tf_ifft_for_rank(rank)(x), feed_dict=feed_dict)\n\n  def _np_fft(self, x, rank, fft_length=None):\n    if rank == 1:\n      return np.fft.fft2(x, s=fft_length, axes=(-1,))\n    elif rank == 2:\n      return np.fft.fft2(x, s=fft_length, axes=(-2, -1))\n    elif rank == 3:\n      return np.fft.fft2(x, s=fft_length, axes=(-3, -2, -1))\n    else:\n      raise ValueError(\"invalid rank\")\n\n  def _np_ifft(self, x, rank, fft_length=None):\n    if rank == 1:\n      return np.fft.ifft2(x, s=fft_length, axes=(-1,))\n    elif rank == 2:\n      return np.fft.ifft2(x, s=fft_length, axes=(-2, -1))\n    elif rank == 3:\n      return np.fft.ifft2(x, s=fft_length, axes=(-3, -2, -1))\n    else:\n      raise ValueError(\"invalid rank\")\n\n  def _tf_fft_for_rank(self, rank):\n    if rank == 1:\n      return fft_ops.fft\n    elif rank == 2:\n      return fft_ops.fft2d\n    elif rank == 3:\n      return fft_ops.fft3d\n    else:\n      raise ValueError(\"invalid rank\")\n\n  def _tf_ifft_for_rank(self, rank):\n    if rank == 1:\n      return fft_ops.ifft\n    elif rank == 2:\n      return fft_ops.ifft2d\n    elif rank == 3:\n      return fft_ops.ifft3d\n    else:\n      raise ValueError(\"invalid rank\")\n\n  @parameterized.parameters(itertools.product(\n      VALID_FFT_RANKS, range(3), (np.complex64, np.complex128)))\n  def test_empty(self, rank, extra_dims, np_type):\n    dims = rank + extra_dims\n    x = np.zeros((0,) * dims).astype(np_type)\n    self.assertEqual(x.shape, self._tf_fft(x, rank).shape)\n    self.assertEqual(x.shape, self._tf_ifft(x, rank).shape)\n\n  @parameterized.parameters(\n      itertools.product(VALID_FFT_RANKS, range(3),\n                        (np.complex64, np.complex128)))\n  def test_basic(self, rank, extra_dims, np_type):\n    dims = rank + extra_dims\n    tol = 1e-4 if np_type == np.complex64 else 1e-8\n    self._compare(\n        np.mod(np.arange(np.power(4, dims)), 10).reshape(\n            (4,) * dims).astype(np_type), rank, rtol=tol, atol=tol)\n\n  @parameterized.parameters(itertools.product(\n      (1,), range(3), (np.complex64, np.complex128)))\n  def test_large_batch(self, rank, extra_dims, np_type):\n    dims = rank + extra_dims\n    tol = 1e-4 if np_type == np.complex64 else 5e-5\n    self._compare(\n        np.mod(np.arange(np.power(128, dims)), 10).reshape(\n            (128,) * dims).astype(np_type), rank, rtol=tol, atol=tol)\n\n  # TODO(yangzihao): Disable before we can figure out a way to\n  # properly test memory fail for large batch fft.\n  # def test_large_batch_memory_fail(self):\n  #   if test.is_gpu_available(cuda_only=True):\n  #     rank = 1\n  #     for dims in range(rank, rank + 3):\n  #       self._check_memory_fail(\n  #           np.mod(np.arange(np.power(128, dims)), 64).reshape(\n  #               (128,) * dims).astype(np.complex64), rank)\n\n  @parameterized.parameters(itertools.product(\n      VALID_FFT_RANKS, range(3), (np.complex64, np.complex128)))\n  def test_placeholder(self, rank, extra_dims, np_type):\n    if context.executing_eagerly():\n      return\n    tol = 1e-4 if np_type == np.complex64 else 1e-8\n    dims = rank + extra_dims\n    self._compare(\n        np.mod(np.arange(np.power(4, dims)), 10).reshape(\n            (4,) * dims).astype(np_type),\n        rank, use_placeholder=True, rtol=tol, atol=tol)\n\n  @parameterized.parameters(itertools.product(\n      VALID_FFT_RANKS, range(3), (np.complex64, np.complex128)))\n  def test_random(self, rank, extra_dims, np_type):\n    tol = 1e-4 if np_type == np.complex64 else 5e-6\n    dims = rank + extra_dims\n    def gen(shape):\n      n = np.prod(shape)\n      re = np.random.uniform(size=n)\n      im = np.random.uniform(size=n)\n      return (re + im * 1j).reshape(shape)\n\n    self._compare(gen((4,) * dims).astype(np_type), rank,\n                  rtol=tol, atol=tol)\n\n  @parameterized.parameters(itertools.product(\n      VALID_FFT_RANKS,\n      # Check a variety of sizes (power-of-2, odd, etc.)\n      [128, 256, 512, 1024, 127, 255, 511, 1023],\n      (np.complex64, np.complex128)))\n  def test_random_1d(self, rank, dim, np_type):\n    has_gpu = test.is_gpu_available(cuda_only=True)\n    tol = {(np.complex64, True): 1e-4,\n           (np.complex64, False): 1e-2,\n           (np.complex128, True): 1e-4,\n           (np.complex128, False): 1e-2}[(np_type, has_gpu)]\n    def gen(shape):\n      n = np.prod(shape)\n      re = np.random.uniform(size=n)\n      im = np.random.uniform(size=n)\n      return (re + im * 1j).reshape(shape)\n\n    self._compare(gen((dim,)).astype(np_type), 1, rtol=tol, atol=tol)\n\n  def test_error(self):\n    # TODO(rjryan): Fix this test under Eager.\n    if context.executing_eagerly():\n      return\n    for rank in VALID_FFT_RANKS:\n      for dims in range(0, rank):\n        x = np.zeros((1,) * dims).astype(np.complex64)\n        with self.assertRaisesWithPredicateMatch(\n            ValueError, \"Shape must be .*rank {}.*\".format(rank)):\n          self._tf_fft(x, rank)\n        with self.assertRaisesWithPredicateMatch(\n            ValueError, \"Shape must be .*rank {}.*\".format(rank)):\n          self._tf_ifft(x, rank)\n\n  @parameterized.parameters(itertools.product(\n      VALID_FFT_RANKS, range(2), (np.float32, np.float64)))\n  def test_grad_simple(self, rank, extra_dims, np_type):\n    tol = 1e-4 if np_type == np.float32 else 1e-10\n    dims = rank + extra_dims\n    re = np.ones(shape=(4,) * dims, dtype=np_type) / 10.0\n    im = np.zeros(shape=(4,) * dims, dtype=np_type)\n    self._check_grad_complex(self._tf_fft_for_rank(rank), re, im,\n                             rtol=tol, atol=tol)\n    self._check_grad_complex(self._tf_ifft_for_rank(rank), re, im,\n                             rtol=tol, atol=tol)\n\n  @parameterized.parameters(itertools.product(\n      VALID_FFT_RANKS, range(2), (np.float32, np.float64)))\n  def test_grad_random(self, rank, extra_dims, np_type):\n    dims = rank + extra_dims\n    tol = 1e-2 if np_type == np.float32 else 1e-10\n    re = np.random.rand(*((3,) * dims)).astype(np_type) * 2 - 1\n    im = np.random.rand(*((3,) * dims)).astype(np_type) * 2 - 1\n    self._check_grad_complex(self._tf_fft_for_rank(rank), re, im,\n                             rtol=tol, atol=tol)\n    self._check_grad_complex(self._tf_ifft_for_rank(rank), re, im,\n                             rtol=tol, atol=tol)\n\n\n@test_util.run_all_in_graph_and_eager_modes\n@test_util.disable_xla(\"b/155276727\")\nclass RFFTOpsTest(BaseFFTOpsTest, parameterized.TestCase):\n\n  def _tf_fft(self, x, rank, fft_length=None, feed_dict=None):\n    with self.cached_session() as sess:\n      return sess.run(\n          self._tf_fft_for_rank(rank)(x, fft_length), feed_dict=feed_dict)\n\n  def _tf_ifft(self, x, rank, fft_length=None, feed_dict=None):\n    with self.cached_session() as sess:\n      return sess.run(\n          self._tf_ifft_for_rank(rank)(x, fft_length), feed_dict=feed_dict)\n\n  def _np_fft(self, x, rank, fft_length=None):\n    if rank == 1:\n      return np.fft.rfft2(x, s=fft_length, axes=(-1,))\n    elif rank == 2:\n      return np.fft.rfft2(x, s=fft_length, axes=(-2, -1))\n    elif rank == 3:\n      return np.fft.rfft2(x, s=fft_length, axes=(-3, -2, -1))\n    else:\n      raise ValueError(\"invalid rank\")\n\n  def _np_ifft(self, x, rank, fft_length=None):\n    if rank == 1:\n      return np.fft.irfft2(x, s=fft_length, axes=(-1,))\n    elif rank == 2:\n      return np.fft.irfft2(x, s=fft_length, axes=(-2, -1))\n    elif rank == 3:\n      return np.fft.irfft2(x, s=fft_length, axes=(-3, -2, -1))\n    else:\n      raise ValueError(\"invalid rank\")\n\n  def _tf_fft_for_rank(self, rank):\n    if rank == 1:\n      return fft_ops.rfft\n    elif rank == 2:\n      return fft_ops.rfft2d\n    elif rank == 3:\n      return fft_ops.rfft3d\n    else:\n      raise ValueError(\"invalid rank\")\n\n  def _tf_ifft_for_rank(self, rank):\n    if rank == 1:\n      return fft_ops.irfft\n    elif rank == 2:\n      return fft_ops.irfft2d\n    elif rank == 3:\n      return fft_ops.irfft3d\n    else:\n      raise ValueError(\"invalid rank\")\n\n  # rocFFT requires/assumes that the input to the irfft transform\n  # is of the form that is a valid output from the rfft transform\n  # (i.e. it cannot be a set of random numbers)\n  # So for ROCm, call rfft and use its output as the input for testing irfft\n  def _generate_valid_irfft_input(self, c2r, np_ctype, r2c, np_rtype, rank,\n                                  fft_length):\n    if test.is_built_with_rocm():\n      return self._np_fft(r2c.astype(np_rtype), rank, fft_length)\n    else:\n      return c2r.astype(np_ctype)\n\n  @parameterized.parameters(itertools.product(\n      VALID_FFT_RANKS, range(3), (np.float32, np.float64)))\n\n  def test_empty(self, rank, extra_dims, np_rtype):\n    np_ctype = np.complex64 if np_rtype == np.float32 else np.complex128\n    dims = rank + extra_dims\n    x = np.zeros((0,) * dims).astype(np_rtype)\n    self.assertEqual(x.shape, self._tf_fft(x, rank).shape)\n    x = np.zeros((0,) * dims).astype(np_ctype)\n    self.assertEqual(x.shape, self._tf_ifft(x, rank).shape)\n\n  @parameterized.parameters(itertools.product(\n      VALID_FFT_RANKS, range(3), (5, 6), (np.float32, np.float64)))\n  def test_basic(self, rank, extra_dims, size, np_rtype):\n    np_ctype = np.complex64 if np_rtype == np.float32 else np.complex128\n    tol = 1e-4 if np_rtype == np.float32 else 5e-5\n    dims = rank + extra_dims\n    inner_dim = size // 2 + 1\n    r2c = np.mod(np.arange(np.power(size, dims)), 10).reshape(\n        (size,) * dims)\n    fft_length = (size,) * rank\n    self._compare_forward(\n        r2c.astype(np_rtype), rank, fft_length, rtol=tol, atol=tol)\n    c2r = np.mod(np.arange(np.power(size, dims - 1) * inner_dim),\n                 10).reshape((size,) * (dims - 1) + (inner_dim,))\n    c2r = self._generate_valid_irfft_input(c2r, np_ctype, r2c, np_rtype, rank,\n                                           fft_length)\n    self._compare_backward(c2r, rank, fft_length, rtol=tol, atol=tol)\n\n  @parameterized.parameters(itertools.product(\n      (1,), range(3), (64, 128), (np.float32, np.float64)))\n  def test_large_batch(self, rank, extra_dims, size, np_rtype):\n    np_ctype = np.complex64 if np_rtype == np.float32 else np.complex128\n    tol = 1e-4 if np_rtype == np.float32 else 1e-5\n    dims = rank + extra_dims\n    inner_dim = size // 2 + 1\n    r2c = np.mod(np.arange(np.power(size, dims)), 10).reshape(\n        (size,) * dims)\n    fft_length = (size,) * rank\n    self._compare_forward(\n        r2c.astype(np_rtype), rank, fft_length, rtol=tol, atol=tol)\n    c2r = np.mod(np.arange(np.power(size, dims - 1) * inner_dim),\n                 10).reshape((size,) * (dims - 1) + (inner_dim,))\n    c2r = self._generate_valid_irfft_input(c2r, np_ctype, r2c, np_rtype, rank,\n                                           fft_length)\n    self._compare_backward(c2r, rank, fft_length, rtol=tol, atol=tol)\n\n  @parameterized.parameters(itertools.product(\n      VALID_FFT_RANKS, range(3), (5, 6), (np.float32, np.float64)))\n  def test_placeholder(self, rank, extra_dims, size, np_rtype):\n    if context.executing_eagerly():\n      return\n    np_ctype = np.complex64 if np_rtype == np.float32 else np.complex128\n    tol = 1e-4 if np_rtype == np.float32 else 1e-8\n    dims = rank + extra_dims\n    inner_dim = size // 2 + 1\n    r2c = np.mod(np.arange(np.power(size, dims)), 10).reshape(\n        (size,) * dims)\n    fft_length = (size,) * rank\n    self._compare_forward(\n        r2c.astype(np_rtype),\n        rank,\n        fft_length,\n        use_placeholder=True,\n        rtol=tol,\n        atol=tol)\n    c2r = np.mod(np.arange(np.power(size, dims - 1) * inner_dim),\n                 10).reshape((size,) * (dims - 1) + (inner_dim,))\n    c2r = self._generate_valid_irfft_input(c2r, np_ctype, r2c, np_rtype, rank,\n                                           fft_length)\n    self._compare_backward(\n        c2r, rank, fft_length, use_placeholder=True, rtol=tol, atol=tol)\n\n  @parameterized.parameters(itertools.product(\n      VALID_FFT_RANKS, range(3), (5, 6), (np.float32, np.float64)))\n  def test_fft_lenth_truncate(self, rank, extra_dims, size, np_rtype):\n    \"\"\"Test truncation (FFT size < dimensions).\"\"\"\n    if test.is_built_with_rocm() and (rank == 3):\n      # TODO(rocm): fix me\n      # rfft fails for rank == 3 on ROCm\n      self.skipTest(\"Test fails on ROCm...fix me\")\n    np_ctype = np.complex64 if np_rtype == np.float32 else np.complex128\n    tol = 1e-4 if np_rtype == np.float32 else 8e-5\n    dims = rank + extra_dims\n    inner_dim = size // 2 + 1\n    r2c = np.mod(np.arange(np.power(size, dims)), 10).reshape(\n        (size,) * dims)\n    c2r = np.mod(np.arange(np.power(size, dims - 1) * inner_dim),\n                 10).reshape((size,) * (dims - 1) + (inner_dim,))\n    fft_length = (size - 2,) * rank\n    self._compare_forward(r2c.astype(np_rtype), rank, fft_length,\n                          rtol=tol, atol=tol)\n    c2r = self._generate_valid_irfft_input(c2r, np_ctype, r2c, np_rtype, rank,\n                                           fft_length)\n    self._compare_backward(c2r, rank, fft_length, rtol=tol, atol=tol)\n    # Confirm it works with unknown shapes as well.\n    if not context.executing_eagerly():\n      self._compare_forward(\n          r2c.astype(np_rtype),\n          rank,\n          fft_length,\n          use_placeholder=True,\n          rtol=tol, atol=tol)\n      self._compare_backward(\n          c2r, rank, fft_length, use_placeholder=True, rtol=tol, atol=tol)\n\n  @parameterized.parameters(itertools.product(\n      VALID_FFT_RANKS, range(3), (5, 6), (np.float32, np.float64)))\n  def test_fft_lenth_pad(self, rank, extra_dims, size, np_rtype):\n    \"\"\"Test padding (FFT size > dimensions).\"\"\"\n    np_ctype = np.complex64 if np_rtype == np.float32 else np.complex128\n    tol = 1e-4 if np_rtype == np.float32 else 8e-5\n    dims = rank + extra_dims\n    inner_dim = size // 2 + 1\n    r2c = np.mod(np.arange(np.power(size, dims)), 10).reshape(\n        (size,) * dims)\n    c2r = np.mod(np.arange(np.power(size, dims - 1) * inner_dim),\n                 10).reshape((size,) * (dims - 1) + (inner_dim,))\n    fft_length = (size + 2,) * rank\n    self._compare_forward(r2c.astype(np_rtype), rank, fft_length,\n                          rtol=tol, atol=tol)\n    c2r = self._generate_valid_irfft_input(c2r, np_ctype, r2c, np_rtype, rank,\n                                           fft_length)\n    self._compare_backward(c2r.astype(np_ctype), rank, fft_length,\n                           rtol=tol, atol=tol)\n    # Confirm it works with unknown shapes as well.\n    if not context.executing_eagerly():\n      self._compare_forward(\n          r2c.astype(np_rtype),\n          rank,\n          fft_length,\n          use_placeholder=True,\n          rtol=tol, atol=tol)\n      self._compare_backward(\n          c2r.astype(np_ctype),\n          rank,\n          fft_length,\n          use_placeholder=True,\n          rtol=tol, atol=tol)\n\n  @parameterized.parameters(itertools.product(\n      VALID_FFT_RANKS, range(3), (5, 6), (np.float32, np.float64)))\n  def test_random(self, rank, extra_dims, size, np_rtype):\n    def gen_real(shape):\n      n = np.prod(shape)\n      re = np.random.uniform(size=n)\n      ret = re.reshape(shape)\n      return ret\n\n    def gen_complex(shape):\n      n = np.prod(shape)\n      re = np.random.uniform(size=n)\n      im = np.random.uniform(size=n)\n      ret = (re + im * 1j).reshape(shape)\n      return ret\n    np_ctype = np.complex64 if np_rtype == np.float32 else np.complex128\n    tol = 1e-4 if np_rtype == np.float32 else 1e-5\n    dims = rank + extra_dims\n    r2c = gen_real((size,) * dims)\n    inner_dim = size // 2 + 1\n    fft_length = (size,) * rank\n    self._compare_forward(\n        r2c.astype(np_rtype), rank, fft_length, rtol=tol, atol=tol)\n    complex_dims = (size,) * (dims - 1) + (inner_dim,)\n    c2r = gen_complex(complex_dims)\n    c2r = self._generate_valid_irfft_input(c2r, np_ctype, r2c, np_rtype, rank,\n                                           fft_length)\n    self._compare_backward(c2r, rank, fft_length, rtol=tol, atol=tol)\n\n  def test_error(self):\n    # TODO(rjryan): Fix this test under Eager.\n    if context.executing_eagerly():\n      return\n    for rank in VALID_FFT_RANKS:\n      for dims in range(0, rank):\n        x = np.zeros((1,) * dims).astype(np.complex64)\n        with self.assertRaisesWithPredicateMatch(\n            ValueError, \"Shape .* must have rank at least {}\".format(rank)):\n          self._tf_fft(x, rank)\n        with self.assertRaisesWithPredicateMatch(\n            ValueError, \"Shape .* must have rank at least {}\".format(rank)):\n          self._tf_ifft(x, rank)\n      for dims in range(rank, rank + 2):\n        x = np.zeros((1,) * rank)\n\n        # Test non-rank-1 fft_length produces an error.\n        fft_length = np.zeros((1, 1)).astype(np.int32)\n        with self.assertRaisesWithPredicateMatch(ValueError,\n                                                 \"Shape .* must have rank 1\"):\n          self._tf_fft(x, rank, fft_length)\n        with self.assertRaisesWithPredicateMatch(ValueError,\n                                                 \"Shape .* must have rank 1\"):\n          self._tf_ifft(x, rank, fft_length)\n\n        # Test wrong fft_length length.\n        fft_length = np.zeros((rank + 1,)).astype(np.int32)\n        with self.assertRaisesWithPredicateMatch(\n            ValueError, \"Dimension must be .*but is {}.*\".format(rank + 1)):\n          self._tf_fft(x, rank, fft_length)\n        with self.assertRaisesWithPredicateMatch(\n            ValueError, \"Dimension must be .*but is {}.*\".format(rank + 1)):\n          self._tf_ifft(x, rank, fft_length)\n\n      # Test that calling the kernel directly without padding to fft_length\n      # produces an error.\n      rffts_for_rank = {\n          1: [gen_spectral_ops.rfft, gen_spectral_ops.irfft],\n          2: [gen_spectral_ops.rfft2d, gen_spectral_ops.irfft2d],\n          3: [gen_spectral_ops.rfft3d, gen_spectral_ops.irfft3d]\n      }\n      rfft_fn, irfft_fn = rffts_for_rank[rank]\n      with self.assertRaisesWithPredicateMatch(\n          errors.InvalidArgumentError,\n          \"Input dimension .* must have length of at least 6 but got: 5\"):\n        x = np.zeros((5,) * rank).astype(np.float32)\n        fft_length = [6] * rank\n        with self.cached_session():\n          self.evaluate(rfft_fn(x, fft_length))\n\n      with self.assertRaisesWithPredicateMatch(\n          errors.InvalidArgumentError,\n          \"Input dimension .* must have length of at least .* but got: 3\"):\n        x = np.zeros((3,) * rank).astype(np.complex64)\n        fft_length = [6] * rank\n        with self.cached_session():\n          self.evaluate(irfft_fn(x, fft_length))\n\n  @parameterized.parameters(itertools.product(\n      VALID_FFT_RANKS, range(2), (5, 6), (np.float32, np.float64)))\n  def test_grad_simple(self, rank, extra_dims, size, np_rtype):\n    # rfft3d/irfft3d do not have gradients yet.\n    if rank == 3:\n      return\n    dims = rank + extra_dims\n    tol = 1e-3 if np_rtype == np.float32 else 1e-10\n    re = np.ones(shape=(size,) * dims, dtype=np_rtype)\n    im = -np.ones(shape=(size,) * dims, dtype=np_rtype)\n    self._check_grad_real(self._tf_fft_for_rank(rank), re,\n                          rtol=tol, atol=tol)\n    if test.is_built_with_rocm():\n      # Fails on ROCm because of irfft peculairity\n      return\n    self._check_grad_complex(\n        self._tf_ifft_for_rank(rank), re, im, result_is_complex=False,\n        rtol=tol, atol=tol)\n\n  @parameterized.parameters(itertools.product(\n      VALID_FFT_RANKS, range(2), (5, 6), (np.float32, np.float64)))\n  def test_grad_random(self, rank, extra_dims, size, np_rtype):\n    # rfft3d/irfft3d do not have gradients yet.\n    if rank == 3:\n      return\n    dims = rank + extra_dims\n    tol = 1e-2 if np_rtype == np.float32 else 1e-10\n    re = np.random.rand(*((size,) * dims)).astype(np_rtype) * 2 - 1\n    im = np.random.rand(*((size,) * dims)).astype(np_rtype) * 2 - 1\n    self._check_grad_real(self._tf_fft_for_rank(rank), re,\n                          rtol=tol, atol=tol)\n    if test.is_built_with_rocm():\n      # Fails on ROCm because of irfft peculairity\n      return\n    self._check_grad_complex(\n        self._tf_ifft_for_rank(rank), re, im, result_is_complex=False,\n        rtol=tol, atol=tol)\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass FFTShiftTest(test.TestCase, parameterized.TestCase):\n\n  def test_definition(self):\n    with self.session():\n      x = [0, 1, 2, 3, 4, -4, -3, -2, -1]\n      y = [-4, -3, -2, -1, 0, 1, 2, 3, 4]\n      self.assertAllEqual(fft_ops.fftshift(x), y)\n      self.assertAllEqual(fft_ops.ifftshift(y), x)\n      x = [0, 1, 2, 3, 4, -5, -4, -3, -2, -1]\n      y = [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4]\n      self.assertAllEqual(fft_ops.fftshift(x), y)\n      self.assertAllEqual(fft_ops.ifftshift(y), x)\n\n  def test_axes_keyword(self):\n    with self.session():\n      freqs = [[0, 1, 2], [3, 4, -4], [-3, -2, -1]]\n      shifted = [[-1, -3, -2], [2, 0, 1], [-4, 3, 4]]\n      self.assertAllEqual(fft_ops.fftshift(freqs, axes=(0, 1)), shifted)\n      self.assertAllEqual(\n          fft_ops.fftshift(freqs, axes=0),\n          fft_ops.fftshift(freqs, axes=(0,)))\n      self.assertAllEqual(fft_ops.ifftshift(shifted, axes=(0, 1)), freqs)\n      self.assertAllEqual(\n          fft_ops.ifftshift(shifted, axes=0),\n          fft_ops.ifftshift(shifted, axes=(0,)))\n      self.assertAllEqual(fft_ops.fftshift(freqs), shifted)\n      self.assertAllEqual(fft_ops.ifftshift(shifted), freqs)\n\n  def test_numpy_compatibility(self):\n    with self.session():\n      x = [0, 1, 2, 3, 4, -4, -3, -2, -1]\n      y = [-4, -3, -2, -1, 0, 1, 2, 3, 4]\n      self.assertAllEqual(fft_ops.fftshift(x), np.fft.fftshift(x))\n      self.assertAllEqual(fft_ops.ifftshift(y), np.fft.ifftshift(y))\n      x = [0, 1, 2, 3, 4, -5, -4, -3, -2, -1]\n      y = [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4]\n      self.assertAllEqual(fft_ops.fftshift(x), np.fft.fftshift(x))\n      self.assertAllEqual(fft_ops.ifftshift(y), np.fft.ifftshift(y))\n      freqs = [[0, 1, 2], [3, 4, -4], [-3, -2, -1]]\n      shifted = [[-1, -3, -2], [2, 0, 1], [-4, 3, 4]]\n      self.assertAllEqual(\n          fft_ops.fftshift(freqs, axes=(0, 1)),\n          np.fft.fftshift(freqs, axes=(0, 1)))\n      self.assertAllEqual(\n          fft_ops.ifftshift(shifted, axes=(0, 1)),\n          np.fft.ifftshift(shifted, axes=(0, 1)))\n\n  @parameterized.parameters(None, 1, ([1, 2],))\n  def test_placeholder(self, axes):\n    if context.executing_eagerly():\n      return\n    x = array_ops.placeholder(shape=[None, None, None], dtype=\"float32\")\n    y_fftshift = fft_ops.fftshift(x, axes=axes)\n    y_ifftshift = fft_ops.ifftshift(x, axes=axes)\n    x_np = np.random.rand(16, 256, 256)\n    with self.session() as sess:\n      y_fftshift_res, y_ifftshift_res = sess.run(\n          [y_fftshift, y_ifftshift],\n          feed_dict={x: x_np})\n    self.assertAllClose(y_fftshift_res, np.fft.fftshift(x_np, axes=axes))\n    self.assertAllClose(y_ifftshift_res, np.fft.ifftshift(x_np, axes=axes))\n\n  def test_negative_axes(self):\n    with self.session():\n      freqs = [[0, 1, 2], [3, 4, -4], [-3, -2, -1]]\n      shifted = [[-1, -3, -2], [2, 0, 1], [-4, 3, 4]]\n      self.assertAllEqual(fft_ops.fftshift(freqs, axes=(0, -1)), shifted)\n      self.assertAllEqual(fft_ops.ifftshift(shifted, axes=(0, -1)), freqs)\n      self.assertAllEqual(\n          fft_ops.fftshift(freqs, axes=-1), fft_ops.fftshift(freqs, axes=(1,)))\n      self.assertAllEqual(\n          fft_ops.ifftshift(shifted, axes=-1),\n          fft_ops.ifftshift(shifted, axes=(1,)))\n\n\nif __name__ == \"__main__\":\n  test.main()"