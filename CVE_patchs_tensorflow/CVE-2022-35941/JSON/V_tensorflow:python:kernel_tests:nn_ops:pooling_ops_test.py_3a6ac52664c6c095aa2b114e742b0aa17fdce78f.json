"# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Functional tests for pooling operations.\"\"\"\n\nimport collections\nimport os\n\nfrom absl.testing import parameterized\nimport numpy as np\n\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_util\nimport tensorflow.python.framework.config as config_exec\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gen_array_ops\nfrom tensorflow.python.ops import gen_nn_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import variables\nimport tensorflow.python.ops.nn_grad  # pylint: disable=unused-import\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.platform import tf_logging\n\n\ndef GetDeviceScope(self, use_gpu=False):\n  if context.executing_eagerly():\n    if use_gpu and test.is_gpu_available():\n      return ops.device(\"GPU:0\")\n    return ops.device(\"CPU:0\")\n  else:\n    return self.session(use_gpu=use_gpu)\n\n\n# TODO(jlebar): Convert the rest of this file to parameters.parameterized().\n# Then remove GetTestConfigs() and rename GetTestConfigsDicts().\ndef GetTestConfigsDicts(v1_fn,\n                        v2_fn=None,\n                        one_dimensional=False,\n                        allow_gpu=True):\n  # (data_format, use_gpu) tuple\n  if one_dimensional:\n    configs0 = [\n        (\"NWC\", False),\n        (\"NWC\", True),\n        (\"NCW\", True),\n    ]\n  else:\n    configs0 = [\n        (\"NHWC\", False),\n        (\"NHWC\", True),\n        (\"NCHW\", True),\n    ]\n    # NCHW_VECT_C only supported for max_pool.\n    if (v1_fn == nn_ops.max_pool or v1_fn == nn_ops.max_pool1d or\n        v2_fn == nn_ops.max_pool_v2 or v2_fn == gen_nn_ops.max_pool_v2):\n      configs0.append((\"NCHW_VECT_C\", True))\n\n  # (data_format, use_gpu, data_type) tuple\n  configs1 = []\n  for data_format, use_gpu in configs0:\n    configs1.append((data_format, use_gpu, dtypes.float32))\n\n    # In our test, VECT_C always uses float32.  (It gets converted to int8 in\n    # the test runner.)\n    if data_format == \"NCHW_VECT_C\":\n      continue\n\n    configs1 += [(data_format, use_gpu, dtypes.float16),\n                 (data_format, use_gpu, dtypes.float64)]\n\n  # Convert from tuple to dict and add v1/v2 versions.\n  ret = []\n  for data_format, use_gpu, data_type in configs1:\n    ret.append({\n        \"pool_func\": v1_fn,\n        \"data_format\": data_format,\n        \"data_type\": data_type,\n        \"use_gpu\": use_gpu,\n        \"v2\": False\n    })\n    if v2_fn:\n      ret.append({\n          \"pool_func\": v2_fn,\n          \"data_format\": data_format,\n          \"data_type\": data_type,\n          \"use_gpu\": use_gpu,\n          \"v2\": False\n      })\n      ret.append({\n          \"pool_func\": v2_fn,\n          \"data_format\": data_format,\n          \"data_type\": data_type,\n          \"use_gpu\": use_gpu,\n          \"v2\": True\n      })\n\n  # Filter out GPU configs if necessary.\n  if not allow_gpu:\n    ret = [c for c in ret if not c[\"use_gpu\"]]\n\n  return ret\n\n\ndef GetTestConfigs(include_nchw_vect_c=False, one_dimensional=False):\n  \"\"\"Get all the valid tests configs to run.\n\n  Args:\n    include_nchw_vect_c: Whether to include NCHW_VECT_C in the test configs.\n    one_dimensional: If it's a 1D test\n\n  Returns:\n    all the valid test configs as tuples of data_format and use_gpu.\n  \"\"\"\n  if one_dimensional:\n    test_configs = [(\"NWC\", False), (\"NWC\", True)]\n    if test.is_gpu_available(cuda_only=True):\n      test_configs += [(\"NCW\", True)]\n    return test_configs\n  test_configs = [(\"NHWC\", False), (\"NHWC\", True)]\n  if not test.is_gpu_available(cuda_only=True):\n    tf_logging.info(\"NCHW and NCHW_VECT_C tests skipped because not run with \"\n                    \"--config=cuda or no GPUs available.\")\n    return test_configs\n  # \"NCHW\" format is currently supported exclusively on CUDA GPUs.\n  test_configs += [(\"NCHW\", True)]\n  if include_nchw_vect_c:\n    if test.is_gpu_available(\n        cuda_only=True, min_cuda_compute_capability=(6, 1)):\n      test_configs += [(\"NCHW_VECT_C\", True)]\n    else:\n      tf_logging.info(\"NCHW_VECT_C test skipped because no GPUs with \"\n                      \"compute capability >= 6.1 are available.\")\n\n  return test_configs\n\n\ndef GetShrunkInceptionMaxPoolShapes(shrink=30):\n  \"\"\"Iterator for some of the max pool ops in the Inception 2015 model.\n\n  Args:\n    shrink: Factor to shrink depth relative to Inception.\n\n  Yields:\n    Tuple (name, input_size, filter_size, out_size, strides, padding)\n  \"\"\"\n  names = [\"maxpool2\", \"maxpool3\", \"maxpool4\", \"maxpool5\"]\n  input_sizes = [[32, 71, 71, 192], [32, 35, 35, 288], [32, 17, 17, 1248],\n                 [32, 8, 8, 2048]]\n  filter_sizes = [[1, 3, 3, 1], [1, 3, 3, 1], [1, 3, 3, 1], [1, 3, 3, 1]]\n  output_sizes = [[32, 35, 35, 192], [32, 17, 17, 288], [32, 8, 8, 1248],\n                  [32, 8, 8, 2048]]\n  strides = [[1, 2, 2, 1], [1, 2, 2, 1], [1, 2, 2, 1], [1, 1, 1, 1]]\n  # Shrink each depth value\n  for i in input_sizes:\n    i[3] //= shrink\n  for o in output_sizes:\n    o[3] //= shrink\n  paddings = [\"VALID\", \"VALID\", \"VALID\", \"SAME\"]\n  for n, i, f, o, s, p in zip(names, input_sizes, filter_sizes, output_sizes,\n                              strides, paddings):\n    yield n, i, f, o, s, p\n\n\n@test_util.with_eager_op_as_function\nclass PoolingTest(test.TestCase, parameterized.TestCase):\n\n  def _isMaxPool(self, func):\n    return func in (nn_ops.max_pool, nn_ops.max_pool_v2)\n\n  def _VerifyOneType(self, pool_func, input_sizes, ksize, strides, padding,\n                     data_format, data_type, expected, use_gpu, v2,\n                     use_negative_input=False):\n    \"\"\"Verifies the output values of the pooling function.\n\n    Args:\n      pool_func: Function to be called, co.MaxPool, co.AvgPool,\n        or the Lua version.\n      input_sizes: Input tensor dimensions.\n      ksize: The kernel size dimensions\n      strides: The stride dimensions\n      padding: Padding type.\n      data_format: The data format we use to run the pooling operation.\n      data_type: The data type to use to run the pooling operation.\n      expected: An array containing the expected operation outputs.\n      use_gpu: Whether we are running on GPU.\n      v2: Whether to use v2 version.\n      use_negative_input: If the input values should be negative.\n    \"\"\"\n    # Check that this test is compatible with the hardware we have.  (Really\n    # this should be done in GetTestConfigsDicts(), but when that runs, we\n    # haven't initialized enough of TF to know what our hardware is!)\n    if use_gpu and not test.is_gpu_available():\n      self.skipTest(\"No GPU is available.\")\n    if use_gpu and data_type == dtypes.float64 and test.is_built_with_rocm():\n      self.skipTest(\"ROCm pooling ops don't support float64.\")\n    if use_gpu and data_format == \"NCHW_VECT_C\" and not test.is_gpu_available(\n        cuda_only=True, min_cuda_compute_capability=(6, 1)):\n      self.skipTest(\"NCHW_VECT_C requires sm61+.\")\n\n    if v2 and data_format != \"NHWC\":\n      self.skipTest(\"v2 not supported for %s\" % data_format)\n    if v2 and not isinstance(padding, str):\n      self.skipTest(\"non-constant ksize/strides requires nonexplicit padding\")\n    if data_format == \"NCHW_VECT_C\":\n      if data_type != dtypes.float32:\n        self.skipTest(\"quantization to qint8 not implemented for %r\" %\n                      data_type)\n      if input_sizes[-1] % 4 != 0:\n        self.skipTest(\"Skipping test for depth %d\" % input_sizes[-1])\n\n    total_size = 1\n    for s in input_sizes:\n      total_size *= s\n    tf_logging.info(\"Running %s test. %r %r %d %r %r %r %s\", data_format, v2,\n                    input_sizes, total_size, pool_func, ksize, strides,\n                    data_type)\n    # Initializes the input tensor with array containing incrementing\n    # numbers from 1, wrapping round to -127 after 127 to support int8.\n    y = -1 if use_negative_input else 1\n    x = [(((f + 128) % 255) - 127)*y for f in range(total_size)]\n    with self.cached_session(use_gpu=use_gpu):\n      t = constant_op.constant(x, shape=input_sizes, dtype=data_type)\n      if data_format in (\"NCHW\", \"NCHW_VECT_C\", \"NCW\"):\n        if data_format == \"NCHW_VECT_C\":\n          t = test_util.NHWCToNCHW_VECT_C(t)\n          t, _, _ = gen_array_ops.quantize_v2(t, -128.0, 127.0, dtypes.qint8)\n        else:\n          t = test_util.NHWCToNCHW(t)\n        ksize = test_util.NHWCToNCHW(ksize)\n        strides = test_util.NHWCToNCHW(strides)\n        if isinstance(padding, list):\n          padding = test_util.NHWCToNCHW(padding)\n      ksize_placeholder = array_ops.placeholder(dtypes.int32, shape=[4])\n      strides_placeholder = array_ops.placeholder(dtypes.int32, shape=[4])\n      if v2:\n        t = pool_func(\n            t,\n            ksize=ksize_placeholder,\n            strides=strides_placeholder,\n            padding=padding,\n            data_format=data_format)\n      else:\n        t = pool_func(\n            t,\n            ksize=ksize,\n            strides=strides,\n            padding=padding,\n            data_format=data_format)\n      if data_format == \"NCHW_VECT_C\":\n        t = gen_array_ops.dequantize(t, -128, 127)\n        t = test_util.NCHW_VECT_CToNHWC(t)\n      elif data_format == \"NCHW\":\n        t = test_util.NCHWToNHWC(t)\n      if v2:\n        actual = t.eval(feed_dict={\n            ksize_placeholder: ksize,\n            strides_placeholder: strides\n        })\n      else:\n        actual = self.evaluate(t)\n        self.assertShapeEqual(actual, t)\n      self.assertAllCloseAccordingToType(expected, actual.flatten())\n\n  def _VerifyOneTest(self, pool_func, input_sizes, ksize, strides, padding,\n                     data_format, expected, use_gpu, v2,\n                     use_negative_input=False):\n    \"\"\"Verifies the output values of the pooling function.\n\n    Args:\n      pool_func: Function to be called, co.MaxPool, co.AvgPool,\n        or the Lua version.\n      input_sizes: Input tensor dimensions.\n      ksize: The kernel size dimensions\n      strides: The stride dimensions\n      padding: Padding type.\n      data_format: The data format we use to run the pooling operation.\n      expected: An array containing the expected operation outputs.\n      use_gpu: Whether we are running on GPU.\n      v2: Whether to use v2 version.\n      use_negative_input: If the input values should be negative.\"\n    \"\"\"\n    if data_format == \"NCHW_VECT_C\":\n      avg_pool_func = nn_ops.avg_pool\n      tf_logging.info(\"pool_func=%s\", pool_func)\n      if pool_func == avg_pool_func:\n        tf_logging.info(\"NCHW_VECT_C not yet implemented for avg_pool\")\n        return\n      if (self._isMaxPool(pool_func) and isinstance(padding, list)):\n        tf_logging.info(\"NCHW_VECT_C not yet implemented for max pool\" +\n                        \" with explicit padding\")\n        return\n\n    self._VerifyOneType(pool_func, input_sizes, ksize, strides, padding,\n                        data_format, dtypes.float32, expected, use_gpu, v2,\n                        use_negative_input)\n    if not test.is_built_with_rocm():\n      # double datatype is not supported for pooling ops on the ROCm platform\n      self._VerifyOneType(pool_func, input_sizes, ksize, strides, padding,\n                          data_format, dtypes.float64, expected, use_gpu, v2,\n                          use_negative_input)\n\n    if not use_gpu or test_util.GpuSupportsHalfMatMulAndConv():\n      self._VerifyOneType(pool_func, input_sizes, ksize, strides, padding,\n                          data_format, dtypes.float16, expected, use_gpu, v2,\n                          use_negative_input)\n\n  def _VerifyValues(self,\n                    pool_func,\n                    input_sizes,\n                    ksize,\n                    strides,\n                    padding,\n                    expected,\n                    use_gpu,\n                    v2=False,\n                    one_dim=False,\n                    use_negative_input=False):\n    \"\"\"Verifies the output values of the pooling function.\n\n    Args:\n      pool_func: Function to be called, co.MaxPool, co.AvgPool,\n        or the Lua version.\n      input_sizes: Input tensor dimensions.\n      ksize: The kernel size dimensions\n      strides: The stride dimensions\n      padding: Padding type.\n      expected: An array containing the expected operation outputs.\n      use_gpu: Whether we are running on GPU.\n      v2: Whether to use v2 version.\n      one_dim: If one dimensional pools should be done instead of two\n        dimensional pools.\n      use_negative_input: If the input values should be negative.\n    \"\"\"\n    for (data_format, use_gpu_2) in GetTestConfigs(\n        include_nchw_vect_c=True, one_dimensional=one_dim):\n      if use_gpu_2 == use_gpu:\n        self._VerifyOneTest(pool_func, input_sizes, ksize, strides, padding,\n                            data_format, expected, use_gpu, v2,\n                            use_negative_input)\n\n  @parameterized.parameters(GetTestConfigsDicts(nn_ops.avg_pool))\n  @test_util.run_deprecated_v1\n  def testAvgPoolValidPadding(self, **kwargs):\n    self._VerifyOneType(\n        input_sizes=[1, 3, 3, 3],\n        ksize=[1, 2, 2, 1],\n        strides=[1, 2, 2, 1],\n        padding=\"VALID\",\n        expected=[7.0, 8.0, 9.0],\n        **kwargs)\n\n  @parameterized.parameters(GetTestConfigsDicts(nn_ops.avg_pool))\n  @test_util.run_deprecated_v1\n  def testAvgPoolEmpty(self, **kwargs):\n    self._VerifyOneType(\n        input_sizes=[1, 3, 3, 0],\n        ksize=[1, 2, 2, 1],\n        strides=[1, 2, 2, 1],\n        padding=\"VALID\",\n        expected=[],\n        **kwargs)\n\n  @parameterized.parameters(GetTestConfigsDicts(nn_ops.avg_pool))\n  @test_util.run_deprecated_v1\n  def testAvgPoolSamePadding(self, **kwargs):\n    self._VerifyOneType(\n        input_sizes=[1, 2, 4, 3],\n        ksize=[1, 2, 2, 1],\n        strides=[1, 2, 2, 1],\n        padding=\"SAME\",\n        expected=[8.5, 9.5, 10.5, 14.5, 15.5, 16.5],\n        **kwargs)\n\n  @parameterized.parameters(GetTestConfigsDicts(nn_ops.avg_pool))\n  @test_util.run_deprecated_v1\n  def testAvgPoolSamePaddingNonSquareWindow(self, **kwargs):\n    # input is:\n    # [1.0, 2.0\n    #  3.0  4.0]\n    #\n    # Window of [x, x] should do:\n    #  [avg(1.0, 2.0), avg(2.0, padded0),\n    #   avg(3.0, 4.0), avg(4.0, padded0)]\n    self._VerifyOneType(\n        input_sizes=[1, 2, 2, 1],\n        ksize=[1, 1, 2, 1],\n        strides=[1, 1, 1, 1],\n        padding=\"SAME\",\n        expected=[1.5, 2.0, 3.5, 4.0],\n        **kwargs)\n\n  @parameterized.parameters(GetTestConfigsDicts(nn_ops.avg_pool))\n  @test_util.run_deprecated_v1\n  def testAvgPoolSamePaddingNonSquareWindow_2(self, **kwargs):\n    # Window of [x,\n    #            x] should do:\n    #  [avg(1.0, 3.0), avg(2.0, 4.0)\n    #   avg(3.0, padded0), avg(4.0, padded0)]\n    self._VerifyOneType(\n        input_sizes=[1, 2, 2, 1],\n        ksize=[1, 2, 1, 1],\n        strides=[1, 1, 1, 1],\n        padding=\"SAME\",\n        expected=[2.0, 3.0, 3.0, 4.0],\n        **kwargs)\n\n  @parameterized.parameters(GetTestConfigsDicts(nn_ops.avg_pool))\n  @test_util.run_deprecated_v1\n  def testAvgPoolSamePaddingNonSquareWindowMultiBatch(self, **kwargs):\n    self._VerifyOneType(\n        input_sizes=[2, 2, 2, 2],\n        ksize=[1, 1, 2, 1],\n        strides=[1, 1, 1, 1],\n        padding=\"SAME\",\n        expected=[\n            2.0, 3.0, 3.0, 4.0, 6.0, 7.0, 7.0, 8.0, 10.0, 11.0, 11.0, 12.0,\n            14.0, 15.0, 15.0, 16.0\n        ],\n        **kwargs)\n\n  @parameterized.parameters(GetTestConfigsDicts(nn_ops.avg_pool))\n  @test_util.run_deprecated_v1\n  def testAvgPoolSamePaddingNonSquareWindowMultiBatch_2(self, **kwargs):\n    self._VerifyOneType(\n        input_sizes=[2, 2, 2, 2],\n        ksize=[1, 2, 1, 1],\n        strides=[1, 1, 1, 1],\n        padding=\"SAME\",\n        expected=[\n            3.0, 4.0, 5.0, 6.0, 5.0, 6.0, 7.0, 8.0, 11.0, 12.0, 13.0, 14.0,\n            13.0, 14.0, 15.0, 16.0\n        ],\n        **kwargs)\n\n  @parameterized.parameters(GetTestConfigsDicts(nn_ops.avg_pool))\n  @test_util.run_deprecated_v1\n  def testAvgPoolValidPaddingUnevenStride(self, **kwargs):\n    self._VerifyOneType(\n        input_sizes=[1, 3, 3, 3],\n        ksize=[1, 2, 2, 1],\n        strides=[1, 1, 2, 1],\n        padding=\"VALID\",\n        expected=[7.0, 8.0, 9.0, 16.0, 17.0, 18.0],\n        **kwargs)\n\n  @parameterized.parameters(GetTestConfigsDicts(nn_ops.avg_pool))\n  @test_util.run_deprecated_v1\n  def testAvgPoolValidPaddingUnevenStride_2(self, **kwargs):\n    self._VerifyOneType(\n        input_sizes=[1, 3, 3, 3],\n        ksize=[1, 2, 2, 1],\n        strides=[1, 2, 1, 1],\n        padding=\"VALID\",\n        expected=[7.0, 8.0, 9.0, 10.0, 11.0, 12.0],\n        **kwargs)\n\n  @parameterized.parameters(GetTestConfigsDicts(nn_ops.avg_pool))\n  @test_util.run_deprecated_v1\n  def testAvgPoolSamePadding_2(self, **kwargs):\n    expected_output = [\n        11.0, 12.0, 13.0, 14.0, 19.0, 20.0, 21.0, 22.0, 43.0, 44.0, 45.0, 46.0,\n        51.0, 52.0, 53.0, 54.0\n    ]\n    self._VerifyOneType(\n        input_sizes=[1, 4, 4, 4],\n        ksize=[1, 2, 2, 1],\n        strides=[1, 2, 2, 1],\n        padding=\"SAME\",\n        expected=expected_output,\n        **kwargs)\n\n  @parameterized.parameters(GetTestConfigsDicts(nn_ops.avg_pool))\n  @test_util.run_deprecated_v1\n  def testAvgPoolSamePaddingPacket_4(self, **kwargs):\n    expected_output = [\n        21.0, 22.0, 23.0, 24.0, 27.0, 28.0, 29.0, 30.0, 45.0, 46.0, 47.0, 48.0,\n        51.0, 52.0, 53.0, 54.0\n    ]\n    self._VerifyOneType(\n        input_sizes=[1, 4, 4, 4],\n        ksize=[1, 3, 3, 1],\n        strides=[1, 2, 2, 1],\n        padding=\"SAME\",\n        expected=expected_output,\n        **kwargs)\n\n  @parameterized.parameters(GetTestConfigsDicts(nn_ops.avg_pool))\n  @test_util.run_deprecated_v1\n  def testAvgPoolSamePaddingPacket_8(self, **kwargs):\n    expected_output = [\n        -12.0, -11.0, -10.0, -9.0, -8.0, -7.0, -6.0, -5.0, 4.0, 5.0, 6.0, 7.0,\n        8.0, 9.0, 10.0, 11.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0,\n        32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, -3.5, -54.0, -53.0, -52.0,\n        -51.0, -50.0, -49.0, -48.0, -47.0, -38.0, -37.0, -36.0, -35.0, -34.0,\n        -33.0, -32.0, -31.0, -22.0, -21.0, -20.0, -19.0, -18.0, -17.0, -16.0,\n        -15.0, -10.0, -9.0, -8.0, -7.0, -6.0, -5.0, -4.0, -3.0, -11.0, -10.0,\n        -9.0, -8.0, -7.0, -6.0, -5.0, -4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0,\n        12.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 33.0, 34.0, 35.0,\n        36.0, 37.0, 38.0, -3.5, -2.5, -85.0, -84.0, -83.0, -82.0, -81.0, -80.0,\n        -79.0, -78.0, -69.0, -68.0, -67.0, -66.0, -65.0, -64.0, -63.0, -62.0,\n        -53.0, -52.0, -51.0, -50.0, -49.0, -48.0, -47.0, -46.0, -41.0, -40.0,\n        -39.0, -38.0, -37.0, -36.0, -35.0, -34.0\n    ]\n    self._VerifyOneType(\n        input_sizes=[1, 8, 8, 8],\n        ksize=[1, 3, 3, 1],\n        strides=[1, 2, 2, 1],\n        padding=\"SAME\",\n        expected=expected_output,\n        **kwargs)\n\n  @parameterized.parameters(GetTestConfigsDicts(nn_ops.avg_pool))\n  @test_util.run_deprecated_v1\n  def testAvgPoolEmptyInput(self, **kwargs):\n    self._VerifyOneType(\n        input_sizes=[0, 8, 8, 8],\n        ksize=[1, 3, 3, 1],\n        strides=[1, 2, 2, 1],\n        padding=\"SAME\",\n        expected=[],\n        **kwargs)\n\n  @parameterized.parameters(\n      GetTestConfigsDicts(nn_ops.max_pool, gen_nn_ops.max_pool_v2))\n  @test_util.run_deprecated_v1\n  def testMaxPoolValidPadding(self, **kwargs):\n    self._VerifyOneType(\n        input_sizes=[1, 3, 3, 3],\n        ksize=[1, 2, 2, 1],\n        strides=[1, 2, 2, 1],\n        padding=\"VALID\",\n        expected=[13.0, 14.0, 15.0],\n        **kwargs)\n\n  @parameterized.parameters(\n      GetTestConfigsDicts(nn_ops.max_pool, gen_nn_ops.max_pool_v2))\n  @test_util.run_deprecated_v1\n  def testMaxPoolSamePadding(self, **kwargs):\n    self._VerifyOneType(\n        input_sizes=[1, 2, 3, 3],\n        ksize=[1, 2, 2, 1],\n        strides=[1, 2, 2, 1],\n        padding=\"SAME\",\n        expected=[13.0, 14.0, 15.0, 16.0, 17.0, 18.0],\n        **kwargs)\n\n  @parameterized.parameters(\n      GetTestConfigsDicts(nn_ops.max_pool, nn_ops.max_pool_v2))\n  @test_util.xla_allow_fallback(\"XLA doesn't support explicit padding\")\n  @test_util.run_deprecated_v1\n  def testMaxPoolZeroExplicitPadding(self, **kwargs):\n    self._VerifyOneType(\n        input_sizes=[1, 3, 3, 1],\n        ksize=[1, 3, 3, 1],\n        strides=[1, 2, 2, 1],\n        padding=[[0, 0], [0, 0], [0, 0], [0, 0]],\n        expected=[9.0],\n        **kwargs)\n\n  @parameterized.parameters(\n      GetTestConfigsDicts(nn_ops.max_pool, nn_ops.max_pool_v2))\n  @test_util.xla_allow_fallback(\"XLA doesn't support explicit padding\")\n  @test_util.run_deprecated_v1\n  def testMaxPoolNegativeInputExpPadding(self, **kwargs):\n    self._VerifyOneType(\n        input_sizes=[1, 3, 3, 1],\n        ksize=[1, 3, 3, 1],\n        strides=[1, 2, 2, 1],\n        padding=[[0, 0], [2, 1], [2, 1], [0, 0]],\n        expected=[-1, -1, -1, -1],\n        use_negative_input=True,\n        **kwargs)\n\n  @parameterized.parameters(\n      GetTestConfigsDicts(nn_ops.max_pool, nn_ops.max_pool_v2))\n  @test_util.xla_allow_fallback(\"XLA doesn't support explicit padding\")\n  @test_util.run_deprecated_v1\n  def testMaxPoolExplicitPadding(self, **kwargs):\n    expected_output = [9.0, 9.0]\n    self._VerifyOneType(\n        input_sizes=[1, 3, 3, 1],\n        ksize=[1, 3, 3, 1],\n        strides=[1, 2, 2, 1],\n        padding=[[0, 0], [0, 2], [0, 1], [0, 0]],\n        expected=expected_output,\n        **kwargs)\n\n  @parameterized.parameters(\n      GetTestConfigsDicts(nn_ops.max_pool, nn_ops.max_pool_v2))\n  @test_util.xla_allow_fallback(\"XLA doesn't support explicit padding\")\n  @test_util.run_deprecated_v1\n  def testMaxPoolExplicitPaddingAdvanced(self, **kwargs):\n    expected_output = [7, 9, 11, 12, 19, 21, 23, 24, 31, 33, 35, 36, 31, 33,\n                       35, 36]\n    self._VerifyOneType(\n        input_sizes=[1, 6, 6, 1],\n        ksize=[1, 3, 3, 1],\n        strides=[1, 2, 2, 1],\n        padding=[[0, 0], [1, 2], [2, 1], [0, 0]],\n        expected=expected_output,\n        **kwargs)\n\n  @parameterized.parameters(\n      GetTestConfigsDicts(nn_ops.max_pool, nn_ops.max_pool_v2))\n  @test_util.xla_allow_fallback(\"XLA doesn't support explicit padding\")\n  @test_util.run_deprecated_v1\n  def testMaxPoolNegativeInputExpPaddingAdv(self, **kwargs):\n    expected_output = [-1, -1, -3, -5, -7, -7, -9, -11, -19, -19, -21, -23, -31,\n                       -31, -33, -35]\n\n    self._VerifyOneType(\n        input_sizes=[1, 6, 6, 1],\n        ksize=[1, 3, 3, 1],\n        strides=[1, 2, 2, 1],\n        padding=[[0, 0], [1, 2], [2, 1], [0, 0]],\n        expected=expected_output,\n        use_negative_input=True,\n        **kwargs)\n\n  @parameterized.parameters(\n      GetTestConfigsDicts(nn_ops.max_pool, nn_ops.max_pool_v2))\n  @test_util.xla_allow_fallback(\"XLA doesn't support explicit padding\")\n  @test_util.run_deprecated_v1\n  def testMaxPoolExplicitPadding2_(self, **kwargs):\n    expected_output = [9.0, 9.0]\n    self._VerifyOneType(\n        input_sizes=[1, 3, 3, 1],\n        ksize=[1, 3, 3, 1],\n        strides=[1, 2, 2, 1],\n        padding=[[0, 0], [0, 2], [0, 1], [0, 0]],\n        expected=expected_output,\n        **kwargs)\n\n  @parameterized.parameters(\n      GetTestConfigsDicts(\n          nn_ops.max_pool1d, nn_ops.max_pool_v2, one_dimensional=True))\n  @test_util.xla_allow_fallback(\"XLA doesn't support explicit padding\")\n  @test_util.run_deprecated_v1\n  def testMaxPoolExplicitPadding_1D(self, **kwargs):\n    self._VerifyOneType(\n        input_sizes=[1, 3, 1],\n        ksize=[1, 2, 1],\n        strides=[1, 2, 1],\n        padding=[[0, 0], [0, 1], [0, 0]],\n        expected=[2.0, 3.0],\n        **kwargs)\n\n  @parameterized.parameters(\n      GetTestConfigsDicts(nn_ops.max_pool, gen_nn_ops.max_pool_v2))\n  @test_util.run_deprecated_v1\n  def testMaxPoolSamePaddingNonSquareWindow(self, **kwargs):\n    # input is:\n    # [1.0, 2.0\n    #  3.0  4.0]\n    #\n    # Window of [x, x] should do:\n    #\n    #  [max(1.0, 2.0), max(2.0, padded0),\n    #   max(3.0, 4.0), max(4.0, padded0)]\n    self._VerifyOneType(\n        input_sizes=[1, 2, 2, 1],\n        ksize=[1, 1, 2, 1],\n        strides=[1, 1, 1, 1],\n        padding=\"SAME\",\n        expected=[2.0, 2.0, 4.0, 4.0],\n        **kwargs)\n\n  @parameterized.parameters(\n      GetTestConfigsDicts(nn_ops.max_pool, gen_nn_ops.max_pool_v2))\n  @test_util.run_deprecated_v1\n  def testMaxPoolValidPaddingUnevenStride(self, **kwargs):\n    self._VerifyOneType(\n        input_sizes=[1, 4, 4, 1],\n        ksize=[1, 2, 2, 1],\n        strides=[1, 1, 2, 1],\n        padding=\"VALID\",\n        expected=[6.0, 8.0, 10.0, 12.0, 14.0, 16.0],\n        **kwargs)\n\n  @parameterized.parameters(\n      GetTestConfigsDicts(nn_ops.max_pool, gen_nn_ops.max_pool_v2))\n  @test_util.run_deprecated_v1\n  def testMaxPoolValidPaddingUnevenStride2_(self, **kwargs):\n    self._VerifyOneType(\n        input_sizes=[1, 4, 4, 1],\n        ksize=[1, 2, 2, 1],\n        strides=[1, 2, 1, 1],\n        padding=\"VALID\",\n        expected=[6.0, 7.0, 8.0, 14.0, 15.0, 16.0],\n        **kwargs)\n\n  @parameterized.parameters(\n      GetTestConfigsDicts(nn_ops.max_pool, gen_nn_ops.max_pool_v2))\n  @test_util.run_deprecated_v1\n  def testMaxPoolSamePaddingPacket4_(self, **kwargs):\n    expected_output = [\n        21.0, 22.0, 23.0, 24.0, 29.0, 30.0, 31.0, 32.0, 53.0, 54.0, 55.0, 56.0,\n        61.0, 62.0, 63.0, 64.0\n    ]\n    self._VerifyOneType(\n        input_sizes=[1, 4, 4, 4],\n        ksize=[1, 2, 2, 1],\n        strides=[1, 2, 2, 1],\n        padding=\"SAME\",\n        expected=expected_output,\n        **kwargs)\n\n  @parameterized.parameters(\n      GetTestConfigsDicts(nn_ops.max_pool, gen_nn_ops.max_pool_v2))\n  @test_util.run_deprecated_v1\n  def testMaxPoolSamePaddingPacket8_(self, **kwargs):\n    expected_output = [\n        81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 97.0, 98.0, 99.0, 100.0,\n        101.0, 102.0, 103.0, 104.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0,\n        119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 120.0,\n        18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 34.0, 35.0, 36.0, 37.0,\n        38.0, 39.0, 40.0, 41.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0,\n        58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 82.0, 83.0, 84.0, 85.0,\n        86.0, 87.0, 88.0, 89.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0,\n        105.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0,\n        123.0, 124.0, 125.0, 126.0, 127.0, 120.0, 121.0, -45.0, -44.0, -43.0,\n        -42.0, -41.0, -40.0, -39.0, -38.0, -29.0, -28.0, -27.0, -26.0, -25.0,\n        -24.0, -23.0, -22.0, -13.0, -12.0, -11.0, -10.0, -9.0, -8.0, -7.0, -6.0,\n        -5.0, -4.0, -3.0, -2.0, -1.0, 0.0, 1.0, 2.0\n    ]\n    self._VerifyOneType(\n        input_sizes=[1, 8, 8, 8],\n        ksize=[1, 3, 3, 1],\n        strides=[1, 2, 2, 1],\n        padding=\"SAME\",\n        expected=expected_output,\n        **kwargs)\n\n  @parameterized.parameters(\n      GetTestConfigsDicts(nn_ops.max_pool, gen_nn_ops.max_pool_v2))\n  @test_util.run_deprecated_v1\n  def testMaxPoolEmptyInput(self, **kwargs):\n    self._VerifyOneType(\n        input_sizes=[0, 8, 8, 8],\n        ksize=[1, 3, 3, 1],\n        strides=[1, 2, 2, 1],\n        padding=\"SAME\",\n        expected=[],\n        **kwargs)\n\n  # Tests for DepthwiseMaxPooling on CPU only.\n  @parameterized.parameters(\n      GetTestConfigsDicts(\n          nn_ops.max_pool, gen_nn_ops.max_pool_v2, allow_gpu=False))\n  @test_util.run_deprecated_v1\n  def testDepthwiseMaxPool1x1DepthWindow(self, **kwargs):\n    # input is:\n    # [1.0, ..., 10.0] along depth,\n    #\n    # We maxpool by depth in patches of 2.\n    self._VerifyOneType(\n        input_sizes=[1, 1, 1, 10],\n        ksize=[1, 1, 1, 2],\n        strides=[1, 1, 1, 2],\n        padding=\"SAME\",\n        expected=[2.0, 4.0, 6.0, 8.0, 10.0],\n        **kwargs)\n\n  @parameterized.parameters(\n      GetTestConfigsDicts(\n          nn_ops.max_pool, gen_nn_ops.max_pool_v2, allow_gpu=False))\n  @test_util.run_deprecated_v1\n  def testDepthwiseMaxPool2x2DepthWindow(self, **kwargs):\n    # input is:\n    #\n    # a 2x2x6 cube, and we depthwise max across 3 to produce a 2x2x2\n    # output.  Each node has contiguous values, so the depthwise max\n    # should be multiples of 3.0.\n    self._VerifyOneType(\n        input_sizes=[1, 2, 2, 6],\n        ksize=[1, 1, 1, 3],\n        strides=[1, 1, 1, 3],\n        padding=\"SAME\",\n        expected=[3.0, 6.0, 9.0, 12.0, 15.0, 18.0, 21.0, 24.0],\n        **kwargs)\n\n  @parameterized.parameters(\n      GetTestConfigsDicts(\n          nn_ops.max_pool, gen_nn_ops.max_pool_v2, allow_gpu=False))\n  @test_util.run_deprecated_v1\n  def testMaxPoolKernelSmallerThanStrideValid(self, **kwargs):\n    self._VerifyOneType(\n        input_sizes=[1, 7, 7, 1],\n        ksize=[1, 2, 2, 1],\n        strides=[1, 3, 3, 1],\n        padding=\"VALID\",\n        expected=[9, 12, 30, 33],\n        **kwargs)\n\n  @parameterized.parameters(GetTestConfigsDicts(nn_ops.avg_pool))\n  @test_util.run_deprecated_v1\n  def testAvgPoolKernelSmallerThanStride(self, **kwargs):\n    self._VerifyOneType(\n        input_sizes=[1, 7, 7, 1],\n        ksize=[1, 2, 2, 1],\n        strides=[1, 3, 3, 1],\n        padding=\"VALID\",\n        expected=[5, 8, 26, 29],\n        **kwargs)\n\n  @parameterized.parameters(\n      GetTestConfigsDicts(nn_ops.max_pool, gen_nn_ops.max_pool_v2) +\n      GetTestConfigsDicts(nn_ops.avg_pool))\n  @test_util.run_deprecated_v1\n  def testKernelSmallerThanStrideSame1_(self, **kwargs):\n    self._VerifyOneType(\n        input_sizes=[1, 3, 3, 1],\n        ksize=[1, 1, 1, 1],\n        strides=[1, 2, 2, 1],\n        padding=\"SAME\",\n        expected=[1, 3, 7, 9],\n        **kwargs)\n\n  @parameterized.parameters(\n      GetTestConfigsDicts(nn_ops.max_pool, gen_nn_ops.max_pool_v2) +\n      GetTestConfigsDicts(nn_ops.avg_pool))\n  @test_util.run_deprecated_v1\n  def testKernelSmallerThanStrideSame2_(self, **kwargs):\n    self._VerifyOneType(\n        input_sizes=[1, 4, 4, 1],\n        ksize=[1, 1, 1, 1],\n        strides=[1, 2, 2, 1],\n        padding=\"SAME\",\n        expected=[1, 3, 9, 11],\n        **kwargs)\n\n  def _testDepthwiseMaxPoolInvalidConfig(self,\n                                         in_size,\n                                         ksize,\n                                         strides,\n                                         error_msg,\n                                         use_gpu=False):\n    with self.cached_session(use_gpu=use_gpu):\n      t = constant_op.constant(1.0, shape=in_size)\n      with self.assertRaisesRegex(errors_impl.UnimplementedError, error_msg):\n        t = nn_ops.max_pool(\n            t, ksize=ksize, strides=strides, padding=\"SAME\").eval()\n\n  @test_util.disable_xla(\"b/123338077\")  # Passes with XLA\n  def testDepthwiseMaxPoolInvalidConfigs(self):\n    self._testDepthwiseMaxPoolInvalidConfig(\n        [1, 2, 2, 4], [1, 2, 2, 2], [1, 1, 1, 2],\n        \"exactly one of pooling across depth\")\n    self._testDepthwiseMaxPoolInvalidConfig(\n        [1, 2, 2, 4], [1, 1, 1, 2], [1, 1, 1, 1],\n        \"depth window to equal the depth stride\")\n    self._testDepthwiseMaxPoolInvalidConfig([1, 2, 2, 4], [1, 1, 1, 3],\n                                            [1, 1, 1, 3], \"evenly divide\")\n    if test.is_gpu_available():\n      with self.session():\n        t = variables.Variable(np.ones([1, 2, 2, 4]))\n        self.evaluate(variables.global_variables_initializer())\n        with self.assertRaisesOpError(\"for CPU devices\"):\n          nn_ops.max_pool(\n              t, ksize=[1, 1, 1, 2], strides=[1, 1, 1, 2],\n              padding=\"SAME\").eval()\n\n  # The following are tests that verify that the CPU and GPU implementations\n  # produce the same results.\n  def _CompareMaxPoolingFwd(self, input_shape, ksize, strides, padding):\n    # double datatype is currently not supported for pooling ops\n    # on the ROCm platform\n    for dtype in [np.float32, np.float16] \\\n        + [np.float64] if not test.is_built_with_rocm() else []:\n      tensor_input = np.random.rand(*input_shape).astype(dtype)\n      with self.cached_session():\n        t = constant_op.constant(tensor_input, shape=input_shape)\n        out_op, _ = nn_ops.max_pool_with_argmax(t, ksize, strides, padding)\n        gpu_val = self.evaluate(out_op)\n      with self.cached_session(use_gpu=False):\n        t = constant_op.constant(tensor_input, shape=input_shape)\n        out_op = nn_ops.max_pool(t, ksize, strides, padding)\n        cpu_val = self.evaluate(out_op)\n      self.assertAllCloseAccordingToType(cpu_val, gpu_val)\n\n  def _CompareMaxPoolingBk(self, input_shape, output_shape, ksize, strides,\n                           padding):\n    # double datatype is currently not supported for pooling ops\n    # on the ROCm platform\n    for dtype in [np.float32, np.float16] \\\n        + [np.float64] if not test.is_built_with_rocm() else []:\n      # Generate numbers in a narrow range, so that there are many duplicates\n      # in the input.\n      tensor_input = np.random.random_integers(0, 3, input_shape).astype(dtype)\n      tensor_output = np.random.rand(*output_shape).astype(dtype)\n      with self.cached_session():\n        t = constant_op.constant(tensor_input, shape=input_shape)\n        _, argmax_op = nn_ops.max_pool_with_argmax(t, ksize, strides, padding)\n        argmax = self.evaluate(argmax_op)\n        grad_in = constant_op.constant(tensor_output, shape=output_shape)\n        out_op = gen_nn_ops.max_pool_grad_with_argmax(t, grad_in, argmax, ksize,\n                                                      strides, padding)\n        gpu_val = self.evaluate(out_op)\n        self.assertShapeEqual(gpu_val, out_op)\n      with self.cached_session(use_gpu=False):\n        t = constant_op.constant(tensor_input, shape=input_shape)\n        out_op = nn_ops.max_pool(t, ksize, strides, padding)\n        orig_out = self.evaluate(out_op)\n        grad_in = constant_op.constant(tensor_output, shape=output_shape)\n        out_op = gen_nn_ops.max_pool_grad(t, orig_out, grad_in, ksize, strides,\n                                          padding)\n        cpu_val = self.evaluate(out_op)\n        self.assertShapeEqual(cpu_val, out_op)\n      # The CPU version accumulates its gradient on fp16, so it's less\n      # accurate than the GPU version that does the accumulation on fp32\n      self.assertAllCloseAccordingToType(\n          cpu_val, gpu_val, half_rtol=0.01, half_atol=0.01)\n\n  def _CompareMaxPoolingGradBk(self, input_shape, output_shape, ksize, strides,\n                               padding):\n    # double datatype is currently not supported for pooling ops\n    # on the ROCm platform\n    for dtype in [np.float32, np.float16] \\\n        + [np.float64] if not test.is_built_with_rocm() else []:\n      # Generate numbers in a narrow range, so that there are many duplicates\n      # in the input.\n      tensor_input = np.random.random_integers(0, 3, input_shape).astype(dtype)\n      with self.cached_session(use_gpu=False):\n        t = constant_op.constant(tensor_input, shape=input_shape)\n        _, argmax_op = nn_ops.max_pool_with_argmax(t, ksize, strides, padding)\n        argmax = self.evaluate(argmax_op)\n        grad_in = constant_op.constant(tensor_input, shape=input_shape)\n        out_op = gen_nn_ops.max_pool_grad_grad_with_argmax(\n            t, grad_in, argmax, ksize, strides, padding)\n        gpu_val = self.evaluate(out_op)\n        self.assertShapeEqual(gpu_val, out_op)\n      with self.cached_session(use_gpu=False):\n        t = constant_op.constant(tensor_input, shape=input_shape)\n        out_op = nn_ops.max_pool(t, ksize, strides, padding)\n        orig_out = self.evaluate(out_op)\n        grad_in = constant_op.constant(tensor_input, shape=input_shape)\n        out_op = gen_nn_ops.max_pool_grad_grad(t, orig_out, grad_in, ksize,\n                                               strides, padding)\n        cpu_val = self.evaluate(out_op)\n        self.assertShapeEqual(cpu_val, out_op)\n      # The CPU version accumulates its gradient on fp16, so it's less\n      # accurate than the GPU version that does the accumulation on fp32\n      self.assertAllCloseAccordingToType(\n          cpu_val, gpu_val, half_rtol=0.01, half_atol=0.01)\n\n  def testMaxPoolingWithArgmax(self):\n    tensor_input = [\n        1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0,\n        0.0, 1.0, 0.0, 1.0\n    ]\n\n    Config = collections.namedtuple(\n        \"Config\", [\"use_gpu\", \"include_batch_in_index\", \"argmax\", \"Targmax\"])\n    configs = [\n        Config(False, False, [0, 1, 3, 5, 0, 2, 6, 8], dtypes.int64),\n        Config(False, True, [0, 1, 3, 5, 9, 11, 15, 17], dtypes.int64),\n        Config(False, False, [0, 1, 3, 5, 0, 2, 6, 8], dtypes.int32),\n        Config(False, True, [0, 1, 3, 5, 9, 11, 15, 17], dtypes.int32),\n        Config(True, False, [0, 1, 3, 5, 0, 2, 6, 8], dtypes.int64),\n        Config(True, True, [0, 1, 3, 5, 9, 11, 15, 17], dtypes.int64),\n    ]\n\n    for config in configs:\n      with GetDeviceScope(self, use_gpu=config.use_gpu):\n        t = constant_op.constant(tensor_input, shape=[2, 3, 3, 1])\n        out_op, argmax_op = nn_ops.max_pool_with_argmax(\n            t,\n            ksize=[1, 2, 2, 1],\n            strides=[1, 1, 1, 1],\n            Targmax=config.Targmax,\n            padding=\"VALID\",\n            include_batch_in_index=config.include_batch_in_index)\n        out, argmax = self.evaluate([out_op, argmax_op])\n        self.assertShapeEqual(out, out_op)\n        self.assertShapeEqual(argmax, argmax_op)\n        self.assertAllClose(out.ravel(),\n                            [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n        self.assertAllEqual(argmax.ravel(), config.argmax)\n\n  def testMaxPoolingGradWithArgmax(self):\n    orig_input = [\n        1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0,\n        0.0, 1.0, 0.0, 1.0\n    ]\n    tensor_input = [11.0, 12.0, 13.0, 14.0, 21.0, 22.0, 23.0, 24.0]\n\n    Config = collections.namedtuple(\n        \"Config\", [\"use_gpu\", \"include_batch_in_index\", \"argmax\"])\n    configs = [\n        Config(False, False, [0, 1, 3, 5, 0, 2, 6, 8]),\n        Config(False, True, [0, 1, 3, 5, 9, 11, 15, 17]),\n        Config(True, False, [0, 1, 3, 5, 0, 2, 6, 8]),\n        Config(True, True, [0, 1, 3, 5, 9, 11, 15, 17])\n    ]\n\n    for config in configs:\n      with GetDeviceScope(self, config.use_gpu):\n        orig_in = constant_op.constant(orig_input, shape=[2, 3, 3, 1])\n        t = constant_op.constant(tensor_input, shape=[2, 2, 2, 1])\n        argmax_t = constant_op.constant(\n            config.argmax, shape=[2, 2, 2, 1], dtype=dtypes.int64)\n        out_op = gen_nn_ops.max_pool_grad_with_argmax(\n            orig_in,\n            t,\n            argmax_t,\n            ksize=[1, 2, 2, 1],\n            strides=[1, 1, 1, 1],\n            padding=\"VALID\",\n            include_batch_in_index=config.include_batch_in_index)\n        out = self.evaluate(out_op).flatten()\n        self.assertAllClose(out, [\n            11.0, 12.0, 0.0, 13.0, 0.0, 14.0, 0.0, 0.0, 0.0, 21.0, 0.0, 22.0,\n            0.0, 0.0, 0.0, 23.0, 0.0, 24.0\n        ])\n\n  def testMaxPoolingGradThrowDeterminismError(self):\n    if test.is_gpu_available(cuda_only=True):\n      try:\n        config_exec.enable_op_determinism()\n        orig_input = [\n            1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0,\n            0.0, 1.0, 0.0, 1.0\n        ]\n        tensor_input = [11.0, 12.0, 13.0, 14.0, 21.0, 22.0, 23.0, 24.0]\n\n        with GetDeviceScope(self, True):\n          orig_in = constant_op.constant(orig_input, shape=[2, 3, 3, 1])\n          t = constant_op.constant(tensor_input, shape=[2, 2, 2, 1])\n          argmax_t = constant_op.constant(\n              [0, 1, 3, 5, 0, 2, 6, 8], shape=[2, 2, 2, 1], dtype=dtypes.int64)\n          with self.assertRaisesRegexp(\n              errors_impl.UnimplementedError, \"Determinism is not yet supported \"\n              \"for MaxPoolGradWithArgmax.\"):\n            out_op = gen_nn_ops.max_pool_grad_with_argmax(\n                orig_in,\n                t,\n                argmax_t,\n                ksize=[1, 2, 2, 1],\n                strides=[1, 1, 1, 1],\n                padding=\"VALID\",\n                include_batch_in_index=False)\n            self.evaluate(out_op)\n      finally:\n        config_exec.disable_op_determinism()\n    else:\n      try:\n        config_exec.enable_op_determinism()\n        orig_input = [\n            1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0,\n            0.0, 1.0, 0.0, 1.0\n        ]\n        tensor_input = [11.0, 12.0, 13.0, 14.0, 21.0, 22.0, 23.0, 24.0]\n\n        with GetDeviceScope(self, False):\n          orig_in = constant_op.constant(orig_input, shape=[2, 3, 3, 1])\n          t = constant_op.constant(tensor_input, shape=[2, 2, 2, 1])\n          argmax_t = constant_op.constant(\n              [0, 1, 3, 5, 0, 2, 6, 8], shape=[2, 2, 2, 1], dtype=dtypes.int64)\n          out_op = gen_nn_ops.max_pool_grad_with_argmax(\n              orig_in,\n              t,\n              argmax_t,\n              ksize=[1, 2, 2, 1],\n              strides=[1, 1, 1, 1],\n              padding=\"VALID\",\n              include_batch_in_index=False)\n          self.evaluate(out_op)\n      finally:\n        config_exec.disable_op_determinism()\n\n  def testMaxPoolingGradGradWithArgmax(self):\n    # MaxPoolWithArgMax is implemented only on CUDA.\n    if not test.is_gpu_available(cuda_only=True):\n      return\n    orig_input = [\n        1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0,\n        0.0, 1.0, 0.0, 1.0\n    ]\n    tensor_input = [\n        11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 21.0, 22.0, 23.0,\n        24.0, 25.0, 26.0, 27.0, 28.0, 29.0\n    ]\n\n    Config = collections.namedtuple(\n        \"Config\", [\"use_gpu\", \"include_batch_in_index\", \"argmax\"])\n    configs = [\n        Config(True, False, [0, 1, 3, 5, 0, 2, 6, 8]),\n        Config(True, True, [0, 1, 3, 5, 9, 11, 15, 17])\n    ]\n\n    for config in configs:\n      with GetDeviceScope(self, config.use_gpu):\n        orig_in = constant_op.constant(orig_input, shape=[2, 3, 3, 1])\n        t = constant_op.constant(tensor_input, shape=[2, 3, 3, 1])\n        argmax_t = constant_op.constant(\n            config.argmax, shape=[2, 2, 2, 1], dtype=dtypes.int64)\n        out_op = gen_nn_ops.max_pool_grad_grad_with_argmax(\n            orig_in,\n            t,\n            argmax_t,\n            ksize=[1, 2, 2, 1],\n            strides=[1, 1, 1, 1],\n            padding=\"VALID\",\n            include_batch_in_index=config.include_batch_in_index)\n        out = self.evaluate(out_op).flatten()\n        self.assertAllClose(out,\n                            [11.0, 12.0, 14.0, 16.0, 21.0, 23.0, 27.0, 29.0])\n\n  def _ConstructAndTestGradient(self,\n                                pool_func,\n                                input_sizes,\n                                output_sizes,\n                                window_rows,\n                                window_cols,\n                                row_stride,\n                                col_stride,\n                                padding,\n                                data_format,\n                                use_gpu,\n                                x_init_value=None):\n    \"\"\"Verifies the gradients of the max or avg pooling function.\n\n    Args:\n      pool_func: Function to be called, co.MaxPool, co.AvgPool,\n        or the Lua version.\n      input_sizes: Input tensor dimensions.\n      output_sizes: Output tensor dimensions.\n      window_rows: kernel size in row dim\n      window_cols: kernel size in col dim\n      row_stride: Row Stride.\n      col_stride: Col Stride.\n      padding: Padding type.\n      data_format: Data format.\n      use_gpu: whether we are running on GPU\n      x_init_value: Values to be passed to the gradient checker.\n    \"\"\"\n    assert input_sizes[0] == output_sizes[0]\n    assert input_sizes[3] == output_sizes[3]\n    total_size = 1\n    for s in input_sizes:\n      total_size *= s\n    # Initializes the input tensor with array containing incrementing\n    # numbers from 1.\n    x = [f * 1.0 for f in range(1, total_size + 1)]\n    with self.cached_session(use_gpu=use_gpu):\n      input_tensor = constant_op.constant(x, shape=input_sizes, name=\"input\")\n      if pool_func == nn_ops.avg_pool:\n        func_name = \"avg_pool\"\n        err_tolerance = 1e-4\n      else:\n        if x_init_value is None:\n          x_init_value = np.asfarray(\n              np.arange(1, total_size + 1),\n              dtype=np.float32).reshape(input_sizes)\n        func_name = \"max_pool\"\n        err_tolerance = 1e-3\n      if data_format == \"NCHW\":\n        ksize = [1, 1, window_rows, window_cols]\n        strides = [1, 1, row_stride, col_stride]\n        if isinstance(padding, list):\n          padding = test_util.NHWCToNCHW(padding)\n        t = test_util.NHWCToNCHW(input_tensor)\n      else:\n        ksize = [1, window_rows, window_cols, 1]\n        strides = [1, row_stride, col_stride, 1]\n        t = input_tensor\n      t = pool_func(\n          t,\n          ksize=ksize,\n          strides=strides,\n          padding=padding,\n          data_format=data_format,\n          name=func_name)\n      if data_format == \"NCHW\":\n        t = test_util.NCHWToNHWC(t)\n\n      err = gradient_checker.compute_gradient_error(\n          input_tensor,\n          input_sizes,\n          t,\n          output_sizes,\n          x_init_value=x_init_value,\n          delta=1e-2)\n    tf_logging.info(\"%s gradient error = %.4f\" % (func_name, err))\n    self.assertLess(err, err_tolerance)\n\n  def _ConstructAndTestSecondGradient(self,\n                                      pool_func,\n                                      input_sizes,\n                                      output_sizes,\n                                      window_rows,\n                                      window_cols,\n                                      row_stride,\n                                      col_stride,\n                                      padding,\n                                      data_format,\n                                      use_gpu,\n                                      x_init_value=None):\n    \"\"\"Verifies the second-order gradients of the pooling function.\n\n    Args:\n      pool_func: Function to be called, co.MaxPool, co.AvgPool,\n        or the Lua version.\n      input_sizes: Input tensor dimensions.\n      output_sizes: Output tensor dimensions.\n      window_rows: kernel size in row dim\n      window_cols: kernel size in col dim\n      row_stride: Row Stride.\n      col_stride: Col Stride.\n      padding: Padding type.\n      data_format: Data format.\n      use_gpu: whether we are running on GPU\n      x_init_value: Values to be passed to the gradient checker.\n    \"\"\"\n    assert input_sizes[0] == output_sizes[0]\n    assert input_sizes[3] == output_sizes[3]\n    total_size = 1\n    for s in input_sizes:\n      total_size *= s\n    # Initializes the input tensor with array containing incrementing\n    # numbers from 1.\n    x = [f * 1.0 for f in range(1, total_size + 1)]\n    with self.cached_session(use_gpu=use_gpu):\n      input_tensor = constant_op.constant(x, shape=input_sizes, name=\"input\")\n      if pool_func == nn_ops.avg_pool:\n        func_name = \"avg_pool\"\n        err_tolerance = 1e-3\n      else:\n        if x_init_value is None:\n          x_init_value = np.asfarray(\n              np.arange(1, total_size + 1),\n              dtype=np.float32).reshape(input_sizes)\n        func_name = \"max_pool\"\n        err_tolerance = 1e-2\n      if data_format == \"NCHW\":\n        ksize = [1, 1, window_rows, window_rows]\n        strides = [1, 1, row_stride, col_stride]\n        t = test_util.NHWCToNCHW(input_tensor)\n      else:\n        ksize = [1, window_rows, window_rows, 1]\n        strides = [1, row_stride, col_stride, 1]\n        t = input_tensor\n      t = pool_func(\n          t,\n          ksize=ksize,\n          strides=strides,\n          padding=padding,\n          data_format=data_format,\n          name=func_name)\n      if data_format == \"NCHW\":\n        t = test_util.NHWCToNCHW(t)\n\n      t_g = gradients_impl.gradients(t**2, input_tensor)[0]\n      err = gradient_checker.compute_gradient_error(\n          input_tensor,\n          input_sizes,\n          t_g,\n          input_sizes,\n          x_init_value=x_init_value,\n          delta=1e-2)\n    tf_logging.info(\"%s second-order gradient error = %.4f\" % (func_name, err))\n    self.assertLess(err, err_tolerance)\n\n  def _testMaxPoolGradValidPadding1_1(self, data_format, use_gpu):\n    for pool_func in [gen_nn_ops.max_pool_v2, nn_ops.max_pool]:\n      self._ConstructAndTestGradient(\n          pool_func,\n          input_sizes=[1, 3, 3, 1],\n          output_sizes=[1, 3, 3, 1],\n          window_rows=1,\n          window_cols=1,\n          row_stride=1,\n          col_stride=1,\n          padding=\"VALID\",\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  def _testMaxPoolGradValidPadding2_1_6(self, data_format, use_gpu):\n    for pool_func in [gen_nn_ops.max_pool_v2, nn_ops.max_pool]:\n      self._ConstructAndTestGradient(\n          pool_func,\n          input_sizes=[2, 6, 6, 3],\n          output_sizes=[2, 5, 5, 3],\n          window_rows=2,\n          window_cols=2,\n          row_stride=1,\n          col_stride=1,\n          padding=\"VALID\",\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  def _testMaxPoolGradValidPadding2_1_7(self, data_format, use_gpu):\n    for pool_func in [gen_nn_ops.max_pool_v2, nn_ops.max_pool]:\n      self._ConstructAndTestGradient(\n          pool_func,\n          input_sizes=[2, 7, 7, 3],\n          output_sizes=[2, 6, 6, 3],\n          window_rows=2,\n          window_cols=2,\n          row_stride=1,\n          col_stride=1,\n          padding=\"VALID\",\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  def _testMaxPoolGradValidPadding1_2(self, data_format, use_gpu):\n    for pool_func in [gen_nn_ops.max_pool_v2, nn_ops.max_pool]:\n      self._ConstructAndTestGradient(\n          pool_func,\n          input_sizes=[1, 3, 3, 1],\n          output_sizes=[1, 2, 2, 1],\n          window_rows=1,\n          window_cols=1,\n          row_stride=2,\n          col_stride=2,\n          padding=\"VALID\",\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  def _testMaxPoolGradValidPadding2_2(self, data_format, use_gpu):\n    for pool_func in [gen_nn_ops.max_pool_v2, nn_ops.max_pool]:\n      self._ConstructAndTestGradient(\n          pool_func,\n          input_sizes=[2, 2, 2, 3],\n          output_sizes=[2, 1, 1, 3],\n          window_rows=2,\n          window_cols=2,\n          row_stride=2,\n          col_stride=2,\n          padding=\"VALID\",\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  def _testMaxPoolGradSamePadding1_1(self, data_format, use_gpu):\n    for pool_func in [gen_nn_ops.max_pool_v2, nn_ops.max_pool]:\n      self._ConstructAndTestGradient(\n          pool_func,\n          input_sizes=[2, 2, 4, 3],\n          output_sizes=[2, 2, 4, 3],\n          window_rows=1,\n          window_cols=1,\n          row_stride=1,\n          col_stride=1,\n          padding=\"SAME\",\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  def _testMaxPoolGradSamePadding1_2(self, data_format, use_gpu):\n    for pool_func in [gen_nn_ops.max_pool_v2, nn_ops.max_pool]:\n      self._ConstructAndTestGradient(\n          pool_func,\n          input_sizes=[2, 2, 4, 3],\n          output_sizes=[2, 1, 2, 3],\n          window_rows=1,\n          window_cols=1,\n          row_stride=2,\n          col_stride=2,\n          padding=\"SAME\",\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  def _testMaxPoolGradSamePadding2_1(self, data_format, use_gpu):\n    for pool_func in [gen_nn_ops.max_pool_v2, nn_ops.max_pool]:\n      self._ConstructAndTestGradient(\n          pool_func,\n          input_sizes=[2, 2, 4, 3],\n          output_sizes=[2, 2, 4, 3],\n          window_rows=2,\n          window_cols=2,\n          row_stride=1,\n          col_stride=1,\n          padding=\"SAME\",\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  def _testMaxPoolGradSamePadding2_2(self, data_format, use_gpu):\n    for pool_func in [gen_nn_ops.max_pool_v2, nn_ops.max_pool]:\n      self._ConstructAndTestGradient(\n          pool_func,\n          input_sizes=[2, 2, 4, 3],\n          output_sizes=[2, 1, 2, 3],\n          window_rows=2,\n          window_cols=2,\n          row_stride=2,\n          col_stride=2,\n          padding=\"SAME\",\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  def _testMaxPoolGradSamePadding3_1(self, data_format, use_gpu):\n    for pool_func in [gen_nn_ops.max_pool_v2, nn_ops.max_pool]:\n      self._ConstructAndTestGradient(\n          pool_func,\n          input_sizes=[1, 7, 7, 1],\n          output_sizes=[1, 7, 7, 1],\n          window_rows=3,\n          window_cols=3,\n          row_stride=1,\n          col_stride=1,\n          padding=\"SAME\",\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  def _testMaxPoolExplicitPadding_1(self, data_format, use_gpu):\n    for pool_func in [nn_ops.max_pool]:\n      self._ConstructAndTestGradient(\n          pool_func,\n          input_sizes=[1, 7, 7, 1],\n          output_sizes=[1, 7, 7, 1],\n          window_rows=3,\n          window_cols=3,\n          row_stride=1,\n          col_stride=1,\n          padding=[[0, 0], [1, 1], [1, 1], [0, 0]],\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  def _testMaxPoolExplicitPadding_2(self, data_format, use_gpu):\n    for pool_func in [nn_ops.max_pool]:\n      self._ConstructAndTestGradient(\n          pool_func,\n          input_sizes=[1, 7, 7, 1],\n          output_sizes=[1, 6, 8, 1],\n          window_rows=3,\n          window_cols=5,\n          row_stride=1,\n          col_stride=1,\n          padding=[[0, 0], [0, 1], [2, 3], [0, 0]],\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  def _testMaxPoolExplicitPaddingLeftGreater(self, data_format, use_gpu):\n    for pool_func in [nn_ops.max_pool]:\n      self._ConstructAndTestGradient(\n          pool_func,\n          input_sizes=[1, 7, 7, 1],\n          output_sizes=[1, 6, 8, 1],\n          window_rows=3,\n          window_cols=5,\n          row_stride=1,\n          col_stride=1,\n          padding=[[0, 0], [0, 1], [3, 2], [0, 0]],\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  def _testMaxPoolExplicitPaddingBatchChannel(self, data_format, use_gpu):\n    for pool_func in [nn_ops.max_pool]:\n      self._ConstructAndTestGradient(\n          pool_func,\n          input_sizes=[4, 7, 7, 3],\n          output_sizes=[4, 6, 8, 3],\n          window_rows=3,\n          window_cols=5,\n          row_stride=1,\n          col_stride=1,\n          padding=[[0, 0], [0, 1], [3, 2], [0, 0]],\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  def _testMaxPoolExplicitPaddingStrides(self, data_format, use_gpu):\n    for pool_func in [nn_ops.max_pool]:\n      self._ConstructAndTestGradient(\n          pool_func,\n          input_sizes=[1, 7, 7, 1],\n          output_sizes=[1, 4, 3, 1],\n          window_rows=3,\n          window_cols=3,\n          row_stride=2,\n          col_stride=3,\n          padding=[[0, 0], [1, 1], [1, 1], [0, 0]],\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_deprecated_v1\n  def testMaxPoolGrad(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._testMaxPoolGradValidPadding1_1(data_format, use_gpu)\n      self._testMaxPoolGradValidPadding1_2(data_format, use_gpu)\n      self._testMaxPoolGradValidPadding2_1_6(data_format, use_gpu)\n      self._testMaxPoolGradValidPadding2_1_7(data_format, use_gpu)\n      self._testMaxPoolGradValidPadding2_2(data_format, use_gpu)\n      self._testMaxPoolGradSamePadding1_1(data_format, use_gpu)\n      self._testMaxPoolGradSamePadding1_2(data_format, use_gpu)\n      self._testMaxPoolGradSamePadding2_1(data_format, use_gpu)\n      self._testMaxPoolGradSamePadding2_2(data_format, use_gpu)\n      self._testMaxPoolGradSamePadding3_1(data_format, use_gpu)\n      self._testMaxPoolExplicitPadding_1(data_format, use_gpu)\n      self._testMaxPoolExplicitPadding_2(data_format, use_gpu)\n      self._testMaxPoolExplicitPaddingStrides(data_format, use_gpu)\n      self._testMaxPoolExplicitPaddingLeftGreater(data_format, use_gpu)\n      self._testMaxPoolExplicitPaddingBatchChannel(data_format, use_gpu)\n\n  def _MaxPoolGrad(self, orig_input, orig_output, grad, window_rows,\n                   window_cols, row_stride, col_stride, padding, v2):\n    \"\"\"Max Pooling Gradient.\n\n    Args:\n      orig_input: A float Tensor. The original input tensor.\n      orig_output: A float Tensor. The original output tensor.\n      grad: A float Tensor.\n        The 4D (batch x rows x cols x depth) output backprop.\n      window_rows: integer. Kernel size along rows dimension.\n      window_cols: integer. Kernel size along cols dimension.\n      row_stride: integer. Stride along rows dimension\n      col_stride: integer. Stride along cols dimension\n      padding: PoolingOpDef.Padding.  Padding type.\n\n    Returns:\n      A Tensor.\n    \"\"\"\n    pool_func = gen_nn_ops.max_pool_grad_v2 if v2 else gen_nn_ops.max_pool_grad\n    if v2:\n      return pool_func(orig_input, orig_output, grad,\n                       [1, window_rows, window_cols, 1],\n                       [1, row_stride, col_stride, 1], padding)\n    else:\n      padding, explicit_paddings = nn_ops.convert_padding(padding)\n      return pool_func(orig_input, orig_output, grad,\n                       [1, window_rows, window_cols, 1],\n                       [1, row_stride, col_stride, 1], padding,\n                       explicit_paddings)\n\n  def _testMaxPoolGradDirect(self, input_data, output_backprop,\n                             expected_input_backprop, input_sizes, output_sizes,\n                             window_rows, window_cols, row_stride, col_stride,\n                             padding, use_gpu, v2):\n    pool_func = gen_nn_ops.max_pool_v2 if v2 else nn_ops.max_pool\n    with self.cached_session(use_gpu=use_gpu):\n      input_tensor = variables.Variable(\n          np.array(input_data, dtype=np.float32).reshape(input_sizes))\n      self.evaluate(variables.global_variables_initializer())\n      output_tensor = pool_func(input_tensor, [1, window_rows, window_cols, 1],\n                                [1, row_stride, col_stride, 1], padding)\n      output_backprop_tensor = constant_op.constant(\n          output_backprop, shape=output_sizes)\n\n      input_backprop_tensor = self._MaxPoolGrad(\n          input_tensor, output_tensor, output_backprop_tensor, window_rows,\n          window_cols, row_stride, col_stride, padding, v2)\n\n      actual_input_backprop = self.evaluate(input_backprop_tensor)\n      self.assertShapeEqual(actual_input_backprop, input_backprop_tensor)\n      actual_input_backprop = actual_input_backprop.flatten()\n      actual_input_backprop = self._GetNdArray(actual_input_backprop)\n\n      actual_output = self.evaluate(output_tensor).flatten()\n      actual_output = self._GetNdArray(actual_output)\n\n      self.assertAllClose(\n          expected_input_backprop, actual_input_backprop, rtol=1e-6, atol=1e-6)\n\n  def _testMaxPoolGradDirect1_1(self):\n    input_data = [\n        1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n        1.0, 1.0\n    ]\n    output_backprop = [11.0, 12.0, 13.0, 15.0, 16.0, 17.0, 19.0, 20.0, 21.0]\n    expected_input_backprop = [\n        11.0, 12.0, 13.0, 0.0, 15.0, 16.0, 17.0, 0.0, 19.0, 20.0, 21.0, 0.0,\n        0.0, 0.0, 0.0, 0.0\n    ]\n\n    for use_gpu in True, False:\n      for v2 in [True, False]:\n        self._testMaxPoolGradDirect(\n            input_data,\n            output_backprop,\n            expected_input_backprop,\n            input_sizes=[1, 4, 4, 1],\n            output_sizes=[1, 3, 3, 1],\n            window_rows=2,\n            window_cols=2,\n            row_stride=1,\n            col_stride=1,\n            padding=\"VALID\",\n            use_gpu=use_gpu,\n            v2=v2)\n\n  def _testMaxPoolGradDirect1_2(self):\n    input_data = [\n        1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0,\n        0.0, 1.0\n    ]\n    output_backprop = [11.0, 12.0, 13.0, 15.0, 16.0, 17.0, 19.0, 20.0, 21.0]\n    expected_input_backprop = [\n        11.0, 0.0, 25.0, 0.0, 0.0, 31.0, 0.0, 17.0, 19.0, 0.0, 41.0, 0.0, 0.0,\n        0.0, 0.0, 0.0\n    ]\n\n    for use_gpu in True, False:\n      for v2 in [True, False]:\n        self._testMaxPoolGradDirect(\n            input_data,\n            output_backprop,\n            expected_input_backprop,\n            input_sizes=[1, 4, 4, 1],\n            output_sizes=[1, 3, 3, 1],\n            window_rows=2,\n            window_cols=2,\n            row_stride=1,\n            col_stride=1,\n            padding=\"VALID\",\n            use_gpu=use_gpu,\n            v2=v2)\n\n  def _testMaxPoolGradDirect1_3(self):\n    input_data = [\n        1.0,\n        0.0,\n        1.0,\n        0.0,\n        0.0,\n        1.0,\n        0.0,\n        1.0,\n        1.0,\n        0.0,\n        1.0,\n        0.0,\n        0.0,\n        1.0,\n        0.0,\n        1.0,\n    ]\n    output_backprop = [\n        11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0,\n        23.0, 24.0, 25.0, 26.0\n    ]\n    expected_input_backprop = [\n        54,\n        0.0,\n        62,\n        0.0,\n        0.0,\n        60,\n        0.0,\n        22.0,\n        47,\n        0.0,\n        51,\n        0.0,\n        0.0,\n        0.0,\n        0.0,\n        0.0,\n    ]\n\n    for use_gpu in True, False:\n      for v2 in [True, False]:\n        self._testMaxPoolGradDirect(\n            input_data,\n            output_backprop,\n            expected_input_backprop,\n            input_sizes=[1, 4, 4, 1],\n            output_sizes=[1, 4, 4, 1],\n            window_rows=3,\n            window_cols=3,\n            row_stride=1,\n            col_stride=1,\n            padding=\"SAME\",\n            use_gpu=use_gpu,\n            v2=v2)\n\n  def _testMaxPoolGradZeroExplicitPadding(self):\n    input_data = [\n        1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0,\n        0.0, 1.0\n    ]\n    output_backprop = [11.0, 12.0, 13.0, 15.0, 16.0, 17.0, 19.0, 20.0, 21.0]\n    expected_input_backprop = [\n        11.0, 0.0, 25.0, 0.0, 0.0, 31.0, 0.0, 17.0, 19.0, 0.0, 41.0, 0.0, 0.0,\n        0.0, 0.0, 0.0\n    ]\n\n    for use_gpu in True, False:\n      for v2 in [False]:\n        self._testMaxPoolGradDirect(\n            input_data,\n            output_backprop,\n            expected_input_backprop,\n            input_sizes=[1, 4, 4, 1],\n            output_sizes=[1, 3, 3, 1],\n            window_rows=2,\n            window_cols=2,\n            row_stride=1,\n            col_stride=1,\n            padding=[[0, 0], [0, 0], [0, 0], [0, 0]],\n            use_gpu=use_gpu,\n            v2=v2)\n\n  def _testMaxPoolGradExplicitPadding_1(self):\n    input_data = [\n        1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0,\n        0.0, 1.0\n    ]\n    output_backprop = [11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0,\n                       20.0, 21.0, 22.0]\n    expected_input_backprop = [\n        11.0, 0.0, 25.0, 0.0, 0.0, 31.0, 0.0, 49.0, 19.0, 0.0, 41.0, 0.0, 0.0,\n        0.0, 0.0, 22.0\n    ]\n\n    for use_gpu in True, False:\n      for v2 in [False]:\n        self._testMaxPoolGradDirect(\n            input_data,\n            output_backprop,\n            expected_input_backprop,\n            input_sizes=[1, 4, 4, 1],\n            output_sizes=[1, 3, 4, 1],\n            window_rows=2,\n            window_cols=2,\n            row_stride=1,\n            col_stride=1,\n            padding=[[0, 0], [0, 0], [0, 1], [0, 0]],\n            use_gpu=use_gpu,\n            v2=v2)\n\n  def _testMaxPoolGradExplicitPadding_2(self):\n    input_data = [\n        1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0,\n        0.0, 1.0\n    ]\n    output_backprop = [11.0, 12.0, 13.0, 15.0, 16.0, 17.0, 19.0, 20.0, 21.0]\n    expected_input_backprop = [\n        54.0, 0.0, 30.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.0, 0.0, 21.0, 0.0, 0.0,\n        0.0, 0.0, 0.0\n    ]\n\n    for use_gpu in True, False:\n      for v2 in [False]:\n        self._testMaxPoolGradDirect(\n            input_data,\n            output_backprop,\n            expected_input_backprop,\n            input_sizes=[1, 4, 4, 1],\n            output_sizes=[1, 3, 3, 1],\n            window_rows=3,\n            window_cols=3,\n            row_stride=2,\n            col_stride=2,\n            padding=[[0, 0], [2, 1], [2, 1], [0, 0]],\n            use_gpu=use_gpu,\n            v2=v2)\n\n  def _testMaxPoolGradExplicitPadding_3(self):\n    input_data = [\n        -1.0, -5.0, -1.0, -5.0, -5.0, -1.0, -5.0, -1.0, -1.0, -5.0, -1.0, -5.0,\n        -5.0, -1.0, -5.0, -1.0\n    ]\n    output_backprop = [11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0,\n                       20.0, 21.0, 22.0]\n    expected_input_backprop = [\n        11.0, 0.0, 25.0, 0.0, 0.0, 31.0, 0.0, 49.0, 19.0, 0.0, 41.0, 0.0, 0.0,\n        0.0, 0.0, 22.0\n    ]\n\n    for use_gpu in True, False:\n      for v2 in [False]:\n        self._testMaxPoolGradDirect(\n            input_data,\n            output_backprop,\n            expected_input_backprop,\n            input_sizes=[1, 4, 4, 1],\n            output_sizes=[1, 3, 4, 1],\n            window_rows=2,\n            window_cols=2,\n            row_stride=1,\n            col_stride=1,\n            padding=[[0, 0], [0, 0], [0, 1], [0, 0]],\n            use_gpu=use_gpu,\n            v2=v2)\n\n  @test_util.no_xla_auto_jit(\"b/123923733\")  # NaNs handled differently\n  def _testMaxPoolGradDirectWithNans2_1(self):\n    input_data = [float(\"nan\")] * 16\n    output_backprop = [11.0, 12.0, 13.0, 15.0, 16.0, 17.0, 19.0, 20.0, 21.0]\n    # Test the CPU implementation, which propagates diffs in case of NaN\n    expected_input_backprop_tf_cpu = [\n        11.0, 12.0, 13.0, 0.0, 15.0, 16.0, 17.0, 0.0, 19.0, 20.0, 21.0, 0.0,\n        0.0, 0.0, 0.0, 0.0\n    ]\n    for v2 in [True, False]:\n      self._testMaxPoolGradDirect(\n          input_data,\n          output_backprop,\n          expected_input_backprop_tf_cpu,\n          input_sizes=[1, 4, 4, 1],\n          output_sizes=[1, 3, 3, 1],\n          window_rows=2,\n          window_cols=2,\n          row_stride=1,\n          col_stride=1,\n          padding=\"VALID\",\n          use_gpu=False,\n          v2=v2)\n\n    if not test.is_gpu_available():\n      return\n\n    # The functionality associated with TF_ENABLE_NANPROP is currently\n    # not supported on the ROCm platform, so skip this part of the test\n    # NANs in input lead to non-deterministic results, and hence skipping\n    # the remaining tests altogether on the ROCm platform\n    if test.is_built_with_rocm():\n      return\n\n    # Test the GPU implementation that uses cudnn for now.\n    saved_nanprop = os.environ.get(\"TF_ENABLE_MAXPOOL_NANPROP\")\n    # Do not propagate the diff in cases of NaNs\n    os.environ[\"TF_ENABLE_MAXPOOL_NANPROP\"] = \"0\"\n    expected_input_backprop_cudnn = [\n        0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n        0.0, 0.0\n    ]\n\n    for v2 in [True, False]:\n      self._testMaxPoolGradDirect(\n          input_data,\n          output_backprop,\n          expected_input_backprop_cudnn,\n          input_sizes=[1, 4, 4, 1],\n          output_sizes=[1, 3, 3, 1],\n          window_rows=2,\n          window_cols=2,\n          row_stride=1,\n          col_stride=1,\n          padding=\"VALID\",\n          use_gpu=True,\n          v2=v2)\n\n    # Propagate the diff in cases of NaNs\n    os.environ[\"TF_ENABLE_MAXPOOL_NANPROP\"] = \"1\"\n    expected_input_backprop_cudnn = expected_input_backprop_tf_cpu\n\n    for v2 in [True, False]:\n      self._testMaxPoolGradDirect(\n          input_data,\n          output_backprop,\n          expected_input_backprop_cudnn,\n          input_sizes=[1, 4, 4, 1],\n          output_sizes=[1, 3, 3, 1],\n          window_rows=2,\n          window_cols=2,\n          row_stride=1,\n          col_stride=1,\n          padding=\"VALID\",\n          use_gpu=True,\n          v2=v2)\n\n    if saved_nanprop:\n      os.environ[\"TF_ENABLE_MAXPOOL_NANPROP\"] = saved_nanprop\n    else:\n      del os.environ[\"TF_ENABLE_MAXPOOL_NANPROP\"]\n\n  @test_util.no_xla_auto_jit(\"b/123923733\")  # NaNs handled differently\n  def _testMaxPoolGradDirectWithNans2_2(self):\n    input_data = [float(\"nan\")] * 16\n    output_backprop = [\n        float(\"nan\"), 12.0, 13.0, 15.0,\n        float(\"nan\"), 17.0, 19.0, 20.0,\n        float(\"nan\")\n    ]\n    # Test the CPU implementation, which propagates diffs in case of NaN\n    expected_input_backprop_tf_cpu = [\n        float(\"nan\"), 12.0, 13.0, 0.0, 15.0,\n        float(\"nan\"), 17.0, 0.0, 19.0, 20.0,\n        float(\"nan\"), 0.0, 0.0, 0.0, 0.0, 0.0\n    ]\n    for v2 in [True, False]:\n      self._testMaxPoolGradDirect(\n          input_data,\n          output_backprop,\n          expected_input_backprop_tf_cpu,\n          input_sizes=[1, 4, 4, 1],\n          output_sizes=[1, 3, 3, 1],\n          window_rows=2,\n          window_cols=2,\n          row_stride=1,\n          col_stride=1,\n          padding=\"VALID\",\n          use_gpu=False,\n          v2=v2)\n\n    if not test.is_gpu_available():\n      return\n\n    # The functionality associated with TF_ENABLE_NANPROP is currently\n    # not supported on the ROCm platform, so skip this part of the test\n    # NANs in input lead to non-deterministic results, and hence skipping\n    # the remaining tests altogether on the ROCm platform\n    if test.is_built_with_rocm():\n      return\n\n    # Test the GPU implementation that uses cudnn for now.\n    saved_nanprop = os.environ.get(\"TF_ENABLE_MAXPOOL_NANPROP\")\n    # Do not propagate the diff in cases of NaNs\n    os.environ[\"TF_ENABLE_MAXPOOL_NANPROP\"] = \"0\"\n    expected_input_backprop_cudnn = [\n        0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n        0.0, 0.0\n    ]\n\n    for v2 in [True, False]:\n      self._testMaxPoolGradDirect(\n          input_data,\n          output_backprop,\n          expected_input_backprop_cudnn,\n          input_sizes=[1, 4, 4, 1],\n          output_sizes=[1, 3, 3, 1],\n          window_rows=2,\n          window_cols=2,\n          row_stride=1,\n          col_stride=1,\n          padding=\"VALID\",\n          use_gpu=True,\n          v2=v2)\n\n    # Propagate the diff in cases of NaNs\n    os.environ[\"TF_ENABLE_MAXPOOL_NANPROP\"] = \"1\"\n    expected_input_backprop_cudnn = expected_input_backprop_tf_cpu\n\n    for v2 in [True, False]:\n      self._testMaxPoolGradDirect(\n          input_data,\n          output_backprop,\n          expected_input_backprop_cudnn,\n          input_sizes=[1, 4, 4, 1],\n          output_sizes=[1, 3, 3, 1],\n          window_rows=2,\n          window_cols=2,\n          row_stride=1,\n          col_stride=1,\n          padding=\"VALID\",\n          use_gpu=True,\n          v2=v2)\n\n    if saved_nanprop:\n      os.environ[\"TF_ENABLE_MAXPOOL_NANPROP\"] = saved_nanprop\n    else:\n      del os.environ[\"TF_ENABLE_MAXPOOL_NANPROP\"]\n\n  @test_util.run_deprecated_v1\n  def testMaxPoolGradDirect(self):\n    self._testMaxPoolGradDirect1_1()\n    self._testMaxPoolGradDirect1_2()\n    self._testMaxPoolGradDirect1_3()\n    self._testMaxPoolGradDirectWithNans2_1()\n    self._testMaxPoolGradDirectWithNans2_2()\n    self._testMaxPoolGradZeroExplicitPadding()\n    self._testMaxPoolGradExplicitPadding_1()\n    self._testMaxPoolGradExplicitPadding_2()\n    self._testMaxPoolGradExplicitPadding_3()\n\n  def _testMaxPoolGradGradValidPadding1_1(self, data_format, use_gpu):\n    for pool_func in [gen_nn_ops.max_pool_v2, nn_ops.max_pool]:\n      self._ConstructAndTestSecondGradient(\n          pool_func,\n          input_sizes=[1, 3, 3, 1],\n          output_sizes=[1, 3, 3, 1],\n          window_rows=1,\n          window_cols=1,\n          row_stride=1,\n          col_stride=1,\n          padding=\"VALID\",\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  def _testMaxPoolGradGradValidPadding2_1_6(self, data_format, use_gpu):\n    for pool_func in [gen_nn_ops.max_pool_v2, nn_ops.max_pool]:\n      self._ConstructAndTestSecondGradient(\n          pool_func,\n          input_sizes=[2, 6, 6, 3],\n          output_sizes=[2, 5, 5, 3],\n          window_rows=2,\n          window_cols=2,\n          row_stride=1,\n          col_stride=1,\n          padding=\"VALID\",\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  def _testMaxPoolGradGradValidPadding2_1_7(self, data_format, use_gpu):\n    for pool_func in [gen_nn_ops.max_pool_v2, nn_ops.max_pool]:\n      self._ConstructAndTestSecondGradient(\n          pool_func,\n          input_sizes=[2, 7, 7, 3],\n          output_sizes=[2, 6, 6, 3],\n          window_rows=2,\n          window_cols=2,\n          row_stride=1,\n          col_stride=1,\n          padding=\"VALID\",\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  def _testMaxPoolGradGradValidPadding2_2(self, data_format, use_gpu):\n    for pool_func in [gen_nn_ops.max_pool_v2, nn_ops.max_pool]:\n      self._ConstructAndTestSecondGradient(\n          pool_func,\n          input_sizes=[2, 2, 2, 3],\n          output_sizes=[2, 1, 1, 3],\n          window_rows=2,\n          window_cols=2,\n          row_stride=2,\n          col_stride=2,\n          padding=\"VALID\",\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  def _testMaxPoolGradGradSamePadding1_1(self, data_format, use_gpu):\n    for pool_func in [gen_nn_ops.max_pool_v2, nn_ops.max_pool]:\n      self._ConstructAndTestSecondGradient(\n          pool_func,\n          input_sizes=[2, 2, 4, 3],\n          output_sizes=[2, 2, 4, 3],\n          window_rows=1,\n          window_cols=1,\n          row_stride=1,\n          col_stride=1,\n          padding=\"SAME\",\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  def _testMaxPoolGradGradSamePadding2_1(self, data_format, use_gpu):\n    for pool_func in [gen_nn_ops.max_pool_v2, nn_ops.max_pool]:\n      self._ConstructAndTestSecondGradient(\n          pool_func,\n          input_sizes=[2, 2, 4, 3],\n          output_sizes=[2, 2, 4, 3],\n          window_rows=2,\n          window_cols=2,\n          row_stride=1,\n          col_stride=1,\n          padding=\"SAME\",\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  def _testMaxPoolGradGradSamePadding2_2(self, data_format, use_gpu):\n    for pool_func in [gen_nn_ops.max_pool_v2, nn_ops.max_pool]:\n      self._ConstructAndTestSecondGradient(\n          pool_func,\n          input_sizes=[2, 2, 4, 3],\n          output_sizes=[2, 1, 2, 3],\n          window_rows=2,\n          window_cols=2,\n          row_stride=2,\n          col_stride=2,\n          padding=\"SAME\",\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  def _testMaxPoolGradGradSamePadding3_1(self, data_format, use_gpu):\n    for pool_func in [gen_nn_ops.max_pool_v2, nn_ops.max_pool]:\n      self._ConstructAndTestSecondGradient(\n          pool_func,\n          input_sizes=[1, 7, 7, 1],\n          output_sizes=[1, 7, 7, 1],\n          window_rows=3,\n          window_cols=3,\n          row_stride=1,\n          col_stride=1,\n          padding=\"SAME\",\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_deprecated_v1\n  def testMaxPoolGradGrad(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._testMaxPoolGradGradValidPadding1_1(data_format, use_gpu)\n      self._testMaxPoolGradGradValidPadding2_1_6(data_format, use_gpu)\n      self._testMaxPoolGradGradValidPadding2_1_7(data_format, use_gpu)\n      self._testMaxPoolGradGradValidPadding2_2(data_format, use_gpu)\n      self._testMaxPoolGradGradSamePadding1_1(data_format, use_gpu)\n      self._testMaxPoolGradGradSamePadding2_1(data_format, use_gpu)\n      self._testMaxPoolGradGradSamePadding2_2(data_format, use_gpu)\n      self._testMaxPoolGradGradSamePadding3_1(data_format, use_gpu)\n\n  def _MaxPoolGradGrad(self, orig_input, orig_output, grad, window_rows,\n                       window_cols, row_stride, col_stride, padding):\n    \"\"\"Max Pooling Second-Order Gradient.\n\n    Args:\n      orig_input: A float Tensor. The original input tensor.\n      orig_output: A float Tensor. The original output tensor.\n      grad: A float Tensor.\n        The 4D (batch x out_rows x out_cols x depth) output backprop.\n      window_rows: integer. Kernel size along rows dimension.\n      window_cols: integer. Kernel size along cols dimension.\n      row_stride: integer. Stride along rows dimension\n      col_stride: integer. Stride along cols dimension\n      padding: PoolingOpDef.Padding.  Padding type.\n\n    Returns:\n      A Tensor.\n    \"\"\"\n    return gen_nn_ops.max_pool_grad_grad(\n        orig_input, orig_output, grad, [1, window_rows, window_cols, 1],\n        [1, row_stride, col_stride, 1], padding)\n\n  @test_util.run_deprecated_v1\n  def testAvgPoolGrad(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._testAvgPoolGradValidPadding1_1(data_format, use_gpu)\n      self._testAvgPoolGradValidPadding1_2(data_format, use_gpu)\n      self._testAvgPoolGradValidPadding2_1(data_format, use_gpu)\n      self._testAvgPoolGradValidPadding2_2(data_format, use_gpu)\n      self._testAvgPoolGradSamePadding1_1(data_format, use_gpu)\n      self._testAvgPoolGradSamePadding1_2(data_format, use_gpu)\n      self._testAvgPoolGradSamePadding2_1(data_format, use_gpu)\n      self._testAvgPoolGradSamePadding2_2(data_format, use_gpu)\n      self._testAvgPoolGradSamePadding3_1(data_format, use_gpu)\n\n  def _testAvgPoolGradValidPadding1_1(self, data_format, use_gpu):\n    self._ConstructAndTestGradient(\n        nn_ops.avg_pool,\n        input_sizes=[2, 3, 3, 3],\n        output_sizes=[2, 3, 3, 3],\n        window_rows=1,\n        window_cols=1,\n        row_stride=1,\n        col_stride=1,\n        padding=\"VALID\",\n        data_format=data_format,\n        use_gpu=use_gpu)\n\n  def _testAvgPoolGradValidPadding1_2(self, data_format, use_gpu):\n    self._ConstructAndTestGradient(\n        nn_ops.avg_pool,\n        input_sizes=[2, 3, 3, 3],\n        output_sizes=[2, 2, 2, 3],\n        window_rows=1,\n        window_cols=1,\n        row_stride=2,\n        col_stride=2,\n        padding=\"VALID\",\n        data_format=data_format,\n        use_gpu=use_gpu)\n\n  def _testAvgPoolGradValidPadding2_1(self, data_format, use_gpu):\n    self._ConstructAndTestGradient(\n        nn_ops.avg_pool,\n        input_sizes=[2, 3, 3, 3],\n        output_sizes=[2, 2, 2, 3],\n        window_rows=2,\n        window_cols=2,\n        row_stride=1,\n        col_stride=1,\n        padding=\"VALID\",\n        data_format=data_format,\n        use_gpu=use_gpu)\n\n  def _testAvgPoolGradValidPadding2_2(self, data_format, use_gpu):\n    self._ConstructAndTestGradient(\n        nn_ops.avg_pool,\n        input_sizes=[2, 2, 2, 3],\n        output_sizes=[2, 1, 1, 3],\n        window_rows=2,\n        window_cols=2,\n        row_stride=2,\n        col_stride=2,\n        padding=\"VALID\",\n        data_format=data_format,\n        use_gpu=use_gpu)\n\n  def _testAvgPoolGradSamePadding1_1(self, data_format, use_gpu):\n    self._ConstructAndTestGradient(\n        nn_ops.avg_pool,\n        input_sizes=[2, 2, 4, 3],\n        output_sizes=[2, 2, 4, 3],\n        window_rows=1,\n        window_cols=1,\n        row_stride=1,\n        col_stride=1,\n        padding=\"SAME\",\n        data_format=data_format,\n        use_gpu=use_gpu)\n\n  def _testAvgPoolGradSamePadding1_2(self, data_format, use_gpu):\n    self._ConstructAndTestGradient(\n        nn_ops.avg_pool,\n        input_sizes=[2, 2, 4, 3],\n        output_sizes=[2, 1, 2, 3],\n        window_rows=1,\n        window_cols=1,\n        row_stride=2,\n        col_stride=2,\n        padding=\"SAME\",\n        data_format=data_format,\n        use_gpu=use_gpu)\n\n  def _testAvgPoolGradSamePadding2_1(self, data_format, use_gpu):\n    self._ConstructAndTestGradient(\n        nn_ops.avg_pool,\n        input_sizes=[2, 2, 4, 3],\n        output_sizes=[2, 2, 4, 3],\n        window_rows=2,\n        window_cols=2,\n        row_stride=1,\n        col_stride=1,\n        padding=\"SAME\",\n        data_format=data_format,\n        use_gpu=use_gpu)\n\n  def _testAvgPoolGradSamePadding2_2(self, data_format, use_gpu):\n    self._ConstructAndTestGradient(\n        nn_ops.avg_pool,\n        input_sizes=[2, 2, 4, 3],\n        output_sizes=[2, 1, 2, 3],\n        window_rows=2,\n        window_cols=2,\n        row_stride=2,\n        col_stride=2,\n        padding=\"SAME\",\n        data_format=data_format,\n        use_gpu=use_gpu)\n\n  def _testAvgPoolGradSamePadding3_1(self, data_format, use_gpu):\n    self._ConstructAndTestGradient(\n        nn_ops.avg_pool,\n        input_sizes=[1, 7, 7, 1],\n        output_sizes=[1, 7, 7, 1],\n        window_rows=3,\n        window_cols=3,\n        row_stride=1,\n        col_stride=1,\n        padding=\"SAME\",\n        data_format=data_format,\n        use_gpu=use_gpu)\n\n  @test_util.run_deprecated_v1\n  def testShapeFunctionEdgeCases(self):\n    # All shapes unknown.\n    for pool_func in [nn_ops.max_pool, nn_ops.avg_pool]:\n      p = pool_func(\n          array_ops.placeholder(dtypes.float32),\n          ksize=[1, 1, 1, 1],\n          strides=[1, 1, 1, 1],\n          padding=\"SAME\")\n      self.assertEqual([None, None, None, None], p.get_shape().as_list())\n    p, am = nn_ops.max_pool_with_argmax(\n        array_ops.placeholder(dtypes.float32),\n        ksize=[1, 1, 1, 1],\n        strides=[1, 1, 1, 1],\n        padding=\"SAME\")\n    self.assertEqual([None, None, None, None], p.get_shape().as_list())\n    self.assertEqual([None, None, None, None], am.get_shape().as_list())\n\n    # Incorrect input shape.\n    for pool_func in [\n        nn_ops.max_pool, nn_ops.avg_pool, nn_ops.max_pool_with_argmax\n    ]:\n      with self.assertRaises(ValueError):\n        pool_func(\n            array_ops.placeholder(dtypes.float32, shape=[1, 3]),\n            ksize=[1, 1, 1, 1],\n            strides=[1, 1, 1, 1],\n            padding=\"SAME\")\n\n  @test_util.run_deprecated_v1\n  @test_util.disable_xla(\"b/123337890\")  # Error messages differ\n  def testOpEdgeCases(self):\n    with self.session(use_gpu=test.is_gpu_available()) as sess:\n      pool_funcs = [nn_ops.max_pool, nn_ops.avg_pool]\n      if test.is_gpu_available():\n        pool_funcs.append(nn_ops.max_pool_with_argmax)\n      for pool_func in pool_funcs:\n        if pool_func != nn_ops.max_pool:\n          # Illegal strides.\n          with self.assertRaisesRegex(\n              errors_impl.UnimplementedError,\n              \"Pooling is not yet supported on the batch\"):\n            sess.run(\n                pool_func(\n                    array_ops.placeholder(dtypes.float32),\n                    ksize=[1, 1, 1, 1],\n                    strides=[2, 1, 1, 1],\n                    padding=\"SAME\"))\n\n        # Filter too large.\n        with self.assertRaisesRegex(ValueError, \"Negative dimension size\"):\n          sess.run(\n              pool_func(\n                  array_ops.placeholder(dtypes.float32, shape=[32, 20, 20, 3]),\n                  ksize=[1, 20, 21, 1],\n                  strides=[1, 1, 1, 1],\n                  padding=\"VALID\"))\n        with self.assertRaisesRegex(ValueError, \"Negative dimension size\"):\n          pool_func(\n              array_ops.placeholder(dtypes.float32, shape=[32, 20, 20, 3]),\n              ksize=[1, 21, 20, 1],\n              strides=[1, 1, 1, 1],\n              padding=\"VALID\")\n\n  @test_util.run_deprecated_v1\n  def testEdgeCasesRaiseErrors(self):\n    with self.assertRaisesRegexp(\n        ValueError, \"NCHW_VECT_C.*is not supported with \"\n        \"explicit padding|XLA does not support pooling ops with explicit \"\n        \"padding\"):\n      nn_ops.max_pool(\n          array_ops.placeholder(dtypes.float32, shape=[1, 3, 3, 1]),\n          ksize=[1, 2, 2, 1],\n          strides=[1, 2, 2, 1],\n          padding=[[0, 0], [0, 1], [0, 1], [0, 0]],\n          data_format=\"NCHW_VECT_C\")\n    with self.assertRaisesRegexp(\n        ValueError, \"Explicit padding is not supported with an input \"\n                    \"tensor of rank 5\"):\n      nn_ops.max_pool_v2(\n          array_ops.placeholder(dtypes.float32, shape=[1, 3, 3, 1, 1]),\n          ksize=[1, 2, 2, 1, 1],\n          strides=[1, 2, 2, 1, 1],\n          padding=[[0, 0], [0, 1], [0, 1], [0, 0]],\n          data_format=\"NCHW\")\n    with self.assertRaisesRegexp(\n        ValueError, \"Attr 'padding' of 'MaxPoolV2' Op passed \"\n                    \"string 'EXPLICIT'\"):\n      gen_nn_ops.max_pool_v2(\n          array_ops.placeholder(dtypes.float32, shape=[1, 3, 3, 1, 1]),\n          ksize=[1, 2, 2, 1, 1],\n          strides=[1, 2, 2, 1, 1],\n          padding=\"EXPLICIT\",\n          data_format=\"NHWC\")\n\n  @test_util.run_deprecated_v1\n  def testEdgeCasesExcessPadding(self):\n    with self.session(use_gpu=test.is_gpu_available()) as sess:\n      with self.assertRaisesRegexp(\n          (errors_impl.UnimplementedError, errors_impl.InvalidArgumentError),\n          \"Right padding 2 needs to be smaller than the window size 2|\"\n          \"XLA does not support pooling ops with explicit padding\"):\n        input_sizes = [1, 3, 3, 1]\n        x = [(((f + 128) % 255) - 127) for f in range(9)]\n        t = constant_op.constant(x, shape=input_sizes, dtype=dtypes.float32)\n        sess.run(gen_nn_ops.max_pool(\n            t,\n            ksize=[1, 2, 2, 1],\n            strides=[1, 2, 2, 1],\n            padding=\"EXPLICIT\",\n            explicit_paddings=[0, 0, 0, 1, 0, 2, 0, 0],\n            data_format=\"NHWC\"))\n\n  @test_util.run_deprecated_v1\n  def testNegativePadding(self):\n    with self.session(use_gpu=test.is_gpu_available()) as sess:\n      with self.assertRaisesRegexp(\n          ValueError, \"All elements of explicit_paddings must be \"\n                      \"nonnegative for\"):\n        input_sizes = [1, 3, 3, 1]\n        x = [(((f + 128) % 255) - 127) for f in range(9)]\n        t = constant_op.constant(x, shape=input_sizes, dtype=dtypes.float32)\n        sess.run(gen_nn_ops.max_pool(\n            t,\n            ksize=[1, 2, 2, 1],\n            strides=[1, 2, 2, 1],\n            padding=\"EXPLICIT\",\n            explicit_paddings=[0, 0, -1, -1, -1, -1, 0, 0],\n            data_format=\"NHWC\"))\n\n  @test_util.run_deprecated_v1\n  def testExplicitPaddingBatch(self):\n    with self.session(use_gpu=test.is_gpu_available()) as sess:\n      with self.assertRaisesRegexp(\n          ValueError, \"Nonzero explicit padding in the batch or depth \"\n                      \"dimensions is not supported\"):\n        input_sizes = [1, 3, 3, 1]\n        x = [(((f + 128) % 255) - 127) for f in range(9)]\n        t = constant_op.constant(x, shape=input_sizes, dtype=dtypes.float32)\n        sess.run(gen_nn_ops.max_pool(\n            t,\n            ksize=[1, 2, 2, 1],\n            strides=[1, 2, 2, 1],\n            padding=\"EXPLICIT\",\n            explicit_paddings=[1, 1, 1, 1, 1, 1, 0, 0],\n            data_format=\"NHWC\"))\n\n  @test_util.disable_xla(\n      \"b/205634417\")  # XLA is not throwing shape errors for multiple *Grad ops.\n  def testMaxPoolGradEagerShapeErrors(self):\n    with context.eager_mode():\n      orig_in = array_ops.ones((1, 1, 1, 1))\n\n      # Test invalid orig_out shape\n      orig_out = array_ops.ones((1, 1, 1, 2))\n      grad = array_ops.ones((1, 1, 1, 1))\n      with self.assertRaisesRegex(\n          errors_impl.InvalidArgumentError,\n          r\"Expected orig_output shape to be \\[1,1,1,1\\], but got \\[1,1,1,2\\]\"):\n        gen_nn_ops.max_pool_grad(\n            orig_in, orig_out, grad, ksize=[1, 1, 1, 1], strides=[1, 1, 1, 1],\n            padding=\"VALID\")\n      with self.assertRaisesRegex(\n          errors_impl.InvalidArgumentError,\n          r\"Expected orig_output shape to be \\[1,1,1,1\\], but got \\[1,1,1,2\\]\"):\n        gen_nn_ops.max_pool_grad_grad(\n            orig_in, orig_out, grad, ksize=[1, 1, 1, 1], strides=[1, 1, 1, 1],\n            padding=\"VALID\")\n\n      # Test invalid grad shape\n      orig_out = array_ops.ones((1, 1, 1, 1))\n      grad = array_ops.ones((1, 1, 1, 2))\n      with self.assertRaisesRegex(\n          errors_impl.InvalidArgumentError,\n          r\"Expected grad shape to be \\[1,1,1,1\\], but got \\[1,1,1,2\\]\"):\n        gen_nn_ops.max_pool_grad(\n            orig_in, orig_out, grad, ksize=[1, 1, 1, 1], strides=[1, 1, 1, 1],\n            padding=\"VALID\")\n      with self.assertRaisesRegex(\n          errors_impl.InvalidArgumentError,\n          r\"Expected grad shape to be \\[1,1,1,1\\], but got \\[1,1,1,2\\]\"):\n        gen_nn_ops.max_pool_grad_grad(\n            orig_in, orig_out, grad, ksize=[1, 1, 1, 1], strides=[1, 1, 1, 1],\n            padding=\"VALID\")\n\n  def testMaxPoolGradWithArgmaxEagerShapeErrors(self):\n    with context.eager_mode():\n      inp = array_ops.ones((1, 1, 1, 1))\n\n      # Test invalid grad shape\n      grad = array_ops.ones((1, 1, 1, 2))\n      argmax = array_ops.zeros((1, 1, 1, 1), dtype=dtypes.int64)\n      with self.assertRaisesRegex(\n          errors_impl.InvalidArgumentError,\n          r\"Expected grad shape to be \\[1,1,1,1\\], but got \\[1,1,1,2\\]\"):\n        gen_nn_ops.max_pool_grad_with_argmax(\n            inp, grad, argmax, ksize=[1, 1, 1, 1], strides=[1, 1, 1, 1],\n            padding=\"VALID\")\n      # max_pool_grad_grad_with_argmax is only implemented for GPUs\n      if test.is_gpu_available():\n        with self.assertRaisesRegex(\n            errors_impl.InvalidArgumentError,\n            r\"Expected grad shape to be \\[1,1,1,1\\], but got \\[1,1,1,2\\]\"):\n          gen_nn_ops.max_pool_grad_grad_with_argmax(\n              inp, grad, argmax, ksize=[1, 1, 1, 1], strides=[1, 1, 1, 1],\n              padding=\"VALID\")\n\n      # Test invalid argmax shape\n      grad = array_ops.ones((1, 1, 1, 1))\n      argmax = array_ops.ones((1, 1, 1, 2), dtype=dtypes.int64)\n      with self.assertRaisesRegex(\n          errors_impl.InvalidArgumentError,\n          r\"Expected argmax shape to be \\[1,1,1,1\\], but got \\[1,1,1,2\\]\"):\n        gen_nn_ops.max_pool_grad_with_argmax(\n            inp, grad, argmax, ksize=[1, 1, 1, 1], strides=[1, 1, 1, 1],\n            padding=\"VALID\")\n      # max_pool_grad_grad_with_argmax is only implemented for GPUs\n      if test.is_gpu_available():\n        with self.assertRaisesRegex(\n            errors_impl.InvalidArgumentError,\n            r\"Expected argmax shape to be \\[1,1,1,1\\], but got \\[1,1,1,2\\]\"):\n          gen_nn_ops.max_pool_grad_grad_with_argmax(\n              inp, grad, argmax, ksize=[1, 1, 1, 1], strides=[1, 1, 1, 1],\n              padding=\"VALID\")\n\n\ndef GetMaxPoolFwdTest(input_size, filter_size, strides, padding):\n\n  def Test(self):\n    # MaxPoolWithArgMax is implemented only on CUDA.\n    if not test.is_gpu_available(cuda_only=True):\n      return\n    self._CompareMaxPoolingFwd(input_size, filter_size, strides, padding)\n\n  return Test\n\n\ndef GetMaxPoolGradTest(input_size, filter_size, output_size, strides, padding):\n\n  def Test(self):\n    # MaxPoolWithArgMax is implemented only on CUDA.\n    if not test.is_gpu_available(cuda_only=True):\n      return\n    self._CompareMaxPoolingBk(input_size, output_size, filter_size, strides,\n                              padding)\n\n  return Test\n\n\ndef GetMaxPoolGradGradTest(input_size, filter_size, output_size, strides,\n                           padding):\n\n  def Test(self):\n    # MaxPoolWithArgMax is implemented only on CUDA.\n    if not test.is_gpu_available(cuda_only=True):\n      return\n    self._CompareMaxPoolingGradBk(input_size, output_size, filter_size, strides,\n                                  padding)\n\n  return Test\n\n\nif __name__ == \"__main__\":\n  for (name_, input_size_, filter_size_, output_size_, stride_,\n       padding_) in GetShrunkInceptionMaxPoolShapes():\n    setattr(PoolingTest, \"testMaxPoolFwd_\" + name_,\n            GetMaxPoolFwdTest(input_size_, filter_size_, stride_, padding_))\n    setattr(PoolingTest, \"testMaxPoolGrad_\" + name_,\n            GetMaxPoolGradTest(input_size_, filter_size_, output_size_, stride_,\n                               padding_))\n    setattr(PoolingTest, \"testMaxPoolGradGrad_\" + name_,\n            GetMaxPoolGradGradTest(input_size_, filter_size_, output_size_,\n                                   stride_, padding_))\n  test.main()"