"/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#define EIGEN_USE_THREADS\n\n#include \"tensorflow/core/kernels/searchsorted_op.h\"\n\n#include \"tensorflow/core/framework/bounds_check.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/lib/core/bits.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/platform/threadpool.h\"\n#include \"tensorflow/core/platform/types.h\"\n\nnamespace tensorflow {\ntypedef Eigen::ThreadPoolDevice CPUDevice;\ntypedef Eigen::GpuDevice GPUDevice;\n\nnamespace functor {\ntemplate <typename T, typename OutType>\nstruct UpperBoundFunctor<CPUDevice, T, OutType> {\n  static Status Compute(OpKernelContext* context,\n                        const typename TTypes<T, 1>::ConstTensor& sorted_inputs,\n                        const typename TTypes<T, 1>::ConstTensor& values,\n                        int batch_size, int num_inputs, int num_values,\n                        typename TTypes<OutType, 1>::Tensor* output) {\n    auto work_fn = [&](int64_t first, int64_t last) {\n      for (int b = 0; b < batch_size; ++b) {\n        const T* sorted_inputs_ptr = sorted_inputs.data() + b * num_inputs;\n        OutType* output_ptr = output->data() + b * num_values;\n        for (int i = first; i < last; ++i) {\n          output_ptr[i] = std::upper_bound(sorted_inputs_ptr,\n                                           sorted_inputs_ptr + num_inputs,\n                                           values(i + b * num_values)) -\n                          sorted_inputs_ptr;\n        }\n      }\n    };\n    auto worker_threads = *(context->device()->tensorflow_cpu_worker_threads());\n    thread::ThreadPool* thread_pool = worker_threads.workers;\n    const float kCostMultiplier = 1.f;  // Can be tuned to minimize overhead\n    int64_t cost_per_unit =\n        kCostMultiplier * batch_size * Log2Ceiling(num_inputs);\n    thread_pool->ParallelFor(num_values, cost_per_unit, work_fn);\n    return OkStatus();\n  }\n};\n\ntemplate <typename T, typename OutType>\nstruct LowerBoundFunctor<CPUDevice, T, OutType> {\n  static Status Compute(OpKernelContext* context,\n                        const typename TTypes<T, 1>::ConstTensor& sorted_inputs,\n                        const typename TTypes<T, 1>::ConstTensor& values,\n                        int batch_size, int num_inputs, int num_values,\n                        typename TTypes<OutType, 1>::Tensor* output) {\n    auto work_fn = [&](int64_t first, int64_t last) {\n      for (int b = 0; b < batch_size; ++b) {\n        const T* sorted_inputs_ptr = sorted_inputs.data() + b * num_inputs;\n        OutType* output_ptr = output->data() + b * num_values;\n        for (int i = first; i < last; ++i) {\n          output_ptr[i] = std::lower_bound(sorted_inputs_ptr,\n                                           sorted_inputs_ptr + num_inputs,\n                                           values(i + b * num_values)) -\n                          sorted_inputs_ptr;\n        }\n      }\n    };\n    auto worker_threads = *(context->device()->tensorflow_cpu_worker_threads());\n    thread::ThreadPool* thread_pool = worker_threads.workers;\n    const float kCostMultiplier = 1.f;  // Can be tuned to minimize overhead\n    int64_t cost_per_unit =\n        kCostMultiplier * batch_size * Log2Ceiling(num_inputs);\n    thread_pool->ParallelFor(num_values, cost_per_unit, work_fn);\n    return OkStatus();\n  }\n};\n}  // namespace functor\n\ntemplate <typename Device, typename T, typename OutType>\nclass UpperBoundOp : public OpKernel {\n public:\n  explicit UpperBoundOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& sorted_inputs_t = ctx->input(0);\n    const Tensor& values_t = ctx->input(1);\n\n    // inputs must be at least a matrix\n    OP_REQUIRES(\n        ctx, sorted_inputs_t.shape().dims() >= 2,\n        errors::InvalidArgument(\"sorted input argument must be a matrix\"));\n    // must have same batch dim_size for both\n    OP_REQUIRES(ctx, sorted_inputs_t.dim_size(0) == values_t.dim_size(0),\n                Status(error::INVALID_ARGUMENT,\n                       \"Leading dim_size of both tensors must match.\"));\n\n    // this is required because we do indexing in int32 on the GPU\n    OP_REQUIRES(ctx, values_t.NumElements() < std::numeric_limits<int>::max(),\n                Status(error::INVALID_ARGUMENT,\n                       \"values tensor size must less than INT_MAX\"));\n\n    Tensor* output_t;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, values_t.shape(), &output_t));\n\n    if (output_t->dtype() == DT_INT32) {\n      OP_REQUIRES(ctx,\n                  FastBoundsCheck(sorted_inputs_t.dim_size(1),\n                                  std::numeric_limits<int>::max()),\n                  errors::InvalidArgument(\"trailing dim_size must less than \"\n                                          \"INT_MAX for int32 output type, was \",\n                                          sorted_inputs_t.dim_size(1)));\n    }\n\n    auto output = output_t->template flat<OutType>();\n    const auto sorted_inputs = sorted_inputs_t.template flat<T>();\n    const auto values = values_t.template flat<T>();\n    OP_REQUIRES_OK(\n        ctx, functor::UpperBoundFunctor<Device, T, OutType>::Compute(\n                 ctx, sorted_inputs, values, sorted_inputs_t.dim_size(0),\n                 sorted_inputs_t.dim_size(1), values_t.dim_size(1), &output));\n  }\n};\n\ntemplate <typename Device, typename T, typename OutType>\nclass LowerBoundOp : public OpKernel {\n public:\n  explicit LowerBoundOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& sorted_inputs_t = ctx->input(0);\n    const Tensor& values_t = ctx->input(1);\n\n    // inputs must be at least a matrix\n    OP_REQUIRES(\n        ctx, sorted_inputs_t.shape().dims() >= 2,\n        errors::InvalidArgument(\"sorted input argument must be a matrix\"));\n    // must have same batch dim_size for both\n    OP_REQUIRES(ctx, sorted_inputs_t.dim_size(0) == values_t.dim_size(0),\n                Status(error::INVALID_ARGUMENT,\n                       \"Leading dim_size of both tensors must match.\"));\n\n    // this is required because we do indexing in int32 on the GPU\n    OP_REQUIRES(ctx, values_t.NumElements() < std::numeric_limits<int>::max(),\n                Status(error::INVALID_ARGUMENT,\n                       \"values tensor size must less than INT_MAX\"));\n\n    Tensor* output_t;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, values_t.shape(), &output_t));\n\n    if (output_t->dtype() == DT_INT32) {\n      OP_REQUIRES(ctx,\n                  FastBoundsCheck(sorted_inputs_t.dim_size(1),\n                                  std::numeric_limits<int>::max()),\n                  errors::InvalidArgument(\"trailing dim_size must less than \"\n                                          \"INT_MAX for int32 output type, was \",\n                                          sorted_inputs_t.dim_size(1)));\n    }\n\n    auto output = output_t->template flat<OutType>();\n    const auto sorted_inputs = sorted_inputs_t.template flat<T>();\n    const auto values = values_t.template flat<T>();\n    OP_REQUIRES_OK(\n        ctx, functor::LowerBoundFunctor<Device, T, OutType>::Compute(\n                 ctx, sorted_inputs, values, sorted_inputs_t.dim_size(0),\n                 sorted_inputs_t.dim_size(1), values_t.dim_size(1), &output));\n  }\n};\n\n#define REGISTER_KERNELS(type)                                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"UpperBound\")                      \\\n                              .Device(DEVICE_CPU)                 \\\n                              .TypeConstraint<type>(\"T\")          \\\n                              .TypeConstraint<int32>(\"out_type\"), \\\n                          UpperBoundOp<CPUDevice, type, int32>);\n\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#define REGISTER_KERNELS(type)                                      \\\n  REGISTER_KERNEL_BUILDER(Name(\"UpperBound\")                        \\\n                              .Device(DEVICE_CPU)                   \\\n                              .TypeConstraint<type>(\"T\")            \\\n                              .TypeConstraint<int64_t>(\"out_type\"), \\\n                          UpperBoundOp<CPUDevice, type, int64>);\n\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_KERNELS(type)                                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"UpperBound\")                      \\\n                              .Device(DEVICE_GPU)                 \\\n                              .TypeConstraint<type>(\"T\")          \\\n                              .TypeConstraint<int32>(\"out_type\"), \\\n                          UpperBoundOp<GPUDevice, type, int32>);\n\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#define REGISTER_KERNELS(type)                                      \\\n  REGISTER_KERNEL_BUILDER(Name(\"UpperBound\")                        \\\n                              .Device(DEVICE_GPU)                   \\\n                              .TypeConstraint<type>(\"T\")            \\\n                              .TypeConstraint<int64_t>(\"out_type\"), \\\n                          UpperBoundOp<GPUDevice, type, int64>);\n\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_KERNELS(type)                                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"LowerBound\")                      \\\n                              .Device(DEVICE_CPU)                 \\\n                              .TypeConstraint<type>(\"T\")          \\\n                              .TypeConstraint<int32>(\"out_type\"), \\\n                          LowerBoundOp<CPUDevice, type, int32>);\n\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#define REGISTER_KERNELS(type)                                      \\\n  REGISTER_KERNEL_BUILDER(Name(\"LowerBound\")                        \\\n                              .Device(DEVICE_CPU)                   \\\n                              .TypeConstraint<type>(\"T\")            \\\n                              .TypeConstraint<int64_t>(\"out_type\"), \\\n                          LowerBoundOp<CPUDevice, type, int64>);\n\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_KERNELS(type)                                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"LowerBound\")                      \\\n                              .Device(DEVICE_GPU)                 \\\n                              .TypeConstraint<type>(\"T\")          \\\n                              .TypeConstraint<int32>(\"out_type\"), \\\n                          LowerBoundOp<GPUDevice, type, int32>);\n\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#define REGISTER_KERNELS(type)                                      \\\n  REGISTER_KERNEL_BUILDER(Name(\"LowerBound\")                        \\\n                              .Device(DEVICE_GPU)                   \\\n                              .TypeConstraint<type>(\"T\")            \\\n                              .TypeConstraint<int64_t>(\"out_type\"), \\\n                          LowerBoundOp<GPUDevice, type, int64>);\n\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n}  // namespace tensorflow"