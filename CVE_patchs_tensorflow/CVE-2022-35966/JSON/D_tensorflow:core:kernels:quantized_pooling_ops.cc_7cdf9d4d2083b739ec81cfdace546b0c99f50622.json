"diff --git a/tensorflow/core/kernels/quantized_pooling_ops.cc b/tensorflow/core/kernels/quantized_pooling_ops.cc\nindex b512369b3c4..5673fb6ee00 100644\n--- a/tensorflow/core/kernels/quantized_pooling_ops.cc\n+++ b/tensorflow/core/kernels/quantized_pooling_ops.cc\n@@ -15,18 +15,18 @@ limitations under the License.\n \n // See docs in ../ops/nn_ops.cc.\n \n-#include \"tensorflow/core/framework/op_requires.h\"\n-#include \"tensorflow/core/platform/errors.h\"\n #define EIGEN_USE_THREADS\n \n #include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n #include \"tensorflow/core/framework/numeric_op.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/op_requires.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n #include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/kernels/ops_util.h\"\n #include \"tensorflow/core/kernels/pooling_ops_common.h\"\n #include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/platform/errors.h\"\n #include \"tensorflow/core/platform/logging.h\"\n #include \"tensorflow/core/util/padding.h\"\n #include \"tensorflow/core/util/tensor_format.h\"\n@@ -67,8 +67,20 @@ class QuantizedAvgPoolingOp : public OpKernel {\n       return;\n     }\n \n-    const float min_input = context->input(1).flat<float>()(0);\n-    const float max_input = context->input(2).flat<float>()(0);\n+    const Tensor& min_input_tensor = context->input(1);\n+    const Tensor& max_input_tensor = context->input(2);\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(min_input_tensor.shape()),\n+                errors::InvalidArgument(\n+                    \"min_input shape must be rank 0 but is rank \",\n+                    min_input_tensor.dims(),\n+                    \", received shape: \", min_input_tensor.shape()));\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(max_input_tensor.shape()),\n+                errors::InvalidArgument(\n+                    \"max_input shape must be rank 0 but is rank \",\n+                    max_input_tensor.dims(),\n+                    \", received shape: \", max_input_tensor.shape()));\n+    const float min_input = context->input(1).scalar<float>()();\n+    const float max_input = context->input(2).scalar<float>()();\n \n     OP_REQUIRES(context, params.depth_window == 1,\n                 errors::Unimplemented(\"Non-spatial pooling is not \"\n@@ -119,20 +131,20 @@ class QuantizedMaxPoolingOp : public MaxPoolingOp<Device, T> {\n       : MaxPoolingOp<Device, T>(context) {}\n \n   void Compute(OpKernelContext* context) override {\n-    auto min_input_tensor = context->input(1);\n-    auto max_input_tensor = context->input(2);\n-    OP_REQUIRES(\n-        context, min_input_tensor.NumElements() == 1,\n-        errors::InvalidArgument(\n-            \"min_input must be a scalar float value, got tensor with shape \",\n-            min_input_tensor.shape()));\n-    OP_REQUIRES(\n-        context, max_input_tensor.NumElements() == 1,\n-        errors::InvalidArgument(\n-            \"max_input must be a scalar float value, got tensor with shape \",\n-            max_input_tensor.shape()));\n-    const float min_input = context->input(1).flat<float>()(0);\n-    const float max_input = context->input(2).flat<float>()(0);\n+    const Tensor& min_input_tensor = context->input(1);\n+    const Tensor& max_input_tensor = context->input(2);\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(min_input_tensor.shape()),\n+                errors::InvalidArgument(\n+                    \"min_input shape must be rank 0 but is rank \",\n+                    min_input_tensor.dims(),\n+                    \", received shape: \", min_input_tensor.shape()));\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(max_input_tensor.shape()),\n+                errors::InvalidArgument(\n+                    \"max_input shape must be rank 0 but is rank \",\n+                    max_input_tensor.dims(),\n+                    \", received shape: \", max_input_tensor.shape()));\n+    const float min_input = context->input(1).scalar<float>()();\n+    const float max_input = context->input(2).scalar<float>()();\n     MaxPoolingOp<Device, T>::Compute(context);\n     Tensor* output_min = nullptr;\n     OP_REQUIRES_OK(context, context->allocate_output(1, {}, &output_min));"