"# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for ParameterizedTruncatedNormalOp.\"\"\"\n\nimport functools\nimport math\nimport timeit\n\nimport numpy as np\nfrom six.moves import range  # pylint: disable=redefined-builtin\n\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.python.client import session\nfrom tensorflow.python.eager import backprop\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import random_seed\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import stateless_random_ops as stateless\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.platform import tf_logging\n\n\ndef _get_stddev_inside_bounds_before_using_randn(gpu):\n  # The boundary where the randn sampler is used varies between CPU and GPU.\n  if gpu:\n    return 1.3\n  else:\n    return 1.7\n\n\nclass TruncatedNormalMoments(object):\n  memoized_moments = None\n  mean = None\n  stddev = None\n  minval = None\n  maxval = None\n\n  def __init__(self, mean, stddev, minval, maxval):\n    self.memoized_moments = [1.0]  # 0th moment\n    self.mean = np.double(mean)\n    self.stddev = np.double(stddev)\n    # NOTE(ringwalt): The formula doesn't handle infinite values.\n    self.minval = np.double(max(-10, minval))\n    self.maxval = np.double(min(10, maxval))\n\n  def __getitem__(self, moment):\n    \"\"\"Calculates the truncated normal moments.\n\n    Args:\n      moment: The number for the moment.\n\n    Returns:\n      The value for the given moment.\n\n    Uses the recurrence relation described in:\n        http://www.smp.uq.edu.au/people/YoniNazarathy/teaching_projects\n            /studentWork/EricOrjebin_TruncatedNormalMoments.pdf\n    \"\"\"\n    assert moment > 0\n    # The test case must ensure it can import scipy.stats before this point.\n    import scipy.stats  # pylint: disable=g-import-not-at-top\n    dist = scipy.stats.norm(loc=self.mean, scale=self.stddev)\n    for k in range(len(self.memoized_moments), moment + 1):\n      m_k_minus_2 = self.memoized_moments[k - 2] if k > 1 else np.double(0.0)\n      m_k_minus_1 = self.memoized_moments[k - 1]\n      numerator = (np.power(self.maxval, k - 1) * dist.pdf(self.maxval) -\n                   np.power(self.minval, k - 1) * dist.pdf(self.minval))\n      denominator = dist.cdf(self.maxval) - dist.cdf(self.minval)\n      m = ((k - 1) * self.stddev**2 * m_k_minus_2 + self.mean * m_k_minus_1 -\n           self.stddev * numerator / denominator)\n      assert abs(m) < 1e50  # ensure numerical accuracy\n      self.memoized_moments.append(m)\n    return self.memoized_moments[moment]\n\n\ndef calculate_moments(samples, max_moment):\n  moments = [0.0] * (max_moment + 1)\n  for k in range(len(moments)):\n    moments[k] = np.mean(samples**k, axis=0)\n  return moments\n\n\ndef z_test(real, expected, i, num_samples):\n  numerical_error = 1e-6  # per-operation error\n  moment_mean = expected[i]\n  moment_squared = expected[2 * i]\n  moment_var = moment_squared - moment_mean * moment_mean\n\n  error_per_moment = i * numerical_error\n  total_variance = moment_var / float(num_samples) + error_per_moment\n  return abs((real[i] - moment_mean) / math.sqrt(total_variance))\n\n\nclass ParameterizedTruncatedNormalTest(test.TestCase):\n  z_limit = 6.0\n\n  # Stop at moment 10 to avoid numerical errors in the theoretical moments.\n  max_moment = 10\n\n  def validateMoments(self,\n                      shape,\n                      mean,\n                      stddev,\n                      minval,\n                      maxval,\n                      use_stateless=False,\n                      seed=1618):\n    try:\n      # TruncatedNormalMoments requires scipy.stats.\n      # Give up early if we are unable to import it.\n      random_seed.set_random_seed(seed)\n      with self.cached_session():\n        if use_stateless:\n          # Generate a seed that stateless ops can use.\n          new_seed = random_ops.random_uniform([2],\n                                               seed=seed,\n                                               minval=0,\n                                               maxval=(2**31 - 1),\n                                               dtype=np.int32)\n          samples = stateless.stateless_parameterized_truncated_normal(\n              shape, new_seed, mean, stddev, minval, maxval).eval()\n        else:\n          samples = random_ops.parameterized_truncated_normal(\n              shape, mean, stddev, minval, maxval).eval()\n        assert (~np.isnan(samples)).all()\n      moments = calculate_moments(samples, self.max_moment)\n      expected_moments = TruncatedNormalMoments(mean, stddev, minval, maxval)\n      num_samples = functools.reduce(lambda x, y: x * y, shape, 1)\n      for i in range(1, len(moments)):\n        self.assertLess(\n            z_test(moments, expected_moments, i, num_samples), self.z_limit)\n    except ImportError as e:\n      tf_logging.warn(\"Cannot test truncated normal op: %s\" % str(e))\n\n  def validateKolmogorovSmirnov(self,\n                                shape,\n                                mean,\n                                stddev,\n                                minval,\n                                maxval,\n                                use_stateless=False,\n                                seed=1618):\n    try:\n      import scipy.stats  # pylint: disable=g-import-not-at-top\n      random_seed.set_random_seed(seed)\n      with self.cached_session():\n        if use_stateless:\n          new_seed = random_ops.random_uniform([2],\n                                               seed=seed,\n                                               minval=0,\n                                               maxval=(2**31 - 1),\n                                               dtype=np.int32)\n          samples = stateless.stateless_parameterized_truncated_normal(\n              shape, new_seed, mean, stddev, minval, maxval).eval()\n        else:\n          samples = random_ops.parameterized_truncated_normal(\n              shape, mean, stddev, minval, maxval).eval()\n\n      assert (~np.isnan(samples)).all()\n      minval = max(mean - stddev * 10, minval)\n      maxval = min(mean + stddev * 10, maxval)\n      dist = scipy.stats.norm(loc=mean, scale=stddev)\n      cdf_min = dist.cdf(minval)\n      cdf_max = dist.cdf(maxval)\n\n      def truncated_cdf(x):\n        return np.clip((dist.cdf(x) - cdf_min) / (cdf_max - cdf_min), 0.0, 1.0)\n\n      pvalue = scipy.stats.kstest(samples, truncated_cdf)[1]\n      self.assertGreater(pvalue, 1e-10)\n    except ImportError as e:\n      tf_logging.warn(\"Cannot test truncated normal op: %s\" % str(e))\n\n  @test_util.run_deprecated_v1\n  def testDefaults(self):\n    self.validateMoments([int(1e5)], 0.0, 1.0, -2.0, 2.0)\n    self.validateMoments([int(1e5)], 0.0, 1.0, -2.0, 2.0, use_stateless=True)\n\n  @test_util.run_deprecated_v1\n  def testShifted(self):\n    self.validateMoments([int(1e5)], -1.0, 1.0, -2.0, 2.0)\n    self.validateMoments([int(1e5)], -1.0, 1.0, -2.0, 2.0, use_stateless=True)\n\n  @test_util.run_deprecated_v1\n  def testRightTail(self):\n    self.validateMoments([int(1e5)], 0.0, 1.0, 4.0, np.infty)\n    self.validateMoments([int(1e5)],\n                         0.0,\n                         1.0,\n                         4.0,\n                         np.infty,\n                         use_stateless=True)\n\n  @test_util.run_deprecated_v1\n  def testLeftTail(self):\n    self.validateMoments([int(1e5)], 0.0, 1.0, -np.infty, -4.0)\n    self.validateMoments([int(1e5)],\n                         0.0,\n                         1.0,\n                         -np.infty,\n                         -4.0,\n                         use_stateless=True)\n\n  @test_util.run_deprecated_v1\n  def testLeftTailTwoSidedBounds(self):\n    self.validateMoments([int(1e5)], 0.0, 1.0, -6.0, -3.0)\n    self.validateMoments([int(1e5)], 0.0, 1.0, -6.0, -3.0, use_stateless=True)\n\n  @test_util.run_deprecated_v1\n  @test_util.disable_xla(\"Low probability region\")\n  def testTwoSidedLeftTailShifted(self):\n    self.validateKolmogorovSmirnov([int(1e5)], 6.0, 1.0, -1.0, 1.0)\n    self.validateKolmogorovSmirnov([int(1e5)],\n                                   6.0,\n                                   1.0,\n                                   -1.0,\n                                   1.0,\n                                   use_stateless=True)\n\n  @test_util.run_deprecated_v1\n  @test_util.disable_xla(\"Low probability region\")\n  def testRightTailShifted(self):\n    self.validateMoments([int(1e5)], -5.0, 1.0, 2.0, np.infty)\n    self.validateMoments([int(1e5)],\n                         -5.0,\n                         1.0,\n                         2.0,\n                         np.infty,\n                         use_stateless=True)\n\n  # Take the normal distribution around the mean, but truncating the left tail\n  # far from the mean.\n  @test_util.run_deprecated_v1\n  def testTruncateOnLeft_entireTailOnRight(self):\n    self.validateKolmogorovSmirnov([int(1e5)], 10.0, 1.0, 4.0, np.infty)\n    self.validateKolmogorovSmirnov([int(1e5)],\n                                   10.0,\n                                   1.0,\n                                   4.0,\n                                   np.infty,\n                                   use_stateless=True)\n\n  # Take the normal distribution around the mean, but truncating the right tail.\n  @test_util.run_deprecated_v1\n  def testTruncateOnRight_entireTailOnLeft(self):\n    self.validateKolmogorovSmirnov([int(1e5)], -8, 1.0, -np.infty, -4.0)\n    self.validateKolmogorovSmirnov([int(1e5)],\n                                   -8.,\n                                   1.0,\n                                   -np.infty,\n                                   -4.0,\n                                   use_stateless=True)\n\n  @test_util.run_deprecated_v1\n  def testSmallStddev(self):\n    self.validateKolmogorovSmirnov([int(1e5)], 0.0, 0.1, 0.05, 0.10)\n    self.validateKolmogorovSmirnov([int(1e5)],\n                                   0.0,\n                                   0.1,\n                                   0.05,\n                                   0.10,\n                                   use_stateless=True)\n\n  @test_util.run_deprecated_v1\n  def testSamplingWithSmallStdDevFarFromBound(self):\n    sample_op = random_ops.parameterized_truncated_normal(\n        shape=(int(1e5),), means=0.8, stddevs=0.05, minvals=-1., maxvals=1.)\n    new_seed = random_ops.random_uniform([2],\n                                         seed=1234,\n                                         minval=0,\n                                         maxval=(2**31 - 1),\n                                         dtype=np.int32)\n    sample_op_stateless = stateless.stateless_parameterized_truncated_normal(\n        shape=(int(1e5),),\n        seed=new_seed,\n        means=0.8,\n        stddevs=0.05,\n        minvals=-1.,\n        maxvals=1.)\n\n    with self.session() as sess:\n      samples, samples_stateless = sess.run([sample_op, sample_op_stateless])\n      # 0. is more than 16 standard deviations from the mean, and\n      # should have a likelihood < 1e-57.\n      assert (~np.isnan(samples)).all()\n      assert (~np.isnan(samples_stateless)).all()\n      self.assertAllGreater(samples, 0.)\n      self.assertAllGreater(samples_stateless, 0.)\n\n  def testStatelessParameterizedTruncatedNormalHasGrads(self):\n    mean = variables.Variable(0.01)\n    stddev = variables.Variable(1.)\n    minval = variables.Variable(-1.)\n    maxval = variables.Variable(1.)\n\n    with self.cached_session() as sess:\n      with backprop.GradientTape(persistent=True) as tape:\n        samples = stateless.stateless_parameterized_truncated_normal(\n            [1], [1, 2], mean, stddev, minval, maxval)\n\n      sess.run(variables.variables_initializer([mean, stddev, minval, maxval]))\n      [mean_grad, std_grad], mean_actual_grad, std_actual_grad = sess.run([\n          tape.gradient(samples, [mean, stddev]),\n          array_ops.ones_like(mean),\n          (samples - mean) / stddev])\n      self.assertAllClose(mean_grad, mean_actual_grad)\n      self.assertAllClose(std_grad, std_actual_grad[0])\n\n      try:\n        import scipy.stats  # pylint:disable=g-import-not-at-top\n        truncnorm = scipy.stats.truncnorm(a=-1., b=1., loc=0., scale=1.)\n        samples_np, [minval_grad, maxval_grad] = sess.run([\n            samples, tape.gradient(samples, [minval, maxval])])\n\n        sample_cdf = truncnorm.cdf(samples_np)\n        # These come from the implicit reparameterization trick.\n        scipy_maxval_grad = np.exp(\n            0.5 * (samples_np ** 2 - ((1. - 0.01) / 1.) ** 2) +\n            np.log(sample_cdf))\n\n        scipy_minval_grad = np.exp(\n            0.5 * (samples_np ** 2 - ((-1. - 0.01) / 1.) ** 2) +\n            np.log1p(-sample_cdf))\n\n        self.assertAllClose(minval_grad, scipy_minval_grad[0], rtol=1e-2)\n        self.assertAllClose(maxval_grad, scipy_maxval_grad[0], rtol=1e-2)\n\n      except ImportError as e:\n        tf_logging.warn(\"Cannot test truncated normal op: %s\" % str(e))\n\n  @test_util.run_deprecated_v1\n  def testSamplingAtRandnSwitchover(self):\n    # The randn sampler is used as the bounds are moved farther from the mean,\n    # and the probability of accepting a sample increases the farther the\n    # bounds are from the mean.\n    # This test asserts that at the point of switchover, both samplers are\n    # working (not raising an error or returning nan) and returning the\n    # expected moments.\n    use_gpu = test.is_gpu_available()\n    stddev_inside_bounds_before_using_randn = (\n        _get_stddev_inside_bounds_before_using_randn(use_gpu))\n\n    epsilon = 0.001\n    self.validateMoments(\n        shape=[int(1e6)],\n        mean=0.,\n        stddev=1.0,\n        minval=-epsilon,\n        maxval=stddev_inside_bounds_before_using_randn - epsilon)\n    self.validateMoments(\n        shape=[int(1e6)],\n        mean=0.,\n        stddev=1.0,\n        minval=-epsilon,\n        maxval=stddev_inside_bounds_before_using_randn + epsilon)\n\n    self.validateMoments(\n        shape=[int(1e6)],\n        mean=0.,\n        stddev=1.0,\n        minval=-epsilon,\n        maxval=stddev_inside_bounds_before_using_randn - epsilon,\n        use_stateless=True)\n    self.validateMoments(\n        shape=[int(1e6)],\n        mean=0.,\n        stddev=1.0,\n        minval=-epsilon,\n        maxval=stddev_inside_bounds_before_using_randn + epsilon,\n        use_stateless=True)\n\n\n# Benchmarking code\ndef parameterized_vs_naive(shape, num_iters, use_gpu=False):\n  np.random.seed(1618)  # Make it reproducible.\n\n  # No CSE/CF.\n  optimizer_options = config_pb2.OptimizerOptions(\n      opt_level=config_pb2.OptimizerOptions.L0)\n  config = config_pb2.ConfigProto(graph_options=config_pb2.GraphOptions(\n      optimizer_options=optimizer_options))\n\n  with session.Session(config=config) as sess:\n    with ops.device(\"/cpu:0\" if not use_gpu else None):\n      param_op = control_flow_ops.group(\n          random_ops.parameterized_truncated_normal(shape))\n      naive_op = control_flow_ops.group(random_ops.truncated_normal(shape))\n\n    # Burn-in to avoid session setup costs in the timing.\n    sess.run(param_op)\n    sess.run(param_op)\n    param_dt = timeit.timeit(lambda: sess.run(param_op), number=num_iters)\n    sess.run(naive_op)\n    sess.run(naive_op)\n    naive_dt = timeit.timeit(lambda: sess.run(naive_op), number=num_iters)\n    return param_dt, naive_dt\n\n\ndef randn_sampler_switchover(shape, num_iters, use_gpu=False):\n  # Benchmark by constructing samplers on the threshold of using the randn\n  # rejection sampling and check that this threshold is set correctly by\n  # benchmarking with bounds just above and below this threshold.\n  # The uniform and randn samplers should have about the same performance\n  # at this point.\n\n  stddev_inside_bounds_before_using_randn = (\n      _get_stddev_inside_bounds_before_using_randn(use_gpu))\n\n  epsilon = 0.001\n\n  np.random.seed(1618)  # Make it reproducible.\n\n  # No CSE/CF.\n  optimizer_options = config_pb2.OptimizerOptions(\n      opt_level=config_pb2.OptimizerOptions.L0)\n  config = config_pb2.ConfigProto(\n      graph_options=config_pb2.GraphOptions(\n          optimizer_options=optimizer_options))\n\n  with session.Session(config=config) as sess:\n    with ops.device(\"/cpu:0\" if not use_gpu else \"/gpu:0\"):\n      uniform_sampler_op = control_flow_ops.group(\n          random_ops.parameterized_truncated_normal(\n              shape,\n              means=0.,\n              stddevs=1.0,\n              minvals=-stddev_inside_bounds_before_using_randn + epsilon,\n              maxvals=0.01))\n      randn_sampler_op = control_flow_ops.group(\n          random_ops.parameterized_truncated_normal(\n              shape,\n              means=0.,\n              stddevs=1.0,\n              minvals=-stddev_inside_bounds_before_using_randn - epsilon,\n              maxvals=0.01))\n\n    # Burn-in to avoid session setup costs in the timing.\n    sess.run(uniform_sampler_op)\n    sess.run(uniform_sampler_op)\n    uniform_dt = timeit.timeit(\n        lambda: sess.run(uniform_sampler_op), number=num_iters)\n\n    sess.run(randn_sampler_op)\n    sess.run(randn_sampler_op)\n    randn_dt = timeit.timeit(\n        lambda: sess.run(randn_sampler_op), number=num_iters)\n\n    return randn_dt, uniform_dt\n\n\nclass TruncatedNormalBenchmark(test.Benchmark):\n\n  def benchmarkParameterizedOpVsNaiveOpCpu(self):\n    self._benchmarkParameterizedOpVsNaiveOp(False)\n\n  def benchmarkParameterizedOpVsNaiveOpGpu(self):\n    self._benchmarkParameterizedOpVsNaiveOp(True)\n\n  def _benchmarkParameterizedOpVsNaiveOp(self, use_gpu):\n    num_iters = 50\n    print((\"Composition of new ParameterizedTruncatedNormalOp vs. \"\n           \"naive TruncatedNormalOp [%d iters]\") % num_iters)\n    print(\"Shape\\tsec(parameterized)\\tsec(naive)\\tspeedup\")\n\n    for shape in [[10000, 100], [1000, 1000], [1000000], [100, 100, 100],\n                  [20, 20, 20, 20]]:\n      p_dt, n_dt = parameterized_vs_naive(shape, num_iters, use_gpu)\n      print(\"%s\\t%.3f\\t%.3f\\t%.2f\" % (shape, p_dt, n_dt, p_dt / n_dt))\n\n      shape_str = \"-\".join(map(str, shape))\n      self.report_benchmark(\n          name=\"parameterized_shape\" + shape_str,\n          iters=num_iters,\n          wall_time=p_dt)\n      self.report_benchmark(\n          name=\"naive_shape\" + shape_str, iters=num_iters, wall_time=n_dt)\n\n  def benchmarkRandnSamplerCPU(self):\n    self._benchmarkRandnSampler(False)\n\n  def benchmarkRandnSamplerGPU(self):\n    self._benchmarkRandnSampler(True)\n\n  def _benchmarkRandnSampler(self, use_gpu):\n    num_iters = 100\n    shape = [int(1e6)]\n    randn_dt, uniform_dt = randn_sampler_switchover(shape, num_iters, use_gpu)\n\n    print((\"Randn Sampler vs uniform samplers [%d iters]\\t%.4f\\t%.4f\") %\n          (num_iters, randn_dt, uniform_dt))\n\n    gpu_str = \"_gpu\" if use_gpu else \"_cpu\"\n    self.report_benchmark(\n        name=\"randn_sampler\" + gpu_str, iters=num_iters, wall_time=randn_dt)\n    self.report_benchmark(\n        name=\"uniform_sampler\" + gpu_str, iters=num_iters, wall_time=uniform_dt)\n\n\nif __name__ == \"__main__\":\n  test.main()"