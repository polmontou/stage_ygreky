"# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for local response normalization.\"\"\"\n\nimport copy\n\nimport numpy as np\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.ops import nn\nfrom tensorflow.python.ops import random_ops\nimport tensorflow.python.ops.nn_grad  # pylint: disable=unused-import\nfrom tensorflow.python.platform import test\n\n\nclass LRNOpTest(test.TestCase):\n\n  def _LRN(self, input_image, lrn_depth_radius=5, bias=1.0, alpha=1.0,\n           beta=0.5):\n    \"\"\"Compute expected result.\"\"\"\n    output = copy.deepcopy(input_image)\n    batch_size = input_image.shape[0]\n    rows = input_image.shape[1]\n    cols = input_image.shape[2]\n    depth = input_image.shape[3]\n    for b in range(batch_size):\n      for r in range(rows):\n        for c in range(cols):\n          for d in range(depth):\n            begin = max(0, d - lrn_depth_radius)\n            end = min(depth, d + lrn_depth_radius + 1)\n            patch = input_image[b, r, c, begin:end]\n            output[b, r, c, d] /= (\n                np.power(bias + alpha * np.sum(patch * patch), beta))\n    return output\n\n  def _RunAndVerify(self, dtype):\n    with self.cached_session():\n      # random shape\n      shape = np.random.randint(1, 16, size=4)\n      # Make depth at least 2 to make it meaningful\n      shape[3] += 1\n      p = array_ops.placeholder(dtype, shape=shape)\n      # random depth_radius, bias, alpha, beta. cuDNN requires depth_radius to\n      # be in [1, 7].\n      lrn_depth_radius = np.random.randint(1, min(8, shape[3]))\n\n      bias = 1.0 + np.random.rand()\n      alpha = 2.0 * np.random.rand()\n      # cuDNN requires beta >= 0.01.\n      beta = 0.01 + 2.0 * np.random.rand()\n      lrn_t = nn.local_response_normalization(\n          p,\n          name=\"lrn\",\n          depth_radius=lrn_depth_radius,\n          bias=bias,\n          alpha=alpha,\n          beta=beta)\n      params = {p: np.random.rand(*shape).astype(\"f\")}\n      result = lrn_t.eval(feed_dict=params)\n    expected = self._LRN(\n        params[p],\n        lrn_depth_radius=lrn_depth_radius,\n        bias=bias,\n        alpha=alpha,\n        beta=beta)\n    err = np.amax(np.abs(result - expected))\n    print(\"LRN error for bias \", bias, \"alpha \", alpha, \" beta \", beta, \" is \",\n          err)\n    if dtype == dtypes.float32:\n      self.assertTrue(err < 1e-4)\n    else:\n      self.assertTrue(err < 1e-2)\n    self.assertShapeEqual(expected, lrn_t)\n\n  @test_util.run_deprecated_v1\n  def testCompute(self):\n    for _ in range(2):\n      self._RunAndVerify(dtypes.float32)\n      # Enable when LRN supports tf.float16 on GPU.\n      if not test.is_gpu_available():\n        self._RunAndVerify(dtypes.float16)\n\n  @test_util.run_deprecated_v1\n  def testGradientsZeroInput(self):\n    with self.session():\n      shape = [4, 4, 4, 4]\n      p = array_ops.placeholder(dtypes.float32, shape=shape)\n      inp_array = np.zeros(shape).astype(\"f\")\n      lrn_op = nn.local_response_normalization(p, 2, 1.0, 0.0, 1.0, name=\"lrn\")\n      grad = gradients_impl.gradients([lrn_op], [p])[0]\n      params = {p: inp_array}\n      r = grad.eval(feed_dict=params)\n    expected = np.ones(shape).astype(\"f\")\n    self.assertAllClose(r, expected)\n    self.assertShapeEqual(expected, grad)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testIncompatibleInputAndOutputImageShapes(self):\n    depth_radius = 1\n    bias = 1.59018219\n    alpha = 0.117728651\n    beta = 0.404427052\n    input_grads = random_ops.random_uniform(\n        shape=[4, 4, 4, 4],\n        minval=-10000,\n        maxval=10000,\n        dtype=dtypes.float32,\n        seed=-2033)\n    input_image = random_ops.random_uniform(\n        shape=[4, 4, 4, 4],\n        minval=-10000,\n        maxval=10000,\n        dtype=dtypes.float32,\n        seed=-2033)\n    invalid_output_image = random_ops.random_uniform(\n        shape=[4, 4, 4, 4, 4, 4],\n        minval=-10000,\n        maxval=10000,\n        dtype=dtypes.float32,\n        seed=-2033)\n    with self.assertRaises((ValueError, errors_impl.InvalidArgumentError)):\n      self.evaluate(\n          nn.lrn_grad(\n              input_grads=input_grads,\n              input_image=input_image,\n              output_image=invalid_output_image,\n              depth_radius=depth_radius,\n              bias=bias,\n              alpha=alpha,\n              beta=beta))\n\n  def _RunAndVerifyGradients(self, dtype):\n    with self.cached_session():\n      # random shape\n      shape = np.random.randint(1, 5, size=4)\n      # Make depth at least 2 to make it meaningful\n      shape[3] += 1\n      # random depth_radius, bias, alpha, beta. cuDNN requires depth_radius to\n      # be in [1, 7].\n      lrn_depth_radius = np.random.randint(1, min(8, shape[3]))\n      bias = 1.0 + np.random.rand()\n      alpha = 1.0 * np.random.rand()\n      # cuDNN requires beta >= 0.01.\n      beta = 0.01 + 1.0 * np.random.rand()\n      if dtype == dtypes.float32:\n        inp_array = np.random.rand(*shape).astype(np.float32)\n      else:\n        inp_array = np.random.rand(*shape).astype(np.float16)\n\n      inp = constant_op.constant(\n          list(inp_array.ravel(order=\"C\")), shape=shape, dtype=dtype)\n      lrn_op = nn.local_response_normalization(\n          inp,\n          name=\"lrn\",\n          depth_radius=lrn_depth_radius,\n          bias=bias,\n          alpha=alpha,\n          beta=beta)\n      err = gradient_checker.compute_gradient_error(inp, shape, lrn_op, shape)\n    print(\"LRN Gradient error for bias \", bias, \"alpha \", alpha, \" beta \", beta,\n          \" is \", err)\n    if dtype == dtypes.float32:\n      self.assertLess(err, 1e-4)\n    else:\n      self.assertLess(err, 1.0)\n\n  @test_util.run_deprecated_v1\n  def testGradients(self):\n    for _ in range(2):\n      self._RunAndVerifyGradients(dtypes.float32)\n      # Enable when LRN supports tf.float16 on GPU.\n      if not test.is_gpu_available():\n        self._RunAndVerifyGradients(dtypes.float16)\n\n\nif __name__ == \"__main__\":\n  test.main()"