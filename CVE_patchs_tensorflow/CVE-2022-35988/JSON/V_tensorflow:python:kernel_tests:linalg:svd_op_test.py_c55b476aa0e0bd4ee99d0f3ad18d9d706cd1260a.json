"# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for tensorflow.ops.math_ops.matrix_inverse.\"\"\"\n\nimport numpy as np\n\nfrom tensorflow.python.client import session\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import gradient_checker_v2\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.ops import linalg_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import stateless_random_ops\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.platform import benchmark\nfrom tensorflow.python.platform import test\n\n\ndef _AddTest(test_class, op_name, testcase_name, fn):\n  test_name = \"_\".join([\"test\", op_name, testcase_name])\n  if hasattr(test_class, test_name):\n    raise RuntimeError(\"Test %s defined more than once\" % test_name)\n  setattr(test_class, test_name, fn)\n\n\nclass SvdOpTest(test.TestCase):\n\n  @test_util.run_in_graph_and_eager_modes(use_gpu=True)\n  def testWrongDimensions(self):\n    # The input to svd should be a tensor of at least rank 2.\n    scalar = constant_op.constant(1.)\n    with self.assertRaisesRegex((ValueError, errors_impl.InvalidArgumentError),\n                                \"rank.* 2.*0\"):\n      linalg_ops.svd(scalar)\n    vector = constant_op.constant([1., 2.])\n    with self.assertRaisesRegex((ValueError, errors_impl.InvalidArgumentError),\n                                \"rank.* 2.*1\"):\n      linalg_ops.svd(vector)\n\n  @test_util.run_in_graph_and_eager_modes(use_gpu=True)\n  def testThrowDeterminismError(self):\n    shape = [6, 5]\n    seed = [42, 24]\n    matrix1 = stateless_random_ops.stateless_random_normal(shape, seed)\n    with test_util.deterministic_ops():\n      if test_util.is_gpu_available(cuda_only=True):\n        with self.assertRaisesRegex(\n            errors_impl.UnimplementedError, \"Determinism is not yet supported \"\n            \"for Svd.\"):\n          self.evaluate(linalg_ops.svd(matrix1))\n\n  @test_util.run_in_graph_and_eager_modes(use_gpu=True)\n  def DISABLED_testBadInputs(self):\n    # TODO(b/185822300): re-enable after the bug is fixed in CUDA-11.x\n    # The input to svd should be a tensor of at least rank 2.\n    for bad_val in [np.nan, np.inf]:\n      matrix = np.array([[1, bad_val], [0, 1]])\n      s, u, v = linalg_ops.svd(matrix, compute_uv=True)\n      s, u, v = self.evaluate([s, u, v])\n      for i in range(2):\n        self.assertTrue(np.isnan(s[i]))\n        for j in range(2):\n          self.assertTrue(np.isnan(u[i, j]))\n          self.assertTrue(np.isnan(v[i, j]))\n\n  @test_util.run_in_graph_and_eager_modes(use_gpu=True)\n  def testExecuteMultipleWithoutError(self):\n    all_ops = []\n    shape = [6, 5]\n    seed = [42, 24]\n    for compute_uv_ in True, False:\n      for full_matrices_ in True, False:\n        matrix1 = stateless_random_ops.stateless_random_normal(shape, seed)\n        matrix2 = stateless_random_ops.stateless_random_normal(shape, seed)\n        self.assertAllEqual(matrix1, matrix2)\n        if compute_uv_:\n          s1, u1, v1 = linalg_ops.svd(\n              matrix1, compute_uv=compute_uv_, full_matrices=full_matrices_)\n          s2, u2, v2 = linalg_ops.svd(\n              matrix2, compute_uv=compute_uv_, full_matrices=full_matrices_)\n          all_ops += [s1, s2, u1, u2, v1, v2]\n        else:\n          s1 = linalg_ops.svd(\n              matrix1, compute_uv=compute_uv_, full_matrices=full_matrices_)\n          s2 = linalg_ops.svd(\n              matrix2, compute_uv=compute_uv_, full_matrices=full_matrices_)\n          all_ops += [s1, s2]\n    val = self.evaluate(all_ops)\n    for i in range(0, len(val), 2):\n      self.assertAllEqual(val[i], val[i + 1])\n\n\ndef _GetSvdOpTest(dtype_, shape_, use_static_shape_, compute_uv_,\n                  full_matrices_):\n\n  def CompareSingularValues(self, x, y, tol):\n    atol = (x[0] + y[0]) * tol if len(x) else tol\n    self.assertAllClose(x, y, atol=atol)\n\n  def CompareSingularVectors(self, x, y, rank, tol):\n    # We only compare the first 'rank' singular vectors since the\n    # remainder form an arbitrary orthonormal basis for the\n    # (row- or column-) null space, whose exact value depends on\n    # implementation details. Notice that since we check that the\n    # matrices of singular vectors are unitary elsewhere, we do\n    # implicitly test that the trailing vectors of x and y span the\n    # same space.\n    x = x[..., 0:rank]\n    y = y[..., 0:rank]\n    # Singular vectors are only unique up to sign (complex phase factor for\n    # complex matrices), so we normalize the sign first.\n    sum_of_ratios = np.sum(np.divide(y, x), -2, keepdims=True)\n    phases = np.divide(sum_of_ratios, np.abs(sum_of_ratios))\n    x *= phases\n    self.assertAllClose(x, y, atol=2 * tol)\n\n  def CheckApproximation(self, a, u, s, v, full_matrices_, tol):\n    # Tests that a ~= u*diag(s)*transpose(v).\n    batch_shape = a.shape[:-2]\n    m = a.shape[-2]\n    n = a.shape[-1]\n    diag_s = math_ops.cast(array_ops.matrix_diag(s), dtype=dtype_)\n    if full_matrices_:\n      if m > n:\n        zeros = array_ops.zeros(batch_shape + (m - n, n), dtype=dtype_)\n        diag_s = array_ops.concat([diag_s, zeros], a.ndim - 2)\n      elif n > m:\n        zeros = array_ops.zeros(batch_shape + (m, n - m), dtype=dtype_)\n        diag_s = array_ops.concat([diag_s, zeros], a.ndim - 1)\n    a_recon = math_ops.matmul(u, diag_s)\n    a_recon = math_ops.matmul(a_recon, v, adjoint_b=True)\n    self.assertAllClose(a_recon, a, rtol=tol, atol=tol)\n\n  def CheckUnitary(self, x, tol):\n    # Tests that x[...,:,:]^H * x[...,:,:] is close to the identity.\n    xx = math_ops.matmul(x, x, adjoint_a=True)\n    identity = array_ops.matrix_band_part(array_ops.ones_like(xx), 0, 0)\n    self.assertAllClose(identity, xx, atol=tol)\n\n  @test_util.run_in_graph_and_eager_modes(use_gpu=True)\n  def Test(self):\n    if not use_static_shape_ and context.executing_eagerly():\n      return\n    is_complex = dtype_ in (np.complex64, np.complex128)\n    is_single = dtype_ in (np.float32, np.complex64)\n    tol = 3e-4 if is_single else 1e-12\n    if test.is_gpu_available():\n      # The gpu version returns results that are much less accurate.\n      tol *= 200\n    np.random.seed(42)\n    x_np = np.random.uniform(\n        low=-1.0, high=1.0, size=np.prod(shape_)).reshape(shape_).astype(dtype_)\n    if is_complex:\n      x_np += 1j * np.random.uniform(\n          low=-1.0, high=1.0,\n          size=np.prod(shape_)).reshape(shape_).astype(dtype_)\n\n    if use_static_shape_:\n      x_tf = constant_op.constant(x_np)\n    else:\n      x_tf = array_ops.placeholder(dtype_)\n\n    if compute_uv_:\n      s_tf, u_tf, v_tf = linalg_ops.svd(\n          x_tf, compute_uv=compute_uv_, full_matrices=full_matrices_)\n      if use_static_shape_:\n        s_tf_val, u_tf_val, v_tf_val = self.evaluate([s_tf, u_tf, v_tf])\n      else:\n        with self.session() as sess:\n          s_tf_val, u_tf_val, v_tf_val = sess.run(\n              [s_tf, u_tf, v_tf], feed_dict={x_tf: x_np})\n    else:\n      s_tf = linalg_ops.svd(\n          x_tf, compute_uv=compute_uv_, full_matrices=full_matrices_)\n      if use_static_shape_:\n        s_tf_val = self.evaluate(s_tf)\n      else:\n        with self.session() as sess:\n          s_tf_val = sess.run(s_tf, feed_dict={x_tf: x_np})\n\n    if compute_uv_:\n      u_np, s_np, v_np = np.linalg.svd(\n          x_np, compute_uv=compute_uv_, full_matrices=full_matrices_)\n    else:\n      s_np = np.linalg.svd(\n          x_np, compute_uv=compute_uv_, full_matrices=full_matrices_)\n    # We explicitly avoid the situation where numpy eliminates a first\n    # dimension that is equal to one.\n    s_np = np.reshape(s_np, s_tf_val.shape)\n\n    CompareSingularValues(self, s_np, s_tf_val, tol)\n    if compute_uv_:\n      CompareSingularVectors(self, u_np, u_tf_val, min(shape_[-2:]), tol)\n      CompareSingularVectors(self, np.conj(np.swapaxes(v_np, -2, -1)), v_tf_val,\n                             min(shape_[-2:]), tol)\n      CheckApproximation(self, x_np, u_tf_val, s_tf_val, v_tf_val,\n                         full_matrices_, tol)\n      CheckUnitary(self, u_tf_val, tol)\n      CheckUnitary(self, v_tf_val, tol)\n\n  return Test\n\n\nclass SvdGradOpTest(test.TestCase):\n  pass  # Filled in below\n\n\ndef _NormalizingSvd(tf_a, full_matrices_):\n  tf_s, tf_u, tf_v = linalg_ops.svd(\n      tf_a, compute_uv=True, full_matrices=full_matrices_)\n  # Singular vectors are only unique up to an arbitrary phase. We normalize\n  # the vectors such that the first component of u (if m >=n) or v (if n > m)\n  # have phase 0.\n  m = tf_a.shape[-2]\n  n = tf_a.shape[-1]\n  if m >= n:\n    top_rows = tf_u[..., 0:1, :]\n  else:\n    top_rows = tf_v[..., 0:1, :]\n  if tf_u.dtype.is_complex:\n    angle = -math_ops.angle(top_rows)\n    phase = math_ops.complex(math_ops.cos(angle), math_ops.sin(angle))\n  else:\n    phase = math_ops.sign(top_rows)\n  tf_u *= phase[..., :m]\n  tf_v *= phase[..., :n]\n  return tf_s, tf_u, tf_v\n\n\ndef _GetSvdGradOpTest(dtype_, shape_, compute_uv_, full_matrices_):\n\n  @test_util.run_in_graph_and_eager_modes(use_gpu=True)\n  def Test(self):\n\n    def RandomInput():\n      np.random.seed(42)\n      a = np.random.uniform(low=-1.0, high=1.0, size=shape_).astype(dtype_)\n      if dtype_ in [np.complex64, np.complex128]:\n        a += 1j * np.random.uniform(\n            low=-1.0, high=1.0, size=shape_).astype(dtype_)\n      return a\n\n    # Optimal stepsize for central difference is O(epsilon^{1/3}).\n    # See Equation (21) in:\n    # http://www.karenkopecky.net/Teaching/eco613614/Notes_NumericalDifferentiation.pdf\n    # TODO(rmlarsen): Move step size control to gradient checker.\n    epsilon = np.finfo(dtype_).eps\n    delta = 0.25 * epsilon**(1.0 / 3.0)\n    if dtype_ in [np.float32, np.complex64]:\n      tol = 3e-2\n    else:\n      tol = 1e-6\n    if compute_uv_:\n      funcs = [\n          lambda a: _NormalizingSvd(a, full_matrices_)[0],\n          lambda a: _NormalizingSvd(a, full_matrices_)[1],\n          lambda a: _NormalizingSvd(a, full_matrices_)[2]\n      ]\n    else:\n      funcs = [lambda a: linalg_ops.svd(a, compute_uv=False)]\n\n    for f in funcs:\n      theoretical, numerical = gradient_checker_v2.compute_gradient(\n          f, [RandomInput()], delta=delta)\n      self.assertAllClose(theoretical, numerical, atol=tol, rtol=tol)\n\n  return Test\n\n\nclass SvdGradGradOpTest(test.TestCase):\n  pass  # Filled in below\n\n\ndef _GetSvdGradGradOpTest(dtype_, shape_, compute_uv_, full_matrices_):\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def Test(self):\n    np.random.seed(42)\n    a = np.random.uniform(low=-1.0, high=1.0, size=shape_).astype(dtype_)\n    if dtype_ in [np.complex64, np.complex128]:\n      a += 1j * np.random.uniform(\n          low=-1.0, high=1.0, size=shape_).astype(dtype_)\n    # Optimal stepsize for central difference is O(epsilon^{1/3}).\n    # See Equation (21) in:\n    # http://www.karenkopecky.net/Teaching/eco613614/Notes_NumericalDifferentiation.pdf\n    # TODO(rmlarsen): Move step size control to gradient checker.\n    epsilon = np.finfo(dtype_).eps\n    delta = 0.1 * epsilon**(1.0 / 3.0)\n    tol = 1e-5\n    with self.session():\n      tf_a = constant_op.constant(a)\n      if compute_uv_:\n        tf_s, tf_u, tf_v = _NormalizingSvd(tf_a, full_matrices_)\n        outputs = [tf_s, tf_u, tf_v]\n      else:\n        tf_s = linalg_ops.svd(tf_a, compute_uv=False)\n        outputs = [tf_s]\n      outputs_sums = [math_ops.reduce_sum(o) for o in outputs]\n      tf_func_outputs = math_ops.add_n(outputs_sums)\n      grad = gradients_impl.gradients(tf_func_outputs, tf_a)[0]\n      x_init = np.random.uniform(\n          low=-1.0, high=1.0, size=shape_).astype(dtype_)\n      if dtype_ in [np.complex64, np.complex128]:\n        x_init += 1j * np.random.uniform(\n            low=-1.0, high=1.0, size=shape_).astype(dtype_)\n      theoretical, numerical = gradient_checker.compute_gradient(\n          tf_a,\n          tf_a.get_shape().as_list(),\n          grad,\n          grad.get_shape().as_list(),\n          x_init_value=x_init,\n          delta=delta)\n      self.assertAllClose(theoretical, numerical, atol=tol, rtol=tol)\n  return Test\n\n\nclass SVDBenchmark(test.Benchmark):\n\n  shapes = [\n      (4, 4),\n      (8, 8),\n      (16, 16),\n      (101, 101),\n      (256, 256),\n      (1024, 1024),\n      (2048, 2048),\n      (1, 8, 8),\n      (10, 8, 8),\n      (100, 8, 8),\n      (1000, 8, 8),\n      (1, 32, 32),\n      (10, 32, 32),\n      (100, 32, 32),\n      (1000, 32, 32),\n      (1, 256, 256),\n      (10, 256, 256),\n      (100, 256, 256),\n  ]\n\n  def benchmarkSVDOp(self):\n    for shape_ in self.shapes:\n      with ops.Graph().as_default(), \\\n          session.Session(config=benchmark.benchmark_config()) as sess, \\\n          ops.device(\"/cpu:0\"):\n        matrix_value = np.random.uniform(\n            low=-1.0, high=1.0, size=shape_).astype(np.float32)\n        matrix = variables.Variable(matrix_value)\n        u, s, v = linalg_ops.svd(matrix)\n        self.evaluate(variables.global_variables_initializer())\n        self.run_op_benchmark(\n            sess,\n            control_flow_ops.group(u, s, v),\n            min_iters=25,\n            name=\"SVD_cpu_{shape}\".format(shape=shape_))\n\n      if test.is_gpu_available(True):\n        with ops.Graph().as_default(), \\\n            session.Session(config=benchmark.benchmark_config()) as sess, \\\n            ops.device(\"/device:GPU:0\"):\n          matrix_value = np.random.uniform(\n              low=-1.0, high=1.0, size=shape_).astype(np.float32)\n          matrix = variables.Variable(matrix_value)\n          u, s, v = linalg_ops.svd(matrix)\n          self.evaluate(variables.global_variables_initializer())\n          self.run_op_benchmark(\n              sess,\n              control_flow_ops.group(u, s, v),\n              min_iters=25,\n              name=\"SVD_gpu_{shape}\".format(shape=shape_))\n\n\nif __name__ == \"__main__\":\n  dtypes_to_test = [np.float32, np.float64, np.complex64, np.complex128]\n  for compute_uv in False, True:\n    for full_matrices in False, True:\n      for dtype in dtypes_to_test:\n        for rows in 0, 1, 2, 5, 10, 32, 100:\n          for cols in 0, 1, 2, 5, 10, 32, 100:\n            for batch_dims in [(), (3,)] + [(3, 2)] * (max(rows, cols) < 10):\n              full_shape = batch_dims + (rows, cols)\n              for use_static_shape in set([True, False]):\n                name = \"%s_%s_static_shape_%s__compute_uv_%s_full_%s\" % (\n                    dtype.__name__, \"_\".join(map(str, full_shape)),\n                    use_static_shape, compute_uv, full_matrices)\n                _AddTest(\n                    SvdOpTest, \"Svd\", name,\n                    _GetSvdOpTest(dtype, full_shape, use_static_shape,\n                                  compute_uv, full_matrices))\n  for compute_uv in False, True:\n    for full_matrices in False, True:\n      dtypes = ([np.float32, np.float64] + [np.complex64, np.complex128] *\n                (not compute_uv))\n      for dtype in dtypes:\n        mat_shapes = [(10, 11), (11, 10), (11, 11), (2, 2, 2, 3)]\n        if not full_matrices or not compute_uv:\n          mat_shapes += [(5, 11), (11, 5)]\n        for mat_shape in mat_shapes:\n          for batch_dims in [(), (3,)]:\n            full_shape = batch_dims + mat_shape\n            name = \"%s_%s_compute_uv_%s_full_%s\" % (dtype.__name__, \"_\".join(\n                map(str, full_shape)), compute_uv, full_matrices)\n            _AddTest(\n                SvdGradOpTest, \"SvdGrad\", name,\n                _GetSvdGradOpTest(dtype, full_shape, compute_uv, full_matrices))\n            # The results are too inaccurate for float32.\n            if dtype in (np.float64, np.complex128):\n              _AddTest(\n                  SvdGradGradOpTest, \"SvdGradGrad\", name,\n                  _GetSvdGradGradOpTest(dtype, full_shape, compute_uv,\n                                        full_matrices))\n  test.main()"