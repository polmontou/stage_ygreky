"# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for ops which manipulate lists of tensors.\"\"\"\n\n# pylint: disable=g-bad-name\nfrom absl.testing import parameterized\nimport numpy as np  # pylint: disable=unused-import\n\nfrom tensorflow.core.framework import full_type_pb2\nfrom tensorflow.python.client import session\nfrom tensorflow.python.eager import backprop\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.eager import function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import gen_list_ops\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.ops import list_ops\nfrom tensorflow.python.ops import map_fn\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import resource_variable_ops\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import string_ops\nfrom tensorflow.python.ops import variable_scope as vs\nfrom tensorflow.python.platform import test\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass ListOpsTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n\n  def _testPushPop(self, max_num_elements):\n    l = list_ops.empty_tensor_list(\n        element_dtype=dtypes.float32,\n        element_shape=[],\n        max_num_elements=max_num_elements)\n    l = list_ops.tensor_list_push_back(l, constant_op.constant(1.0))\n    l, e = list_ops.tensor_list_pop_back(l, element_dtype=dtypes.float32)\n    l = list_ops.tensor_list_stack(l, element_dtype=dtypes.float32)\n    l, e = self.evaluate((l, e))\n    self.assertAllEqual(l, [])\n    self.assertAllEqual(e, 1.0)\n\n  @parameterized.named_parameters((\"NoMaxNumElements\", None),\n                                  (\"WithMaxNumElements\", 2))\n  def testPushPop(self, max_num_elements):\n    self._testPushPop(max_num_elements)\n\n  @parameterized.named_parameters((\"NoMaxNumElements\", None),\n                                  (\"WithMaxNumElements\", 2))\n  @test_util.run_gpu_only\n  def testPushPopGPU(self, max_num_elements):\n    with context.device(\"gpu:0\"):\n      self._testPushPop(max_num_elements)\n\n  @test_util.run_deprecated_v1\n  def testPushInFullListFails(self):\n    l = list_ops.empty_tensor_list(\n        element_dtype=dtypes.float32, element_shape=[], max_num_elements=1)\n    l = list_ops.tensor_list_push_back(l, constant_op.constant(1.0))\n    with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                \"Tried to push item into a full list\"):\n      l = list_ops.tensor_list_push_back(l, 2.)\n      self.evaluate(l)\n\n  @parameterized.named_parameters((\"NoMaxNumElements\", None),\n                                  (\"WithMaxNumElements\", 2))\n  @test_util.run_deprecated_v1\n  def testPopFromEmptyTensorListFails(self, max_num_elements):\n    l = list_ops.empty_tensor_list(\n        element_dtype=dtypes.float32,\n        element_shape=[],\n        max_num_elements=max_num_elements)\n    with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                \"Trying to pop from an empty list\"):\n      l = list_ops.tensor_list_pop_back(l, element_dtype=dtypes.float32)\n      self.evaluate(l)\n\n  def testPopUninitializedTensorUseListElementShape(self):\n    l = list_ops.tensor_list_reserve(\n        element_dtype=dtypes.float32, element_shape=[2, 3], num_elements=3)\n    _, e = list_ops.tensor_list_pop_back(l, element_dtype=dtypes.float32)\n    l = list_ops.tensor_list_stack(l, element_dtype=dtypes.float32)\n    l, e = self.evaluate((l, e))\n    self.assertAllEqual(e, np.zeros((2, 3)))\n    self.assertAllEqual(l, np.zeros((3, 2, 3)))\n\n  def testPopUninitializedTensorUseSpecifiedElementShape(self):\n    l = list_ops.tensor_list_reserve(\n        element_dtype=dtypes.float32, element_shape=[None, 3], num_elements=3)\n    _, e = gen_list_ops.tensor_list_pop_back(\n        l, element_dtype=dtypes.float32, element_shape=[4, 3])\n    self.assertAllEqual(e, np.zeros((4, 3)))\n\n  def testPopUninitializedTensorWithInvalidElementShapeFails(self):\n    l = list_ops.tensor_list_reserve(\n        element_dtype=dtypes.float32, element_shape=None, num_elements=3)\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError,\n        \"Trying to read an uninitialized tensor but \"\n        \"element_shape is not fully defined\"):\n      _, e = list_ops.tensor_list_pop_back(l, element_dtype=dtypes.float32)\n      self.evaluate(e)\n\n    l = list_ops.tensor_list_reserve(\n        element_dtype=dtypes.float32, element_shape=[None, 2], num_elements=3)\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError,\n        r\"Incompatible shapes during merge: \\[1,3\\] vs. \\[\\?,2\\]\"):\n      _, e = gen_list_ops.tensor_list_pop_back(\n          l, element_dtype=dtypes.float32, element_shape=[1, 3])\n      self.evaluate(e)\n\n  def testPushGetGrad(self):\n    with backprop.GradientTape() as tape:\n      l = list_ops.empty_tensor_list(\n          element_dtype=dtypes.float32, element_shape=None)\n      c0 = constant_op.constant(5.0)\n      c1 = constant_op.constant([10.0, 20.0])\n      tape.watch(c0)\n      tape.watch(c1)\n      l = list_ops.tensor_list_push_back(l, c0)\n      l = list_ops.tensor_list_push_back(l, c1)\n      t1 = list_ops.tensor_list_get_item(l, 1, element_dtype=dtypes.float32)\n      self.assertAllEqual(self.evaluate(t1), [10.0, 20.0])\n      # t1 == c1 so the gradient should be [0., [1., 1.]]\n      # This tests that the gradient of push_back correctly converts DT_INVALID\n      # tensors to zeros. The list returned by the gradient of GetItem will\n      # have only have tensor at index 1 set and others set to DT_INVALID.\n      dt0, dt1 = tape.gradient(t1, [c0, c1])\n      self.assertAllEqual(self.evaluate(dt1), [1.0, 1.0])\n      self.assertEqual(self.evaluate(dt0), 0.0)\n\n  def _testStack(self, max_num_elements):\n    l = list_ops.empty_tensor_list(\n        element_dtype=dtypes.float32,\n        element_shape=[],\n        max_num_elements=max_num_elements)\n    l = list_ops.tensor_list_push_back(l, constant_op.constant(1.0))\n    l = list_ops.tensor_list_push_back(l, constant_op.constant(2.0))\n    t = list_ops.tensor_list_stack(l, element_dtype=dtypes.float32)\n    if not context.executing_eagerly():\n      self.assertAllEqual(t.shape.as_list(), [None])\n    self.assertAllEqual(self.evaluate(t), [1.0, 2.0])\n\n  @parameterized.named_parameters((\"NoMaxNumElements\", None),\n                                  (\"WithMaxNumElements\", 2))\n  def testStack(self, max_num_elements):\n    self._testStack(max_num_elements)\n\n  @parameterized.named_parameters((\"NoMaxNumElements\", None),\n                                  (\"WithMaxNumElements\", 2))\n  @test_util.run_gpu_only\n  def testStackGPU(self, max_num_elements):\n    with context.device(\"gpu:0\"):\n      self._testStack(max_num_elements)\n\n  @parameterized.named_parameters((\"NoMaxNumElements\", None),\n                                  (\"WithMaxNumElements\", 3))\n  @test_util.run_deprecated_v1\n  def testStackWithUnknownElementShape(self, max_num_elements):\n    l = list_ops.empty_tensor_list(\n        element_dtype=dtypes.float32,\n        element_shape=None,\n        max_num_elements=max_num_elements)\n    l = list_ops.tensor_list_push_back(l, constant_op.constant(1.0))\n    l = list_ops.tensor_list_push_back(l, constant_op.constant(2.0))\n\n    t = list_ops.tensor_list_stack(l, element_dtype=dtypes.float32)\n    self.assertAllEqual(self.evaluate(t), [1.0, 2.0])\n\n    # Should raise an error when the element tensors do not all have the same\n    # shape.\n    with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                \"Incompatible ranks during merge: 0 vs. 1\"):\n      l = list_ops.tensor_list_push_back(l, constant_op.constant([3.0, 4.0]))\n      t = list_ops.tensor_list_stack(l, element_dtype=dtypes.float32)\n      self.evaluate(t)\n\n  @parameterized.named_parameters((\"NoMaxNumElements\", None),\n                                  (\"WithMaxNumElements\", 3))\n  @test_util.run_deprecated_v1\n  def testStackWithPartiallyDefinedElementShape(self, max_num_elements):\n    l = list_ops.empty_tensor_list(\n        element_dtype=dtypes.float32,\n        element_shape=[None],\n        max_num_elements=max_num_elements)\n    l = list_ops.tensor_list_push_back(l, constant_op.constant([1.0]))\n    l = list_ops.tensor_list_push_back(l, constant_op.constant([2.0]))\n\n    t = list_ops.tensor_list_stack(l, element_dtype=dtypes.float32)\n    self.assertAllEqual(self.evaluate(t), [[1.0], [2.0]])\n\n    # Should raise an error when the element tensors do not all have the same\n    # shape.\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError,\n        r\"Incompatible shapes during merge: \\[1\\] vs. \\[2\\]\"):\n      l = list_ops.tensor_list_push_back(l, constant_op.constant([2.0, 3.0]))\n      t = list_ops.tensor_list_stack(l, element_dtype=dtypes.float32)\n      self.evaluate(t)\n\n  @parameterized.named_parameters((\"NoMaxNumElements\", None),\n                                  (\"WithMaxNumElements\", 2))\n  @test_util.run_deprecated_v1\n  def testStackEmptyList(self, max_num_elements):\n    # Should be able to stack empty lists with fully defined element_shape.\n    l = list_ops.empty_tensor_list(\n        element_dtype=dtypes.float32,\n        element_shape=[1, 2],\n        max_num_elements=max_num_elements)\n    t = list_ops.tensor_list_stack(l, element_dtype=dtypes.float32)\n    self.assertAllEqual(self.evaluate(t).shape, (0, 1, 2))\n\n    # Should not be able to stack empty lists with partially defined\n    # element_shape.\n    with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                \"non-fully-defined\"):\n      l = list_ops.empty_tensor_list(\n          element_dtype=dtypes.float32,\n          element_shape=[None, 2],\n          max_num_elements=max_num_elements)\n      t = list_ops.tensor_list_stack(l, element_dtype=dtypes.float32)\n      self.evaluate(t)\n\n    # Should not be able to stack empty lists with undefined element_shape.\n    with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                \"non-fully-defined\"):\n      l = list_ops.empty_tensor_list(\n          element_dtype=dtypes.float32,\n          element_shape=None,\n          max_num_elements=max_num_elements)\n      t = list_ops.tensor_list_stack(l, element_dtype=dtypes.float32)\n      self.evaluate(t)\n\n  def _testStackWithUninitializedTensors(self):\n    l = list_ops.tensor_list_reserve(\n        element_dtype=dtypes.float32, element_shape=[], num_elements=3)\n    t = list_ops.tensor_list_stack(l, element_dtype=dtypes.float32)\n    self.assertAllEqual(t, [0., 0., 0.])\n\n  def testStackWithUninitializedTensors(self):\n    self._testStackWithUninitializedTensors()\n\n  @test_util.run_gpu_only\n  def testStackWithUninitializedTensorsGpu(self):\n    with context.device(\"gpu:0\"):\n      self._testStackWithUninitializedTensors()\n\n  def _testStackWithUninitializedTensorsInferShape(self):\n    l = list_ops.tensor_list_reserve(\n        element_dtype=dtypes.float32, element_shape=None, num_elements=3)\n    l = list_ops.tensor_list_set_item(l, 1, [1., 2.])\n    t = list_ops.tensor_list_stack(l, element_dtype=dtypes.float32)\n    self.assertAllEqual(t, [[0., 0.], [1., 2.], [0., 0.]])\n\n  def testStackWithUninitializedTensorsInferShape(self):\n    self._testStackWithUninitializedTensorsInferShape()\n\n  @test_util.run_gpu_only\n  def testStackWithUninitializedTensorsInferShapeGpu(self):\n    with context.device(\"gpu:0\"):\n      self._testStackWithUninitializedTensorsInferShape()\n\n  def testStackReservedListWithNoElementsAndPartialElementShapeFails(self):\n    l = list_ops.tensor_list_reserve(\n        element_dtype=dtypes.float32, element_shape=None, num_elements=3)\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError, \"Tried to stack list which only contains \"\n        \"uninitialized tensors and has a \"\n        \"non-fully-defined element_shape: <unknown>\"):\n      t = list_ops.tensor_list_stack(l, element_dtype=dtypes.float32)\n      self.evaluate(t)\n\n  def testStackUsingSpecifiedElementShape(self):\n    l = list_ops.tensor_list_reserve(\n        element_dtype=dtypes.float32, element_shape=None, num_elements=3)\n    t = gen_list_ops.tensor_list_stack(\n        l, element_dtype=dtypes.float32, element_shape=[])\n    if context.executing_eagerly():\n      self.assertEqual(t.shape.as_list(), [3])\n    else:\n      self.assertEqual(t.shape.as_list(), [None])\n    self.assertAllEqual(self.evaluate(t), np.zeros((3,)))\n\n  @parameterized.named_parameters((\"NoMaxNumElements\", None),\n                                  (\"WithMaxNumElements\", 2))\n  def testGatherGrad(self, max_num_elements):\n    with backprop.GradientTape() as tape:\n      l = list_ops.empty_tensor_list(\n          element_dtype=dtypes.float32,\n          element_shape=[],\n          max_num_elements=max_num_elements)\n      c0 = constant_op.constant(1.0)\n      tape.watch(c0)\n      l = list_ops.tensor_list_push_back(l, c0)\n      l = list_ops.tensor_list_push_back(l, constant_op.constant(2.0))\n      t = list_ops.tensor_list_gather(l, [1, 0], element_dtype=dtypes.float32)\n      self.assertAllEqual(self.evaluate(t), [2.0, 1.0])\n      s = (t[0] + t[1]) * (t[0] + t[1])\n    dt = tape.gradient(s, c0)\n    self.assertAllEqual(self.evaluate(dt), 6.0)\n\n  @parameterized.named_parameters((\"NoMaxNumElements\", None),\n                                  (\"WithMaxNumElements\", 3))\n  @test_util.run_deprecated_v1\n  def testGatherWithUnknownElementShape(self, max_num_elements):\n    l = list_ops.empty_tensor_list(\n        element_dtype=dtypes.float32,\n        element_shape=None,\n        max_num_elements=max_num_elements)\n    l = list_ops.tensor_list_push_back(l, constant_op.constant(1.0))\n    l = list_ops.tensor_list_push_back(l, constant_op.constant(2.0))\n    l = list_ops.tensor_list_push_back(l, constant_op.constant([3.0, 4.0]))\n\n    t = list_ops.tensor_list_gather(l, [1, 0], element_dtype=dtypes.float32)\n    self.assertAllEqual(self.evaluate(t), [2.0, 1.0])\n\n    t = list_ops.tensor_list_gather(l, [2], element_dtype=dtypes.float32)\n    self.assertAllEqual(self.evaluate(t), [[3.0, 4.0]])\n\n    # Should raise an error when the requested tensors do not all have the same\n    # shape.\n    with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                \"Incompatible ranks during merge: 0 vs. 1\"):\n      t = list_ops.tensor_list_gather(l, [0, 2], element_dtype=dtypes.float32)\n      self.evaluate(t)\n\n  @parameterized.named_parameters((\"NoMaxNumElements\", None),\n                                  (\"WithMaxNumElements\", 3))\n  @test_util.run_deprecated_v1\n  def testGatherWithPartiallyDefinedElementShape(self, max_num_elements):\n    l = list_ops.empty_tensor_list(\n        element_dtype=dtypes.float32,\n        element_shape=[None],\n        max_num_elements=max_num_elements)\n    l = list_ops.tensor_list_push_back(l, constant_op.constant([1.0]))\n    l = list_ops.tensor_list_push_back(l, constant_op.constant([2.0, 3.0]))\n    l = list_ops.tensor_list_push_back(l, constant_op.constant([4.0, 5.0]))\n\n    t = list_ops.tensor_list_gather(l, [0], element_dtype=dtypes.float32)\n    self.assertAllEqual(self.evaluate(t), [[1.0]])\n\n    t = list_ops.tensor_list_gather(l, [1, 2], element_dtype=dtypes.float32)\n    self.assertAllEqual(self.evaluate(t), [[2.0, 3.0], [4.0, 5.0]])\n\n    # Should raise an error when the requested tensors do not all have the same\n    # shape.\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError,\n        r\"Incompatible shapes during merge: \\[1\\] vs. \\[2\\]\"):\n      t = list_ops.tensor_list_gather(l, [0, 2], element_dtype=dtypes.float32)\n      self.evaluate(t)\n\n  @parameterized.named_parameters((\"NoMaxNumElements\", None),\n                                  (\"WithMaxNumElements\", 3))\n  @test_util.run_deprecated_v1\n  def testGatherEmptyList(self, max_num_elements):\n    # Should be able to gather from empty lists with fully defined\n    # element_shape.\n    l = list_ops.empty_tensor_list(\n        element_dtype=dtypes.float32,\n        element_shape=[1, 2],\n        max_num_elements=max_num_elements)\n    t = list_ops.tensor_list_gather(l, [], element_dtype=dtypes.float32)\n    self.assertAllEqual((0, 1, 2), self.evaluate(t).shape)\n\n    # Should not be able to gather from empty lists with partially defined\n    # element_shape.\n    with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                \"non-fully-defined\"):\n      l = list_ops.empty_tensor_list(\n          element_dtype=dtypes.float32,\n          element_shape=[None, 2],\n          max_num_elements=max_num_elements)\n      t = list_ops.tensor_list_gather(l, [], element_dtype=dtypes.float32)\n      self.evaluate(t)\n\n    # Should not be able to gather from empty lists with undefined\n    # element_shape.\n    with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                \"non-fully-defined\"):\n      l = list_ops.empty_tensor_list(\n          element_dtype=dtypes.float32,\n          element_shape=None,\n          max_num_elements=max_num_elements)\n      t = list_ops.tensor_list_gather(l, [], element_dtype=dtypes.float32)\n      self.evaluate(t)\n\n  def testGatherGradWithNonContiguousIndices(self):\n    with backprop.GradientTape(persistent=True) as tape:\n      t = constant_op.constant([1.0, 2.0, 3.0])\n      l = list_ops.tensor_list_from_tensor(t, element_shape=[])\n      c = constant_op.constant(5.0)\n      tape.watch(c)\n      l = list_ops.tensor_list_set_item(l, 1, c)\n      t = list_ops.tensor_list_gather(l, [1], element_dtype=dtypes.float32)\n      self.assertAllEqual(self.evaluate(t), [5.0])\n      s = t[0] * t[0]\n    dt = tape.gradient(s, c)\n    self.assertAllEqual(self.evaluate(dt), 10.0)\n    dl = tape.gradient(t, l)\n    dl_length = list_ops.tensor_list_length(dl)\n    self.assertAllEqual(self.evaluate(dl_length), 3)\n\n  def _testGatherWithUninitializedTensors(self):\n    l = list_ops.tensor_list_reserve(\n        element_dtype=dtypes.float32, element_shape=[], num_elements=3)\n    t = list_ops.tensor_list_gather(l, [0, 2], element_dtype=dtypes.float32)\n    self.assertAllEqual(self.evaluate(t), [0., 0.])\n\n  def testGatherWithUninitializedTensors(self):\n    self._testGatherWithUninitializedTensors()\n\n  @test_util.run_gpu_only\n  def testGatherWithUninitializedTensorsGpu(self):\n    with context.device(\"gpu:0\"):\n      self._testGatherWithUninitializedTensors()\n\n  def _testGatherWithUninitializedTensorsInferShape(self):\n    l = list_ops.tensor_list_reserve(\n        element_dtype=dtypes.float32, element_shape=None, num_elements=3)\n    l = list_ops.tensor_list_set_item(l, 1, [1., 2.])\n    t = list_ops.tensor_list_gather(l, [1, 2], element_dtype=dtypes.float32)\n    self.assertAllEqual(self.evaluate(t), [[1., 2.], [0., 0.]])\n\n  def testGatherWithUninitializedTensorsInferShape(self):\n    self._testGatherWithUninitializedTensorsInferShape()\n\n  @test_util.run_gpu_only\n  def testGatherWithUninitializedTensorsInferShapeGpu(self):\n    with context.device(\"gpu:0\"):\n      self._testGatherWithUninitializedTensorsInferShape()\n\n  def testGatherReservedListWithNoElementsAndPartialElementShapeFails(self):\n    l = list_ops.tensor_list_reserve(\n        element_dtype=dtypes.float32, element_shape=None, num_elements=3)\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError,\n        \"Tried to gather uninitialized tensors from a\"\n        \" list with non-fully-defined element_shape\"):\n      t = list_ops.tensor_list_gather(l, [0], element_dtype=dtypes.float32)\n      self.evaluate(t)\n\n  def testGatherUsingSpecifiedElementShape(self):\n    l = list_ops.tensor_list_reserve(\n        element_dtype=dtypes.float32, element_shape=None, num_elements=3)\n    t = gen_list_ops.tensor_list_gather(\n        l, [0, 1, 2], element_dtype=dtypes.float32, element_shape=[])\n    self.assertEqual(t.shape.as_list(), [3])\n    self.assertAllEqual(self.evaluate(t), np.zeros((3,)))\n\n  def testScatterOutputListSize(self):\n    c0 = constant_op.constant([1.0, 2.0])\n    l = list_ops.tensor_list_scatter(c0, [1, 3], [])\n    # TensorListScatter should return a list with size largest index + 1.\n    self.assertAllEqual(list_ops.tensor_list_length(l), 4)\n\n  def testScatterOutputListSizeWithNumElementsSpecified(self):\n    c0 = constant_op.constant([1.0, 2.0])\n    l = gen_list_ops.tensor_list_scatter_v2(\n        c0, [1, 3], list_ops._build_element_shape([]), num_elements=5)\n    # TensorListScatter should return a list with size num_elements.\n    self.assertAllEqual(list_ops.tensor_list_length(l), 5)\n\n  def testScatterFailsWhenElementShapeIsNotVector(self):\n    c0 = constant_op.constant([1.0, 2.0])\n    # In Eager mode, InvalidArgumentError is generated by the Compute function.\n    # In graph mode, ValueError is generated by the shape function.\n    with self.assertRaisesRegex(\n        (errors.InvalidArgumentError, ValueError),\n        \"must be at most rank 1\"):\n      l = gen_list_ops.tensor_list_scatter(\n          # Wrong element_shape. Should be at most rank 1.\n          c0, [1, 3], element_shape=[[1]])\n      self.evaluate(l)\n\n  def testScatterV2FailsWhenElementShapeIsNotVector(self):\n    c0 = constant_op.constant([1.0, 2.0])\n    # In Eager mode, InvalidArgumentError is generated by the Compute function.\n    # In graph mode, ValueError is generated by the shape function.\n    with self.assertRaisesRegex(\n        (errors.InvalidArgumentError, ValueError),\n        \"must be at most rank 1\"):\n      l = gen_list_ops.tensor_list_scatter_v2(\n          # Wrong element_shape. Should be at most rank 1.\n          c0, [1, 3], element_shape=[[1]], num_elements=2)\n      self.evaluate(l)\n\n  def testScatterFailsWhenIndexLargerThanNumElements(self):\n    c0 = constant_op.constant([1.0, 2.0])\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError,\n        \"TensorListScatter: Trying to scatter at index 3 in list with size 3\"):\n      l = gen_list_ops.tensor_list_scatter_v2(\n          c0, [1, 3], list_ops._build_element_shape([]), num_elements=3)\n      self.evaluate(l)\n\n  def testScatterFailsWithInvalidNumElements(self):\n    c0 = constant_op.constant([1.0, 2.0])\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError,\n        \"TensorListScatter expects num_elements >= -1, found: -2\"):\n      l = gen_list_ops.tensor_list_scatter_v2(\n          c0, [1, 3], list_ops._build_element_shape([]), num_elements=-2)\n      self.evaluate(l)\n\n  def testScatterWithInvalidRowsInInputTensorFails(self):\n    c0 = constant_op.constant([1.0, 2.0])\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError,\n        \"Invalid number of rows in input tensor. Expected: 3 Actual: 2\"):\n      l = list_ops.tensor_list_scatter(c0, [1, 0, 2], [])\n      self.evaluate(l)\n\n  def testScatterWithNegativeIndicesFails(self):\n    c0 = constant_op.constant([1.0, 2.0])\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError,\n        \"Indices in TensorListScatter must all be non-negative.\"):\n      l = list_ops.tensor_list_scatter(c0, [-1, -2], element_shape=[])\n      self.evaluate(l)\n\n  def testScatterIntoExistingList(self):\n    l = list_ops.tensor_list_reserve(\n        element_dtype=dtypes.float32, element_shape=[], num_elements=3)\n    l = list_ops.tensor_list_scatter(tensor=[1.], indices=[0], element_shape=[])\n    l = list_ops.tensor_list_scatter(\n        tensor=[2., 3.], indices=[1, 2], element_shape=[], input_handle=l)\n    self.assertAllEqual(\n        list_ops.tensor_list_stack(l, element_dtype=dtypes.float32),\n        [1., 2., 3.])\n\n  def testScatterGrad(self):\n    with backprop.GradientTape() as tape:\n      c0 = constant_op.constant([1.0, 2.0])\n      tape.watch(c0)\n      l = list_ops.tensor_list_scatter(c0, [1, 0], element_shape=[])\n      t0 = list_ops.tensor_list_get_item(l, 0, element_dtype=dtypes.float32)\n      t1 = list_ops.tensor_list_get_item(l, 1, element_dtype=dtypes.float32)\n      self.assertAllEqual(self.evaluate(t0), 2.0)\n      self.assertAllEqual(self.evaluate(t1), 1.0)\n      loss = t0 * t0 + t1 * t1\n    dt = tape.gradient(loss, c0)\n    self.assertAllEqual(self.evaluate(dt), [2., 4.])\n\n  def testScatterWithPartialReadGrad(self):\n    with backprop.GradientTape() as tape:\n      c0 = constant_op.constant([1.0, 2.0])\n      tape.watch(c0)\n      l = list_ops.tensor_list_scatter(c0, [1, 0], element_shape=[])\n      t0 = list_ops.tensor_list_get_item(l, 0, element_dtype=dtypes.float32)\n      self.assertAllEqual(self.evaluate(t0), 2.0)\n      loss = t0 * t0\n    dt = tape.gradient(loss, c0)\n    self.assertAllEqual(self.evaluate(dt), [0., 4.])\n\n  def testTensorListFromTensor(self):\n    t = constant_op.constant([1.0, 2.0])\n    l = list_ops.tensor_list_from_tensor(t, element_shape=[])\n    e = list_ops.tensor_list_get_item(l, 0, element_dtype=dtypes.float32)\n    self.assertAllEqual(e, 1.0)\n    l, e = list_ops.tensor_list_pop_back(l, element_dtype=dtypes.float32)\n    self.assertAllEqual(e, 2.0)\n    l, e = list_ops.tensor_list_pop_back(l, element_dtype=dtypes.float32)\n    self.assertAllEqual(e, 1.0)\n    self.assertAllEqual(list_ops.tensor_list_length(l), 0)\n\n  @test_util.run_gpu_only\n  def testFromTensorGPU(self):\n    with context.device(\"gpu:0\"):\n      self.testTensorListFromTensor()\n\n  def testGetSetBool(self):\n    t = constant_op.constant([True, False])\n    l = list_ops.tensor_list_from_tensor(t, element_shape=[])\n    e0 = list_ops.tensor_list_get_item(l, 0, element_dtype=dtypes.bool)\n    self.assertAllEqual(self.evaluate(e0), True)\n    l = list_ops.tensor_list_set_item(l, 0, False)\n    t = list_ops.tensor_list_stack(l, element_dtype=dtypes.bool)\n    self.assertAllEqual(self.evaluate(t), [False, False])\n\n  @test_util.run_gpu_only\n  def testGetSetBoolGPU(self):\n    with context.device(\"gpu:0\"):\n      self.testGetSetBool()\n\n  def _testGetSetNumeric(self, dtype):\n    t = constant_op.constant([1.0, 2.0], dtype=dtype)\n    l = list_ops.tensor_list_from_tensor(t, element_shape=[])\n    e0 = list_ops.tensor_list_get_item(l, 0, element_dtype=dtype)\n    self.assertAllEqual(self.evaluate(e0), 1.0)\n    l = list_ops.tensor_list_set_item(\n        l, 0, constant_op.constant(3.0, dtype=dtype))\n    t = list_ops.tensor_list_stack(l, element_dtype=dtype)\n    self.assertAllEqual(self.evaluate(t), [3.0, 2.0])\n\n  @parameterized.parameters([dtypes.float32, dtypes.float64,\n                             dtypes.complex64, dtypes.complex128])\n  def testGetSetNumeric(self, dtype):\n    self._testGetSetNumeric(dtype)\n\n  @parameterized.parameters([dtypes.float32, dtypes.float64,\n                             dtypes.complex64, dtypes.complex128])\n  @test_util.run_gpu_only\n  def testGetSetNumericGPU(self, dtype):\n    with context.device(\"gpu:0\"):\n      self._testGetSetNumeric(dtype)\n\n  def testGetSetReserved(self):\n    l = list_ops.tensor_list_reserve(\n        element_dtype=dtypes.float32, element_shape=[], num_elements=2)\n    e0 = list_ops.tensor_list_get_item(l, 0, element_dtype=dtypes.float32)\n    self.assertAllEqual(e0, 0.0)\n    l = list_ops.tensor_list_set_item(l, 0, 3.0)\n    t = list_ops.tensor_list_stack(l, element_dtype=dtypes.float32)\n    self.assertAllEqual(t, [3.0, 0.0])\n\n  @test_util.run_gpu_only\n  def testGetSetReservedGPU(self):\n    with context.device(\"gpu:0\"):\n      self.testGetSetReserved()\n\n  def testSetGetGrad(self):\n    with backprop.GradientTape() as tape:\n      t = constant_op.constant(5.)\n      tape.watch(t)\n      l = list_ops.tensor_list_reserve(\n          element_dtype=dtypes.float32, element_shape=[], num_elements=3)\n      l = list_ops.tensor_list_set_item(l, 1, 2. * t)\n      e = list_ops.tensor_list_get_item(l, 1, element_dtype=dtypes.float32)\n      self.assertAllEqual(self.evaluate(e), 10.0)\n    self.assertAllEqual(self.evaluate(tape.gradient(e, t)), 2.0)\n\n  def testGetUninitializedTensorUseListElementShape(self):\n    l = list_ops.tensor_list_reserve(\n        element_dtype=dtypes.float32, element_shape=[], num_elements=3)\n    l = list_ops.tensor_list_set_item(l, 0, 5.)\n    e1 = list_ops.tensor_list_get_item(l, 1, element_dtype=dtypes.float32)\n    e2 = list_ops.tensor_list_get_item(l, 2, element_dtype=dtypes.float32)\n    self.assertEqual(self.evaluate(e1), 0.)\n    self.assertEqual(self.evaluate(e2), 0.)\n\n  def testGetUninitializedTensorUseSpecifiedElementShape(self):\n    l = list_ops.tensor_list_reserve(\n        element_dtype=dtypes.float32, element_shape=None, num_elements=3)\n    e0 = gen_list_ops.tensor_list_get_item(\n        l, 0, element_shape=[], element_dtype=dtypes.float32)\n    e1 = gen_list_ops.tensor_list_get_item(\n        l, 1, element_shape=[2, 3], element_dtype=dtypes.float32)\n    self.assertEqual(e0.shape.as_list(), [])\n    self.assertEqual(e1.shape.as_list(), [2, 3])\n    self.assertEqual(self.evaluate(e0), 0.)\n    self.assertAllEqual(self.evaluate(e1), np.zeros((2, 3)))\n\n    l = list_ops.tensor_list_reserve(\n        element_dtype=dtypes.float32, element_shape=[None, 3], num_elements=3)\n    e1 = gen_list_ops.tensor_list_get_item(\n        l, 1, element_shape=[2, 3], element_dtype=dtypes.float32)\n    self.assertAllEqual(self.evaluate(e1), np.zeros((2, 3)))\n\n  def testGetUninitializedTensorWithInvalidElementShapeFails(self):\n    l = list_ops.tensor_list_reserve(\n        element_dtype=dtypes.float32, element_shape=None, num_elements=3)\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError,\n        \"Trying to read an uninitialized tensor but \"\n        \"element_shape is not fully defined\"):\n      e0 = list_ops.tensor_list_get_item(l, 0, element_dtype=dtypes.float32)\n      self.evaluate(e0)\n\n    l = list_ops.tensor_list_reserve(\n        element_dtype=dtypes.float32, element_shape=[None, 2], num_elements=3)\n\n    # In eager mode the shape mismatch is caught in the TensorListGetItem\n    # kernel which raises an InvalidArgumentError.\n    # In graph mode the shape mismatch is caught in the C++ shape inference\n    # which raises a ValueError.\n    if context.executing_eagerly():\n      error_type = errors.InvalidArgumentError\n    else:\n      error_type = ValueError\n    with self.assertRaisesRegex(error_type, r\"shapes\"):\n      e0 = gen_list_ops.tensor_list_get_item(\n          l, 0, element_dtype=dtypes.float32, element_shape=[1, 3])\n      self.evaluate(e0)\n\n  @test_util.run_deprecated_v1\n  @test_util.enable_control_flow_v2\n  def testSkipEagerSetItemIndexOutOfBounds(self):\n    l = list_ops.empty_tensor_list(\n        element_dtype=dtypes.float32, element_shape=[])\n    e0 = constant_op.constant(5.)\n    l = list_ops.tensor_list_set_item(\n        l, 0, 2. * e0, resize_if_index_out_of_bounds=True)\n    l = list_ops.tensor_list_set_item(\n        l, 1, 1., resize_if_index_out_of_bounds=True)\n    t = list_ops.tensor_list_stack(l, element_dtype=dtypes.float32)\n    grad = gradients_impl.gradients(t, e0)[0]\n    self.assertAllEqual(self.evaluate(grad), 2.)\n\n  @test_util.run_deprecated_v1\n  def testSetOnEmptyListWithMaxNumElementsFails(self):\n    l = list_ops.empty_tensor_list(\n        element_dtype=dtypes.float32, element_shape=[], max_num_elements=3)\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError,\n        \"Trying to modify element 0 in a list with 0 elements.\"):\n      l = list_ops.tensor_list_set_item(l, 0, 1.)\n      self.evaluate(l)\n\n  def testUnknownShape(self):\n    l = list_ops.empty_tensor_list(\n        element_dtype=dtypes.float32, element_shape=None)\n    l = list_ops.tensor_list_push_back(l, constant_op.constant(1.0))\n    l = list_ops.tensor_list_push_back(l, constant_op.constant([1.0, 2.0]))\n    l, e = list_ops.tensor_list_pop_back(l, element_dtype=dtypes.float32)\n    self.assertAllEqual(self.evaluate(e), [1.0, 2.0])\n    l, e = list_ops.tensor_list_pop_back(l, element_dtype=dtypes.float32)\n    self.assertAllEqual(self.evaluate(e), 1.0)\n\n  @test_util.run_gpu_only\n  def testCPUGPUCopy(self):\n    t = constant_op.constant([1.0, 2.0])\n    l = list_ops.tensor_list_from_tensor(t, element_shape=[])\n    with context.device(\"gpu:0\"):\n      l_gpu = array_ops.identity(l)\n      self.assertAllEqual(\n          self.evaluate(\n              list_ops.tensor_list_pop_back(\n                  l_gpu, element_dtype=dtypes.float32)[1]), 2.0)\n    l_cpu = array_ops.identity(l_gpu)\n    self.assertAllEqual(\n        self.evaluate(\n            list_ops.tensor_list_pop_back(\n                l_cpu, element_dtype=dtypes.float32)[1]), 2.0)\n\n  @test_util.run_gpu_only\n  def testCPUGPUCopyNested(self):\n    t = constant_op.constant([1.0, 2.0])\n    child_l = list_ops.tensor_list_from_tensor(t, element_shape=[])\n    l = list_ops.empty_tensor_list(\n        element_shape=constant_op.constant([], dtype=dtypes.int32),\n        element_dtype=dtypes.variant)\n    l = list_ops.tensor_list_push_back(l, child_l)\n    with context.device(\"gpu:0\"):\n      l_gpu = array_ops.identity(l)\n      _, child_l_gpu = list_ops.tensor_list_pop_back(\n          l_gpu, element_dtype=dtypes.variant)\n      self.assertAllEqual(\n          self.evaluate(\n              list_ops.tensor_list_pop_back(\n                  child_l_gpu, element_dtype=dtypes.float32)[1]), 2.0)\n    l_cpu = array_ops.identity(l_gpu)\n    _, child_l_cpu = list_ops.tensor_list_pop_back(\n        l_cpu, element_dtype=dtypes.variant)\n    self.assertAllEqual(\n        self.evaluate(\n            list_ops.tensor_list_pop_back(\n                child_l_cpu, element_dtype=dtypes.float32)[1]), 2.0)\n\n  def testGraphStack(self):\n    with self.cached_session():\n      tl = list_ops.empty_tensor_list(\n          element_shape=constant_op.constant([1], dtype=dtypes.int32),\n          element_dtype=dtypes.int32)\n      tl = list_ops.tensor_list_push_back(tl, [1])\n      self.assertAllEqual(\n          self.evaluate(\n              list_ops.tensor_list_stack(tl, element_dtype=dtypes.int32)),\n          [[1]])\n\n  def testSkipEagerStackInLoop(self):\n    with self.cached_session():\n      t1 = list_ops.empty_tensor_list(\n          element_shape=constant_op.constant([], dtype=dtypes.int32),\n          element_dtype=dtypes.int32)\n      i = constant_op.constant(0, dtype=dtypes.int32)\n\n      def body(i, t1):\n        t1 = list_ops.tensor_list_push_back(t1, i)\n        i += 1\n        return i, t1\n\n      i, t1 = control_flow_ops.while_loop(lambda i, t1: math_ops.less(i, 4),\n                                          body, [i, t1])\n      s1 = list_ops.tensor_list_stack(t1, element_dtype=dtypes.int32)\n      self.assertAllEqual(self.evaluate(s1), [0, 1, 2, 3])\n\n  def testSkipEagerStackSwitchDtype(self):\n    with self.cached_session():\n      list_ = list_ops.empty_tensor_list(\n          element_shape=constant_op.constant([], dtype=dtypes.int32),\n          element_dtype=dtypes.int32)\n      m = constant_op.constant([1, 2, 3], dtype=dtypes.float32)\n\n      def body(list_, m):\n        list_ = control_flow_ops.cond(\n            math_ops.equal(list_ops.tensor_list_length(list_), 0),\n            lambda: list_ops.empty_tensor_list(m.shape, m.dtype), lambda: list_)\n        list_ = list_ops.tensor_list_push_back(list_, m)\n        return list_, m\n\n      for _ in range(2):\n        list_, m = body(list_, m)\n\n      s1 = list_ops.tensor_list_stack(list_, element_dtype=dtypes.float32)\n      np_s1 = np.array([[1, 2, 3], [1, 2, 3]], dtype=np.float32)\n      self.assertAllEqual(self.evaluate(s1), np_s1)\n\n  def testSkipEagerStackInLoopSwitchDtype(self):\n    with self.cached_session():\n      t1 = list_ops.empty_tensor_list(\n          element_shape=constant_op.constant([], dtype=dtypes.int32),\n          element_dtype=dtypes.int32)\n      i = constant_op.constant(0, dtype=dtypes.float32)\n      m = constant_op.constant([1, 2, 3], dtype=dtypes.float32)\n\n      def body(i, m, t1):\n        t1 = control_flow_ops.cond(\n            math_ops.equal(list_ops.tensor_list_length(t1), 0),\n            lambda: list_ops.empty_tensor_list(m.shape, m.dtype), lambda: t1)\n\n        t1 = list_ops.tensor_list_push_back(t1, m * i)\n        i += 1.0\n        return i, m, t1\n\n      i, m, t1 = control_flow_ops.while_loop(\n          lambda i, m, t1: math_ops.less(i, 4), body, [i, m, t1])\n      s1 = list_ops.tensor_list_stack(t1, element_dtype=dtypes.float32)\n      np_s1 = np.vstack([np.arange(1, 4) * i for i in range(4)])\n      self.assertAllEqual(self.evaluate(s1), np_s1)\n\n  def testSerialize(self):\n    worker = test_util.create_local_cluster(num_workers=1, num_ps=1)[0][0]\n    with ops.Graph().as_default(), session.Session(target=worker.target):\n      with ops.device(\"/job:worker\"):\n        t = constant_op.constant([[1.0], [2.0]])\n        l = list_ops.tensor_list_from_tensor(t, element_shape=[1])\n      with ops.device(\"/job:ps\"):\n        l_ps = array_ops.identity(l)\n        l_ps, e = list_ops.tensor_list_pop_back(\n            l_ps, element_dtype=dtypes.float32)\n      with ops.device(\"/job:worker\"):\n        worker_e = array_ops.identity(e)\n      self.assertAllEqual(self.evaluate(worker_e), [2.0])\n\n  def testSerializeListWithInvalidTensors(self):\n    worker = test_util.create_local_cluster(num_workers=1, num_ps=1)[0][0]\n    with ops.Graph().as_default(), session.Session(target=worker.target):\n      with ops.device(\"/job:worker\"):\n        l = list_ops.tensor_list_reserve(\n            element_dtype=dtypes.float32, element_shape=[], num_elements=2)\n        l = list_ops.tensor_list_set_item(l, 0, 1.)\n      with ops.device(\"/job:ps\"):\n        l_ps = array_ops.identity(l)\n        l_ps = list_ops.tensor_list_set_item(l_ps, 1, 2.)\n        t = list_ops.tensor_list_stack(l_ps, element_dtype=dtypes.float32)\n      with ops.device(\"/job:worker\"):\n        worker_t = array_ops.identity(t)\n      self.assertAllEqual(self.evaluate(worker_t), [1.0, 2.0])\n\n  def testSerializeListWithUnknownRank(self):\n    worker = test_util.create_local_cluster(num_workers=1, num_ps=1)[0][0]\n    with ops.Graph().as_default(), session.Session(target=worker.target):\n      with ops.device(\"/job:worker\"):\n        t = constant_op.constant([[1.0], [2.0]])\n        l = list_ops.tensor_list_from_tensor(t, element_shape=None)\n      with ops.device(\"/job:ps\"):\n        l_ps = array_ops.identity(l)\n        element_shape = list_ops.tensor_list_element_shape(\n            l_ps, shape_type=dtypes.int32)\n      with ops.device(\"/job:worker\"):\n        element_shape = array_ops.identity(element_shape)\n      self.assertEqual(self.evaluate(element_shape), -1)\n\n  def testSerializeListWithMaxNumElements(self):\n    worker = test_util.create_local_cluster(num_workers=1, num_ps=1)[0][0]\n    with ops.Graph().as_default(), session.Session(target=worker.target):\n      with ops.device(\"/job:worker\"):\n        l = list_ops.empty_tensor_list(\n            element_shape=None,\n            element_dtype=dtypes.float32,\n            max_num_elements=2)\n        l = list_ops.tensor_list_push_back(l, 1.)\n      with ops.device(\"/job:ps\"):\n        l_ps = array_ops.identity(l)\n        l_ps = list_ops.tensor_list_push_back(l_ps, 2.)\n      with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                  \"Tried to push item into a full list\"):\n        with ops.device(\"/job:worker\"):\n          l_worker = array_ops.identity(l_ps)\n          l_worker = list_ops.tensor_list_push_back(l_worker, 3.0)\n          self.evaluate(l_worker)\n\n  def testPushPopGradients(self):\n    with backprop.GradientTape() as tape:\n      l = list_ops.empty_tensor_list(\n          element_dtype=dtypes.float32, element_shape=[])\n      c = constant_op.constant(1.0)\n      tape.watch(c)\n      l = list_ops.tensor_list_push_back(l, c)\n      l, e = list_ops.tensor_list_pop_back(l, element_dtype=dtypes.float32)\n      e = 2 * e\n    self.assertAllEqual(self.evaluate(tape.gradient(e, [c])[0]), 2.0)\n\n  def testStackFromTensorGradients(self):\n    with backprop.GradientTape() as tape:\n      c = constant_op.constant([1.0, 2.0])\n      tape.watch(c)\n      l = list_ops.tensor_list_from_tensor(c, element_shape=[])\n      c2 = list_ops.tensor_list_stack(\n          l, element_dtype=dtypes.float32, num_elements=2)\n      result = c2 * 2.0\n    grad = tape.gradient(result, [c])[0]\n    self.assertAllEqual(self.evaluate(grad), [2.0, 2.0])\n\n  def testGetSetGradients(self):\n    with backprop.GradientTape() as tape:\n      c = constant_op.constant([1.0, 2.0])\n      tape.watch(c)\n      l = list_ops.tensor_list_from_tensor(c, element_shape=[])\n      c2 = constant_op.constant(3.0)\n      tape.watch(c2)\n      l = list_ops.tensor_list_set_item(l, 0, c2)\n      e = list_ops.tensor_list_get_item(l, 0, element_dtype=dtypes.float32)\n      ee = list_ops.tensor_list_get_item(l, 1, element_dtype=dtypes.float32)\n      y = e * e + ee * ee\n    grad_c, grad_c2 = tape.gradient(y, [c, c2])\n    self.assertAllEqual(self.evaluate(grad_c), [0.0, 4.0])\n    self.assertAllEqual(self.evaluate(grad_c2), 6.0)\n\n  @test_util.run_deprecated_v1\n  def testSetOutOfBounds(self):\n    c = constant_op.constant([1.0, 2.0])\n    l = list_ops.tensor_list_from_tensor(c, element_shape=[])\n    with self.assertRaises(errors.InvalidArgumentError):\n      self.evaluate(list_ops.tensor_list_set_item(l, 20, 3.0))\n\n  @test_util.run_deprecated_v1\n  def testSkipEagerSetItemWithMismatchedShapeFails(self):\n    with self.cached_session() as sess:\n      ph = array_ops.placeholder(dtypes.float32)\n      c = constant_op.constant([1.0, 2.0])\n      l = list_ops.tensor_list_from_tensor(c, element_shape=[])\n      # Set a placeholder with unknown shape to satisfy the shape inference\n      # at graph building time.\n      l = list_ops.tensor_list_set_item(l, 0, ph)\n      l_0 = list_ops.tensor_list_get_item(l, 0, element_dtype=dtypes.float32)\n      with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                  \"incompatible shape\"):\n        sess.run(l_0, {ph: [3.0]})\n\n  def testResourceVariableScatterGather(self):\n    c = constant_op.constant([1.0, 2.0], dtype=dtypes.float32)\n    l = list_ops.tensor_list_from_tensor(c, element_shape=[])\n    v = vs.get_variable(\"var\", initializer=[l] * 10, use_resource=True)\n    v_r_0_stacked = list_ops.tensor_list_stack(v[0], dtypes.float32)\n    self.evaluate(v.initializer)\n    self.assertAllEqual([1.0, 2.0], self.evaluate(v_r_0_stacked))\n    v_r_sparse_stacked = list_ops.tensor_list_stack(\n        v.sparse_read(0), dtypes.float32)\n    self.assertAllEqual([1.0, 2.0], self.evaluate(v_r_sparse_stacked))\n    l_new_0 = list_ops.tensor_list_from_tensor([3.0, 4.0], element_shape=[])\n    l_new_1 = list_ops.tensor_list_from_tensor([5.0, 6.0], element_shape=[])\n    updated_v = state_ops.scatter_update(v, [3, 5], [l_new_0, l_new_1])\n    updated_v_elems = array_ops.unstack(updated_v)\n    updated_v_stacked = [\n        list_ops.tensor_list_stack(el, dtypes.float32) for el in updated_v_elems\n    ]\n    expected = ([[1.0, 2.0]] * 3 + [[3.0, 4.0], [1.0, 2.0], [5.0, 6.0]] +\n                [[1.0, 2.0]] * 4)\n    self.assertAllEqual(self.evaluate(updated_v_stacked), expected)\n\n  def testResourceVariableScatterGatherInt64(self):\n    c = constant_op.constant([1, 2], dtype=dtypes.int64)\n    l = list_ops.tensor_list_from_tensor(c, element_shape=[])\n    v = vs.get_variable(\"var\", initializer=[l] * 10, use_resource=True)\n    v_r_0_stacked = list_ops.tensor_list_stack(v[0], dtypes.int64)\n    self.evaluate(v.initializer)\n    self.assertAllEqual([1, 2], self.evaluate(v_r_0_stacked))\n    v_r_sparse_stacked = list_ops.tensor_list_stack(\n        v.sparse_read(0), dtypes.int64)\n    self.assertAllEqual([1, 2], self.evaluate(v_r_sparse_stacked))\n    c34 = constant_op.constant([3, 4], dtype=dtypes.int64)\n    l_new_0 = list_ops.tensor_list_from_tensor(c34, element_shape=[])\n    c56 = constant_op.constant([5, 6], dtype=dtypes.int64)\n    l_new_1 = list_ops.tensor_list_from_tensor(c56, element_shape=[])\n    updated_v = state_ops.scatter_update(v, [3, 5], [l_new_0, l_new_1])\n    updated_v_elems = array_ops.unstack(updated_v)\n    updated_v_stacked = [\n        list_ops.tensor_list_stack(el, dtypes.int64) for el in updated_v_elems\n    ]\n    expected = ([[1, 2]] * 3 + [[3, 4], [1, 2], [5, 6]] +\n                [[1, 2]] * 4)\n    self.assertAllEqual(self.evaluate(updated_v_stacked), expected)\n\n  @test_util.run_deprecated_v1\n  def testConcat(self):\n    c = constant_op.constant([1.0, 2.0], dtype=dtypes.float32)\n    l0 = list_ops.tensor_list_from_tensor(c, element_shape=[])\n    l1 = list_ops.tensor_list_from_tensor([-1.0], element_shape=[])\n    l_batch_0 = array_ops.stack([l0, l1])\n    l_batch_1 = array_ops.stack([l1, l0])\n\n    l_concat_01 = list_ops.tensor_list_concat_lists(\n        l_batch_0, l_batch_1, element_dtype=dtypes.float32)\n    l_concat_10 = list_ops.tensor_list_concat_lists(\n        l_batch_1, l_batch_0, element_dtype=dtypes.float32)\n    l_concat_00 = list_ops.tensor_list_concat_lists(\n        l_batch_0, l_batch_0, element_dtype=dtypes.float32)\n    l_concat_11 = list_ops.tensor_list_concat_lists(\n        l_batch_1, l_batch_1, element_dtype=dtypes.float32)\n\n    expected_0 = [[1.0, 2.0], [-1.0]]\n    expected_1 = [[-1.0], [1.0, 2.0]]\n    expected_00 = [[1.0, 2.0, 1.0, 2.0], [-1.0, -1.0]]\n    expected_01 = [[1.0, 2.0, -1.0], [-1.0, 1.0, 2.0]]\n    expected_10 = [[-1.0, 1.0, 2.0], [1.0, 2.0, -1.0]]\n    expected_11 = [[-1.0, -1.0], [1.0, 2.0, 1.0, 2.0]]\n\n    for i, (concat, expected) in enumerate(zip(\n        [l_batch_0, l_batch_1,\n         l_concat_00, l_concat_01, l_concat_10, l_concat_11],\n        [expected_0, expected_1,\n         expected_00, expected_01, expected_10, expected_11])):\n      splitted = array_ops.unstack(concat)\n      splitted_stacked_ret = self.evaluate(\n          (list_ops.tensor_list_stack(splitted[0], dtypes.float32),\n           list_ops.tensor_list_stack(splitted[1], dtypes.float32)))\n      print(\"Test concat %d: %s, %s, %s, %s\"\n            % (i, expected[0], splitted_stacked_ret[0],\n               expected[1], splitted_stacked_ret[1]))\n      self.assertAllClose(expected[0], splitted_stacked_ret[0])\n      self.assertAllClose(expected[1], splitted_stacked_ret[1])\n\n    # Concatenating mismatched shapes fails.\n    with self.assertRaises((errors.InvalidArgumentError, ValueError)):\n      self.evaluate(\n          list_ops.tensor_list_concat_lists(\n              l_batch_0,\n              list_ops.empty_tensor_list([], dtypes.float32),\n              element_dtype=dtypes.float32))\n\n    if context.executing_eagerly():\n      expected_error = (\n          errors.InvalidArgumentError,\n          \"element shapes are not identical at index 0\")\n    else:\n      expected_error = (ValueError, \"Shapes must be equal rank\")\n    with self.assertRaisesRegex(*expected_error):\n      l_batch_of_vec_tls = array_ops.stack(\n          [list_ops.tensor_list_from_tensor([[1.0]], element_shape=[1])] * 2)\n      self.evaluate(\n          list_ops.tensor_list_concat_lists(l_batch_0, l_batch_of_vec_tls,\n                                            element_dtype=dtypes.float32))\n\n    if context.executing_eagerly():\n      expected_error = (errors.InvalidArgumentError,\n                        r\"input_b\\[0\\].dtype != element_dtype.\")\n    else:\n      expected_error = (ValueError, \"input_b.type != element_dtype\")\n    with self.assertRaisesRegex(*expected_error):\n      l_batch_of_int_tls = array_ops.stack(\n          [list_ops.tensor_list_from_tensor([1], element_shape=[])] * 2)\n      self.evaluate(\n          list_ops.tensor_list_concat_lists(l_batch_0, l_batch_of_int_tls,\n                                            element_dtype=dtypes.float32))\n\n  @test_util.run_deprecated_v1\n  def testPushBackBatch(self):\n    c = constant_op.constant([1.0, 2.0], dtype=dtypes.float32)\n    l0 = list_ops.tensor_list_from_tensor(c, element_shape=[])\n    l1 = list_ops.tensor_list_from_tensor([-1.0], element_shape=[])\n    l_batch = array_ops.stack([l0, l1])\n    l_push = list_ops.tensor_list_push_back_batch(l_batch, [3.0, 4.0])\n    l_unstack = array_ops.unstack(l_push)\n    l0_ret = list_ops.tensor_list_stack(l_unstack[0], dtypes.float32)\n    l1_ret = list_ops.tensor_list_stack(l_unstack[1], dtypes.float32)\n    self.assertAllClose([1.0, 2.0, 3.0], self.evaluate(l0_ret))\n    self.assertAllClose([-1.0, 4.0], self.evaluate(l1_ret))\n\n    with ops.control_dependencies([l_push]):\n      l_unstack_orig = array_ops.unstack(l_batch)\n      l0_orig_ret = list_ops.tensor_list_stack(l_unstack_orig[0],\n                                               dtypes.float32)\n      l1_orig_ret = list_ops.tensor_list_stack(l_unstack_orig[1],\n                                               dtypes.float32)\n\n    # Check that without aliasing, push_back_batch still works; and\n    # that it doesn't modify the input.\n    l0_r_v, l1_r_v, l0_orig_v, l1_orig_v = self.evaluate(\n        (l0_ret, l1_ret, l0_orig_ret, l1_orig_ret))\n    self.assertAllClose([1.0, 2.0, 3.0], l0_r_v)\n    self.assertAllClose([-1.0, 4.0], l1_r_v)\n    self.assertAllClose([1.0, 2.0], l0_orig_v)\n    self.assertAllClose([-1.0], l1_orig_v)\n\n    # Pushing back mismatched shapes fails.\n    with self.assertRaises((errors.InvalidArgumentError, ValueError)):\n      self.evaluate(list_ops.tensor_list_push_back_batch(l_batch, []))\n\n    with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                \"incompatible shape to a list at index 0\"):\n      self.evaluate(\n          list_ops.tensor_list_push_back_batch(l_batch, [[3.0], [4.0]]))\n\n    if context.executing_eagerly():\n      expected_error = (errors.InvalidArgumentError, \"Invalid data type\")\n    else:\n      expected_error = (ValueError, \"wrong element dtype\")\n    with self.assertRaisesRegex(*expected_error):\n      self.evaluate(list_ops.tensor_list_push_back_batch(l_batch, [3, 4]))\n\n  def testZerosLike(self):\n    for dtype in (dtypes.uint8, dtypes.uint16, dtypes.int8, dtypes.int16,\n                  dtypes.int32, dtypes.int64, dtypes.float16, dtypes.float32,\n                  dtypes.float64, dtypes.complex64, dtypes.complex128,\n                  dtypes.bool):\n      l_empty = list_ops.empty_tensor_list(\n          element_dtype=dtype, element_shape=[])\n      l_empty_zeros = array_ops.zeros_like(l_empty)\n      t_empty_zeros = list_ops.tensor_list_stack(\n          l_empty_zeros, element_dtype=dtype)\n\n      l_full = list_ops.tensor_list_push_back(l_empty,\n                                              math_ops.cast(0, dtype=dtype))\n      l_full = list_ops.tensor_list_push_back(l_full,\n                                              math_ops.cast(1, dtype=dtype))\n      l_full_zeros = array_ops.zeros_like(l_full)\n      t_full_zeros = list_ops.tensor_list_stack(\n          l_full_zeros, element_dtype=dtype)\n\n      self.assertAllEqual(self.evaluate(t_empty_zeros), [])\n      self.assertAllEqual(\n          self.evaluate(t_full_zeros), np.zeros(\n              (2,), dtype=dtype.as_numpy_dtype))\n\n  def testZerosLikeNested(self):\n    for dtype in (dtypes.uint8, dtypes.uint16, dtypes.int8, dtypes.int16,\n                  dtypes.int32, dtypes.int64, dtypes.float16, dtypes.float32,\n                  dtypes.float64, dtypes.complex64, dtypes.complex128,\n                  dtypes.bool):\n      l = list_ops.empty_tensor_list(\n          element_dtype=dtypes.variant, element_shape=[])\n\n      sub_l = list_ops.empty_tensor_list(element_dtype=dtype, element_shape=[])\n      l = list_ops.tensor_list_push_back(l, sub_l)\n      sub_l = list_ops.tensor_list_push_back(sub_l, math_ops.cast(\n          1, dtype=dtype))\n      l = list_ops.tensor_list_push_back(l, sub_l)\n      sub_l = list_ops.tensor_list_push_back(sub_l, math_ops.cast(\n          2, dtype=dtype))\n      l = list_ops.tensor_list_push_back(l, sub_l)\n\n      # l : [[],\n      #      [1],\n      #      [1, 2]]\n      #\n      # l_zeros : [[],\n      #            [0],\n      #            [0, 0]]\n      l_zeros = array_ops.zeros_like(l)\n\n      outputs = []\n      for _ in range(3):\n        l_zeros, out = list_ops.tensor_list_pop_back(\n            l_zeros, element_dtype=dtypes.variant)\n        outputs.append(list_ops.tensor_list_stack(out, element_dtype=dtype))\n\n      # Note: `outputs` contains popped values so the order is reversed.\n      self.assertAllEqual(self.evaluate(outputs[2]), [])\n      self.assertAllEqual(\n          self.evaluate(outputs[1]), np.zeros((1,), dtype=dtype.as_numpy_dtype))\n      self.assertAllEqual(\n          self.evaluate(outputs[0]), np.zeros((2,), dtype=dtype.as_numpy_dtype))\n\n  def testElementShape(self):\n    l = list_ops.empty_tensor_list(\n        element_dtype=dtypes.float32, element_shape=None)\n    shape = list_ops.tensor_list_element_shape(l, shape_type=dtypes.int32)\n    self.assertEqual(self.evaluate(shape), -1)\n\n  def testZerosLikeUninitialized(self):\n    l0 = list_ops.tensor_list_reserve([], 3, element_dtype=dtypes.float32)\n    l1 = list_ops.tensor_list_set_item(l0, 0, 1.)  # [1., _, _]\n    zeros_1 = array_ops.zeros_like(l1)  # [0., _, _]\n    l2 = list_ops.tensor_list_set_item(l1, 2, 2.)  # [1., _, 2.]\n    zeros_2 = array_ops.zeros_like(l2)  # [0., _, 0.]\n\n    # Gather indices with zeros in `zeros_1`.\n    res_1 = list_ops.tensor_list_gather(\n        zeros_1, [0], element_dtype=dtypes.float32)\n    # Gather indices with zeros in `zeros_2`.\n    res_2 = list_ops.tensor_list_gather(\n        zeros_2, [0, 2], element_dtype=dtypes.float32)\n\n    self.assertAllEqual(self.evaluate(res_1), [0.])\n    self.assertAllEqual(self.evaluate(res_2), [0., 0.])\n\n  @test_util.run_deprecated_v1\n  def testSkipEagerTensorListGetItemGradAggregation(self):\n    l = list_ops.tensor_list_reserve(\n        element_shape=[], num_elements=1, element_dtype=dtypes.float32)\n    x = constant_op.constant(1.0)\n    l = list_ops.tensor_list_set_item(l, 0, x)\n    l_read1 = list_ops.tensor_list_get_item(l, 0, element_dtype=dtypes.float32)\n    l_read2 = list_ops.tensor_list_get_item(l, 0, element_dtype=dtypes.float32)\n    grad = gradients_impl.gradients([l_read1, l_read2], [x])\n    with self.cached_session() as sess:\n      self.assertSequenceEqual(self.evaluate(grad), [2.])\n\n  @test_util.run_deprecated_v1\n  def testSkipEagerBuildElementShape(self):\n    fn = list_ops._build_element_shape\n    # Unknown shape -> -1.\n    self.assertEqual(fn(None), -1)\n    self.assertEqual(fn(tensor_shape.unknown_shape()), -1)\n    # Scalar shape -> [] with type int32.\n    self.assertEqual(fn([]).dtype, dtypes.int32)\n    self.assertEqual(fn(tensor_shape.TensorShape([])).dtype, dtypes.int32)\n    self.assertAllEqual(self.evaluate(fn([])), np.array([], np.int32))\n    self.assertAllEqual(\n        self.evaluate(fn(tensor_shape.TensorShape([]))), np.array([], np.int32))\n    # Tensor -> Tensor\n    shape = constant_op.constant(1)\n    self.assertIs(fn(shape), shape)\n    # Shape with unknown dims -> shape list with -1's.\n    shape = [None, 5]\n    self.assertAllEqual(fn(shape), [-1, 5])\n    self.assertAllEqual(fn(tensor_shape.TensorShape(shape)), [-1, 5])\n    # Shape with unknown dims and tensor dims -> shape list with -1's and tensor\n    # dims.\n    t = array_ops.placeholder(dtypes.int32)\n    shape = [None, 5, t]\n    result = fn(shape)\n    self.assertAllEqual(result[:2], [-1, 5])\n    self.assertIs(result[2], t)\n\n  def testAddN(self):\n    l1 = list_ops.tensor_list_from_tensor([1.0, 2.0], element_shape=[])\n    l2 = list_ops.tensor_list_from_tensor([3.0, 4.0], element_shape=[])\n    l3 = list_ops.tensor_list_from_tensor([5.0, 6.0], element_shape=[])\n    result = math_ops.add_n((l1, l2, l3))\n    result_t = list_ops.tensor_list_stack(result, element_dtype=dtypes.float32)\n    self.assertAllEqual(self.evaluate(result_t), [9., 12.])\n\n  def testAddNNestedList(self):\n    l1 = list_ops.tensor_list_from_tensor([1.0, 2.0], element_shape=[])\n    l2 = list_ops.tensor_list_from_tensor([3.0, 4.0], element_shape=[])\n    l3 = list_ops.tensor_list_from_tensor([5.0, 6.0], element_shape=[])\n    l4 = list_ops.tensor_list_from_tensor([7.0, 8.0], element_shape=[])\n    a = list_ops.empty_tensor_list(\n        element_dtype=dtypes.variant, element_shape=[])\n    a = list_ops.tensor_list_push_back(a, l1)\n    a = list_ops.tensor_list_push_back(a, l2)\n    b = list_ops.empty_tensor_list(\n        element_dtype=dtypes.variant, element_shape=[])\n    b = list_ops.tensor_list_push_back(b, l3)\n    b = list_ops.tensor_list_push_back(b, l4)\n    result = math_ops.add_n((a, b))\n    result_0 = list_ops.tensor_list_stack(\n        list_ops.tensor_list_get_item(result, 0, element_dtype=dtypes.variant),\n        element_dtype=dtypes.float32)\n    result_1 = list_ops.tensor_list_stack(\n        list_ops.tensor_list_get_item(result, 1, element_dtype=dtypes.variant),\n        element_dtype=dtypes.float32)\n    self.assertAllEqual(self.evaluate(result_0), [6., 8.])\n    self.assertAllEqual(self.evaluate(result_1), [10., 12.])\n\n  def testAddTensorListsFailsIfLeadingDimsMismatch(self):\n    l1 = list_ops.tensor_list_reserve(\n        element_shape=[], element_dtype=dtypes.float32, num_elements=2)\n    l2 = list_ops.tensor_list_reserve(\n        element_shape=[], element_dtype=dtypes.float32, num_elements=3)\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError,\n        \"Trying to add two lists of tensors with different lengths\"):\n      l = math_ops.add_n([l1, l2])\n      self.evaluate(list_ops.tensor_list_stack(l, element_dtype=dtypes.float32))\n\n  @test_util.run_v1_only(\"Uses placeholders\")\n  def testSkipEagerAddTensorListsFailsIfElementShapesMismatch(self):\n    with self.cached_session() as sess:\n      # Use placeholders instead of constant values for shapes to prevent TF's\n      # shape inference from catching this early.\n      l1_element_shape = array_ops.placeholder(dtype=dtypes.int32)\n      l2_element_shape = array_ops.placeholder(dtype=dtypes.int32)\n      l1 = list_ops.tensor_list_reserve(\n          element_shape=l1_element_shape,\n          element_dtype=dtypes.float32,\n          num_elements=3)\n      l2 = list_ops.tensor_list_reserve(\n          element_shape=l2_element_shape,\n          element_dtype=dtypes.float32,\n          num_elements=3)\n      l = math_ops.add_n([l1, l2])\n      with self.assertRaisesRegex(\n          errors.InvalidArgumentError,\n          \"Trying to add two lists of tensors with incompatible element shapes\"\n      ):\n        sess.run(\n            list_ops.tensor_list_stack(l, element_dtype=dtypes.float32), {\n                l1_element_shape: [],\n                l2_element_shape: [2]\n            })\n\n  @test_util.run_deprecated_v1\n  def testSkipEagerConcatShapeInference(self):\n\n    def BuildTensor(element_shape):\n      l = list_ops.empty_tensor_list(\n          element_dtype=dtypes.float32, element_shape=element_shape)\n      return list_ops.tensor_list_concat(l, element_dtype=dtypes.float32)\n\n    self.assertIsNone(BuildTensor(None).shape.rank)\n    self.assertAllEqual(BuildTensor([None, 2, 3]).shape.as_list(), [None, 2, 3])\n    self.assertAllEqual(\n        BuildTensor([None, 2, None]).shape.as_list(), [None, 2, None])\n    self.assertAllEqual(BuildTensor([1, 2, 3]).shape.as_list(), [None, 2, 3])\n\n  def testConcatWithFullyDefinedElementShape(self):\n    l = list_ops.empty_tensor_list(\n        element_dtype=dtypes.float32, element_shape=[2, 2])\n    l = list_ops.tensor_list_push_back(l, [[0., 1.], [2., 3.]])\n    l = list_ops.tensor_list_push_back(l, [[4., 5.], [6., 7.]])\n    t = list_ops.tensor_list_concat(l, element_dtype=dtypes.float32)\n    self.assertAllEqual(\n        self.evaluate(t), [[0., 1.], [2., 3.], [4., 5.], [6., 7.]])\n\n  def testConcatWithNonFullyDefinedElementShape(self):\n    l = list_ops.empty_tensor_list(\n        element_dtype=dtypes.float32, element_shape=[None, 2])\n    l = list_ops.tensor_list_push_back(l, [[0., 1.]])\n    l = list_ops.tensor_list_push_back(l, [[2., 3.], [4., 5.]])\n    t = list_ops.tensor_list_concat(l, element_dtype=dtypes.float32)\n    self.assertAllEqual(self.evaluate(t), [[0., 1.], [2., 3.], [4., 5.]])\n\n  def testConcatWithMismatchingTensorShapesFails(self):\n    l = list_ops.empty_tensor_list(\n        element_dtype=dtypes.float32, element_shape=None)\n    l = list_ops.tensor_list_push_back(l, [[0., 1.]])\n    l = list_ops.tensor_list_push_back(l, [[2.], [4.]])\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError, r\"Incompatible shapes during merge: \"\n        r\"\\[2\\] vs. \\[1\\]\"):\n      t = list_ops.tensor_list_concat(l, element_dtype=dtypes.float32)\n      self.evaluate(t)\n\n  def testConcatEmptyListWithFullyDefinedElementShape(self):\n    l = list_ops.empty_tensor_list(\n        element_dtype=dtypes.float32, element_shape=[5, 2])\n    t = list_ops.tensor_list_concat(l, element_dtype=dtypes.float32)\n    self.assertAllEqual(self.evaluate(t).shape, (0, 2))\n    l = list_ops.empty_tensor_list(\n        element_dtype=dtypes.float32, element_shape=[None, 2])\n    t = list_ops.tensor_list_concat(l, element_dtype=dtypes.float32)\n    self.assertAllEqual(self.evaluate(t).shape, (0, 2))\n\n  def testConcatEmptyListWithUnknownElementShapeFails(self):\n    l = list_ops.empty_tensor_list(\n        element_dtype=dtypes.float32, element_shape=None)\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError,\n        \"All except the first dimension must be fully\"\n        \" defined when concating an empty tensor list\"):\n      t = list_ops.tensor_list_concat(l, element_dtype=dtypes.float32)\n      self.evaluate(t)\n\n  def testConcatEmptyListWithPartiallyDefinedElementShapeFails(self):\n    l = list_ops.empty_tensor_list(\n        element_dtype=dtypes.float32, element_shape=[2, None])\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError,\n        \"All except the first dimension must be fully\"\n        \" defined when concating an empty tensor list\"):\n      t = list_ops.tensor_list_concat(l, element_dtype=dtypes.float32)\n      self.evaluate(t)\n\n  def testConcatListWithScalarElementShapeFails(self):\n    l = list_ops.empty_tensor_list(\n        element_dtype=dtypes.float32,\n        element_shape=tensor_shape.TensorShape([]))\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError,\n        \"Concat requires elements to be at least vectors, \"\n        \"found scalars instead\"):\n      t = list_ops.tensor_list_concat(l, element_dtype=dtypes.float32)\n      self.evaluate(t)\n\n  def testConcatListWithScalarElementsFails(self):\n    l = list_ops.empty_tensor_list(\n        element_dtype=dtypes.float32, element_shape=None)\n    l1 = list_ops.tensor_list_push_back(l, 1.)\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError, \"Concat saw a scalar shape at index 0\"\n        \" but requires at least vectors\"):\n      t = list_ops.tensor_list_concat(l1, element_dtype=dtypes.float32)\n      self.evaluate(t)\n    l1 = list_ops.tensor_list_push_back(l, [1.])\n    l1 = list_ops.tensor_list_push_back(l1, 2.)\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError, \"Concat saw a scalar shape at index 1\"\n        \" but requires at least vectors\"):\n      t = list_ops.tensor_list_concat(l1, element_dtype=dtypes.float32)\n      self.evaluate(t)\n\n  def testConcatWithUninitializedTensorsUseListElementShape(self):\n    l = list_ops.tensor_list_reserve(\n        element_dtype=dtypes.float32, element_shape=[2, 3], num_elements=3)\n    t = list_ops.tensor_list_concat(l, element_dtype=dtypes.float32)\n    self.assertAllEqual(np.zeros((6, 3)), t)\n\n  def testConcatWithUninitializedTensorsUseProvidedElementShape(self):\n    l = list_ops.tensor_list_reserve(\n        element_dtype=dtypes.float32, element_shape=None, num_elements=3)\n    t = list_ops.tensor_list_concat(\n        l, element_dtype=dtypes.float32, element_shape=(2, 3))\n    self.assertAllEqual(np.zeros((6, 3)), t)\n\n  def testConcatWithUninitializedTensorsUseProvidedElementShapeAndLengths(self):\n    l = list_ops.tensor_list_reserve(\n        element_dtype=dtypes.float32, element_shape=None, num_elements=3)\n    t, _ = gen_list_ops.tensor_list_concat_v2(\n        l,\n        element_dtype=dtypes.float32,\n        element_shape=list_ops._build_element_shape((None, 3)),\n        leading_dims=[2, 3, 5])\n    self.assertAllEqual(np.zeros((10, 3)), t)\n    l = list_ops.tensor_list_set_item(l, 1, [[2., 3.], [4., 5.], [6., 7.]])\n    t, _ = gen_list_ops.tensor_list_concat_v2(\n        l,\n        element_dtype=dtypes.float32,\n        element_shape=list_ops._build_element_shape((None, 2)),\n        leading_dims=[2, 3, 4])\n    self.assertAllEqual([[0., 0.], [0., 0.], [2., 3.], [4., 5.], [6., 7.],\n                         [0., 0.], [0., 0.], [0., 0.], [0., 0.]], t)\n\n  def testConcatWithUninitializedTensorsInferShapeFromElements(self):\n    l = list_ops.tensor_list_reserve(\n        element_dtype=dtypes.float32, element_shape=None, num_elements=3)\n    l = list_ops.tensor_list_set_item(l, 1, [[2., 3.], [4., 5.], [6., 7.]])\n    t = list_ops.tensor_list_concat(l, element_dtype=dtypes.float32)\n    self.assertAllEqual([[0., 0.], [0., 0.], [0., 0.], [2., 3.], [4., 5.],\n                         [6., 7.], [0., 0.], [0., 0.], [0., 0.]], t)\n\n  def testConcatWithUninitializedTensorsFailsIfNoElementShape(self):\n    l = list_ops.tensor_list_reserve(\n        element_dtype=dtypes.float32, element_shape=None, num_elements=3)\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError,\n        r\"Trying to concat list with only uninitialized tensors \"\n        r\"but element_shape_except_first_dim is not fully defined\"):\n      t = list_ops.tensor_list_concat(l, element_dtype=dtypes.float32)\n      self.evaluate(t)\n\n  def testConcatWithUninitializedTensorsFailsIfNoInputLengths(self):\n    l = list_ops.tensor_list_reserve(\n        element_dtype=dtypes.float32, element_shape=[None, 3], num_elements=3)\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError,\n        r\"List contains uninitialized tensor at index 0\"\n        r\" but leading_dims has only 0 elements.\"):\n      t = list_ops.tensor_list_concat(l, element_dtype=dtypes.float32)\n      self.evaluate(t)\n\n  def testEmptyTensorListInvalidShape(self):\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                r\"Shape must be at most rank 1 but is rank 2\"):\n      t = gen_list_ops.EmptyTensorList(\n          element_shape=array_ops.ones(dtype=dtypes.int32, shape=[1, 0]),\n          max_num_elements=constant_op.constant(1),\n          element_dtype=dtypes.int32)\n      self.evaluate(t)\n\n  def testEvenSplit(self):\n\n    def RunTest(input_tensor, lengths, expected_stacked_output):\n      l = list_ops.tensor_list_split(\n          input_tensor, element_shape=None, lengths=lengths)\n      self.assertAllEqual(\n          list_ops.tensor_list_stack(l, element_dtype=dtypes.float32),\n          expected_stacked_output)\n\n    RunTest([1., 2., 3.], [1, 1, 1], [[1.], [2.], [3.]])\n    RunTest([1., 2., 3., 4.], [2, 2], [[1., 2.], [3., 4.]])\n    RunTest([[1., 2.], [3., 4.]], [1, 1], [[[1., 2.]], [[3., 4.]]])\n\n  def testUnevenSplit(self):\n    l = list_ops.tensor_list_split([1., 2., 3., 4., 5],\n                                   element_shape=None,\n                                   lengths=[3, 2])\n    self.assertAllEqual(list_ops.tensor_list_length(l), 2)\n    self.assertAllEqual(\n        list_ops.tensor_list_get_item(l, 0, element_dtype=dtypes.float32),\n        [1., 2., 3.])\n    self.assertAllEqual(\n        list_ops.tensor_list_get_item(l, 1, element_dtype=dtypes.float32),\n        [4., 5.])\n\n  @test_util.run_deprecated_v1\n  def testSkipEagerSplitWithInvalidTensorShapeFails(self):\n    with self.cached_session():\n      tensor = array_ops.placeholder(dtype=dtypes.float32)\n      l = list_ops.tensor_list_split(tensor, element_shape=None, lengths=[1])\n      with self.assertRaisesRegex(\n          errors.InvalidArgumentError,\n          r\"Tensor must be at least a vector, but saw shape: \\[\\]\"):\n        l.eval({tensor: 1})\n\n  @test_util.run_deprecated_v1\n  def testSkipEagerSplitWithInvalidLengthsShapeFails(self):\n    with self.cached_session():\n      lengths = array_ops.placeholder(dtype=dtypes.int64)\n      l = list_ops.tensor_list_split([1., 2.],\n                                     element_shape=None,\n                                     lengths=lengths)\n      with self.assertRaisesRegex(\n          errors.InvalidArgumentError,\n          r\"Expected lengths to be a vector, received shape: \\[\\]\"):\n        l.eval({lengths: 1})\n\n  def testSplitWithInvalidLengthsFails(self):\n    with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                r\"Invalid value in lengths: -1\"):\n      l = list_ops.tensor_list_split([1., 2.],\n                                     element_shape=None,\n                                     lengths=[1, -1])\n      self.evaluate(l)\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError,\n        r\"Attempting to slice \\[0, 3\\] from tensor with length 2\"):\n      l = list_ops.tensor_list_split([1., 2.], element_shape=None, lengths=[3])\n      self.evaluate(l)\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError,\n        r\"Unused values in tensor. Length of tensor: 2 Values used: 1\"):\n      l = list_ops.tensor_list_split([1., 2.], element_shape=None, lengths=[1])\n      self.evaluate(l)\n\n  @test_util.run_deprecated_v1\n  def testSkipEagerSplitWithScalarElementShapeFails(self):\n    with self.assertRaisesRegex(ValueError,\n                                r\"Shapes must be equal rank, but are 1 and 0\"):\n      l = list_ops.tensor_list_split([1., 2.], element_shape=[], lengths=[1, 1])\n    with self.cached_session():\n      with self.assertRaisesRegex(\n          errors.InvalidArgumentError,\n          r\"TensorListSplit requires element_shape to be at least of rank 1, \"\n          r\"but saw: \\[\\]\"):\n        element_shape = array_ops.placeholder(dtype=dtypes.int32)\n        l = list_ops.tensor_list_split([1., 2.],\n                                       element_shape=element_shape,\n                                       lengths=[1, 1])\n        l.eval({element_shape: []})\n\n  def testEagerOnlySplitWithScalarElementShapeFails(self):\n    if context.executing_eagerly():\n      with self.assertRaisesRegex(\n          errors.InvalidArgumentError,\n          r\"TensorListSplit requires element_shape to be at least of rank 1, \"\n          r\"but saw: \\[\\]\"):\n        list_ops.tensor_list_split([1., 2.], element_shape=[], lengths=[1, 1])\n\n  @test_util.run_deprecated_v1\n  def testSkipEagerSplitWithIncompatibleTensorShapeAndElementShapeFails(self):\n    with self.assertRaisesRegex(ValueError,\n                                r\"Shapes must be equal rank, but are 2 and 1\"):\n      l = list_ops.tensor_list_split([[1.], [2.]],\n                                     element_shape=[1],\n                                     lengths=[1, 1])\n\n    with self.cached_session():\n      with self.assertRaisesRegex(\n          errors.InvalidArgumentError,\n          r\"tensor shape \\[2,1\\] is not compatible with element_shape \\[1\\]\"):\n        element_shape = array_ops.placeholder(dtype=dtypes.int32)\n        l = list_ops.tensor_list_split([[1.], [2.]],\n                                       element_shape=element_shape,\n                                       lengths=[1, 1])\n        l.eval({element_shape: [1]})\n\n  def testEagerOnlySplitWithIncompatibleTensorShapeAndElementShapeFails(self):\n    if context.executing_eagerly():\n      with self.assertRaisesRegex(\n          errors.InvalidArgumentError,\n          r\"tensor shape \\[2,1\\] is not compatible with element_shape \\[1\\]\"):\n        list_ops.tensor_list_split([[1.], [2.]],\n                                   element_shape=[1],\n                                   lengths=[1, 1])\n\n  def testResizeGrow(self):\n    l = list_ops.tensor_list_from_tensor([1., 2.], element_shape=[])\n    l = list_ops.tensor_list_resize(l, 4)\n    self.assertEqual(self.evaluate(list_ops.tensor_list_length(l)), 4)\n    self.assertEqual(\n        self.evaluate(\n            list_ops.tensor_list_get_item(l, 0, element_dtype=dtypes.float32)),\n        1.)\n    self.assertEqual(\n        self.evaluate(\n            list_ops.tensor_list_get_item(l, 1, element_dtype=dtypes.float32)),\n        2.)\n\n  def testResizeShrink(self):\n    l = list_ops.tensor_list_from_tensor([1., 2., 3.], element_shape=[])\n    l = list_ops.tensor_list_resize(l, 2)\n    self.assertEqual(self.evaluate(list_ops.tensor_list_length(l)), 2)\n    self.assertAllEqual(\n        self.evaluate(\n            list_ops.tensor_list_stack(l, element_dtype=dtypes.float32)),\n        [1., 2.])\n\n  def testResizeWithInvalidSizeFails(self):\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError,\n        \"TensorListSlice expects size to be non-negative\"):\n      l = list_ops.tensor_list_from_tensor([1., 2., 3.], element_shape=[])\n      l = list_ops.tensor_list_resize(l, -1)\n      self.evaluate(l)\n\n  @test_util.run_deprecated_v1\n  @test_util.enable_control_flow_v2\n  def testSkipEagerResizeGrad(self):\n    t = constant_op.constant([1., 2., 3.])\n    l = list_ops.tensor_list_from_tensor(t, element_shape=[])\n    l = list_ops.tensor_list_set_item(\n        l, 3, 4., resize_if_index_out_of_bounds=True)\n    t1 = list_ops.tensor_list_stack(l, element_dtype=dtypes.float32)\n    grad = gradients_impl.gradients(t1, t)[0]\n    self.assertAllEqual(self.evaluate(grad), [1., 1., 1.])\n\n  def testHandleDataAcrossFunctionCall(self):\n\n    @def_function.function\n    def func():\n      t = constant_op.constant([1., 2., 3.])\n      l = list_ops.tensor_list_from_tensor(t, element_shape=[])\n      handle_data = resource_variable_ops.get_eager_safe_handle_data(l)\n      self.assertTrue(handle_data.is_set)\n      self.assertEqual(handle_data.shape_and_type[0].type.type_id,\n                       full_type_pb2.TFT_ARRAY)\n      return l\n\n    tensor_list = func()\n    handle_data = resource_variable_ops.get_eager_safe_handle_data(tensor_list)\n    self.assertTrue(handle_data.is_set)\n    self.assertEqual(dtypes.float32, handle_data.shape_and_type[0].dtype)\n    self.assertEqual(handle_data.shape_and_type[0].type.type_id,\n                     full_type_pb2.TFT_ARRAY)\n    element = list_ops.tensor_list_get_item(\n        tensor_list, 0, element_dtype=dtypes.float32)\n    self.assertAllEqual(element.shape.as_list(), [])\n\n  @test_util.run_gpu_only\n  def testNestedListDevicetoDeviceCopy(self):\n    if context.num_gpus() < 2:\n      self.skipTest(\"Need at least 2 GPUs for this test, found %d\" %\n                    context.num_gpus())\n    with ops.device(\"gpu:0\"):\n      t = constant_op.constant([1.0, 2.0, 3.0])\n      inner_l = list_ops.tensor_list_from_tensor(t, element_shape=[])\n      outer_l = list_ops.empty_tensor_list(\n          element_dtype=dtypes.variant, element_shape=[])\n      outer_l = list_ops.tensor_list_push_back(outer_l, inner_l)\n\n    # Stress test.\n    for _ in range(1024):\n      with ops.device(\"gpu:1\"):\n        outer_l = array_ops.identity(outer_l)\n      with ops.device(\"gpu:0\"):\n        outer_l = array_ops.identity(outer_l)\n\n    with ops.device(\"gpu:1\"):\n      _, inner_l = list_ops.tensor_list_pop_back(\n          outer_l, element_dtype=dtypes.variant)\n      t = list_ops.tensor_list_stack(inner_l, element_dtype=dtypes.float32)\n      self.assertAllEqual(t, [1.0, 2.0, 3.0])\n\n  def testTensorListStrings(self):\n    @def_function.function\n    def f():\n      return map_fn.map_fn(string_ops.string_upper,\n                           constant_op.constant([\"a\", \"b\", \"c\"]))\n\n    self.assertAllEqual(f(), [b\"A\", b\"B\", b\"C\"])\n\n  def testTensorListStringsNoInline(self):\n    # Generator function output type is a variant with a host-only underlying\n    # data type. \"ColocationGraph::AddHostOnlyDataTypesConstraints\" needs to\n    # have \"deep op inspection\" to be able to correctly place the while loop\n    # generated from map_fn.\n    self.skipTest(\"b/150742232\")\n\n    @function.defun_with_attributes(attributes={\"_noinline\": True})\n    def generator():\n      c = constant_op.constant([\"a\", \"b\", \"c\"])\n      return list_ops.tensor_list_from_tensor(c, element_shape=[])\n\n    @def_function.function\n    def f():\n      l = generator()\n\n      def upper(i):\n        e = list_ops.tensor_list_get_item(l, i, element_dtype=dtypes.string)\n        return string_ops.string_upper(e)\n\n      return map_fn.map_fn(\n          upper, constant_op.constant([0, 1, 2]), dtype=dtypes.string)\n\n    self.assertAllEqual(f(), [b\"A\", b\"B\", b\"C\"])\n\n  def testPopBackGrad(self):\n    # https://github.com/tensorflow/tensorflow/issues/37230\n\n    @def_function.function\n    def g(x):\n      x_prod = constant_op.constant([1.])\n      for unused_i in math_ops.range(3):\n        x_prod = x_prod * x\n      return x_prod\n\n    x = constant_op.constant(1.)\n    with backprop.GradientTape() as t:\n      t.watch(x)\n      with backprop.GradientTape() as tt:\n        tt.watch(x)\n        loss = g(x)\n      jac = tt.gradient(loss, x)\n    hess = t.gradient(jac, x)\n    self.assertAllEqual(hess, 6.)\n\n  def testTensorListElementShapeShapeInference(self):\n\n    @def_function.function\n    def f():\n      l = list_ops.empty_tensor_list(\n          element_dtype=dtypes.float32, element_shape=None)\n      l_element_shape = list_ops.tensor_list_element_shape(l, dtypes.int32)\n      self.assertIsNone(l_element_shape.shape.rank)\n      shape_l = list_ops.empty_tensor_list(\n          element_dtype=dtypes.int32, element_shape=l_element_shape.shape)\n      shape_l = list_ops.tensor_list_push_back(shape_l, l_element_shape)\n      return list_ops.tensor_list_pop_back(shape_l, dtypes.int32)[1]\n\n    self.assertAllEqual(f(), -1)\n\n  def testElementShapeArgOfTensorListFromTensor(self):\n\n    @def_function.function\n    def f():\n      t = array_ops.ones([3, 3])\n      l = list_ops.tensor_list_from_tensor(t, element_shape=[-1])\n      l = list_ops.tensor_list_push_back(l, array_ops.ones([4]))\n      read_val = list_ops.tensor_list_get_item(\n          l, 3, element_dtype=dtypes.float32)\n      self.assertAllEqual(read_val.shape.as_list(), [None])\n      return read_val\n\n    self.assertAllEqual(f(), [1.0, 1.0, 1.0, 1.0])\n\n\nif __name__ == \"__main__\":\n  test.main()"