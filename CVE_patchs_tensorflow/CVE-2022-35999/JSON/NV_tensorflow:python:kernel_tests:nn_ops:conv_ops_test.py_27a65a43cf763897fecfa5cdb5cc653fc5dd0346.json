"# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Functional tests for convolutional operations.\"\"\"\n\nimport os\nimport time\n\nimport numpy as np\n\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.core.protobuf import rewriter_config_pb2\nfrom tensorflow.python.client import session as session_lib\nfrom tensorflow.python.eager import backprop\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.layers import convolutional\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_impl\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import variables\nimport tensorflow.python.ops.nn_grad  # pylint: disable=unused-import\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.platform import tf_logging\nfrom tensorflow.python.util.compat import collections_abc\n\n\ndef GetShrunkInceptionShapes(shrink=10):\n  \"\"\"Iterator for smaller versions of convolution shapes in 2015 Inception.\n\n  Relative to inception, each depth value is `depth // shrink`.\n\n  Args:\n    shrink: Factor to shrink each depth value by relative to Inception.\n\n  Yields:\n    Tuple (input_size, filter_size, out_size, stride, padding), the convolution\n    parameters of Inception layers.\n  \"\"\"\n  input_sizes = [[4, 5, 5, 1248], [4, 8, 8, 384], [4, 8, 8, 384],\n                 [4, 8, 8, 2048], [4, 8, 8, 448], [4, 8, 8, 2048],\n                 [4, 8, 8, 2048], [4, 8, 8, 2048], [4, 8, 8, 1760],\n                 [4, 8, 8, 1760], [4, 8, 8, 1760], [4, 8, 8, 1760],\n                 [4, 17, 17, 192], [4, 17, 17, 192], [4, 17, 17, 1248],\n                 [4, 17, 17, 128], [4, 17, 17, 1248], [4, 17, 17, 224],\n                 [4, 17, 17, 192], [4, 17, 17, 192], [4, 17, 17, 1216],\n                 [4, 17, 17, 1216], [4, 17, 17, 224], [4, 17, 17, 192],\n                 [4, 17, 17, 192], [4, 17, 17, 1152], [4, 17, 17, 1152],\n                 [4, 17, 17, 192], [4, 17, 17, 160], [4, 17, 17, 1152],\n                 [4, 17, 17, 1024], [4, 17, 17, 128], [4, 17, 17, 1024],\n                 [4, 17, 17, 128], [4, 17, 17, 1024], [4, 17, 17, 128],\n                 [4, 17, 17, 768], [4, 17, 17, 128], [4, 17, 17, 128],\n                 [4, 17, 17, 768], [4, 17, 17, 768], [4, 35, 35, 96],\n                 [4, 35, 35, 288], [4, 35, 35, 64], [4, 35, 35, 288],\n                 [4, 35, 35, 256], [4, 35, 35, 48], [4, 35, 35, 256],\n                 [4, 35, 35, 96], [4, 35, 35, 192], [4, 35, 35, 192],\n                 [4, 35, 35, 192], [4, 73, 73, 64], [4, 73, 73, 64],\n                 [4, 147, 147, 24]]\n  filter_sizes = [[1, 1, 1248, 128], [1, 3, 384, 384], [3, 1, 384, 384],\n                  [1, 1, 2048, 192], [3, 3, 448, 384], [1, 1, 2048, 320],\n                  [1, 1, 2048, 448], [1, 1, 2048, 384], [1, 1, 1760, 384],\n                  [1, 1, 1760, 192], [1, 1, 1760, 448], [1, 1, 1760, 320],\n                  [3, 3, 192, 192], [3, 3, 192, 192], [1, 1, 1248, 192],\n                  [3, 3, 128, 320], [1, 1, 1248, 128], [1, 3, 224, 224],\n                  [3, 1, 192, 256], [1, 3, 192, 256], [1, 1, 1216, 192],\n                  [1, 1, 1216, 96], [3, 1, 224, 224], [3, 3, 192, 224],\n                  [1, 3, 192, 192], [1, 1, 1152, 192], [1, 1, 1152, 128],\n                  [3, 1, 192, 192], [3, 3, 160, 192], [1, 1, 1152, 160],\n                  [1, 1, 1024, 128], [1, 3, 128, 192], [1, 1, 1024, 160],\n                  [3, 1, 128, 192], [1, 1, 1024, 256], [3, 1, 128, 128],\n                  [1, 1, 768, 192], [1, 3, 128, 128], [3, 3, 128, 128],\n                  [1, 1, 768, 128], [1, 1, 768, 320], [3, 3, 96, 96],\n                  [3, 3, 288, 384], [3, 3, 64, 96], [1, 1, 288, 64],\n                  [1, 1, 256, 64], [5, 5, 48, 64], [1, 1, 256, 48],\n                  [3, 3, 96, 96], [1, 1, 192, 32], [1, 1, 192, 64],\n                  [1, 1, 192, 48], [3, 3, 64, 192], [1, 1, 64, 64],\n                  [1, 1, 24, 64]]\n  out_sizes = [[4, 5, 5, 128], [4, 8, 8, 384], [4, 8, 8, 384],\n               [4, 8, 8, 192], [4, 8, 8, 384], [4, 8, 8, 320],\n               [4, 8, 8, 448], [4, 8, 8, 384], [4, 8, 8, 384],\n               [4, 8, 8, 192], [4, 8, 8, 448], [4, 8, 8, 320],\n               [4, 8, 8, 192], [4, 17, 17, 192], [4, 17, 17, 192],\n               [4, 8, 8, 320], [4, 17, 17, 128], [4, 17, 17, 224],\n               [4, 17, 17, 256], [4, 17, 17, 256], [4, 17, 17, 192],\n               [4, 17, 17, 96], [4, 17, 17, 224], [4, 17, 17, 224],\n               [4, 17, 17, 192], [4, 17, 17, 192], [4, 17, 17, 128],\n               [4, 17, 17, 192], [4, 17, 17, 192], [4, 17, 17, 160],\n               [4, 17, 17, 128], [4, 17, 17, 192], [4, 17, 17, 160],\n               [4, 17, 17, 192], [4, 17, 17, 256], [4, 17, 17, 128],\n               [4, 17, 17, 192], [4, 17, 17, 128], [4, 17, 17, 128],\n               [4, 17, 17, 128], [4, 17, 17, 320], [4, 17, 17, 96],\n               [4, 17, 17, 384], [4, 35, 35, 96], [4, 35, 35, 64],\n               [4, 35, 35, 64], [4, 35, 35, 64], [4, 35, 35, 48],\n               [4, 35, 35, 96], [4, 35, 35, 32], [4, 35, 35, 64],\n               [4, 35, 35, 48], [4, 71, 71, 192], [4, 73, 73, 64],\n               [4, 147, 147, 64]]\n  strides = [\n      1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n      1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1,\n      1, 1, 1, 1, 1\n  ]\n  # Shrink sizes to make the test faster\n  for i in input_sizes:\n    i[3] //= shrink\n  for f in filter_sizes:\n    f[2] //= shrink\n    f[3] //= shrink\n  for o in out_sizes:\n    o[3] //= shrink\n  # pylint: disable=invalid-name\n  VALID = \"VALID\"\n  SAME = \"SAME\"\n  # pylint: enable=invalid-name\n  paddings = [\n      SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME,\n      VALID, SAME, SAME, VALID, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME,\n      SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME,\n      SAME, SAME, SAME, SAME, SAME, VALID, VALID, SAME, SAME, SAME, SAME, SAME,\n      SAME, SAME, SAME, SAME, VALID, VALID, VALID\n  ]\n  for i, f, o, s, p in zip(input_sizes, filter_sizes, out_sizes, strides,\n                           paddings):\n    yield i, f, o, s, p\n\n\ndef GetTestConfigs():\n  \"\"\"Get all the valid tests configs to run.\n\n  Returns:\n    all the valid test configs as tuples of data_format and use_gpu.\n  \"\"\"\n  test_configs = [(\"NHWC\", False), (\"NHWC\", True)]\n  if test.is_gpu_available(cuda_only=True):\n    # \"NCHW\" format is only supported on CUDA.\n    test_configs += [(\"NCHW\", True)]\n  return test_configs\n\n\n@test_util.run_all_without_tensor_float_32(\"Avoid TF32 conv on GPU\")\nclass Conv2DTest(test.TestCase):\n\n  def _DtypesToTest(self, use_gpu):\n    if test_util.IsMklEnabled():\n      return [dtypes.float32]\n    # double datatype is currently not supported for convolution ops\n    # on the ROCm platform\n    optional_float64 = [] if test.is_built_with_rocm() else [dtypes.float64]\n    if use_gpu and not test_util.GpuSupportsHalfMatMulAndConv():\n      return [dtypes.float32] + optional_float64\n    else:\n      # It is important that float32 comes before float16 here,\n      # as we will be using its gradients as reference for fp16 gradients.\n      return [dtypes.float32, dtypes.float16] + optional_float64\n\n  def _CreateNumpyTensor(self, shape):\n    total_size = 1\n    for s in shape:\n      total_size *= s\n    return np.arange(1, total_size + 1, dtype=np.float32).reshape(shape)\n\n  def _SetupValuesForDevice(self, tensor_in_sizes, filter_in_sizes, dilations,\n                            strides, padding, data_format, dtype, use_gpu):\n    \"\"\"Verifies the output values of the convolution function.\n\n    Args:\n      tensor_in_sizes: Input tensor dimensions in\n        [batch, input_rows, input_cols, input_depth].\n      filter_in_sizes: Filter tensor dimensions in\n        [kernel_rows, kernel_cols, input_depth, output_depth].\n      dilations: Dilated rate: [col_dilation, row_dilation]\n      strides: Stride: [col_stride, row_stride]\n      padding: Padding type.\n      data_format: Format of the data tensors.\n      dtype: Data type for inputs and outputs.\n      use_gpu: True if the operations should be run on GPU\n    Returns:\n      Symbolic tensor value that can be used to execute the computation\n    \"\"\"\n    x1 = self._CreateNumpyTensor(tensor_in_sizes)\n    x2 = self._CreateNumpyTensor(filter_in_sizes)\n\n    with test_util.device(use_gpu):\n      t1 = constant_op.constant(x1, shape=tensor_in_sizes, dtype=dtype)\n      t2 = constant_op.constant(x2, shape=filter_in_sizes, dtype=dtype)\n      strides = [1] + strides + [1]\n      dilations = [1] + dilations + [1]\n      if isinstance(padding, (list, tuple)):\n        padding = [(0, 0)] + padding + [(0, 0)]\n      if data_format == \"NCHW\":\n        t1 = test_util.NHWCToNCHW(t1)\n        strides = test_util.NHWCToNCHW(strides)\n        dilations = test_util.NHWCToNCHW(dilations)\n        if isinstance(padding, (list, tuple)):\n          padding = test_util.NHWCToNCHW(padding)\n      conv = nn_ops.conv2d(\n          t1,\n          t2,\n          dilations=dilations,\n          strides=strides,\n          padding=padding,\n          data_format=data_format)\n      self.assertEqual(conv.dtype, dtype)\n      if data_format == \"NCHW\":\n        conv = test_util.NCHWToNHWC(conv)\n\n      return conv\n\n  def _CompareFwdValues(self, tensor_in_sizes, filter_in_sizes, conv_strides,\n                        padding):\n    \"\"\"Verifies that CPU and GPU produce the same values.\n\n    Args:\n      tensor_in_sizes: Input tensor dimensions in\n        [batch, input_rows, input_cols, input_depth].\n      filter_in_sizes: Filter tensor dimensions in\n        [kernel_rows, kernel_cols, input_depth, output_depth].\n      conv_strides: [row_stride, col_stride] for the convolution;\n      padding: Padding type.\n    \"\"\"\n    x1 = np.random.rand(*tensor_in_sizes).astype(np.float32)\n    x2 = np.random.rand(*filter_in_sizes).astype(np.float32)\n\n    def _SetupVal(data_format, use_gpu):\n      with test_util.device(use_gpu):\n        t1 = constant_op.constant(x1, shape=tensor_in_sizes)\n        t2 = constant_op.constant(x2, shape=filter_in_sizes)\n        strides = [1] + conv_strides + [1]\n        if data_format == \"NCHW\":\n          t1 = test_util.NHWCToNCHW(t1)\n          strides = test_util.NHWCToNCHW(strides)\n        conv = nn_ops.conv2d(\n            t1, t2, strides=strides, padding=padding, data_format=data_format)\n        if data_format == \"NCHW\":\n          conv = test_util.NCHWToNHWC(conv)\n        return conv\n\n    tensors = []\n    for (data_format, use_gpu) in GetTestConfigs():\n      tensors.append(_SetupVal(data_format, use_gpu))\n    values = self.evaluate(tensors)\n    for i in range(1, len(values)):\n      self.assertAllClose(values[0], values[i], rtol=1e-3, atol=1e-3)\n\n  def _ComputeReferenceDilatedConv(self, tensor_in_sizes, filter_in_sizes,\n                                   stride, dilation, padding, data_format,\n                                   use_gpu):\n    x1 = self._CreateNumpyTensor(tensor_in_sizes)\n    x2 = self._CreateNumpyTensor(filter_in_sizes)\n    with test_util.device(use_gpu):\n      t1 = constant_op.constant(x1, shape=tensor_in_sizes)\n      t2 = constant_op.constant(x2, shape=filter_in_sizes)\n      if isinstance(stride, collections_abc.Iterable):\n        strides = list(stride)\n      else:\n        strides = [stride, stride]\n      if data_format == \"NCHW\":\n        t1 = test_util.NHWCToNCHW(t1)\n        full_strides = [1, 1] + strides\n        full_dilation = [1, 1] + dilation\n      else:\n        full_strides = [1] + strides + [1]\n        full_dilation = [1] + dilation + [1]\n      expected = nn_ops.convolution(\n          t1,\n          t2,\n          padding=padding,\n          strides=strides,\n          dilation_rate=dilation,\n          data_format=data_format)\n      computed = nn_ops.conv2d(\n          t1,\n          t2,\n          strides=full_strides,\n          dilations=full_dilation,\n          padding=padding,\n          data_format=data_format)\n      if data_format == \"NCHW\":\n        expected = test_util.NCHWToNHWC(expected)\n        computed = test_util.NCHWToNHWC(computed)\n    return expected, computed\n\n  def _VerifyDilatedConvValues(self, tensor_in_sizes, filter_in_sizes, strides,\n                               padding, dilations, rtol=1e-4):\n    expected_results = []\n    computed_results = []\n    for data_format, use_gpu in GetTestConfigs():\n      expected, computed = self._ComputeReferenceDilatedConv(\n          tensor_in_sizes, filter_in_sizes, strides, dilations, padding,\n          data_format, use_gpu)\n      expected_results.append(expected)\n      computed_results.append(computed)\n    tolerance = 1e-2 if use_gpu else 1e-5\n    expected_values = self.evaluate(expected_results)\n    computed_values = self.evaluate(computed_results)\n    for e_value, c_value in zip(expected_values, computed_values):\n      tf_logging.debug(\"expected = %s\", e_value)\n      tf_logging.debug(\"actual = %s\", c_value)\n      self.assertAllClose(\n          e_value.flatten(), c_value.flatten(), atol=tolerance, rtol=rtol)\n\n  def _VerifyValues(self,\n                    tensor_in_sizes,\n                    filter_in_sizes,\n                    strides,\n                    padding,\n                    expected,\n                    dilations=(1, 1),\n                    gpu_only=False,\n                    test_grappler_layout_optimizer=False,\n                    tol=1e-5,\n                    fp16_tol=1e-3):\n    if gpu_only and not test.is_gpu_available(cuda_only=True):\n      return\n    tensors = []\n    dilations = list(dilations)\n    for (data_format, use_gpu) in GetTestConfigs():\n      if gpu_only and not use_gpu:\n        continue\n      dtypes_to_test = self._DtypesToTest(use_gpu)\n      if not test_grappler_layout_optimizer and data_format == \"NHWC\":\n        dtypes_to_test.append(dtypes.int32)\n      for dtype in dtypes_to_test:\n        result = self._SetupValuesForDevice(\n            tensor_in_sizes,\n            filter_in_sizes,\n            dilations,\n            strides,\n            padding,\n            data_format,\n            dtype,\n            use_gpu=use_gpu)\n        if test_grappler_layout_optimizer and data_format == \"NHWC\" and use_gpu:\n          # Grappler's layout optimizer will not optimize a fetch node, so\n          # this identity allows Grappler to optimize the Conv2D node.\n          result = array_ops.identity(result)\n        tensors.append(result)\n      values = self.evaluate(tensors)\n      for i in range(len(tensors)):\n        conv = tensors[i]\n        value = values[i]\n        tf_logging.debug(\"expected = %s\", expected)\n        tf_logging.debug(\"actual = %s\", value)\n        tol_to_use = fp16_tol if value.dtype == np.float16 else tol\n        if np.issubdtype(value.dtype, np.integer):\n          self.assertAllEqual(np.rint(expected), np.ravel(value))\n        else:\n          self.assertAllClose(expected, np.ravel(value), atol=tol_to_use,\n                              rtol=tol_to_use)\n        self.assertShapeEqual(value, conv)\n        self.assertEqual(value.dtype, conv.dtype.as_numpy_dtype)\n\n  def _VerifyExplicitPaddings(self,\n                              tensor_in_sizes,\n                              filter_in_sizes,\n                              strides,\n                              padding,\n                              dilations=(1, 1),\n                              test_grappler_layout_optimizer=False,\n                              tol=1e-5,\n                              fp16_tol=1e-3):\n    \"\"\"Verifies Conv2D with explicit padding generates correct values.\n\n    It does this by comparing with Conv2D without explicit padding. This\n    function assumes Conv2D without explicit padding works correctly.\n\n    Args:\n      tensor_in_sizes: Input tensor dimensions in [batch, input_rows,\n        input_cols, input_depth].\n      filter_in_sizes: Filter tensor dimensions in [kernel_rows, kernel_cols,\n        input_depth, output_depth].\n      strides: [row_stride, col_stride] for the convolution;\n      padding: Explicit padding amounts.\n      dilations: Dilation values\n      test_grappler_layout_optimizer: If True, allow the Grappler layout\n        optimizer to run, which turns NHWC Conv2Ds on the GPU to NCHW Conv2Ds.\n      tol: The absolute and relative tolerance for non-fp16 dtypes.\n      fp16_tol: The absolute and relative tolerance for fp16.\n    \"\"\"\n    input_tensor = self._CreateNumpyTensor(tensor_in_sizes)\n    filter_tensor = self._CreateNumpyTensor(filter_in_sizes)\n    input_tensor = array_ops.pad(input_tensor, [(0, 0)] + padding + [(0, 0)])\n    dilations = list(dilations)\n    conv2d_result = nn_ops.conv2d(\n        input_tensor,\n        filter_tensor, [1] + list(strides) + [1],\n        \"VALID\",\n        dilations=[1] + dilations + [1])\n    expected = list(self.evaluate(array_ops.reshape(conv2d_result, [-1])))\n    self._VerifyValues(\n        tensor_in_sizes,\n        filter_in_sizes,\n        strides,\n        padding,\n        expected,\n        dilations,\n        test_grappler_layout_optimizer=test_grappler_layout_optimizer,\n        tol=tol,\n        fp16_tol=fp16_tol)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D1x1Filter(self):\n    expected_output = [\n        30.0, 36.0, 42.0, 66.0, 81.0, 96.0, 102.0, 126.0, 150.0, 138.0, 171.0,\n        204.0, 174.0, 216.0, 258.0, 210.0, 261.0, 312.0\n    ]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[1, 1, 3, 3],\n        strides=[1, 1],\n        padding=\"VALID\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DExpandedBatch(self):\n    tensor_in_sizes_batch = [10, 2, 3, 3]\n    tensor_in_sizes_expanded_batch = [2, 5, 2, 3, 3]\n    filter_in_sizes = [1, 1, 3, 3]\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    x1 = self._CreateNumpyTensor(tensor_in_sizes_batch)\n    x2 = x1.reshape(tensor_in_sizes_expanded_batch)\n    conv1 = nn_ops.conv2d(\n        x1,\n        filter_in,\n        strides=[1, 1],\n        padding=\"VALID\")\n    conv2 = nn_ops.conv2d(\n        x2,\n        filter_in,\n        strides=[1, 1],\n        padding=\"VALID\")\n    self.assertEqual(conv1.shape, tensor_in_sizes_batch)\n    self.assertEqual(conv2.shape, tensor_in_sizes_expanded_batch)\n    self.assertAllEqual(\n        conv1,\n        self.evaluate(conv2).reshape(conv1.shape))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConvolutionClass2DExpandedBatch(self):\n    tensor_in_sizes_batch = [10, 2, 3, 3]\n    tensor_in_sizes_expanded_batch = [2, 5, 2, 3, 3]\n    filter_in_sizes = [1, 1, 3, 3]\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    x1 = self._CreateNumpyTensor(tensor_in_sizes_batch)\n    x2 = x1.reshape(tensor_in_sizes_expanded_batch)\n    convolver1 = nn_ops.Convolution(\n        input_shape=x1.shape,\n        filter_shape=filter_in.shape,\n        strides=[1, 1],\n        padding=\"VALID\")\n    self.assertEqual(convolver1.num_batch_dims, 1)\n    convolver2 = nn_ops.Convolution(\n        input_shape=x2.shape,\n        filter_shape=filter_in.shape,\n        strides=[1, 1],\n        padding=\"VALID\")\n    self.assertEqual(convolver2.num_batch_dims, 2)\n    conv1 = convolver1(x1, filter_in)\n    conv2 = convolver2(x2, filter_in)\n    self.assertEqual(conv1.shape, tensor_in_sizes_batch)\n    self.assertEqual(conv2.shape, tensor_in_sizes_expanded_batch)\n    self.assertAllEqual(\n        conv1,\n        self.evaluate(conv2).reshape(conv1.shape))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConvolutionWith2SpatialDimensionsAndExpandedBatch(self):\n    tensor_in_sizes_batch = [10, 2, 3, 3]\n    tensor_in_sizes_expanded_batch = [2, 5, 2, 3, 3]\n    filter_in_sizes = [1, 1, 3, 3]\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    x1 = self._CreateNumpyTensor(tensor_in_sizes_batch)\n    x2 = x1.reshape(tensor_in_sizes_expanded_batch)\n    conv1 = nn_ops.convolution(\n        x1,\n        filter_in,\n        strides=[1, 1],\n        padding=\"VALID\")\n    conv2 = nn_ops.convolution(\n        x2,\n        filter_in,\n        strides=[1, 1],\n        padding=\"VALID\")\n    self.assertEqual(conv1.shape, tensor_in_sizes_batch)\n    self.assertEqual(conv2.shape, tensor_in_sizes_expanded_batch)\n    self.assertAllEqual(\n        conv1,\n        self.evaluate(conv2).reshape(conv1.shape))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Filter2x1Dilation(self):\n    self._VerifyDilatedConvValues(\n        tensor_in_sizes=[1, 4, 4, 1],\n        filter_in_sizes=[2, 2, 1, 1],\n        strides=[1, 1],\n        dilations=[2, 1],\n        padding=\"VALID\")\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DEmpty(self):\n    expected_output = []\n    self._VerifyValues(\n        tensor_in_sizes=[0, 2, 3, 3],\n        filter_in_sizes=[1, 1, 3, 3],\n        strides=[1, 1],\n        padding=\"VALID\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DEmptyDilation(self):\n    self._VerifyDilatedConvValues(\n        tensor_in_sizes=[0, 2, 3, 3],\n        filter_in_sizes=[1, 1, 3, 3],\n        strides=[1, 1],\n        dilations=[2, 1],\n        padding=\"VALID\")\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Filter(self):\n    # The outputs are computed using third_party/py/IPython/notebook.\n    expected_output = [2271.0, 2367.0, 2463.0, 2901.0, 3033.0, 3165.0]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[2, 2, 3, 3],\n        strides=[1, 1],\n        padding=\"VALID\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2FilterDilation(self):\n    self._VerifyDilatedConvValues(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[2, 2, 3, 3],\n        strides=[1, 1],\n        dilations=[1, 2],\n        padding=\"VALID\")\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D1x2Filter(self):\n    # The outputs are computed using third_party/py/IPython/notebook.\n    expected_output = [\n        231.0, 252.0, 273.0, 384.0, 423.0, 462.0, 690.0, 765.0, 840.0, 843.0,\n        936.0, 1029.0\n    ]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[1, 2, 3, 3],\n        strides=[1, 1],\n        padding=\"VALID\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D1x2FilterDilation(self):\n    self._VerifyDilatedConvValues(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[1, 2, 3, 3],\n        strides=[1, 1],\n        dilations=[2, 1],\n        padding=\"VALID\")\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2FilterStride2(self):\n    expected_output = [2271.0, 2367.0, 2463.0]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[2, 2, 3, 3],\n        strides=[2, 2],\n        padding=\"VALID\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2FilterStride2Same(self):\n    expected_output = [2271.0, 2367.0, 2463.0, 1230.0, 1305.0, 1380.0]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[2, 2, 3, 3],\n        strides=[2, 2],\n        padding=\"SAME\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2FilterStride1x2(self):\n    expected_output = [58.0, 78.0, 98.0, 118.0, 138.0, 158.0]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 3, 6, 1],\n        filter_in_sizes=[2, 2, 1, 1],\n        strides=[1, 2],\n        padding=\"VALID\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DKernelSmallerThanStrideValid(self):\n    expected_output = [65, 95, 275, 305]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 7, 7, 1],\n        filter_in_sizes=[2, 2, 1, 1],\n        strides=[3, 3],\n        padding=\"VALID\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DKernelSmallerThanStrideSame(self):\n    self._VerifyValues(\n        tensor_in_sizes=[1, 3, 3, 1],\n        filter_in_sizes=[1, 1, 1, 1],\n        strides=[2, 2],\n        padding=\"SAME\",\n        expected=[1, 3, 7, 9])\n\n    self._VerifyValues(\n        tensor_in_sizes=[1, 4, 4, 1],\n        filter_in_sizes=[1, 1, 1, 1],\n        strides=[2, 2],\n        padding=\"SAME\",\n        expected=[1, 3, 9, 11])\n\n    self._VerifyValues(\n        tensor_in_sizes=[1, 4, 4, 1],\n        filter_in_sizes=[2, 2, 1, 1],\n        strides=[3, 3],\n        padding=\"SAME\",\n        expected=[44, 28, 41, 16])\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DKernelSizeMatchesInputSize(self):\n    self._VerifyValues(\n        tensor_in_sizes=[1, 2, 2, 1],\n        filter_in_sizes=[2, 2, 1, 2],\n        strides=[1, 1],\n        padding=\"VALID\",\n        expected=[50, 60])\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DKernelSizeMatchesInputSizeDilation(self):\n    self._VerifyDilatedConvValues(\n        tensor_in_sizes=[1, 3, 3, 1],\n        filter_in_sizes=[2, 2, 1, 2],\n        strides=[1, 1],\n        dilations=[2, 2],\n        padding=\"VALID\")\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D0x0Padding(self):\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[2, 2, 3, 3],\n        strides=[1, 1],\n        padding=[[0, 0], [0, 0]])\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[3, 4, 3, 2],\n        filter_in_sizes=[1, 1, 2, 1],\n        strides=[2, 2],\n        padding=[[0, 0], [0, 0]])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D1x1Padding(self):\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 3, 2],\n        filter_in_sizes=[2, 2, 2, 2],\n        strides=[1, 1],\n        padding=[[1, 1], [1, 1]])\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 2, 1],\n        filter_in_sizes=[1, 1, 1, 2],\n        strides=[1, 1],\n        padding=[[1, 1], [1, 1]])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Padding(self):\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 1, 2],\n        filter_in_sizes=[2, 1, 2, 1],\n        strides=[1, 1],\n        padding=[[2, 2], [2, 2]])\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 1, 2],\n        filter_in_sizes=[1, 1, 2, 1],\n        strides=[2, 1],\n        padding=[[2, 2], [2, 2]])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2DOnlyBottomPadding(self):\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[2, 2, 3, 2],\n        strides=[1, 1],\n        padding=[[0, 3], [0, 0]], tol=2e-5)\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[2, 2, 4, 3],\n        filter_in_sizes=[1, 2, 3, 2],\n        strides=[2, 2],\n        padding=[[0, 3], [0, 0]])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2DOnlyTopRightPadding(self):\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[2, 2, 3, 2],\n        strides=[1, 1],\n        padding=[[1, 0], [0, 2]],\n        tol=5e-5)\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 4, 2],\n        filter_in_sizes=[2, 2, 2, 2],\n        strides=[1, 3],\n        padding=[[1, 0], [0, 2]])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2DLotsPadding(self):\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 1, 1, 3],\n        filter_in_sizes=[2, 2, 3, 3],\n        strides=[1, 1],\n        padding=[[3, 4], [4, 2]])\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 1, 1],\n        filter_in_sizes=[2, 2, 1, 3],\n        strides=[2, 1],\n        padding=[[3, 4], [4, 2]])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2DExplicitPaddingWithDilations(self):\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 3, 2, 1],\n        filter_in_sizes=[1, 2, 1, 2],\n        strides=[1, 1],\n        padding=[[1, 0], [0, 1]],\n        dilations=[2, 1])\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 3, 2],\n        filter_in_sizes=[3, 2, 2, 1],\n        strides=[1, 1],\n        padding=[[2, 1], [1, 2]],\n        dilations=[2, 3])\n\n  def testConv2DExplicitPaddingWithLayoutOptimizer(self):\n    # Test with Grappler's layout optimizer, to ensure the layout optimizer\n    # handles explicit padding correctly.\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 3, 2, 1],\n        filter_in_sizes=[1, 2, 1, 2],\n        strides=[1, 1],\n        padding=[[1, 0], [0, 1]],\n        dilations=[2, 1],\n        test_grappler_layout_optimizer=True)\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 3, 2],\n        filter_in_sizes=[3, 2, 2, 1],\n        strides=[1, 1],\n        padding=[[2, 1], [1, 2]],\n        dilations=[2, 3],\n        test_grappler_layout_optimizer=True)\n\n  def _VerifyGroupConvFwd(self, tensor_in_sizes, filter_in_sizes, dilations,\n                          strides, padding, data_format, dtype):\n    \"\"\"Verify the output of group convolution is equal to a for-loop implementation.\n\n    Args:\n      tensor_in_sizes: Input tensor dimensions in [batch, input_rows,\n        input_cols, input_depth].\n      filter_in_sizes: Filter tensor dimensions in [kernel_rows, kernel_cols,\n        input_depth, output_depth].\n      dilations: Dilated rate: [col_dilation, row_dilation]\n      strides: Stride: [col_stride, row_stride]\n      padding: Padding type.\n      data_format: Format of the data tensors.\n      dtype: Data type for inputs and outputs.\n    \"\"\"\n    tensor_in = self._CreateNumpyTensor(tensor_in_sizes)\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    num_groups = tensor_in_sizes[3] // filter_in_sizes[2]\n    assert num_groups > 1 and \\\n        filter_in_sizes[2] * num_groups == tensor_in_sizes[3]\n    with test_util.device(True):\n      t1 = constant_op.constant(tensor_in, dtype=dtype)\n      t2 = constant_op.constant(filter_in, dtype=dtype)\n      strides = [1] + strides + [1]\n      dilations = [1] + dilations + [1]\n      if data_format == \"NCHW\":\n        t1 = test_util.NHWCToNCHW(t1)\n        strides = test_util.NHWCToNCHW(strides)\n        dilations = test_util.NHWCToNCHW(dilations)\n        t1_splits = array_ops.split(t1, num_groups, axis=1)\n      else:\n        t1_splits = array_ops.split(t1, num_groups, axis=3)\n      t2_splits = array_ops.split(t2, num_groups, axis=3)\n\n      def MakeConv2d(inputs, filters):\n        return nn_ops.conv2d(\n            inputs,\n            filters,\n            strides,\n            padding,\n            dilations=dilations,\n            data_format=data_format)\n\n      group_conv = MakeConv2d(t1, t2)\n      group_conv_loop = array_ops.concat(\n          [MakeConv2d(t1s, t2s) for t1s, t2s in zip(t1_splits, t2_splits)],\n          axis=1 if data_format == \"NCHW\" else 3)\n\n      results = self.evaluate([group_conv, group_conv_loop])\n      tol_to_use = 1e-5\n      self.assertAllClose(\n          results[0], results[1], atol=tol_to_use, rtol=tol_to_use)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DGroupConvFwd(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      data_formats = [\"NHWC\", \"NCHW\"]\n    else:\n      data_formats = [\"NHWC\"]\n    for data_format in data_formats:\n      for dilation in [1, 2]:\n        for stride in [1, 2]:\n          for filter_dims in [[3, 3, 4, 8], [1, 1, 2, 16]]:\n            self._VerifyGroupConvFwd([10, 32, 32, 16], filter_dims,\n                                     dilations=[dilation, dilation],\n                                     strides=[stride, stride],\n                                     padding=\"SAME\",\n                                     data_format=data_format,\n                                     dtype=dtypes.float32)\n\n  @test_util.deprecated_graph_mode_only\n  @test_util.run_cuda_only\n  def testInputGradientGroupConv(self):\n    for data_format in [\"NCHW\", \"NHWC\"]:\n      for test_input in [True, False]:\n        self.ConstructAndTestGradient(\n            batch=2,\n            input_rows=5,\n            input_cols=4,\n            filter_rows=3,\n            filter_cols=3,\n            num_groups=2,\n            padding=\"VALID\",\n            in_depth=4,\n            out_depth=6,\n            stride_rows=1,\n            stride_cols=1,\n            test_input=test_input,\n            data_format=data_format,\n            use_gpu=True,\n            max_err=0.005)\n\n  @test_util.deprecated_graph_mode_only\n  @test_util.run_cuda_only\n  def testFilterGradientGroupConv(self):\n    for data_format in [\"NCHW\", \"NHWC\"]:\n      for test_input in [True, False]:\n        self.ConstructAndTestGradient(\n            batch=2,\n            input_rows=5,\n            input_cols=4,\n            filter_rows=3,\n            filter_cols=3,\n            num_groups=2,\n            padding=\"VALID\",\n            in_depth=4,\n            out_depth=6,\n            stride_rows=1,\n            stride_cols=1,\n            test_input=test_input,\n            data_format=data_format,\n            use_gpu=True,\n            max_err=0.005)\n  # TODO(yzhwang): this currently fails.\n  # self._VerifyValues(tensor_in_sizes=[1, 8, 8, 1],\n  #                   filter_in_sizes=[2, 2, 1, 1],\n  #                   strides=[4, 4], padding=\"SAME\",\n  #                   expected=[72, 112, 392, 432])\n\n  # Testing for backprops\n  def _RunAndVerifyBackpropInput(self,\n                                 input_sizes,\n                                 filter_sizes,\n                                 output_sizes,\n                                 strides,\n                                 padding,\n                                 expected,\n                                 data_format,\n                                 use_gpu,\n                                 err,\n                                 dilations=(1, 1)):\n    if use_gpu and not test.is_gpu_available(cuda_only=True):\n      return\n    x1 = self._CreateNumpyTensor(filter_sizes)\n    x2 = self._CreateNumpyTensor(output_sizes)\n    dilations = list(dilations)\n    with test_util.device(use_gpu):\n      if len(input_sizes) == 4:\n        if data_format == \"NCHW\":\n          input_sizes = test_util.NHWCToNCHW(input_sizes)\n      t0 = constant_op.constant(input_sizes, shape=[len(input_sizes)])\n      t1 = constant_op.constant(x1, shape=filter_sizes)\n      t2 = constant_op.constant(x2, shape=output_sizes)\n      strides = [1] + strides + [1]\n      dilations = [1] + dilations + [1]\n      if isinstance(padding, (list, tuple)):\n        padding = [(0, 0)] + padding + [(0, 0)]\n      if data_format == \"NCHW\":\n        t2 = test_util.NHWCToNCHW(t2)\n        strides = test_util.NHWCToNCHW(strides)\n        dilations = test_util.NHWCToNCHW(dilations)\n        if isinstance(padding, (list, tuple)):\n          padding = test_util.NHWCToNCHW((padding))\n      conv = nn_ops.conv2d_backprop_input(\n          t0,\n          t1,\n          t2,\n          strides=strides,\n          padding=padding,\n          data_format=data_format,\n          dilations=dilations)\n      if data_format == \"NCHW\":\n        conv = test_util.NCHWToNHWC(conv)\n      # \"values\" consists of two tensors for two backprops\n      value = self.evaluate(conv)\n      self.assertShapeEqual(value, conv)\n    tf_logging.debug(\"expected = %s\", expected)\n    tf_logging.debug(\"actual = %s\", value)\n    self.assertAllCloseAccordingToType(expected, value.flatten(), atol=1e-5)\n\n  def _CompareBackpropInput(self, input_sizes, filter_sizes, output_sizes,\n                            conv_strides, padding):\n    x1 = np.random.rand(*filter_sizes).astype(np.float32)\n    x2 = np.random.rand(*output_sizes).astype(np.float32)\n\n    def _GetVal(data_format, use_gpu):\n      with test_util.device(use_gpu):\n        if data_format == \"NCHW\":\n          new_input_sizes = test_util.NHWCToNCHW(input_sizes)\n        else:\n          new_input_sizes = input_sizes\n        t0 = constant_op.constant(new_input_sizes, shape=[len(new_input_sizes)])\n        t1 = constant_op.constant(x1, shape=filter_sizes)\n        t2 = constant_op.constant(x2, shape=output_sizes)\n        strides = [1] + conv_strides + [1]\n        if data_format == \"NCHW\":\n          t2 = test_util.NHWCToNCHW(t2)\n          strides = test_util.NHWCToNCHW(strides)\n        conv = nn_ops.conv2d_backprop_input(\n            t0,\n            t1,\n            t2,\n            strides=strides,\n            padding=padding,\n            data_format=data_format)\n        if data_format == \"NCHW\":\n          conv = test_util.NCHWToNHWC(conv)\n        ret = self.evaluate(conv)\n        self.assertShapeEqual(ret, conv)\n        return ret\n\n    values = []\n    for (data_format, use_gpu) in GetTestConfigs():\n      values.append(_GetVal(data_format, use_gpu))\n\n    for i in range(1, len(values)):\n      self.assertAllClose(values[0], values[i], rtol=1e-2, atol=1e-2)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Depth1ValidBackpropInput(self):\n    expected_output = [1.0, 4.0, 4.0, 3.0, 10.0, 8.0]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInput(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 1, 2, 1],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DEmptyBackpropInput(self):\n    expected_output = []\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInput(\n          input_sizes=[0, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[0, 1, 2, 1],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Depth3ValidBackpropInput(self):\n    expected_output = [\n        14.0, 32.0, 50.0, 100.0, 163.0, 226.0, 167.0, 212.0, 257.0, 122.0,\n        140.0, 158.0, 478.0, 541.0, 604.0, 437.0, 482.0, 527.0\n    ]\n    for (data_format, use_gpu) in GetTestConfigs():\n      # The GPU version of this test is not very stable. So adjusting the\n      # error threshold to 1e-4.\n      self._RunAndVerifyBackpropInput(\n          input_sizes=[1, 2, 3, 3],\n          filter_sizes=[2, 2, 3, 3],\n          output_sizes=[1, 1, 2, 3],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-4)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Depth3ValidBackpropInputStride1x2(self):\n    expected_output = [\n        1.0, 2.0, 2.0, 4.0, 3.0, 6.0, 7.0, 12.0, 11.0, 18.0, 15.0, 24.0, 12.0,\n        16.0, 15.0, 20.0, 18.0, 24.0\n    ]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInput(\n          input_sizes=[1, 3, 6, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 2, 3, 1],\n          strides=[1, 2],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DStrideTwoFilterOneSameBackpropInput(self):\n    expected_output = [\n        1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 0.0, 4.0, 0.0, 0.0, 0.0,\n        0.0, 0.0\n    ]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInput(\n          input_sizes=[1, 4, 4, 1],\n          filter_sizes=[1, 1, 1, 1],\n          output_sizes=[1, 2, 2, 1],\n          strides=[2, 2],\n          padding=\"SAME\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DKernelSizeMatchesInputSizeBackpropInput(self):\n    expected_output = [5.0, 11.0, 17.0, 23.0]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInput(\n          input_sizes=[1, 2, 2, 1],\n          filter_sizes=[2, 2, 1, 2],\n          output_sizes=[1, 1, 1, 2],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  @test_util.disable_xla(\"XLA requires input_sizes to be a 4D shape.\")\n  def testConv2DInputSizesContainsOnlySpatialDimensionsBackpropInput(self):\n    expected_output = [5.0, 11.0, 17.0, 23.0]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInput(\n          input_sizes=[2, 2],\n          filter_sizes=[2, 2, 1, 2],\n          output_sizes=[1, 1, 1, 2],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  @test_util.disable_xla(\"b/239598470\")\n  def testConv2DBackpropInputDegenerateBackpropInput(self):\n    input_sizes = [3, 1, 1, 2]\n    expected_output = np.zeros(input_sizes).flatten()\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInput(\n          input_sizes=input_sizes,\n          filter_sizes=[1, 3, 2, 3],\n          output_sizes=[3, 1, 0, 3],\n          strides=[1, 2],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-5)\n\n  # Testing for backprops\n  def _RunAndVerifyBackpropFilter(self,\n                                  input_sizes,\n                                  filter_sizes,\n                                  output_sizes,\n                                  strides,\n                                  padding,\n                                  expected,\n                                  data_format,\n                                  use_gpu,\n                                  dilations=(1, 1),\n                                  err=1e-5):\n    x0 = self._CreateNumpyTensor(input_sizes)\n    x2 = self._CreateNumpyTensor(output_sizes)\n    dilations = list(dilations)\n    explicit_strides = [1] + strides + [1]\n    new_padding = padding\n    new_dilations = [1] + dilations + [1]\n    if isinstance(new_padding, (list, tuple)):\n      new_padding = [(0, 0)] + new_padding + [(0, 0)]\n    if data_format == \"NCHW\":\n      explicit_strides = test_util.NHWCToNCHW(explicit_strides)\n      new_dilations = test_util.NHWCToNCHW(new_dilations)\n      if isinstance(padding, (list, tuple)):\n        new_padding = test_util.NHWCToNCHW(new_padding)\n    for dtype in self._DtypesToTest(use_gpu=use_gpu):\n      with test_util.device(use_gpu):\n        t0 = constant_op.constant(x0, shape=input_sizes, dtype=dtype)\n        t1 = constant_op.constant(filter_sizes, shape=[len(filter_sizes)])\n        t2 = constant_op.constant(x2, shape=output_sizes, dtype=dtype)\n        if data_format == \"NCHW\":\n          t0 = test_util.NHWCToNCHW(t0)\n          t2 = test_util.NHWCToNCHW(t2)\n        conv = nn_ops.conv2d_backprop_filter(\n            t0,\n            t1,\n            t2,\n            strides=explicit_strides,\n            padding=new_padding,\n            dilations=new_dilations,\n            data_format=data_format)\n        value = self.evaluate(conv)\n        self.assertShapeEqual(value, conv)\n      tf_logging.debug(\"expected = %s\", expected)\n      tf_logging.debug(\"actual = %s\", value)\n      self.assertArrayNear(expected, value.flatten(), err)\n\n  def _CompareBackFilter(self, input_sizes, filter_sizes, output_sizes,\n                         conv_strides, padding):\n    x0 = np.random.rand(*input_sizes).astype(np.float32)\n    x2 = np.random.rand(*output_sizes).astype(np.float32)\n\n    def _GetVal(data_format, use_gpu):\n      with test_util.device(use_gpu):\n        t0 = constant_op.constant(x0, shape=input_sizes)\n        t1 = constant_op.constant(filter_sizes, shape=[len(filter_sizes)])\n        t2 = constant_op.constant(x2, shape=output_sizes)\n        strides = [1] + conv_strides + [1]\n        if data_format == \"NCHW\":\n          t0 = test_util.NHWCToNCHW(t0)\n          t2 = test_util.NHWCToNCHW(t2)\n          strides = test_util.NHWCToNCHW(strides)\n        conv = nn_ops.conv2d_backprop_filter(\n            t0,\n            t1,\n            t2,\n            strides=strides,\n            padding=padding,\n            data_format=data_format)\n        ret = self.evaluate(conv)\n        self.assertShapeEqual(ret, conv)\n        return ret\n\n    values = []\n    for (data_format, use_gpu) in GetTestConfigs():\n      values.append(_GetVal(data_format, use_gpu))\n    for i in range(1, len(values)):\n      self.assertAllClose(values[0], values[i], rtol=1e-4, atol=1e-4)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Depth1ValidBackpropFilter(self):\n    expected = [5.0, 8.0, 14.0, 17.0]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilter(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 1, 2, 1],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DEmptyBackpropFilter(self):\n    expected = []\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilter(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 0],\n          output_sizes=[1, 1, 2, 0],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DBackpropFilterWithEmptyInput(self):\n    expected = [0, 0, 0, 0]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilter(\n          input_sizes=[0, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[0, 1, 2, 1],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Depth3ValidBackpropFilter(self):\n    expected = [\n        17.0, 22.0, 27.0, 22.0, 29.0, 36.0, 27.0, 36.0, 45.0, 32.0, 43.0, 54.0,\n        37.0, 50.0, 63.0, 42.0, 57.0, 72.0, 62.0, 85.0, 108.0, 67.0, 92.0,\n        117.0, 72.0, 99.0, 126.0, 77.0, 106.0, 135.0, 82.0, 113.0, 144.0, 87.0,\n        120.0, 153.0\n    ]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilter(\n          input_sizes=[1, 2, 3, 3],\n          filter_sizes=[2, 2, 3, 3],\n          output_sizes=[1, 1, 2, 3],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Depth3ValidBackpropFilterStride1x2(self):\n    expected = [161.0, 182.0, 287.0, 308.0]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilter(\n          input_sizes=[1, 3, 6, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 2, 3, 1],\n          strides=[1, 2],\n          padding=\"VALID\",\n          expected=expected,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DStrideTwoFilterOneSameBackpropFilter(self):\n    expected_output = [78.]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilter(\n          input_sizes=[1, 4, 4, 1],\n          filter_sizes=[1, 1, 1, 1],\n          output_sizes=[1, 2, 2, 1],\n          strides=[2, 2],\n          padding=\"SAME\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DKernelSizeMatchesInputSizeBackpropFilter(self):\n    expected_output = [1.0, 2.0, 2.0, 4.0, 3.0, 6.0, 4.0, 8.0]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilter(\n          input_sizes=[1, 2, 2, 1],\n          filter_sizes=[2, 2, 1, 2],\n          output_sizes=[1, 1, 1, 2],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  # Testing for backprops\n  def _RunAndVerifyBackpropInputDilation(self, input_sizes, filter_sizes,\n                                         output_sizes, strides, dilations,\n                                         padding, data_format, use_gpu, err):\n    x1 = self._CreateNumpyTensor(input_sizes)\n    x2 = self._CreateNumpyTensor(filter_sizes)\n    default_dilations = (dilations[0] == 1 and dilations[1] == 1)\n    if default_dilations or use_gpu:\n      with self.cached_session(use_gpu=use_gpu) as sess:\n        if data_format == \"NCHW\":\n          input_sizes = test_util.NHWCToNCHW(input_sizes)\n        t1 = constant_op.constant(x1, shape=input_sizes)\n        t2 = constant_op.constant(x2, shape=filter_sizes)\n        full_strides = [1] + strides + [1]\n        full_dilations = [1] + dilations + [1]\n        if data_format == \"NCHW\":\n          full_strides = test_util.NHWCToNCHW(full_strides)\n          full_dilations = test_util.NHWCToNCHW(full_dilations)\n        conv_forward = nn_ops.conv2d(\n            t1,\n            t2,\n            strides=full_strides,\n            dilations=full_dilations,\n            padding=padding,\n            data_format=data_format)\n        conv_forward_2 = nn_ops.convolution(\n            t1,\n            t2,\n            padding=padding,\n            strides=strides,\n            dilation_rate=dilations,\n            data_format=data_format)\n        if data_format == \"NCHW\":\n          conv_forward = test_util.NCHWToNHWC(conv_forward)\n          conv_forward_2 = test_util.NCHWToNHWC(conv_forward_2)\n        conv = gradients_impl.gradients(conv_forward, t1)[0]\n        conv_2 = gradients_impl.gradients(conv_forward_2, t1)[0]\n        # \"values\" consists of two tensors for two backprops\n        value = self.evaluate(conv)\n        value_2 = self.evaluate(conv_2)\n        self.assertShapeEqual(value, conv)\n        self.assertShapeEqual(value_2, conv_2)\n      tf_logging.debug(\"expected = %s\", value_2)\n      tf_logging.debug(\"actual = %s\", value)\n      self.assertArrayNear(value_2.flatten(), value.flatten(), err)\n\n  # Testing for backprops\n  def _RunAndVerifyBackpropFilterDilation(self, input_sizes, filter_sizes,\n                                          output_sizes, strides, dilations,\n                                          padding, data_format, use_gpu, err):\n    x1 = self._CreateNumpyTensor(input_sizes)\n    x2 = self._CreateNumpyTensor(filter_sizes)\n    default_dilations = (dilations[0] == 1 and dilations[1] == 1)\n    if default_dilations or use_gpu:\n      with self.cached_session(use_gpu=use_gpu) as sess:\n        if data_format == \"NCHW\":\n          input_sizes = test_util.NHWCToNCHW(input_sizes)\n        t1 = constant_op.constant(x1, shape=input_sizes)\n        t2 = constant_op.constant(x2, shape=filter_sizes)\n        full_strides = [1] + strides + [1]\n        full_dilations = [1] + dilations + [1]\n        if data_format == \"NCHW\":\n          full_strides = test_util.NHWCToNCHW(full_strides)\n          full_dilations = test_util.NHWCToNCHW(full_dilations)\n        conv_forward = nn_ops.conv2d(\n            t1,\n            t2,\n            strides=full_strides,\n            dilations=full_dilations,\n            padding=padding,\n            data_format=data_format)\n        conv_forward_2 = nn_ops.convolution(\n            t1,\n            t2,\n            padding=padding,\n            strides=strides,\n            dilation_rate=dilations,\n            data_format=data_format)\n        if data_format == \"NCHW\":\n          conv_forward = test_util.NCHWToNHWC(conv_forward)\n          conv_forward_2 = test_util.NCHWToNHWC(conv_forward_2)\n        conv = gradients_impl.gradients(conv_forward, t2)[0]\n        conv_2 = gradients_impl.gradients(conv_forward, t2)[0]\n        value = self.evaluate(conv)\n        value_2 = self.evaluate(conv_2)\n        self.assertShapeEqual(value, conv)\n        self.assertShapeEqual(value_2, conv_2)\n      tf_logging.debug(\"expected = %s\", value_2)\n      tf_logging.debug(\"actual = %s\", value)\n      self.assertArrayNear(value_2.flatten(), value.flatten(), err)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2D2x2Depth3ValidBackpropFilterStride1x1Dilation2x1(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropFilterDilation(\n            input_sizes=[1, 3, 6, 1],\n            filter_sizes=[2, 2, 1, 1],\n            output_sizes=[1, 1, 5, 1],\n            strides=[1, 1],\n            dilations=[2, 1],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2D2x2Depth1ValidBackpropFilterDilation1x2(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropFilterDilation(\n            input_sizes=[1, 2, 3, 1],\n            filter_sizes=[2, 2, 1, 1],\n            output_sizes=[1, 1, 2, 1],\n            strides=[1, 1],\n            dilations=[1, 2],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2DEmptyBackpropFilterDilation1x2(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropFilterDilation(\n            input_sizes=[1, 2, 3, 1],\n            filter_sizes=[2, 2, 1, 0],\n            output_sizes=[1, 1, 2, 0],\n            strides=[1, 1],\n            dilations=[1, 2],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2D2x2Depth3ValidBackpropFilterDilation2x2(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropFilterDilation(\n            input_sizes=[1, 3, 4, 3],\n            filter_sizes=[2, 2, 3, 3],\n            output_sizes=[1, 1, 2, 3],\n            strides=[1, 1],\n            dilations=[2, 2],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2DKernelSizeMatchesInputSizeBackpropFilterDilation2x2(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropFilterDilation(\n            input_sizes=[1, 3, 3, 1],\n            filter_sizes=[2, 2, 1, 2],\n            output_sizes=[1, 1, 1, 2],\n            strides=[1, 1],\n            dilations=[2, 2],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2D2x2Depth3ValidBackpropInputStride1x1Dilation2x1(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropInputDilation(\n            input_sizes=[1, 3, 6, 1],\n            filter_sizes=[2, 2, 1, 1],\n            output_sizes=[1, 1, 5, 1],\n            strides=[1, 1],\n            dilations=[2, 1],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2D2x2Depth1ValidBackpropInputDilation1x2(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropInputDilation(\n            input_sizes=[1, 2, 3, 1],\n            filter_sizes=[2, 2, 1, 1],\n            output_sizes=[1, 1, 2, 1],\n            strides=[1, 1],\n            dilations=[1, 2],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2DEmptyBackpropInputDilation1x2(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropInputDilation(\n            input_sizes=[0, 2, 3, 1],\n            filter_sizes=[2, 2, 1, 1],\n            output_sizes=[0, 1, 2, 1],\n            strides=[1, 1],\n            dilations=[1, 2],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2D2x2Depth3ValidBackpropInputDilation2x1(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        # The GPU version of this test is not very stable. So adjusting the\n        # error threshold to 1e-4.\n        self._RunAndVerifyBackpropInputDilation(\n            input_sizes=[1, 3, 2, 3],\n            filter_sizes=[2, 2, 3, 3],\n            output_sizes=[1, 1, 2, 3],\n            strides=[1, 1],\n            dilations=[2, 1],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-4)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2DKernelSizeMatchesInputSizeBackpropInputDilation2x2(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropInputDilation(\n            input_sizes=[1, 3, 3, 1],\n            filter_sizes=[2, 2, 1, 2],\n            output_sizes=[1, 1, 1, 2],\n            strides=[1, 1],\n            dilations=[2, 2],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  def _RunAndVerifyBackpropInputExplicitPadding(self,\n                                                input_sizes,\n                                                filter_sizes,\n                                                output_sizes,\n                                                strides,\n                                                padding,\n                                                data_format,\n                                                use_gpu,\n                                                dilations=(1, 1),\n                                                err=2e-5):\n    if use_gpu and not test.is_gpu_available(cuda_only=True):\n      return\n    if not use_gpu and dilations != (1, 1):\n      return  # Non-default dilations is currently not supported on the CPU.\n\n    x1 = self._CreateNumpyTensor(filter_sizes)\n    x2 = self._CreateNumpyTensor(output_sizes)\n    dilations = list(dilations)\n    padded_input_sizes = input_sizes[:]\n    padded_input_sizes[1] += padding[0][0] + padding[0][1]\n    padded_input_sizes[2] += padding[1][0] + padding[1][1]\n    c = nn_ops.conv2d_backprop_input(\n        padded_input_sizes,\n        x1,\n        x2,\n        strides=[1] + strides + [1],\n        padding=\"VALID\",\n        dilations=[1] + dilations + [1])\n    c = c[:, padding[0][0]:(c.shape[1] - padding[0][1]), padding[1][0]:(\n        c.shape[2] - padding[1][1]), :]\n    expected = list(self.evaluate(array_ops.reshape(c, [-1])))\n    self._RunAndVerifyBackpropInput(\n        input_sizes,\n        filter_sizes,\n        output_sizes,\n        strides,\n        padding,\n        expected,\n        data_format,\n        use_gpu=use_gpu,\n        err=err,\n        dilations=dilations)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding0x0BackpropInput(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 1, 2, 1],\n          strides=[1, 1],\n          padding=[[0, 0], [0, 0]],\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 3, 4, 2],\n          filter_sizes=[2, 2, 2, 3],\n          output_sizes=[1, 1, 2, 3],\n          strides=[2, 2],\n          padding=[[0, 0], [0, 0]],\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding1x1BackpropInput(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 2],\n          output_sizes=[1, 3, 4, 2],\n          strides=[1, 1],\n          padding=[[1, 1], [1, 1]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-4)\n\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 2, 3, 2],\n          filter_sizes=[1, 1, 2, 1],\n          output_sizes=[1, 4, 3, 1],\n          strides=[1, 2],\n          padding=[[1, 1], [1, 1]],\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 4, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 4, 2, 1],\n          strides=[1, 2],\n          padding=[[1, 1], [1, 1]],\n          data_format=data_format,\n          dilations=[2, 2], use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding2x2BackpropInput(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[2, 3, 1, 1],\n          filter_sizes=[2, 1, 1, 1],\n          output_sizes=[2, 2, 5, 1],\n          strides=[3, 1],\n          padding=[[2, 2], [2, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 3, 6, 1],\n          filter_sizes=[3, 2, 1, 1],\n          output_sizes=[1, 3, 4, 1],\n          strides=[1, 2],\n          padding=[[2, 2], [2, 2]],\n          data_format=data_format,\n          dilations=[2, 3],\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding_1_8_4_1_BackpropInput(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 10, 8, 1],\n          strides=[1, 1],\n          padding=[[1, 8], [4, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=5e-5)\n\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 5, 3, 1],\n          filter_sizes=[3, 2, 1, 1],\n          output_sizes=[1, 4, 8, 1],\n          strides=[3, 1],\n          padding=[[1, 8], [4, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding_5_0_2_2_BackpropInput(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 3, 3, 1],\n          filter_sizes=[2, 1, 1, 1],\n          output_sizes=[1, 7, 7, 1],\n          strides=[1, 1],\n          padding=[[5, 0], [2, 2]],\n          data_format=data_format,\n          err=5e-5,\n          use_gpu=use_gpu)\n\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 4, 2, 1],\n          filter_sizes=[3, 3, 1, 1],\n          output_sizes=[1, 5, 2, 1],\n          strides=[1, 2],\n          padding=[[5, 0], [2, 2]],\n          data_format=data_format,\n          dilations=[2, 1],\n          use_gpu=use_gpu)\n\n  def _RunAndVerifyBackpropFilterExplicitPadding(self,\n                                                 input_sizes,\n                                                 filter_sizes,\n                                                 output_sizes,\n                                                 strides,\n                                                 padding,\n                                                 data_format,\n                                                 use_gpu,\n                                                 dilations=(1, 1),\n                                                 err=1e-5):\n    if use_gpu and not test.is_gpu_available(cuda_only=True):\n      return\n    if not use_gpu and dilations != (1, 1):\n      return  # Non-default dilations is currently not supported on the CPU.\n\n    x0 = self._CreateNumpyTensor(input_sizes)\n    x2 = self._CreateNumpyTensor(output_sizes)\n    dilations = list(dilations)\n\n    x0 = np.pad(x0, [(0, 0)] + padding + [(0, 0)], \"constant\")\n    c = nn_ops.conv2d_backprop_filter(\n        x0,\n        filter_sizes,\n        x2,\n        strides=[1] + strides + [1],\n        padding=\"VALID\",\n        dilations=[1] + dilations + [1])\n    expected = list(self.evaluate(array_ops.reshape(c, [-1])))\n    self._RunAndVerifyBackpropFilter(\n        input_sizes,\n        filter_sizes,\n        output_sizes,\n        strides,\n        padding,\n        expected,\n        data_format,\n        use_gpu=use_gpu,\n        dilations=dilations,\n        err=err)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding0x0BackpropFilter(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 1, 2, 1],\n          strides=[1, 1],\n          padding=[[0, 0], [0, 0]],\n          data_format=data_format, use_gpu=use_gpu)\n\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 3, 4, 2],\n          filter_sizes=[2, 2, 2, 3],\n          output_sizes=[1, 1, 2, 3],\n          strides=[2, 2],\n          padding=[[0, 0], [0, 0]],\n          data_format=data_format, use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding1x1BackpropFilter(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 2],\n          output_sizes=[1, 3, 4, 2],\n          strides=[1, 1],\n          padding=[[1, 1], [1, 1]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=5e-5)\n\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 2, 3, 2],\n          filter_sizes=[1, 1, 2, 1],\n          output_sizes=[1, 4, 3, 1],\n          strides=[1, 2],\n          padding=[[1, 1], [1, 1]],\n          use_gpu=use_gpu,\n          data_format=data_format)\n\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 4, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 4, 2, 1],\n          strides=[1, 2],\n          padding=[[1, 1], [1, 1]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          dilations=[2, 2])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding2x2BackpropFilter(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[2, 3, 1, 1],\n          filter_sizes=[2, 1, 1, 1],\n          output_sizes=[2, 2, 5, 1],\n          strides=[3, 1],\n          padding=[[2, 2], [2, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 3, 6, 1],\n          filter_sizes=[3, 2, 1, 1],\n          output_sizes=[1, 3, 4, 1],\n          strides=[1, 2],\n          padding=[[2, 2], [2, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          dilations=[2, 3])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding_1_8_4_1_BackpropFilter(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 10, 8, 1],\n          strides=[1, 1],\n          padding=[[1, 8], [4, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-4)\n\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 5, 3, 1],\n          filter_sizes=[3, 2, 1, 1],\n          output_sizes=[1, 4, 8, 1],\n          strides=[3, 1],\n          padding=[[1, 8], [4, 2]],\n          use_gpu=use_gpu,\n          data_format=data_format)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding_5_0_2_2_BackpropFilter(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 3, 3, 1],\n          filter_sizes=[2, 1, 1, 1],\n          output_sizes=[1, 7, 7, 1],\n          strides=[1, 1],\n          padding=[[5, 0], [2, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-4)\n\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 4, 2, 1],\n          filter_sizes=[3, 3, 1, 1],\n          output_sizes=[1, 5, 2, 1],\n          strides=[1, 2],\n          padding=[[5, 0], [2, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          dilations=[2, 1])\n\n  # Gradient checkers\n  def ConstructAndTestGradient(self,\n                               batch,\n                               input_rows,\n                               input_cols,\n                               filter_rows,\n                               filter_cols,\n                               in_depth,\n                               out_depth,\n                               stride_rows,\n                               stride_cols,\n                               padding,\n                               test_input,\n                               data_format,\n                               use_gpu,\n                               num_groups=1,\n                               max_err=0.003):\n    assert in_depth % num_groups == 0 and out_depth % num_groups == 0\n    input_shape = [batch, input_rows, input_cols, in_depth]\n    filter_shape = [filter_rows, filter_cols, in_depth // num_groups, out_depth]\n    # TODO(yangke): re-factor the computation of output shape.\n    if padding == \"VALID\":\n      output_rows = (input_rows - filter_rows + stride_rows) // stride_rows\n      output_cols = (input_cols - filter_cols + stride_cols) // stride_cols\n    elif padding == \"SAME\":\n      output_rows = (input_rows + stride_rows - 1) // stride_rows\n      output_cols = (input_cols + stride_cols - 1) // stride_cols\n    else:\n      self.assertIsInstance(padding, (list, tuple))\n      output_rows = (input_rows + padding[1][0] + padding[1][1] - filter_rows +\n                     stride_rows) // stride_rows\n      output_cols = (input_cols + padding[2][0] + padding[2][1] - filter_cols +\n                     stride_cols) // stride_cols\n    output_shape = [batch, output_rows, output_cols, out_depth]\n    input_size = 1\n    for x in input_shape:\n      input_size *= x\n    filter_size = 1\n    for x in filter_shape:\n      filter_size *= x\n    input_data = [x * 1.0 / input_size for x in range(0, input_size)]\n    filter_data = [x * 1.0 / filter_size for x in range(0, filter_size)]\n    # Conv2DGrad functions are not compiled for double due to\n    # a problem in the way Eigen's Conv2DGrad works for double.\n    # So we disable the DOUBLE path.  We should re-enable this\n    # when double support returns for CPU and/or GPU.\n    for dtype in self._DtypesToTest(use_gpu=use_gpu):\n      with self.cached_session(use_gpu=use_gpu):\n        input_tensor = constant_op.constant(\n            input_data, shape=input_shape, dtype=dtype, name=\"input\")\n        filter_tensor = constant_op.constant(\n            filter_data, shape=filter_shape, dtype=dtype, name=\"filter\")\n        strides = [1, stride_rows, stride_cols, 1]\n        new_padding = padding\n        if data_format == \"NCHW\":\n          new_input_tensor = test_util.NHWCToNCHW(input_tensor)\n          strides = test_util.NHWCToNCHW(strides)\n          if isinstance(padding, (list, tuple)):\n            new_padding = test_util.NHWCToNCHW(padding)\n        else:\n          new_input_tensor = input_tensor\n        conv = nn_ops.conv2d(\n            new_input_tensor,\n            filter_tensor,\n            strides,\n            new_padding,\n            data_format=data_format,\n            name=\"conv\")\n        if data_format == \"NCHW\":\n          conv = test_util.NCHWToNHWC(conv)\n        self.assertEqual(output_shape, conv.get_shape())\n        if test_input:\n          jacob_t, jacob_n = gradient_checker.compute_gradient(input_tensor,\n                                                               input_shape,\n                                                               conv,\n                                                               output_shape)\n        else:\n          jacob_t, jacob_n = gradient_checker.compute_gradient(filter_tensor,\n                                                               filter_shape,\n                                                               conv,\n                                                               output_shape)\n        if dtype == dtypes.float32:\n          reference_jacob_t = jacob_t\n          err = np.fabs(jacob_t - jacob_n).max()\n        else:\n          # Compare fp16 theoretical gradients to fp32 theoretical gradients,\n          # since fp16 numerical gradients are too imprecise.\n          err = np.fabs(jacob_t - reference_jacob_t).max()\n\n        tf_logging.debug(\"conv_2d gradient error = %s\", err)\n        self.assertLess(err, max_err)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradientValidPaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=5,\n          input_cols=4,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=\"VALID\",\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientValidPaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=4,\n          input_rows=6,\n          input_cols=5,\n          filter_rows=2,\n          filter_cols=2,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=\"VALID\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradientValidPaddingStrideTwo(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=4,\n          input_cols=5,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=2,\n          stride_cols=2,\n          padding=\"VALID\",\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientValidPaddingStrideTwo(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=4,\n          input_rows=6,\n          input_cols=5,\n          filter_rows=2,\n          filter_cols=2,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=2,\n          stride_cols=2,\n          padding=\"VALID\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradientValidPaddingStrideThree(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=7,\n          input_cols=6,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=4,\n          out_depth=5,\n          stride_rows=3,\n          stride_cols=3,\n          padding=\"VALID\",\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientValidPaddingStrideThree(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=8,\n          input_cols=7,\n          filter_rows=4,\n          filter_cols=4,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=3,\n          stride_cols=3,\n          padding=\"VALID\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradientSamePaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=7,\n          input_cols=6,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=\"SAME\",\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientSamePaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=4,\n          input_rows=6,\n          input_cols=5,\n          filter_rows=2,\n          filter_cols=2,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=\"SAME\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradientSamePaddingStrideTwo(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=5,\n          input_cols=4,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=3,\n          out_depth=3,\n          stride_rows=2,\n          stride_cols=2,\n          padding=\"SAME\",\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientSamePaddingStrideTwo(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=4,\n          input_rows=6,\n          input_cols=5,\n          filter_rows=2,\n          filter_cols=2,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=2,\n          stride_cols=2,\n          padding=\"SAME\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradientSamePaddingStrideThree(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=7,\n          input_cols=6,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=4,\n          out_depth=5,\n          stride_rows=3,\n          stride_cols=3,\n          padding=\"SAME\",\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientSamePaddingStrideThree(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=8,\n          input_cols=7,\n          filter_rows=4,\n          filter_cols=4,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=3,\n          stride_cols=3,\n          padding=\"SAME\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientSamePaddingStride2x1(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=8,\n          input_cols=7,\n          filter_rows=4,\n          filter_cols=4,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=2,\n          stride_cols=1,\n          padding=\"SAME\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradientKernelSizeMatchesInputSize(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=4,\n          input_cols=3,\n          filter_rows=4,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=\"VALID\",\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientKernelSizeMatchesInputSize(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=4,\n          input_cols=3,\n          filter_rows=4,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=\"VALID\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradient1x1PaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=5,\n          input_cols=4,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=[[0, 0], [1, 1], [1, 1], [0, 0]],\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          max_err=0.0025)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradient1x1PaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=5,\n          input_cols=4,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=[[0, 0], [1, 1], [1, 1], [0, 0]],\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradient1x1PaddingStrideTwo(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=4,\n          input_cols=5,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=2,\n          stride_cols=2,\n          padding=[[0, 0], [1, 1], [1, 1], [0, 0]],\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradient1x1PaddingStrideTwo(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=4,\n          input_cols=5,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=2,\n          stride_cols=2,\n          padding=[[0, 0], [1, 1], [1, 1], [0, 0]],\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradient2x2PaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=5,\n          input_cols=4,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=[[0, 0], [2, 2], [2, 2], [0, 0]],\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          max_err=0.003)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradient2x2PaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=5,\n          input_cols=4,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=[[0, 0], [2, 2], [2, 2], [0, 0]],\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          max_err=0.005)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradient1_2_3_4PaddingStride3x2(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=8,\n          input_cols=5,\n          filter_rows=4,\n          filter_cols=2,\n          in_depth=3,\n          out_depth=2,\n          stride_rows=3,\n          stride_cols=2,\n          padding=[[0, 0], [1, 2], [3, 4], [0, 0]],\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradient1_2_3_4PaddingStride3x2(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=8,\n          input_cols=5,\n          filter_rows=4,\n          filter_cols=2,\n          in_depth=3,\n          out_depth=2,\n          stride_rows=3,\n          stride_cols=2,\n          padding=[[0, 0], [1, 2], [3, 4], [0, 0]],\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradient4_3_2_1PaddingStride2x1(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=3,\n          input_rows=5,\n          input_cols=7,\n          filter_rows=3,\n          filter_cols=2,\n          in_depth=1,\n          out_depth=2,\n          stride_rows=2,\n          stride_cols=1,\n          padding=[[0, 0], [4, 3], [2, 1], [0, 0]],\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradient4_3_2_1PaddingStride2x1(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=3,\n          input_rows=5,\n          input_cols=7,\n          filter_rows=3,\n          filter_cols=2,\n          in_depth=1,\n          out_depth=2,\n          stride_rows=2,\n          stride_cols=1,\n          padding=[[0, 0], [4, 3], [2, 1], [0, 0]],\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradient0_0_0_5PaddingStride1x2(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=6,\n          input_cols=7,\n          filter_rows=3,\n          filter_cols=4,\n          in_depth=3,\n          out_depth=2,\n          stride_rows=1,\n          stride_cols=2,\n          padding=[[0, 0], [0, 0], [0, 5], [0, 0]],\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradient0_0_0_5PaddingStride1x2(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=6,\n          input_cols=7,\n          filter_rows=3,\n          filter_cols=4,\n          in_depth=3,\n          out_depth=2,\n          stride_rows=1,\n          stride_cols=2,\n          padding=[[0, 0], [0, 0], [0, 5], [0, 0]],\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testShapeFunctionEdgeCases(self):\n    # All shapes unknown.\n    c1 = nn_ops.conv2d(\n        array_ops.placeholder(dtypes.float32),\n        array_ops.placeholder(dtypes.float32),\n        strides=[1, 1, 1, 1],\n        padding=\"SAME\")\n    self.assertEqual([None, None, None, None], c1.get_shape().as_list())\n\n    # Incorrect input shape.\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(\n              dtypes.float32, shape=[1, 3]),\n          array_ops.placeholder(dtypes.float32),\n          strides=[1, 1, 1, 1],\n          padding=\"SAME\")\n\n    # Incorrect filter shape.\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(\n              dtypes.float32, shape=[1, 3]),\n          strides=[1, 1, 1, 1],\n          padding=\"SAME\")\n\n    # Depth mismatch.\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(\n              dtypes.float32, shape=[32, 20, 20, 3]),\n          array_ops.placeholder(\n              dtypes.float32, shape=[4, 4, 2, 2]),\n          strides=[1, 1, 1, 1],\n          padding=\"SAME\")\n\n    # Input depth divisible by filter depth (group convolution).\n    # No exceptions should appear.\n    nn_ops.conv2d(\n        array_ops.placeholder(dtypes.float32, shape=[32, 20, 20, 8]),\n        array_ops.placeholder(dtypes.float32, shape=[4, 4, 2, 16]),\n        strides=[1, 1, 1, 1],\n        padding=\"SAME\")\n\n    # Negative padding.\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(dtypes.float32),\n          strides=[1, 1, 1, 1],\n          padding=[[0, 0], [0, -1], [1, 2], [0, 0]])\n\n    # Nonzero padding in nonspatial dimension.\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(dtypes.float32),\n          strides=[1, 1, 1, 1],\n          padding=[[1, 0], [0, 0], [0, 0], [0, 0]])\n\n    # Nonzero NCHW padding in nonspatial dimension.\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(dtypes.float32),\n          strides=[1, 1, 1, 1],\n          padding=[[0, 0], [0, 1], [0, 0], [0, 0]],\n          data_format=\"NCHW\")\n\n    # Wrong amount of padding\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(dtypes.float32),\n          strides=[1, 1, 1, 1],\n          padding=[[0, 0], [0, 0], [0, 0]])\n\n    # Only specify one padding amount per dimension\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(dtypes.float32),\n          strides=[1, 1, 1, 1],\n          padding=[[0], [0], [0], [0]])\n\n    # Explicit padding elements are not lists\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(dtypes.float32),\n          strides=[1, 1, 1, 1],\n          padding=[0, 0, 0, 0])\n\n  def testOpEdgeCases(self):\n    # Illegal strides.\n    with self.assertRaisesRegex((ValueError, errors_impl.UnimplementedError),\n                                \"strides in the batch and depth\"):\n      input_val = np.ones([2, 4, 10, 10])\n      filter_val = np.ones([2, 4, 10, 10])\n      self.evaluate(\n          nn_ops.conv2d(\n              input_val, filter_val, strides=[2, 1, 1, 1], padding=\"SAME\"))\n    with self.assertRaisesRegex((ValueError, errors_impl.UnimplementedError),\n                                \"strides in the batch and depth\"):\n      input_val = np.ones([2, 4, 10, 10])\n      filter_val = np.ones([2, 4, 10, 10])\n      self.evaluate(\n          nn_ops.conv2d(\n              input_val, filter_val, strides=[1, 1, 1, 2], padding=\"SAME\"))\n\n    # TODO(b/195689143): Will enable when fixed for V2 behavior\n    # # Filter larger than input.\n    # with self.assertRaisesRegex(ValueError, \"Negative dimension size\"):\n    #   input_val = np.ones([32, 20, 20, 3])\n    #   filter_val = np.ones([20, 21, 3, 2])\n    #   self.evaluate(\n    #       nn_ops.conv2d(\n    #           input_val, filter_val, strides=[1, 1, 1, 1], padding=\"VALID\"))\n    # with self.assertRaisesRegex(ValueError, \"Negative dimension size\"):\n    #   input_val = np.ones([32, 20, 20, 3])\n    #   filter_val = np.ones([21, 20, 3, 2])\n    #   self.evaluate(\n    #       nn_ops.conv2d(\n    #           input_val, filter_val, strides=[1, 1, 1, 1], padding=\"VALID\"))\n    #\n    # # Filter larger than input + padding.\n    # with self.assertRaisesRegex(ValueError, \"Negative dimension size\"):\n    #   input_val = np.ones([32, 20, 20, 3])\n    # filter_val = np.ones([24, 25, 3, 2])\n    #   self.evaluate(\n    #       nn_ops.conv2d(\n    #           input_val,\n    #           filter_val,\n    #           strides=[1, 1, 1, 1],\n    #           padding=[[0, 0], [2, 2], [2, 2], [0, 0]]))\n\n    # Filter dimensions must be greater than 0.\n    with self.assertRaisesRegex(\n        errors_impl.InvalidArgumentError, \"filter must not have zero elements\"\n        \"|has a non-positive dimension\"):\n      input_val = np.ones([1, 1, 1, 1])\n      filter_val = np.ones([1, 0, 1, 1])\n      self.evaluate(\n          nn_ops.conv2d(\n              input_val, filter_val, strides=[1, 1, 1, 1], padding=\"SAME\"))\n\n    # Negative padding during backprop.\n    with self.assertRaisesRegex(\n        errors_impl.InvalidArgumentError,\n        \"All elements of explicit_paddings must be nonnegative\"):\n      filter_val = np.ones([18, 18, 3, 2])\n      out_backprop_val = np.ones([32, 3, 2, 2])\n      self.evaluate(\n          nn_ops.conv2d_backprop_input([32, 20, 20, 3],\n                                       filter_val,\n                                       out_backprop_val,\n                                       strides=[1, 1, 1, 1],\n                                       padding=[[0, 0], [-1, 0], [0, 0], [0,\n                                                                          0]]))\n    with self.assertRaisesRegex(\n        errors_impl.InvalidArgumentError,\n        \"All elements of explicit_paddings must be nonnegative\"):\n      input_val = np.ones([32, 20, 20, 3])\n      out_backprop_val = np.ones([32, 3, 2, 2])\n      self.evaluate(\n          nn_ops.conv2d_backprop_filter(\n              input_val, [18, 18, 3, 2],\n              out_backprop_val,\n              strides=[1, 1, 1, 1],\n              padding=[[0, 0], [-1, 0], [0, 0], [0, 0]]))\n\n\n@test_util.run_all_without_tensor_float_32(\"Avoid TF32 conv on GPU\")\nclass DepthwiseConv2DTest(test.TestCase):\n\n  def _VerifyValues(self, tensor_in_sizes, filter_in_sizes, stride, padding,\n                    expected):\n    \"\"\"Verifies the output values of the convolution function.\n\n    Args:\n      tensor_in_sizes: Input tensor dimensions in [batch, input_rows,\n        input_cols, input_depth].\n      filter_in_sizes: Filter tensor dimensions in [filter_rows, filter_cols,\n        input_depth, depth_multiplier].\n      stride: Stride.\n      padding: Padding type.\n      expected: An array containing the expected operation outputs.\n    \"\"\"\n    total_size_1 = 1\n    total_size_2 = 1\n    for s in tensor_in_sizes:\n      total_size_1 *= s\n    for s in filter_in_sizes:\n      total_size_2 *= s\n    # Initializes the input tensor with array containing incrementing\n    # numbers from 1.\n    x1 = [f * 1.0 for f in range(1, total_size_1 + 1)]\n    x2 = [f * 1.0 for f in range(1, total_size_2 + 1)]\n    with self.cached_session() as sess:\n      t1 = constant_op.constant(x1, shape=tensor_in_sizes)\n      t1.set_shape(tensor_in_sizes)\n      t2 = constant_op.constant(x2, shape=filter_in_sizes)\n      conv = nn_impl.depthwise_conv2d(\n          t1, t2, strides=[1, stride, stride, 1], padding=padding)\n      value = self.evaluate(conv)\n    tf_logging.debug(\"value = %s\", value)\n    self.assertArrayNear(expected, np.ravel(value), 1e-5)\n    self.assertShapeEqual(value, conv)\n\n  def testConv2D2x2Filter(self):\n    # The inputs look like this (it's a 3 x 2 matrix, each of depth 2):\n    #\n    # [ (1.0, 2.0), (3.0,  4.0), ( 5.0,  6.0) ]\n    # [ (7.0, 8.0), (9.0, 10.0), (11.0, 12.0) ]\n    #  We can view this as two inputs\n    #\n    #  input depth 0:\n    #\n    #  [ 1.0,  3.0,  5.0 ]\n    #  [ 7.0,  9.0, 11.0 ]\n    #\n    #  input depth 1:\n    #\n    #  [ 2.0,  4.0,  6.0 ]\n    #  [ 8.0, 10.0, 12.0 ]\n    #\n    # The filter looks like this (it has two 2 x 2 patches, each generating 2\n    # depths):\n    #\n    #  filter #0:\n    #\n    #  [ (1.0,  3.0), ( 5.0,  7.0)]\n    #  [ (9.0, 11.0), (13.0, 15.0)]\n    #\n    #  filter #1:\n    #\n    #  [ ( 2.0,  4.0), ( 6.0,  8.0)]\n    #  [ (10.0, 12.0), (14.0, 16.0)]\n    #\n    # So the outputs are:\n    #\n    # (position 0, 0: in_depth 0, output_depth 0 -- using filter #0)\n    #  1.0 * 1.0 + 7.0 * 9.0 + 3.0 * 5.0 + 9.0 * 13.0 = 196\n    # (position 0, 0: in_depth 0, output_depth 1 -- using filter #1)\n    #  1.0 * 2.0 + 7.0 * 10.0 + 3.0 * 6.0 + 9.0 * 14.0 = 216\n    # (position 0, 0: in_depth 1, output_depth 2 -- using filter #0)\n    #  2.0 * 3.0 + 8.0 * 11.0 + 4.0 * 7.0 + 10.0 * 15.0 = 272\n    # (position 0, 0: in_depth 1, output_depth 3 -- using filter #1)\n    #  2.0 * 4.0 + 8.0 * 12.0 + 4.0 * 8.0 + 10.0 * 16.0 = 296\n    #\n    # (position 1, 0: in_depth 0, output_depth 0 -- using filter #0)\n    #  3.0 * 1.0 + 9.0 * 9.0 + 5.0 * 5.0 + 11.0 * 13.0 = 252\n    # (position 1, 0: in_depth 0, output_depth 1 -- using filter #1)\n    #  3.0 * 2.0 + 9.0 * 10.0 + 5.0 * 6.0 + 11.0 * 14.0 = 280\n    # (position 1, 0: in_depth 1, output_depth 2 -- using filter #0)\n    #  4.0 * 3.0 + 10.0 * 11.0 + 6.0 * 7.0 + 12.0 * 15.0 = 344\n    # (position 1, 0: in_depth 1, output_depth 3 -- using filter #1)\n    #  4.0 * 4.0 + 10.0 * 12.0 + 6.0 * 8.0 + 12.0 * 16.0 = 376\n    expected_output = [196, 216, 272, 296, 252, 280, 344, 376]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 2, 3, 2],\n        filter_in_sizes=[2, 2, 2, 2],\n        stride=1,\n        padding=\"VALID\",\n        expected=expected_output)\n\n\n@test_util.run_all_without_tensor_float_32(\"Avoid TF32 conv on GPU\")\nclass SeparableConv2DTest(test.TestCase):\n\n  def _InitValues(self, sizes):\n    \"\"\"Initializes values for input tensors.\n\n    Args:\n      sizes: Tensor dimensions.\n\n    Returns:\n      Tensor initialized to values.\n    \"\"\"\n    total_size = 1\n    for s in sizes:\n      total_size *= s\n    x = [f * 0.5 for f in range(1, total_size + 1)]\n    return constant_op.constant(x, shape=sizes)\n\n  def _VerifyValues(self,\n                    tensor_in_sizes,\n                    depthwise_filter_in_sizes,\n                    pointwise_filter_in_sizes,\n                    stride,\n                    padding,\n                    expected,\n                    data_format=\"NHWC\"):\n    \"\"\"Verifies the output values of the separable convolution function.\n\n    Args:\n      tensor_in_sizes: Input tensor dimensions.\n      depthwise_filter_in_sizes: Depthwise filter tensor dimensions.\n      pointwise_filter_in_sizes: Pointwise filter tensor dimensions.\n      stride: Stride.\n      padding: Padding type.\n      expected: An array containing the expected operation outputs.\n      data_format: string data format for input tensor.\n    \"\"\"\n    with self.cached_session():\n      t1 = self._InitValues(tensor_in_sizes)\n      f1 = self._InitValues(depthwise_filter_in_sizes)\n      f1.set_shape(depthwise_filter_in_sizes)\n      f2 = self._InitValues(pointwise_filter_in_sizes)\n\n      real_t1 = t1\n      strides = [1, stride, stride, 1]\n      if data_format == \"NCHW\":\n        real_t1 = array_ops.transpose(t1, [0, 3, 1, 2])\n        strides = [1, 1, stride, stride]\n        if isinstance(padding, list):\n          padding = [padding[0], padding[3], padding[1], padding[2]]\n\n      conv = nn_impl.separable_conv2d(\n          real_t1,\n          f1,\n          f2,\n          strides=strides,\n          padding=padding,\n          data_format=data_format)\n\n      if data_format == \"NCHW\":\n        conv = array_ops.transpose(conv, [0, 2, 3, 1])\n\n      value = self.evaluate(conv)\n    tf_logging.debug(\"value = %s\", value)\n    self.assertArrayNear(expected, np.ravel(value), 2e-3)\n    self.assertShapeEqual(value, conv)\n\n  def _testSeparableConv2D(self, data_format):\n    # The output is the result of two convolutions:\n    # First with tensor_in[1, 4, 4, 2] * filter1[2, 2, 2, 3].\n    # Second with intermediate_out[1, 4, 4, 6] * filter2[1, 1, 6, 7].\n    # Complexity is O(2*3*2*2 + 6*7*1*1) as opposed to O(2*7*2*2).\n    expected_output = [\n        6644.5, 6971.5, 7298.5, 7625.5, 7952.5, 8279.5, 8606.5, 8154.5, 8556.5,\n        8958.5, 9360.5, 9762.5, 10164.5, 10566.5, 9664.5, 10141.5, 10618.5,\n        11095.5, 11572.5, 12049.5, 12526.5, 4145.5, 4346.5, 4547.5, 4748.5,\n        4949.5, 5150.5, 5351.5, 12684.5, 13311.5, 13938.5, 14565.5, 15192.5,\n        15819.5, 16446.5, 14194.5, 14896.5, 15598.5, 16300.5, 17002.5, 17704.5,\n        18406.5, 15704.5, 16481.5, 17258.5, 18035.5, 18812.5, 19589.5, 20366.5,\n        6499.5, 6814.5, 7129.5, 7444.5, 7759.5, 8074.5, 8389.5, 18724.5,\n        19651.5, 20578.5, 21505.5, 22432.5, 23359.5, 24286.5, 20234.5, 21236.5,\n        22238.5, 23240.5, 24242.5, 25244.5, 26246.5, 21744.5, 22821.5, 23898.5,\n        24975.5, 26052.5, 27129.5, 28206.5, 8853.5, 9282.5, 9711.5, 10140.5,\n        10569.5, 10998.5, 11427.5, 5746.75, 6010.75, 6274.75, 6538.75, 6802.75,\n        7066.75, 7330.75, 6168.75, 6452.25, 6735.75, 7019.25, 7302.75, 7586.25,\n        7869.75, 6590.75, 6893.75, 7196.75, 7499.75, 7802.75, 8105.75, 8408.75,\n        2036.25, 2119.5, 2202.75, 2286.0, 2369.25, 2452.5, 2535.75\n    ]\n\n    self._VerifyValues(\n        tensor_in_sizes=[1, 4, 4, 2],\n        depthwise_filter_in_sizes=[2, 2, 2, 3],\n        pointwise_filter_in_sizes=[1, 1, 6, 7],\n        stride=1,\n        padding=\"SAME\",\n        expected=expected_output,\n        data_format=data_format)\n\n  def testSeparableConv2D(self):\n    self._testSeparableConv2D(\"NHWC\")\n\n  def disabledtestSeparableConv2DNCHW(self):\n    if not test.is_gpu_available():\n      return\n    self._testSeparableConv2D(\"NCHW\")\n\n  def _testSeparableConv2DEqualInputOutputDepth(self, data_format):\n    # The output is the result of two convolutions:\n    # First with tensor_in[1, 4, 4, 2] * filter1[2, 2, 3, 3].\n    # Second with intermediate_out[1, 4, 4, 6] * filter2[1, 1, 6, 6].\n    # Complexity is O(2*3*2*2 + 6*6*1*1) as opposed to O(2*6*2*2).\n    expected_output = [\n        5742.0, 6069.0, 6396.0, 6723.0, 7050.0, 7377.0, 7047.0, 7449.0, 7851.0,\n        8253.0, 8655.0, 9057.0, 8352.0, 8829.0, 9306.0, 9783.0, 10260.0,\n        10737.0, 3582.0, 3783.0, 3984.0, 4185.0, 4386.0, 4587.0, 10962.0,\n        11589.0, 12216.0, 12843.0, 13470.0, 14097.0, 12267.0, 12969.0, 13671.0,\n        14373.0, 15075.0, 15777.0, 13572.0, 14349.0, 15126.0, 15903.0, 16680.0,\n        17457.0, 5616.0, 5931.0, 6246.0, 6561.0, 6876.0, 7191.0, 16182.0,\n        17109.0, 18036.0, 18963.0, 19890.0, 20817.0, 17487.0, 18489.0, 19491.0,\n        20493.0, 21495.0, 22497.0, 18792.0, 19869.0, 20946.0, 22023.0, 23100.0,\n        24177.0, 7650.0, 8079.0, 8508.0, 8937.0, 9366.0, 9795.0, 4963.5, 5227.5,\n        5491.5, 5755.5, 6019.5, 6283.5, 5328.0, 5611.5, 5895.0, 6178.5, 6462.0,\n        6745.5, 5692.5, 5995.5, 6298.5, 6601.5, 6904.5, 7207.5, 1757.25, 1840.5,\n        1923.75, 2007.0, 2090.25, 2173.5\n    ]\n\n    self._VerifyValues(\n        tensor_in_sizes=[1, 4, 4, 2],\n        depthwise_filter_in_sizes=[2, 2, 2, 3],\n        pointwise_filter_in_sizes=[1, 1, 6, 6],\n        stride=1,\n        padding=\"SAME\",\n        expected=expected_output,\n        data_format=data_format)\n\n  @test_util.deprecated_graph_mode_only\n  def testSeparableConv2DEqualInputOutputDepth(self):\n    self._testSeparableConv2DEqualInputOutputDepth(\"NHWC\")\n\n  def testSeparableConv2DEqualInputOutputDepthNCHW(self):\n    if not test.is_gpu_available():\n      return\n    self._testSeparableConv2DEqualInputOutputDepth(\"NCHW\")\n\n  def _testSeparableConv2dExplicitPadding(self, data_format):\n    tensor_in_sizes = [1, 4, 4, 2]\n    depthwise_filter_in_sizes = [2, 2, 2, 3]\n    pointwise_filter_in_sizes = [1, 1, 6, 7]\n    padding = [[0, 0], [1, 2], [3, 4], [0, 0]]\n    with self.cached_session():\n      # Compute the 'expected' values by manually padding before calling\n      # separable_conv2d\n      t1 = self._InitValues(tensor_in_sizes)\n      t1 = array_ops.pad(t1, padding)\n      f1 = self._InitValues(depthwise_filter_in_sizes)\n      f1.set_shape(depthwise_filter_in_sizes)\n      f2 = self._InitValues(pointwise_filter_in_sizes)\n      conv = nn_impl.separable_conv2d(\n          t1,\n          f1,\n          f2,\n          strides=[1, 1, 1, 1],\n          padding=\"VALID\",\n          data_format=\"NHWC\")\n      expected = self.evaluate(conv)\n      expected = np.ravel(expected)\n    self._VerifyValues(\n        tensor_in_sizes=tensor_in_sizes,\n        depthwise_filter_in_sizes=depthwise_filter_in_sizes,\n        pointwise_filter_in_sizes=pointwise_filter_in_sizes,\n        stride=1,\n        padding=padding,\n        expected=expected,\n        data_format=data_format)\n\n  def testSeparableConv2dExplicitPadding(self):\n    self._testSeparableConv2dExplicitPadding(\"NHWC\")\n\n  def testSeparableConv2dExplicitPaddingNCHW(self):\n    if not test.is_gpu_available():\n      return\n    self._testSeparableConv2dExplicitPadding(\"NCHW\")\n\n\n@test_util.run_all_without_tensor_float_32(\"Avoid TF32 conv on GPU\")\nclass DeepConv2DTest(test.TestCase):\n\n  def _CompareFwdConv2D(self, tensor_in_sizes, filter_in_sizes, conv_strides,\n                        padding):\n    \"\"\"Verifies that DeepConv2D and Conv2D produce the same values.\n\n    Args:\n      tensor_in_sizes: Input tensor dimensions in\n        [batch, input_rows, input_cols, input_depth].\n      filter_in_sizes: Filter tensor dimensions in\n        [kernel_rows, kernel_cols, input_depth, output_depth].\n      conv_strides: [row_stride, col_stride] for the convolution;\n      padding: Padding type.\n    \"\"\"\n    x1 = np.random.rand(*tensor_in_sizes).astype(np.float32)\n    x2 = np.random.rand(*filter_in_sizes).astype(np.float32)\n\n    with self.cached_session(use_gpu=False) as sess:\n      t1 = constant_op.constant(x1, shape=tensor_in_sizes)\n      t2 = constant_op.constant(x2, shape=filter_in_sizes)\n      strides = [1] + conv_strides + [1]\n\n      conv = nn_ops.conv2d(t1, t2, strides=strides, padding=padding)\n\n      os.environ[\"TF_USE_DEEP_CONV2D\"] = \"0\"\n      values_expect = self.evaluate([conv])\n\n      os.environ[\"TF_USE_DEEP_CONV2D\"] = \"1\"\n      values_test = self.evaluate([conv])\n\n      self.assertAllClose(values_expect, values_test, rtol=1e-5, atol=1e-5)\n\n  def _RunTestCases(self, conv_strides, padding):\n    input_sizes = [[5, 5, 5, 1248], [3, 17, 17, 192], [2, 35, 35, 288],\n                   [2, 6, 8, 517], [2, 7, 4, 81], [3, 11, 3, 77]]\n    filter_sizes = [[3, 3, 1248, 128], [3, 3, 192, 192], [3, 3, 288, 384],\n                    [3, 3, 517, 64], [3, 3, 81, 77], [3, 3, 77, 181]]\n    for input_shape, filter_shape in zip(input_sizes, filter_sizes):\n      self._CompareFwdConv2D(input_shape, filter_shape, conv_strides, padding)\n\n  def testConv2D3x3FilterStride1x1Valid(self):\n    self._RunTestCases([1, 1], \"VALID\")\n\n  def testConv2D3x3FilterStride1x1Same(self):\n    self._RunTestCases([1, 1], \"SAME\")\n\n\nclass Conv2DBenchmark(test.Benchmark):\n\n  def benchmarkGPUConvStackFirst(self):\n    # Benchmark the first iteration of a conv-net with many identical conv\n    # operations.\n    if not test.is_gpu_available():\n      return\n\n    with ops.Graph().as_default(), session_lib.Session() as session:\n      batch_size = 1\n      timesteps = 600\n      features = 1\n\n      inputs = random_ops.random_uniform(\n          [batch_size, 1, timesteps, features], seed=1234)\n      num_outputs_list = [512] * 40 + [1]\n      kernel_w = 3\n      x = inputs\n      for num_outputs in num_outputs_list:\n        x = convolutional.conv2d(x, num_outputs, [1, kernel_w])\n      outputs = x\n\n      self.evaluate(variables.global_variables_initializer())\n      num_iterations = 4\n      for iter_index in range(num_iterations):\n        start = time.time()\n        session.run(outputs)\n        wall_time = time.time() - start\n        self.report_benchmark(\n            name=\"conv_stack_iter_%d\" % iter_index, wall_time=wall_time)\n        tf_logging.info(\"conv_stack_iter_%d: %.4f\" % (iter_index, wall_time))\n\n  def _bench_op(self, name, op, burn_iters, num_iters):\n    config = config_pb2.ConfigProto()\n    # Prevent Grappler from optimizing away the entire graph.\n    config.graph_options.rewrite_options.dependency_optimization = (\n        rewriter_config_pb2.RewriterConfig.OFF)\n    with session_lib.Session(config=config) as session:\n      self.evaluate(variables.global_variables_initializer())\n      self.run_op_benchmark(\n          session, op, burn_iters=burn_iters, min_iters=num_iters, name=name)\n\n  def benchmarkExplicitVsManualPadding(self):\n    \"\"\"Compare performance of EXPLICIT padding and calling tf.pad.\n\n    A Conv2D op with EXPLICIT padding is benchmarked, and a tf.pad with the same\n    padding followed by an equivalent Conv2D op is benchmarked.\n    \"\"\"\n    if not test.is_gpu_available():\n      return\n\n    with ops.Graph().as_default():\n      burn_iters = 15\n      num_iters = 300\n      batch_size = 64\n      # The input and filter correspond to the first layer of Resnet50.\n      input = variables.Variable(  # pylint: disable=redefined-builtin\n          random_ops.random_uniform([\n              batch_size,\n              3,\n              224,\n              224\n          ]))\n      filter = variables.Variable(random_ops.random_uniform([7, 7, 3, 64]))  # pylint: disable=redefined-builtin\n      strides = [1, 1, 2, 2]\n      padding = [(0, 0), (0, 0), (3, 3), (3, 3)]\n      output_explicit_pad = nn_ops.conv2d(\n          input, filter, strides, padding=padding, data_format=\"NCHW\")\n      input_padded = array_ops.pad(input, padding)\n      output_manual_pad = nn_ops.conv2d(\n          input_padded, filter, strides, padding=\"VALID\", data_format=\"NCHW\")\n      # Benchmark just the forward pass.\n      self._bench_op(\"explicit_pad_forward\", output_explicit_pad.op, burn_iters,\n                     num_iters)\n      self._bench_op(\"manual_pad_forward\", output_manual_pad.op, burn_iters,\n                     num_iters)\n\n      # Benchmark both the forward and backwards passes.\n      input_grad_explicit_pad, filter_grad_explicit_pad = (\n          gradients_impl.gradients(output_explicit_pad, [input, filter]))\n      self._bench_op(\n          \"explicit_pad_backward\",\n          control_flow_ops.group(input_grad_explicit_pad,\n                                 filter_grad_explicit_pad), burn_iters,\n          num_iters)\n      input_grad_manual_pad, filter_grad_manual_pad = gradients_impl.gradients(\n          output_manual_pad, [input, filter])\n      self._bench_op(\n          \"manual_pad_backward\",\n          control_flow_ops.group(input_grad_manual_pad, filter_grad_manual_pad),\n          burn_iters, num_iters)\n\n  def benchmarkExplicitVsSamePaddingGraph(self):\n    \"\"\"Compare performance of EXPLICIT and SAME padding in graph mode.\n\n    A Conv2D op with SAME padding is benchmarked, and an equivalent Conv2D op\n    with explicit padding is benchmarked, where the padding is the same as in\n    the SAME case. The purpose is to ensure EXPLICIT padding is just as\n    efficient as the SAME case\n    \"\"\"\n    if not test.is_gpu_available():\n      return\n\n    with ops.Graph().as_default():\n      burn_iters = 15\n      num_convs = 20\n      num_iters = 50\n      batch_size = 64\n      # The input and filter correspond to a middle layer of Resnet50.\n      input = variables.Variable(  # pylint: disable=redefined-builtin\n          random_ops.random_uniform([\n              batch_size,\n              256,\n              14,\n              14\n          ]))\n      filter = variables.Variable(random_ops.random_uniform([3, 3, 256, 256]))  # pylint: disable=redefined-builtin\n      strides = [1, 1, 1, 1]\n      padding = [(0, 0), (0, 0), (1, 1), (1, 1)]\n      output_explicit_pad = input\n      output_same_pad = input\n\n      for _ in range(num_convs):\n        output_explicit_pad = nn_ops.conv2d(\n            output_explicit_pad,\n            filter,\n            strides,\n            padding=padding,\n            data_format=\"NCHW\")\n        output_same_pad = nn_ops.conv2d(\n            output_same_pad,\n            filter,\n            strides,\n            padding=\"SAME\",\n            data_format=\"NCHW\")\n      grad_explicit_pad, = gradients_impl.gradients(output_explicit_pad, filter)\n      grad_same_pad, = gradients_impl.gradients(output_same_pad, filter)\n      self._bench_op(\"graph_explicit_pad\", grad_explicit_pad.op, burn_iters,\n                     num_iters)\n      self._bench_op(\"graph_same_pad\", grad_same_pad.op, burn_iters, num_iters)\n\n  def benchmarkExplicitVsSamePaddingEager(self):\n    \"\"\"Compare performance of EXPLICIT and SAME padding in eager mode.\n\n    A Conv2D op with SAME padding is benchmarked, and an equivalent Conv2D op\n    with explicit padding is benchmarked, where the padding is the same as in\n    the SAME case. Currently, EXPLICIT padding is slightly slower, due to the\n    fact the Python padding list must be checked and processed before the Conv2D\n    op can run.\n    \"\"\"\n    # TODO(reedwm): Make EXPLICIT padding as fast as SAME padding.\n    if not test.is_gpu_available():\n      return\n\n    with context.eager_mode():\n      burn_iters = 15\n      num_convs = 20\n      num_iters = 50\n      batch_size = 64\n      # The input and filter correspond to a middle layer of Resnet50.\n      input = variables.Variable(  # pylint: disable=redefined-builtin\n          random_ops.random_uniform([\n              batch_size,\n              256,\n              14,\n              14\n          ]))\n      filter = variables.Variable(random_ops.random_uniform([3, 3, 256, 256]))  # pylint: disable=redefined-builtin\n      strides = [1, 1, 1, 1]\n      padding = [(0, 0), (0, 0), (1, 1), (1, 1)]\n      output_explicit_pad = input\n      output_same_pad = input\n      for _ in range(burn_iters):\n        output_explicit_pad = nn_ops.conv2d(\n            output_explicit_pad,\n            filter,\n            strides,\n            padding=padding,\n            data_format=\"NCHW\")\n        output_same_pad = nn_ops.conv2d(\n            output_same_pad,\n            filter,\n            strides,\n            padding=\"SAME\",\n            data_format=\"NCHW\")\n\n      start = time.time()\n      for _ in range(num_iters):\n        with backprop.GradientTape() as tape:\n          for _ in range(num_convs):\n            output_explicit_pad = nn_ops.conv2d(\n                output_explicit_pad,\n                filter,\n                strides,\n                padding=padding,\n                data_format=\"NCHW\")\n          tape.gradient(output_explicit_pad, filter)\n      end = time.time()\n      self.report_benchmark(\n          name=\"eager_explicit_pad\",\n          wall_time=(end - start) / num_iters,\n          iters=num_iters)\n\n      start = time.time()\n      for _ in range(num_iters):\n        with backprop.GradientTape() as tape:\n          for _ in range(num_convs):\n            output_same_pad = nn_ops.conv2d(\n                output_same_pad,\n                filter,\n                strides,\n                padding=\"SAME\",\n                data_format=\"NCHW\")\n          tape.gradient(output_same_pad, filter)\n      end = time.time()\n      self.report_benchmark(\n          name=\"eager_same_pad\",\n          wall_time=(end - start) / num_iters,\n          iters=num_iters)\n\n\ndef GetInceptionFwdTest(input_size, filter_size, stride, padding,\n                        gpu_only=False):\n\n  def Test(self):\n    if gpu_only and not test.is_gpu_available():\n      tf_logging.info(\"Skipping InceptionFwd %s\", (input_size, filter_size,\n                                                   stride, padding))\n      return\n    tf_logging.info(\"Testing InceptionFwd %s\", (input_size, filter_size, stride,\n                                                padding))\n    self._CompareFwdValues(input_size, filter_size, [stride, stride], padding)\n\n  return Test\n\n\ndef GetInceptionFwdDilatedConvTest(input_size, filter_size, stride, padding):\n\n  def Test(self):\n    if stride == 1:\n      tf_logging.info(\"Testing InceptionFwd with dilations %s\",\n                      (input_size, filter_size, stride, padding))\n      self._VerifyDilatedConvValues(\n          tensor_in_sizes=input_size,\n          filter_in_sizes=filter_size,\n          strides=[stride, stride],\n          dilations=[2, 2],\n          padding=padding,\n          rtol=5e-4)\n\n  return Test\n\n\ndef GetInceptionBackInputTest(input_size, filter_size, output_size, stride,\n                              padding,\n                              gpu_only=False):\n\n  def Test(self):\n    if gpu_only and not test.is_gpu_available():\n      tf_logging.info(\"Skipping InceptionBackInput %s\",\n                      (input_size, filter_size, output_size, stride, padding))\n      return\n    tf_logging.info(\"Testing InceptionBackInput %s\",\n                    (input_size, filter_size, output_size, stride, padding))\n    self._CompareBackpropInput(input_size, filter_size, output_size,\n                               [stride, stride], padding)\n\n  return Test\n\n\ndef GetInceptionBackFilterTest(input_size, filter_size, output_size, strides,\n                               padding, gpu_only=False):\n\n  def Test(self):\n    if gpu_only and not test.is_gpu_available():\n      tf_logging.info(\"Skipping InceptionBackFilter %s\",\n                      (input_size, filter_size, output_size, strides, padding))\n      return\n    tf_logging.info(\"Testing InceptionBackFilter %s\",\n                    (input_size, filter_size, output_size, strides, padding))\n    self._CompareBackFilter(input_size, filter_size, output_size, strides,\n                            padding)\n\n  return Test\n\n\n@test_util.run_all_without_tensor_float_32(\"Avoid TF32 conv on GPU\")\nclass FusedConv2DTest(test.TestCase):\n\n  def _CreateNumpyTensor(self, shape):\n    total_size = np.prod(shape)\n    return np.arange(1, total_size + 1, dtype=np.float32).reshape(shape)\n\n  def _CreateConv2D(self,\n                    input_values,\n                    filters,\n                    strides=[1, 1],\n                    padding=\"SAME\"):\n    return nn_ops.convolution(\n        input_values, filters, strides=strides, padding=padding)\n\n  # Tests tensor forwarding of a fused Conv2D+BiasAdd+Add op when the input to\n  # Add has refcount 1.\n  @test_util.run_in_graph_and_eager_modes(use_gpu=False)\n  def testAddWithRefCountOne(self):\n    expected_output = [\n        113377, 125570, 77305, 86738, 19433, 22226, 60681, 70722, 36291, 43718,\n        7143, 9206, 9785, 12098, 4783, 6366, 779, 1134\n    ]\n    tensor_in_sizes = [1, 3, 3, 2]\n    filter_in_sizes = [2, 2, 2, 2]\n    bias_in_sizes = [2]\n\n    x = self._CreateNumpyTensor(tensor_in_sizes)\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    bias_in = self._CreateNumpyTensor(bias_in_sizes)\n    # To get different weights for filter\n    offset = 1\n\n    conv1 = self._CreateConv2D(x, filter_in)\n    conv2 = self._CreateConv2D(conv1, filter_in + offset)\n\n    conv = self._CreateConv2D(conv1, filter_in - offset)\n    bias_add = nn_ops.bias_add(conv, bias_in)\n    add = math_ops.add_n([bias_add, conv2])\n\n    self.assertAllEqual(\n        np.rint(expected_output),\n        self.evaluate(add).reshape(-1))\n\n  # Tests tensor forwarding of a fused Conv2D+BiasAdd+Add op when the input to\n  # Add has a total refcount of 2, and Add is its last consumer.\n  @test_util.run_in_graph_and_eager_modes(use_gpu=False)\n  def testAddWithRefCountTwoAndRunAddLast(self):\n    expected_output = [\n        1.907175e+06, 2.253505e+06, 7.809210e+05, 9.537180e+05, 1.184170e+05,\n        1.523070e+05, 5.367010e+05, 6.803700e+05, 1.867090e+05, 2.529460e+05,\n        2.362300e+04, 3.522600e+04, 5.121700e+04, 7.168300e+04, 1.494300e+04,\n        2.347400e+04, 1.558000e+03, 2.903000e+03\n    ]\n    tensor_in_sizes = [1, 3, 3, 2]\n    filter_in_sizes = [2, 2, 2, 2]\n    bias_in_sizes = [2]\n\n    x = self._CreateNumpyTensor(tensor_in_sizes)\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    bias_in = self._CreateNumpyTensor(bias_in_sizes)\n    # To get different weights for filter\n    offset = 1\n\n    conv1 = self._CreateConv2D(x, filter_in)\n    conv2 = self._CreateConv2D(conv1, filter_in + offset)\n\n    conv = self._CreateConv2D(conv2, filter_in - offset)\n    bias_add = nn_ops.bias_add(conv, bias_in)\n    add = math_ops.add_n([bias_add, conv1])\n\n    self.assertAllEqual(\n        np.rint(expected_output),\n        self.evaluate(add).reshape(-1))\n\n  # Tests tensor forwarding of a fused Conv2D+BiasAdd+Add op when the input to\n  # Add has refcount 2 and Add (in the fused Conv2D op) is its first consumer.\n  @test_util.run_in_graph_and_eager_modes(use_gpu=False)\n  def testAddWithRefCountTwoAndRunAddFirst(self):\n    expected_output = [\n        176161, 194450, 120673, 134822, 30545, 34734, 96041, 111102, 58149,\n        69289, 11745, 14839, 15833, 19302, 7965, 10339, 1345, 1877\n    ]\n    tensor_in_sizes = [1, 3, 3, 2]\n    filter_in_sizes = [2, 2, 2, 2]\n    bias_in_sizes = [2]\n\n    x = self._CreateNumpyTensor(tensor_in_sizes)\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    bias_in = self._CreateNumpyTensor(bias_in_sizes)\n    # To get different weights for filter\n    offset = 1\n\n    conv1 = self._CreateConv2D(x, filter_in)\n    conv2 = self._CreateConv2D(conv1, filter_in + offset)\n\n    conv = self._CreateConv2D(conv1, filter_in - offset)\n    bias_add = nn_ops.bias_add(conv, bias_in)\n    add = math_ops.add_n([bias_add, conv2])\n\n    relu = nn_ops.relu(add)\n    output = math_ops.add_n([relu, conv2])\n\n    self.assertAllEqual(\n        np.rint(expected_output),\n        self.evaluate(output).reshape(-1))\n\n  # Tests tensor forwarding of a fused Conv2D+BiasAdd+Add op when the input to\n  # Add has refcount 2, and there is no dependency between its two consumers.\n  @test_util.run_in_graph_and_eager_modes(use_gpu=False)\n  def testAddWithRefCountTwoAndNoDependence(self):\n    expected_output = [\n        176161, 194450, 120673, 134822, 30545, 34734, 96041, 111102, 58149,\n        69289, 11745, 14839, 15833, 19302, 7965, 10339, 1345, 1877\n    ]\n    tensor_in_sizes = [1, 3, 3, 2]\n    filter_in_sizes = [2, 2, 2, 2]\n    bias_in_sizes = [2]\n\n    x = self._CreateNumpyTensor(tensor_in_sizes)\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    bias_in = self._CreateNumpyTensor(bias_in_sizes)\n    # To get different weights for filter\n    offset = 1\n\n    conv1 = self._CreateConv2D(x, filter_in)\n    conv2 = self._CreateConv2D(conv1, filter_in + offset)\n\n    conv = self._CreateConv2D(conv1, filter_in - offset)\n    bias_add = nn_ops.bias_add(conv, bias_in)\n    add = math_ops.add_n([bias_add, conv2])\n\n    relu1 = nn_ops.relu(add)\n    relu2 = nn_ops.relu(conv2)\n    output = math_ops.add_n([relu1, relu2])\n\n    self.assertAllEqual(\n        np.rint(expected_output),\n        self.evaluate(output).reshape(-1))\n\n  # Tests tensor forwarding of a fused Conv2D+BiasAdd+Add op when the input to\n  # Add is the same as the input to the fused Conv2D op and needs a tensor\n  # buffer.\n  @test_util.run_in_graph_and_eager_modes(use_gpu=False)\n  def testAddWithSameSrcAndAddTensorBuffer(self):\n    expected_output = [\n        57157, 63298, 39249, 44026, 9971, 11402, 31193, 36306, 19126, 22948,\n        3970, 5060, 5135, 6350, 2666, 3524, 461, 674\n    ]\n    tensor_in_sizes = [1, 3, 3, 2]\n    filter_in_sizes = [2, 2, 2, 2]\n    bias_in_sizes = [2]\n\n    x = self._CreateNumpyTensor(tensor_in_sizes)\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    bias_in = self._CreateNumpyTensor(bias_in_sizes)\n\n    conv1 = self._CreateConv2D(x, filter_in)\n\n    conv = self._CreateConv2D(conv1, filter_in)\n    bias_add = nn_ops.bias_add(conv, bias_in)\n    add = math_ops.add_n([bias_add, conv1])\n\n    self.assertAllEqual(\n        np.rint(expected_output),\n        self.evaluate(add).reshape(-1))\n\n\nif __name__ == \"__main__\":\n  for index, (input_size_, filter_size_, output_size_, stride_,\n              padding_) in enumerate(GetShrunkInceptionShapes()):\n    setattr(Conv2DTest, \"testInceptionFwd_\" + str(index),\n            test_util.run_in_graph_and_eager_modes(\n                GetInceptionFwdTest(input_size_, filter_size_, stride_,\n                                    padding_)))\n    setattr(\n        Conv2DTest, \"testInceptionFwdDilatedConv_\" + str(index),\n        test_util.run_in_graph_and_eager_modes(GetInceptionFwdDilatedConvTest(\n            input_size_, filter_size_, stride_, padding_)))\n    setattr(Conv2DTest, \"testInceptionBackInput_\" + str(index),\n            test_util.run_in_graph_and_eager_modes(\n                GetInceptionBackInputTest(input_size_, filter_size_,\n                                          output_size_, stride_, padding_)))\n    setattr(Conv2DTest, \"testInceptionBackFilter_\" + str(index),\n            test_util.run_in_graph_and_eager_modes(\n                GetInceptionBackFilterTest(input_size_, filter_size_,\n                                           output_size_, [stride_, stride_],\n                                           padding_)))\n\n  # TODO(b/35359731)\n  # Fwd, BckInput, and BackFilter to test that for certain input parameter\n  # set, winograd nonfused algorithm will be excluded from conv autotune. If\n  # in such case, winograd nonfused algorithm is added as one option of the\n  # conv autotune, and cuDNN version is smaller than 7, the following tests\n  # will fail.\n  ishape = [1, 400, 400, 1]\n  fshape = [1, 1, 1, 256]\n  oshape = [1, 400, 400, 256]\n  setattr(Conv2DTest, \"testInceptionFwd_No_Winograd_Nonfused\",\n          test_util.run_in_graph_and_eager_modes(\n              GetInceptionFwdTest(ishape, fshape, 1, \"SAME\", gpu_only=True)))\n  setattr(Conv2DTest, \"testInceptionFwdDilatedConv_No_Winograd_Nonfused\",\n          test_util.run_in_graph_and_eager_modes(\n              GetInceptionFwdDilatedConvTest(ishape, fshape, 1, \"SAME\")))\n  setattr(Conv2DTest, \"testInceptionBackInput_No_Winograd_Nonfused\",\n          test_util.run_in_graph_and_eager_modes(\n              GetInceptionBackInputTest(ishape, fshape, oshape, 1, \"SAME\",\n                                        gpu_only=True)))\n  setattr(Conv2DTest, \"testInceptionBackFilter_No_Winograd_Nonfused\",\n          test_util.run_in_graph_and_eager_modes(\n              GetInceptionBackFilterTest(ishape, fshape, oshape, [1, 1], \"SAME\",\n                                         gpu_only=True)))\n  test.main()"