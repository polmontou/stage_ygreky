"# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for tensorflow.ops.linalg_ops.eig.\"\"\"\n\nimport numpy as np\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes as dtypes_lib\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gradient_checker_v2\nfrom tensorflow.python.ops import linalg_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import sort_ops\nfrom tensorflow.python.platform import test\n\n\ndef _AddTest(test_class, op_name, testcase_name, fn):\n  test_name = \"_\".join([\"test\", op_name, testcase_name])\n  if hasattr(test_class, test_name):\n    raise RuntimeError(\"Test %s defined more than once\" % test_name)\n  setattr(test_class, test_name, fn)\n\n\nclass EigTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testWrongDimensions(self):\n    # The input to self_adjoint_eig should be a tensor of\n    # at least rank 2.\n    scalar = constant_op.constant(1.)\n    with self.assertRaises(ValueError):\n      linalg_ops.eig(scalar)\n    vector = constant_op.constant([1., 2.])\n    with self.assertRaises(ValueError):\n      linalg_ops.eig(vector)\n\n  @test_util.run_deprecated_v1\n  def testConcurrentExecutesWithoutError(self):\n    all_ops = []\n    with self.session():\n      for compute_v_ in True, False:\n        matrix1 = random_ops.random_normal([5, 5], seed=42)\n        matrix2 = random_ops.random_normal([5, 5], seed=42)\n        if compute_v_:\n          e1, v1 = linalg_ops.eig(matrix1)\n          e2, v2 = linalg_ops.eig(matrix2)\n          all_ops += [e1, v1, e2, v2]\n        else:\n          e1 = linalg_ops.eigvals(matrix1)\n          e2 = linalg_ops.eigvals(matrix2)\n          all_ops += [e1, e2]\n      val = self.evaluate(all_ops)\n      self.assertAllEqual(val[0], val[2])\n      # The algorithm is slightly different for compute_v being True and False,\n      # so require approximate equality only here.\n      self.assertAllClose(val[2], val[4])\n      self.assertAllEqual(val[4], val[5])\n      self.assertAllEqual(val[1], val[3])\n\n  def testMatrixThatFailsWhenFlushingDenormsToZero(self):\n    # Test a 32x32 matrix which is known to fail if denorm floats are flushed to\n    # zero.\n    matrix = np.genfromtxt(\n        test.test_src_dir_path(\n            \"python/kernel_tests/linalg/testdata/\"\n            \"self_adjoint_eig_fail_if_denorms_flushed.txt\")).astype(np.float32)\n    self.assertEqual(matrix.shape, (32, 32))\n    matrix_tensor = constant_op.constant(matrix)\n    with self.session() as _:\n      (e, v) = self.evaluate(linalg_ops.self_adjoint_eig(matrix_tensor))\n      self.assertEqual(e.size, 32)\n      self.assertAllClose(\n          np.matmul(v, v.transpose()), np.eye(32, dtype=np.float32), atol=2e-3)\n      self.assertAllClose(matrix,\n                          np.matmul(np.matmul(v, np.diag(e)), v.transpose()))\n\n\ndef SortEigenValues(e):\n  perm = np.argsort(e.real + e.imag, -1)\n  return np.take(e, perm, -1)\n\n\ndef SortEigenDecomposition(e, v):\n  if v.ndim < 2:\n    return e, v\n  perm = np.argsort(e.real + e.imag, -1)\n  return np.take(e, perm, -1), np.take(v, perm, -1)\n\n\ndef EquilibrateEigenVectorPhases(x, y):\n  \"\"\"Equilibrate the phase of the Eigenvectors in the columns of `x` and `y`.\n\n  Eigenvectors are only unique up to an arbitrary phase. This function rotates x\n  such that it matches y. Precondition: The columns of x and y differ by a\n  multiplicative complex phase factor only.\n\n  Args:\n    x: `np.ndarray` with Eigenvectors\n    y: `np.ndarray` with Eigenvectors\n\n  Returns:\n    `np.ndarray` containing an equilibrated version of x.\n  \"\"\"\n  phases = np.sum(np.conj(x) * y, -2, keepdims=True)\n  phases /= np.abs(phases)\n  return phases * x\n\n\ndef _GetEigTest(dtype_, shape_, compute_v_):\n\n  def CompareEigenVectors(self, x, y, tol):\n    x = EquilibrateEigenVectorPhases(x, y)\n    self.assertAllClose(x, y, atol=tol)\n\n  def CompareEigenDecompositions(self, x_e, x_v, y_e, y_v, tol):\n    num_batches = int(np.prod(x_e.shape[:-1]))\n    n = x_e.shape[-1]\n    x_e = np.reshape(x_e, [num_batches] + [n])\n    x_v = np.reshape(x_v, [num_batches] + [n, n])\n    y_e = np.reshape(y_e, [num_batches] + [n])\n    y_v = np.reshape(y_v, [num_batches] + [n, n])\n    for i in range(num_batches):\n      x_ei, x_vi = SortEigenDecomposition(x_e[i, :], x_v[i, :, :])\n      y_ei, y_vi = SortEigenDecomposition(y_e[i, :], y_v[i, :, :])\n      self.assertAllClose(x_ei, y_ei, atol=tol, rtol=tol)\n      CompareEigenVectors(self, x_vi, y_vi, tol)\n\n  def Test(self):\n    np.random.seed(1)\n    n = shape_[-1]\n    batch_shape = shape_[:-2]\n    np_dtype = dtype_.as_numpy_dtype\n\n    def RandomInput():\n      # Most matrices are diagonalizable\n      a = np.random.uniform(\n          low=-1.0, high=1.0, size=n * n).reshape([n, n]).astype(np_dtype)\n      if dtype_.is_complex:\n        a += 1j * np.random.uniform(\n            low=-1.0, high=1.0, size=n * n).reshape([n, n]).astype(np_dtype)\n      a = np.tile(a, batch_shape + (1, 1))\n      return a\n\n    if dtype_ in (dtypes_lib.float32, dtypes_lib.complex64):\n      atol = 1e-4\n    else:\n      atol = 1e-12\n\n    a = RandomInput()\n    np_e, np_v = np.linalg.eig(a)\n    with self.session():\n      if compute_v_:\n        tf_e, tf_v = linalg_ops.eig(constant_op.constant(a))\n\n        # Check that V*diag(E)*V^(-1) is close to A.\n        a_ev = math_ops.matmul(\n            math_ops.matmul(tf_v, array_ops.matrix_diag(tf_e)),\n            linalg_ops.matrix_inverse(tf_v))\n        self.assertAllClose(self.evaluate(a_ev), a, atol=atol)\n\n        # Compare to numpy.linalg.eig.\n        CompareEigenDecompositions(self, np_e, np_v, self.evaluate(tf_e),\n                                   self.evaluate(tf_v), atol)\n      else:\n        tf_e = linalg_ops.eigvals(constant_op.constant(a))\n        self.assertAllClose(\n            SortEigenValues(np_e),\n            SortEigenValues(self.evaluate(tf_e)),\n            atol=atol)\n\n  return Test\n\n\nclass EigGradTest(test.TestCase):\n  pass  # Filled in below\n\n\ndef _GetEigGradTest(dtype_, shape_, compute_v_):\n\n  def Test(self):\n    np.random.seed(1)\n    n = shape_[-1]\n    batch_shape = shape_[:-2]\n    np_dtype = dtype_.as_numpy_dtype\n\n    def RandomInput():\n      # Most matrices are diagonalizable\n      a = np.random.uniform(\n          low=-1.0, high=1.0, size=n * n).reshape([n, n]).astype(np_dtype)\n      if dtype_.is_complex:\n        a += 1j * np.random.uniform(\n            low=-1.0, high=1.0, size=n * n).reshape([n, n]).astype(np_dtype)\n      a = np.tile(a, batch_shape + (1, 1))\n      return a\n\n    # Optimal stepsize for central difference is O(epsilon^{1/3}).\n    epsilon = np.finfo(np_dtype).eps\n    delta = 0.1 * epsilon**(1.0 / 3.0)\n    # tolerance obtained by looking at actual differences using\n    # np.linalg.norm(theoretical-numerical, np.inf) on -mavx build\n    # after discarding one random input sample\n    _ = RandomInput()\n    if dtype_ in (dtypes_lib.float32, dtypes_lib.complex64):\n      tol = 1e-2\n    else:\n      tol = 1e-7\n    with self.session():\n\n      def Compute(x):\n        e, v = linalg_ops.eig(x)\n\n        # We sort eigenvalues by e.real+e.imag to have consistent\n        # order between runs\n        b_dims = len(e.shape) - 1\n        idx = sort_ops.argsort(math_ops.real(e) + math_ops.imag(e), axis=-1)\n        e = array_ops.gather(e, idx, batch_dims=b_dims)\n        v = array_ops.gather(v, idx, batch_dims=b_dims)\n\n        # (complex) Eigenvectors are only unique up to an arbitrary phase\n        # We normalize the vectors such that the first component has phase 0.\n        top_rows = v[..., 0:1, :]\n        angle = -math_ops.angle(top_rows)\n        phase = math_ops.complex(math_ops.cos(angle), math_ops.sin(angle))\n        v *= phase\n        return e, v\n\n      if compute_v_:\n        funcs = [lambda x: Compute(x)[0], lambda x: Compute(x)[1]]\n      else:\n        funcs = [linalg_ops.eigvals]\n\n      for f in funcs:\n        theoretical, numerical = gradient_checker_v2.compute_gradient(\n            f, [RandomInput()], delta=delta)\n        self.assertAllClose(theoretical, numerical, atol=tol, rtol=tol)\n\n  return Test\n\n\nif __name__ == \"__main__\":\n  dtypes_to_test = [\n      dtypes_lib.float32, dtypes_lib.float64, dtypes_lib.complex64,\n      dtypes_lib.complex128\n  ]\n  for compute_v in True, False:\n    for dtype in dtypes_to_test:\n      for size in 1, 2, 5, 10:\n        for batch_dims in [(), (3,)] + [(3, 2)] * (max(size, size) < 10):\n          shape = batch_dims + (size, size)\n          name = \"%s_%s_%s\" % (dtype.name, \"_\".join(map(str, shape)), compute_v)\n          _AddTest(EigTest, \"Eig\", name, _GetEigTest(dtype, shape, compute_v))\n\n          if dtype not in [dtypes_lib.float32, dtypes_lib.float64]:\n            _AddTest(EigGradTest, \"EigGrad\", name,\n                     _GetEigGradTest(dtype, shape, compute_v))\n  test.main()"