"# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Tests for the currently experimental in-graph batch ops.\"\"\"\nimport threading\nimport time\nimport numpy as np\n\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import function\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.framework.errors import InvalidArgumentError\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import batch_ops\nfrom tensorflow.python.ops import gen_batch_ops\nfrom tensorflow.python.ops import gen_functional_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import resource_variable_ops\nfrom tensorflow.python.ops import script_ops\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.platform import test\n\n\ndef delayed_plus1(x):\n  \"\"\"Sleeps for 100ms then returns x+1.\"\"\"\n  time.sleep(0.1)\n  return x + 1\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass BatchOpsTest(test.TestCase):\n  \"\"\"Tests for batch_ops.{un,}batch.\"\"\"\n\n  # Test for only non eager mode as batching in eager context as a functionality\n  # is TBD.\n  def testBasicBatch(self):\n    \"\"\"Tests that a single batched tensor executes together and only once.\"\"\"\n    if context.executing_eagerly():\n      return\n    with self.cached_session() as sess:\n      inp = array_ops.placeholder(dtype=dtypes.int32, shape=[1])\n      batched, index, _ = batch_ops.batch(\n          [inp], num_batch_threads=1, max_batch_size=2,\n          batch_timeout_micros=36000000, grad_timeout_micros=0,\n          batching_queue=\"\")\n      thread_results = []\n\n      def worker():\n        thread_results.extend(\n            sess.run([batched, index], feed_dict={inp: [1]}))\n\n      worker_thread = threading.Thread(target=worker)\n      worker_thread.start()\n      main_results = sess.run([batched, index], feed_dict={inp: [2]})\n      worker_thread.join()\n\n      # At this point either the thread or the main did the batch and the other\n      # should have empty results.\n      if list(thread_results[0][0]):\n        batch_t = thread_results[0][0]\n        index_t = thread_results[1]\n        empty_b = main_results[0][0]\n        empty_m = main_results[1]\n      else:\n        batch_t = main_results[0][0]\n        index_t = main_results[1]\n        empty_b = thread_results[0][0]\n        empty_m = thread_results[1]\n\n      # Check that both the inputs made it out exactly once.\n      self.assertAllEqual(sorted(batch_t), (1, 2))\n      # Check that we get 2 rows in the index tensor.\n      self.assertEqual(len(index_t), 2)\n      # Check that the other ones are empty.\n      self.assertEqual(len(empty_b), 0)\n      self.assertEqual(len(empty_m), 0)\n\n  def testBatchWithPadding(self):\n    \"\"\"Test that batching with padding up to an allowed batch size works.\"\"\"\n    if context.executing_eagerly():\n      return\n    with self.cached_session() as sess:\n      inp = array_ops.placeholder(dtype=dtypes.int32, shape=[2])\n      batched, index, _ = batch_ops.batch(\n          [inp], num_batch_threads=1, max_batch_size=10,\n          batch_timeout_micros=100000,  # 100ms\n          allowed_batch_sizes=[5, 10],\n          grad_timeout_micros=0, batching_queue=\"\")\n      thread_results = []\n\n      def worker():\n        thread_results.extend(\n            sess.run([batched, index], feed_dict={inp: [1, 3]}))\n\n      worker_thread = threading.Thread(target=worker)\n      worker_thread.start()\n      main_results = sess.run([batched, index], feed_dict={inp: [2, 4]})\n      worker_thread.join()\n\n      # At this point either the thread or the main did the batch and the other\n      # should have empty results.\n      if list(thread_results[0][0]):\n        batch_t = thread_results[0][0]\n      else:\n        batch_t = main_results[0][0]\n\n      # Check that the batch tensor incorporates the padding.\n      self.assertEqual(len(batch_t), 5)\n\n  def testMultipleBatch(self):\n    \"\"\"Tests that multiple batched tensors execute together.\"\"\"\n    if context.executing_eagerly():\n      return\n    with self.cached_session() as sess:\n      inp0 = array_ops.placeholder(dtype=dtypes.int32, shape=[1])\n      inp1 = array_ops.placeholder(dtype=dtypes.int32, shape=[1])\n      batched, _, _ = batch_ops.batch(\n          [inp0, inp1],\n          num_batch_threads=1,\n          max_batch_size=2,\n          batch_timeout_micros=36000000,\n          grad_timeout_micros=0,\n          batching_queue=\"\")\n      thread_results = []\n\n      def worker():\n        thread_results.extend(\n            sess.run([batched], feed_dict={inp0: [1],\n                                           inp1: [2]}))\n\n      worker_thread = threading.Thread(target=worker)\n      worker_thread.start()\n      main_results = sess.run([batched], feed_dict={inp0: [2], inp1: [3]})\n      worker_thread.join()\n\n      # At this point either the thread or the main did the batch and the other\n      # should have empty results.\n      if list(thread_results[0][0]):\n        batch_t = thread_results[0]\n        empty_t = main_results[0]\n      else:\n        batch_t = main_results[0]\n        empty_t = thread_results[0]\n\n      # Assert that the tensors were batched together.\n      self.assertAllEqual(sorted(batch_t[0]), [1, 2])\n      self.assertAllEqual(sorted(batch_t[1]), [2, 3])\n      self.assertAllEqual(empty_t[0], [])\n      self.assertAllEqual(empty_t[1], [])\n\n  def testIllegalBatchDifferentDim0Sizes(self):\n    \"\"\"Tests illegally feeding tensors with different dim0 sizes.\"\"\"\n    if context.executing_eagerly():\n      return\n    with self.cached_session() as sess:\n      inp0 = array_ops.placeholder(dtype=dtypes.int32, shape=[1])\n      inp1 = array_ops.placeholder(dtype=dtypes.int32, shape=[2])\n      batched, index, _ = batch_ops.batch(\n          [inp0, inp1], num_batch_threads=1, max_batch_size=2,\n          batch_timeout_micros=0, grad_timeout_micros=0, batching_queue=\"\")\n      with self.assertRaises(Exception) as raised:\n        _ = sess.run([batched, index], feed_dict={inp0: [0], inp1: [1, 2]})\n      self.assertGreater(\n          raised.exception.message.find(\"must have equal 0th-dimension size\"),\n          0)\n\n  def testBasicUnbatch(self):\n    \"\"\"Tests that batch and unbatch work together.\"\"\"\n    if context.executing_eagerly():\n      return\n    with self.cached_session() as sess:\n      inp = array_ops.placeholder(dtype=dtypes.int32, shape=[1])\n      batched, index, id_t = batch_ops.batch(\n          [inp], num_batch_threads=1, max_batch_size=10,\n          batch_timeout_micros=100000,  # 100ms\n          allowed_batch_sizes=[3, 10],\n          grad_timeout_micros=0, batching_queue=\"\")\n      computation = batched[0] + 1\n      result = batch_ops.unbatch(computation, index, id_t,\n                                 timeout_micros=1000000, shared_name=\"unbatch\")\n      thread_results = []\n\n      def worker():\n        thread_results.extend(sess.run([result], feed_dict={inp: [1]}))\n\n      worker_thread = threading.Thread(target=worker)\n      worker_thread.start()\n      main_results = sess.run([result], feed_dict={inp: [2]})\n      worker_thread.join()\n      self.assertEqual(thread_results[0], [2])\n      self.assertEqual(main_results[0], [3])\n\n  def testBasicUnbatchDecorated(self):\n    \"\"\"Tests that the batch_function decorator works.\"\"\"\n    if context.executing_eagerly():\n      return\n    with self.cached_session() as sess:\n      # TODO(apassos): Removing this line causes test flakiness! Ideally should\n      # be investigated.\n      default_inp = array_ops.placeholder_with_default(2, shape=[])  # pylint: disable=unused-variable\n\n      @batch_ops.batch_function(1, 10, 100000)\n      def computation(in_t):\n        self.assertTrue(in_t.shape is not None)\n        return in_t + 1\n\n      inp = array_ops.placeholder(dtype=dtypes.int32, shape=[1])\n      result = computation(inp)\n      thread_results = []\n\n      def worker():\n        thread_results.extend(sess.run([result], feed_dict={inp: [1]}))\n\n      worker_thread = threading.Thread(target=worker)\n      worker_thread.start()\n      main_results = sess.run([result], feed_dict={inp: [2]})\n      worker_thread.join()\n      self.assertEqual(thread_results[0], [2])\n      self.assertEqual(main_results[0], [3])\n\n  def testUnbatchInvalidIdArg(self):\n    \"\"\"Tests that unbatch work together.\"\"\"\n    if context.executing_eagerly():\n      batched_tensor = constant_op.constant(\n          value=np.random.random(size=(3, 3, 1)), dtype=dtypes.float64)\n      batched_index = constant_op.constant(\n          value=np.random.randint(0, 100, size=(3, 3, 1)), dtype=dtypes.int64)\n      arg_id = constant_op.constant(\n          value=np.random.randint(0, 100, size=(3, 3, 1)), dtype=dtypes.int64)\n\n      with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                  \"Input id should be scalar;\"):\n        batch_ops.unbatch(\n            batched_tensor=batched_tensor,\n            batch_index=batched_index,\n            id=arg_id,\n            timeout_micros=50,\n            container=\"\",\n            shared_name=\"\")\n\n  def testBatchDecoratedWithCapturedInput(self):\n    \"\"\"Tests that the batch_function decorator works.\"\"\"\n    if context.executing_eagerly():\n      return\n    with self.cached_session() as sess:\n      captured_inp0 = array_ops.placeholder_with_default(2., shape=[])\n      captured_inp1 = resource_variable_ops.ResourceVariable(3.)\n      with ops.device(\"/cpu:0\"):\n        captured_inp2 = resource_variable_ops.ResourceVariable(4.)\n\n      @batch_ops.batch_function(1, 10, 100000)\n      def computation(in_t):\n        return in_t + captured_inp0 + captured_inp1 + captured_inp2\n\n      inp = array_ops.placeholder(dtype=dtypes.float32, shape=[1])\n      result = computation(inp)\n      thread_results = []\n\n      def worker():\n        thread_results.extend(sess.run([result], feed_dict={inp: [1]}))\n\n      sess.run(variables.global_variables_initializer())\n      worker_thread = threading.Thread(target=worker)\n      worker_thread.start()\n      main_results = sess.run([result], feed_dict={inp: [2]})\n      worker_thread.join()\n      self.assertEqual(thread_results[0], [10])\n      self.assertEqual(main_results[0], [11])\n\n  @test_util.disable_xla(\"DeviceIndex returns sentinel value with XLA\")\n  def testBatchDecoratedGpu(self):\n    if context.executing_eagerly():\n      return\n    with self.cached_session() as sess:\n\n      @batch_ops.batch_function(1, 10, 100000)\n      def computation(in_t):\n        # index is 0 on CPU and 1 on GPU\n        index = gen_functional_ops.DeviceIndex(device_names=[\"CPU\", \"GPU\"])\n        return in_t + math_ops.cast(index, dtypes.float32)\n\n      inp = array_ops.placeholder(dtype=dtypes.float32, shape=[1])\n      result = computation(inp)\n      thread_results = []\n\n      def worker():\n        thread_results.extend(sess.run([result], feed_dict={inp: [10.]}))\n\n      worker_thread = threading.Thread(target=worker)\n      worker_thread.start()\n      main_results = sess.run([result], feed_dict={inp: [20.]})\n      worker_thread.join()\n      self.assertEqual(thread_results[0], [10 + test_util.is_gpu_available()])\n      self.assertEqual(main_results[0], [20 + test_util.is_gpu_available()])\n\n  def testParallelRunsWithCpuAndGpu(self):\n    # Run multiple instances of a batch function in parallel. This is a\n    # regression test: this used to fail because _Send nodes for one call would\n    # send the tensor to the _Recv node for a different call.\n    if context.executing_eagerly():\n      return\n    @batch_ops.batch_function(1, 2, 1)\n    def f(x):\n      with ops.device(\"/GPU:0\"):\n        x = x + 1.\n      with ops.device(\"/CPU:0\"):\n        return x + 1\n    num_calls = 10\n    placeholders = [array_ops.placeholder(dtypes.float32, shape=(1,))\n                    for _ in range(num_calls)]\n    results = []\n    for p in placeholders:\n      result = f(p)\n      results.append(result)\n    inputs = [[float(i)] for i in range(num_calls)]\n    expected = [[float(i + 2)] for i in range(num_calls)]\n    with self.session() as sess:\n      outputs = sess.run(results, feed_dict=dict(zip(placeholders, inputs)))\n      self.assertAllEqual(outputs, expected)\n\n  def testSoftPlacement(self):\n    if context.executing_eagerly():\n      return\n\n    @batch_ops.batch_function(1, 10, 100000)\n    def computation(in_t):\n      with ops.device(\"/GPU:0\"):\n        return in_t + 1.\n\n    inp = array_ops.placeholder(dtype=dtypes.float32, shape=[1])\n    result = computation(inp)\n\n    # With soft placement, the function will run even without a GPU\n    config = config_pb2.ConfigProto(allow_soft_placement=True)\n    with self.session(config=config) as sess:\n      sess.run([result], feed_dict={inp: [20.]})\n\n    # Without soft placement, the function fails without a GPU due to the\n    # addition explicitly being placed on the GPU\n    config.allow_soft_placement = False\n    with self.session(config=config) as sess:\n      if test_util.is_gpu_available():\n        sess.run([result], feed_dict={inp: [20.]})\n      else:\n        with self.assertRaisesRegex(InvalidArgumentError,\n                                    \"Cannot assign a device for operation\"):\n          sess.run([result], feed_dict={inp: [20.]})\n\n  def testBatchFunctionOp(self):\n    \"\"\"Tests that the batch_function op works.\"\"\"\n    if context.executing_eagerly():\n      return\n    with self.cached_session() as sess:\n\n      @function.Defun(dtypes.int32)\n      def computation(in_t):\n        return in_t + 1\n\n      inp = array_ops.placeholder(dtype=dtypes.int32, shape=[1])\n      result = gen_batch_ops.batch_function(\n          [inp],\n          num_batch_threads=1,\n          max_batch_size=10,\n          batch_timeout_micros=100000,\n          Tout=[dtypes.int32],\n          f=computation,\n          captured_tensors=computation.captured_inputs)\n      thread_results = []\n\n      def worker():\n        thread_results.extend(sess.run([result], feed_dict={inp: [1]}))\n\n      worker_thread = threading.Thread(target=worker)\n      worker_thread.start()\n      main_results = sess.run([result], feed_dict={inp: [2]})\n      worker_thread.join()\n      self.assertEqual(thread_results[0], [2])\n      self.assertEqual(main_results[0], [3])\n\n  def testBatchFunctionOpWithCapturedInput(self):\n    \"\"\"Tests that batch_function op works with captured input.\"\"\"\n    if context.executing_eagerly():\n      return\n    with self.cached_session() as sess:\n      captured_inp0 = array_ops.placeholder_with_default(2, shape=[])\n      captured_inp1 = array_ops.placeholder_with_default(1, shape=[])\n      inp = array_ops.placeholder(dtype=dtypes.int32, shape=[1])\n\n      @function.Defun(dtypes.int32)\n      def computation(inp):\n        return inp + captured_inp0 - captured_inp1\n\n      result = gen_batch_ops.batch_function(\n          num_batch_threads=1,\n          max_batch_size=10,\n          batch_timeout_micros=100000,  # 100ms\n          allowed_batch_sizes=[3, 10],\n          batching_queue=\"\",\n          f=computation,\n          in_tensors=[inp],\n          captured_tensors=computation.captured_inputs,\n          Tout=[o.type for o in computation.definition.signature.output_arg])\n\n      thread_results = []\n\n      def worker():\n        thread_results.extend(sess.run([result], feed_dict={inp: [1]}))\n\n      worker_thread = threading.Thread(target=worker)\n      worker_thread.start()\n      main_results = sess.run([result], feed_dict={inp: [2]})\n      worker_thread.join()\n      self.assertEqual(thread_results[0], [2])\n      self.assertEqual(main_results[0], [3])\n\n  def testBatchFunctionOpWithInputError(self):\n    \"\"\"Tests that batch_function op works with error in the inputs.\"\"\"\n    if context.executing_eagerly():\n      return\n    with self.cached_session() as sess:\n      inp = array_ops.placeholder(dtype=dtypes.int32, shape=[1])\n\n      @function.Defun(dtypes.int32, dtypes.int32)\n      def computation(in0, in1):\n        return in0 + in1\n\n      result = gen_batch_ops.batch_function(\n          [inp],  # computation actually expects 2 inputs.\n          num_batch_threads=1,\n          max_batch_size=10,\n          batch_timeout_micros=100000,  # 100ms\n          batching_queue=\"\",\n          f=computation,\n          captured_tensors=computation.captured_inputs,\n          Tout=[o.type for o in computation.definition.signature.output_arg])\n\n      with self.assertRaisesRegex(\n          InvalidArgumentError,\n          r\"Function takes 2 argument\\(s\\) but 1 argument\\(s\\) were passed\"):\n        sess.run([result], feed_dict={inp: [2]})\n\n  def testBatchFunctionOpWithLargeBatchSplitted(self):\n    \"\"\"Tests that the batch_function op works with large batch splitted.\"\"\"\n    if context.executing_eagerly():\n      return\n\n    with self.cached_session() as sess:\n\n      @function.Defun(dtypes.int32)\n      def computation(in_t):\n        return in_t + 3\n\n      inp = array_ops.placeholder(dtype=dtypes.int32)\n      result = gen_batch_ops.batch_function(\n          [inp],\n          num_batch_threads=2,\n          # enable_large_batch_splitting is True, so it's valid as long as\n          # max('allowed_batch_sizes') <= 'max_batch_size'.\n          allowed_batch_sizes=[1, 2],\n          max_batch_size=5,\n          batch_timeout_micros=100000,  # 100ms\n          Tout=[dtypes.int32],\n          enable_large_batch_splitting=True,\n          f=computation,\n          captured_tensors=computation.captured_inputs)\n      thread1_results = []\n      thread2_results = []\n\n      # Input sizes of worker1 and main thread are larger than\n      # max(allowed_batch_sizes), while input size of worker2 is smaller.\n      def worker1():\n        thread1_results.extend(\n            sess.run([result], feed_dict={inp: [5, 6, 7, 8, 9]}))\n\n      worker_thread1 = threading.Thread(target=worker1)\n      worker_thread1.start()\n\n      def worker2():\n        thread2_results.extend(sess.run([result], feed_dict={inp: [10]}))\n\n      worker_thread2 = threading.Thread(target=worker2)\n      worker_thread2.start()\n\n      main_results = sess.run([result], feed_dict={inp: [2, 3, 4]})\n      worker_thread1.join()\n      worker_thread2.join()\n      self.assertTrue(\n          np.all(np.equal(thread2_results[0], np.array([13], dtype=np.int32))))\n      self.assertTrue(\n          np.all(\n              np.equal(thread1_results[0],\n                       np.array([8, 9, 10, 11, 12], dtype=np.int32))))\n      self.assertTrue(\n          np.all(\n              np.equal(main_results[0], np.array([5, 6, 7], dtype=np.int32))))\n\n  def testBasicUnbatchDecoratedWithReshape(self):\n    \"\"\"Tests that the batch_function decorator works.\"\"\"\n    if context.executing_eagerly():\n      return\n    with self.cached_session() as sess:\n\n      @batch_ops.batch_function(1, 10, 100000)\n      def computation(in_t):\n        return array_ops.reshape(in_t, [-1]) + 1\n\n      inp = array_ops.placeholder(dtype=dtypes.int32, shape=[1, 1])\n      result = computation(inp)\n      thread_results = []\n\n      def worker():\n        thread_results.extend(sess.run([result], feed_dict={inp: [[1]]}))\n\n      worker_thread = threading.Thread(target=worker)\n      worker_thread.start()\n      main_results = sess.run([result], feed_dict={inp: [[2]]})\n      worker_thread.join()\n      self.assertEqual(thread_results[0], [2])\n      self.assertEqual(main_results[0], [3])\n\n  def testUnbatchTimeout(self):\n    \"\"\"Tests that the unbatch timeout works.\"\"\"\n    if context.executing_eagerly():\n      return\n    with self.cached_session() as sess:\n      inp = array_ops.placeholder(dtype=dtypes.int32, shape=[1])\n      batched, index, id_t = batch_ops.batch(\n          [inp], num_batch_threads=1, max_batch_size=2,\n          batch_timeout_micros=36000000, grad_timeout_micros=0,\n          batching_queue=\"\")\n      computation = batched[0] + 1\n      timeout_micros = 10\n      result = batch_ops.unbatch(computation, index, id_t, timeout_micros,\n                                 shared_name=\"shared_unbatch\")\n      # Set up a parallel pipeline that delays the computation, but uses the\n      # same unbatch resource object as the non-delayed pipeline.\n      computation_delayed = script_ops.py_func(delayed_plus1,\n                                               [batched[0]],\n                                               dtypes.int32)\n      result_delayed = batch_ops.unbatch(computation_delayed,\n                                         index,\n                                         id_t,\n                                         timeout_micros,\n                                         shared_name=\"shared_unbatch\")\n\n      thread_results = []\n      def worker():\n        # A first call using the non-delayed pipeline. The batcher will send an\n        # empty tensor along the non-delayed pipeline.\n        thread_results.extend(sess.run([result], feed_dict={inp: [1]}))\n      worker_thread = threading.Thread(target=worker)\n      worker_thread.start()\n      time.sleep(0.1)  # Ensure the thread's call starts first.\n      # A second call using the delayed pipeline.  The batcher will send the\n      # batched tensor along the delayed pipeline, thus delaying the arrival of\n      # the batched tensor at the unbatch op, relative to the empty tensor.\n      #\n      # TODO(olston, apassos): Avoid relying on the order in which the batch op\n      # emits the empty tensor versus the batched one.\n      _ = sess.run([result_delayed], feed_dict={inp: [2]})\n      worker_thread.join()\n      # The thread's call should hit the timeout, and thus get 0 results.\n      self.assertEqual(len(thread_results), 0)\n\n  def testUnbatchGradInvalidId(self):\n    with self.assertRaises(errors.InvalidArgumentError):\n      self.evaluate(\n          gen_batch_ops.unbatch_grad(\n              original_input=constant_op.constant([1]),\n              batch_index=constant_op.constant([\n                  [0, 0, 0],\n              ], dtype=dtypes.int64),\n              grad=constant_op.constant([\n                  1,\n              ]),\n              id=constant_op.constant([\n                  1,\n                  1,\n              ], dtype=dtypes.int64)))\n\n  def testUnbatchGradInvalidBatchId(self):\n    with self.assertRaises(errors.InvalidArgumentError):\n      self.evaluate(\n          gen_batch_ops.unbatch_grad(\n              original_input=constant_op.constant([1]),\n              batch_index=constant_op.constant([\n                  [0, 0],\n              ], dtype=dtypes.int64),\n              grad=constant_op.constant([\n                  1,\n              ]),\n              id=constant_op.constant([\n                  1,\n              ], dtype=dtypes.int64)))\n\n  def testUnbatchGradInvalidArgs(self):\n    original_input = random_ops.random_uniform(\n        shape=(3, 1), dtype=dtypes.float64, maxval=None)\n    batch_index = random_ops.random_uniform(\n        shape=(3, 1), dtype=dtypes.int64, maxval=65536)\n    grad = random_ops.random_uniform(\n        shape=(3, 1), dtype=dtypes.float64, maxval=None)\n    batch_id = random_ops.random_uniform(\n        shape=(3, 1), dtype=dtypes.int64, maxval=65536)\n    with self.assertRaises(errors.InvalidArgumentError):\n      self.evaluate(\n          gen_batch_ops.unbatch_grad(\n              original_input=original_input,\n              batch_index=batch_index,\n              grad=grad,\n              id=batch_id,\n              container=\"\",\n              shared_name=\"\",\n              name=\"\"))\n\nif __name__ == \"__main__\":\n  test.main()"