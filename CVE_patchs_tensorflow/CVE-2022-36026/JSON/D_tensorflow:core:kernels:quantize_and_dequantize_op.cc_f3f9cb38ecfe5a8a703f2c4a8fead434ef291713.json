"diff --git a/tensorflow/core/kernels/quantize_and_dequantize_op.cc b/tensorflow/core/kernels/quantize_and_dequantize_op.cc\nindex da9257fb9c9..ae02b57861a 100644\n--- a/tensorflow/core/kernels/quantize_and_dequantize_op.cc\n+++ b/tensorflow/core/kernels/quantize_and_dequantize_op.cc\n@@ -21,19 +21,23 @@ limitations under the License.\n #define EIGEN_USE_GPU\n #endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n \n-#include \"tensorflow/core/kernels/quantize_and_dequantize_op.h\"\n-\n #include \"tensorflow/core/framework/op.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n #include \"tensorflow/core/framework/register_types.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/framework/type_traits.h\"\n #include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/kernels/quantize_and_dequantize_op.h\"\n #include \"tensorflow/core/lib/core/errors.h\"\n \n namespace tensorflow {\n+namespace {\n \n-typedef Eigen::ThreadPoolDevice CPUDevice;\n-typedef Eigen::GpuDevice GPUDevice;\n+using CpuDevice = ::Eigen::ThreadPoolDevice;\n+using GpuDevice = ::Eigen::GpuDevice;\n+using ::tensorflow::errors::InvalidArgument;\n+\n+}  // namespace\n \n // Simulate quantization precision loss in a float tensor by:\n // 1. Quantize the tensor to fixed point numbers, which should match the target\n@@ -49,8 +53,8 @@ class QuantizeAndDequantizeV2Op : public OpKernel {\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"axis\", &axis_));\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"num_bits\", &num_bits_));\n     OP_REQUIRES(ctx, num_bits_ > 0 && num_bits_ < (signed_input_ ? 62 : 63),\n-                errors::InvalidArgument(\"num_bits is out of range: \", num_bits_,\n-                                        \" with signed_input_ \", signed_input_));\n+                InvalidArgument(\"num_bits is out of range: \", num_bits_,\n+                                \" with signed_input_ \", signed_input_));\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"range_given\", &range_given_));\n \n     string round_mode_string;\n@@ -58,10 +62,10 @@ class QuantizeAndDequantizeV2Op : public OpKernel {\n     OP_REQUIRES(\n         ctx,\n         (round_mode_string == \"HALF_UP\" || round_mode_string == \"HALF_TO_EVEN\"),\n-        errors::InvalidArgument(\"Round mode string must be \"\n-                                \"'HALF_UP' or \"\n-                                \"'HALF_TO_EVEN', is '\" +\n-                                round_mode_string + \"'\"));\n+        InvalidArgument(\"Round mode string must be \"\n+                        \"'HALF_UP' or \"\n+                        \"'HALF_TO_EVEN', is '\" +\n+                        round_mode_string + \"'\"));\n     if (round_mode_string == \"HALF_UP\") {\n       round_mode_ = ROUND_HALF_UP;\n     } else if (round_mode_string == \"HALF_TO_EVEN\") {\n@@ -72,12 +76,10 @@ class QuantizeAndDequantizeV2Op : public OpKernel {\n \n   void Compute(OpKernelContext* ctx) override {\n     const Tensor& input = ctx->input(0);\n-    OP_REQUIRES(\n-        ctx, axis_ >= -1,\n-        errors::InvalidArgument(\"Axis must be at least -1. Found \", axis_));\n-    OP_REQUIRES(\n-        ctx, (axis_ == -1 || axis_ < input.shape().dims()),\n-        errors::InvalidArgument(\"Shape must be at least rank \", axis_ + 1,\n+    OP_REQUIRES(ctx, axis_ >= -1,\n+                InvalidArgument(\"Axis must be at least -1. Found \", axis_));\n+    OP_REQUIRES(ctx, (axis_ == -1 || axis_ < input.shape().dims()),\n+                InvalidArgument(\"Shape must be at least rank \", axis_ + 1,\n                                 \" but is rank \", input.shape().dims()));\n     const int depth = (axis_ == -1) ? 1 : input.dim_size(axis_);\n     Tensor input_min_tensor;\n@@ -91,21 +93,21 @@ class QuantizeAndDequantizeV2Op : public OpKernel {\n         auto min_val = input_min_tensor.scalar<T>()();\n         auto max_val = input_max_tensor.scalar<T>()();\n         OP_REQUIRES(ctx, min_val <= max_val,\n-                    errors::InvalidArgument(\"Invalid range: input_min \",\n-                                            min_val, \" > input_max \", max_val));\n+                    InvalidArgument(\"Invalid range: input_min \", min_val,\n+                                    \" > input_max \", max_val));\n       } else {\n-        OP_REQUIRES(ctx, input_min_tensor.dim_size(0) == depth,\n-                    errors::InvalidArgument(\n-                        \"input_min_tensor has incorrect size, was \",\n-                        input_min_tensor.dim_size(0), \" expected \", depth,\n-                        \" to match dim \", axis_, \" of the input \",\n-                        input_min_tensor.shape()));\n-        OP_REQUIRES(ctx, input_max_tensor.dim_size(0) == depth,\n-                    errors::InvalidArgument(\n-                        \"input_max_tensor has incorrect size, was \",\n-                        input_max_tensor.dim_size(0), \" expected \", depth,\n-                        \" to match dim \", axis_, \" of the input \",\n-                        input_max_tensor.shape()));\n+        OP_REQUIRES(\n+            ctx, input_min_tensor.dim_size(0) == depth,\n+            InvalidArgument(\"input_min_tensor has incorrect size, was \",\n+                            input_min_tensor.dim_size(0), \" expected \", depth,\n+                            \" to match dim \", axis_, \" of the input \",\n+                            input_min_tensor.shape()));\n+        OP_REQUIRES(\n+            ctx, input_max_tensor.dim_size(0) == depth,\n+            InvalidArgument(\"input_max_tensor has incorrect size, was \",\n+                            input_max_tensor.dim_size(0), \" expected \", depth,\n+                            \" to match dim \", axis_, \" of the input \",\n+                            input_max_tensor.shape()));\n       }\n     } else {\n       auto range_shape = (axis_ == -1) ? TensorShape({}) : TensorShape({depth});\n@@ -158,38 +160,34 @@ class QuantizeAndDequantizeV4GradientOp : public OpKernel {\n     Tensor* input_backprop = nullptr;\n     OP_REQUIRES_OK(ctx,\n                    ctx->allocate_output(0, input.shape(), &input_backprop));\n-    OP_REQUIRES(\n-        ctx, axis_ >= -1,\n-        errors::InvalidArgument(\"Axis must be at least -1. Found \", axis_));\n+    OP_REQUIRES(ctx, axis_ >= -1,\n+                InvalidArgument(\"Axis must be at least -1. Found \", axis_));\n     OP_REQUIRES(ctx, (axis_ == -1 || axis_ < input.shape().dims()),\n-                errors::InvalidArgument(\n+                InvalidArgument(\n                     \"Axis should be -1 or 0 or a positive value less than \",\n                     input.shape().dims(), \"but given axis value was \", axis_));\n \n-    OP_REQUIRES(\n-        ctx, input.IsSameSize(gradient),\n-        errors::InvalidArgument(\"gradient and input must be the same size\"));\n+    OP_REQUIRES(ctx, input.IsSameSize(gradient),\n+                InvalidArgument(\"gradient and input must be the same size\"));\n     const int depth = (axis_ == -1) ? 1 : input.dim_size(axis_);\n     const Tensor& input_min_tensor = ctx->input(2);\n     OP_REQUIRES(ctx,\n                 input_min_tensor.dims() == 0 || input_min_tensor.dims() == 1,\n-                errors::InvalidArgument(\n+                InvalidArgument(\n                     \"Input min tensor must have dimension 0 or 1. Received \",\n                     input_min_tensor.dims(), \".\"));\n     const Tensor& input_max_tensor = ctx->input(3);\n     OP_REQUIRES(ctx,\n                 input_max_tensor.dims() == 0 || input_max_tensor.dims() == 1,\n-                errors::InvalidArgument(\n+                InvalidArgument(\n                     \"Input max tensor must have dimension 0 or 1. Received \",\n                     input_max_tensor.dims(), \".\"));\n     if (axis_ != -1) {\n-      OP_REQUIRES(\n-          ctx, input_min_tensor.dim_size(0) == depth,\n-          errors::InvalidArgument(\"min has incorrect size, expected \", depth,\n+      OP_REQUIRES(ctx, input_min_tensor.dim_size(0) == depth,\n+                  InvalidArgument(\"min has incorrect size, expected \", depth,\n                                   \" was \", input_min_tensor.dim_size(0)));\n-      OP_REQUIRES(\n-          ctx, input_max_tensor.dim_size(0) == depth,\n-          errors::InvalidArgument(\"max has incorrect size, expected \", depth,\n+      OP_REQUIRES(ctx, input_max_tensor.dim_size(0) == depth,\n+                  InvalidArgument(\"max has incorrect size, expected \", depth,\n                                   \" was \", input_max_tensor.dim_size(0)));\n     }\n \n@@ -203,12 +201,12 @@ class QuantizeAndDequantizeV4GradientOp : public OpKernel {\n                    ctx->allocate_output(2, min_max_shape, &input_max_backprop));\n \n     if (axis_ == -1) {\n-      OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(input_min_tensor.shape()),\n-                  errors::InvalidArgument(\n-                      \"input_min must be a scalar if axis is unspecified\"));\n-      OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(input_max_tensor.shape()),\n-                  errors::InvalidArgument(\n-                      \"input_max must be a scalar if axis is unspecified\"));\n+      OP_REQUIRES(\n+          ctx, TensorShapeUtils::IsScalar(input_min_tensor.shape()),\n+          InvalidArgument(\"input_min must be a scalar if axis is unspecified\"));\n+      OP_REQUIRES(\n+          ctx, TensorShapeUtils::IsScalar(input_max_tensor.shape()),\n+          InvalidArgument(\"input_max must be a scalar if axis is unspecified\"));\n       functor::QuantizeAndDequantizeOneScaleGradientFunctor<Device, T> f;\n       f(ctx->eigen_device<Device>(), gradient.template flat<T>(),\n         input.template flat<T>(), input_min_tensor.scalar<T>(),\n@@ -252,21 +250,25 @@ class QuantizeAndDequantizeV3Op : public OpKernel {\n   void Compute(OpKernelContext* ctx) override {\n     const Tensor& input = ctx->input(0);\n     OP_REQUIRES(ctx, axis_ < input.dims(),\n-                errors::InvalidArgument(\n+                InvalidArgument(\n                     \"Axis requested is larger than input dimensions. Axis: \",\n                     axis_, \" Input Dimensions: \", input.dims()));\n     const int depth = (axis_ == -1) ? 1 : input.dim_size(axis_);\n     Tensor* output = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));\n \n-    Tensor num_bits_tensor;\n-    num_bits_tensor = ctx->input(3);\n-    int num_bits_val = num_bits_tensor.scalar<int32>()();\n+    // Get num_bits and validate.\n+    const Tensor num_bits_tensor = ctx->input(3);\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(num_bits_tensor.shape()),\n+                InvalidArgument(\"Invalid shape. The `num_bits` tensor should \"\n+                                \"be a scalar. Got dimensions: \",\n+                                num_bits_tensor.dims()));\n \n-    OP_REQUIRES(\n-        ctx, num_bits_val > 0 && num_bits_val < (signed_input_ ? 62 : 63),\n-        errors::InvalidArgument(\"num_bits is out of range: \", num_bits_val,\n-                                \" with signed_input_ \", signed_input_));\n+    const int num_bits_val = num_bits_tensor.scalar<int32>()();\n+    OP_REQUIRES(ctx,\n+                num_bits_val > 0 && num_bits_val < (signed_input_ ? 62 : 63),\n+                InvalidArgument(\"num_bits is out of range: \", num_bits_val,\n+                                \" with `signed_input_` \", signed_input_));\n \n     Tensor input_min_tensor;\n     Tensor input_max_tensor;\n@@ -274,24 +276,24 @@ class QuantizeAndDequantizeV3Op : public OpKernel {\n       input_min_tensor = ctx->input(1);\n       input_max_tensor = ctx->input(2);\n       if (axis_ == -1) {\n-        auto min_val = input_min_tensor.scalar<T>()();\n-        auto max_val = input_max_tensor.scalar<T>()();\n+        const auto min_val = input_min_tensor.scalar<T>()();\n+        const auto max_val = input_max_tensor.scalar<T>()();\n         OP_REQUIRES(ctx, min_val <= max_val,\n-                    errors::InvalidArgument(\"Invalid range: input_min \",\n-                                            min_val, \" > input_max \", max_val));\n+                    InvalidArgument(\"Invalid range: input_min \", min_val,\n+                                    \" > input_max \", max_val));\n       } else {\n-        OP_REQUIRES(ctx, input_min_tensor.dim_size(0) == depth,\n-                    errors::InvalidArgument(\n-                        \"input_min_tensor has incorrect size, was \",\n-                        input_min_tensor.dim_size(0), \" expected \", depth,\n-                        \" to match dim \", axis_, \" of the input \",\n-                        input_min_tensor.shape()));\n-        OP_REQUIRES(ctx, input_max_tensor.dim_size(0) == depth,\n-                    errors::InvalidArgument(\n-                        \"input_max_tensor has incorrect size, was \",\n-                        input_max_tensor.dim_size(0), \" expected \", depth,\n-                        \" to match dim \", axis_, \" of the input \",\n-                        input_max_tensor.shape()));\n+        OP_REQUIRES(\n+            ctx, input_min_tensor.dim_size(0) == depth,\n+            InvalidArgument(\"input_min_tensor has incorrect size, was \",\n+                            input_min_tensor.dim_size(0), \" expected \", depth,\n+                            \" to match dim \", axis_, \" of the input \",\n+                            input_min_tensor.shape()));\n+        OP_REQUIRES(\n+            ctx, input_max_tensor.dim_size(0) == depth,\n+            InvalidArgument(\"input_max_tensor has incorrect size, was \",\n+                            input_max_tensor.dim_size(0), \" expected \", depth,\n+                            \" to match dim \", axis_, \" of the input \",\n+                            input_max_tensor.shape()));\n       }\n     } else {\n       auto range_shape = (axis_ == -1) ? TensorShape({}) : TensorShape({depth});\n@@ -331,15 +333,14 @@ class QuantizeAndDequantizeOp : public OpKernel {\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"signed_input\", &signed_input_));\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"num_bits\", &num_bits_));\n     OP_REQUIRES(ctx, num_bits_ > 0 && num_bits_ < (signed_input_ ? 62 : 63),\n-                errors::InvalidArgument(\"num_bits is out of range: \", num_bits_,\n-                                        \" with signed_input_ \", signed_input_));\n+                InvalidArgument(\"num_bits is out of range: \", num_bits_,\n+                                \" with signed_input_ \", signed_input_));\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"range_given\", &range_given_));\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"input_min\", &input_min_));\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"input_max\", &input_max_));\n     if (range_given_) {\n-      OP_REQUIRES(\n-          ctx, input_min_ <= input_max_,\n-          errors::InvalidArgument(\"Invalid range: input_min \", input_min_,\n+      OP_REQUIRES(ctx, input_min_ <= input_max_,\n+                  InvalidArgument(\"Invalid range: input_min \", input_min_,\n                                   \" > input_max \", input_max_));\n     }\n   }\n@@ -371,53 +372,53 @@ class QuantizeAndDequantizeOp : public OpKernel {\n   float input_max_;\n };\n \n-// Specializations for CPUDevice.\n+// Specializations for CpuDevice.\n \n namespace functor {\n template <typename T>\n-struct QuantizeAndDequantizeOneScaleFunctor<CPUDevice, T> {\n-  void operator()(const CPUDevice& d, typename TTypes<T>::ConstVec input,\n+struct QuantizeAndDequantizeOneScaleFunctor<CpuDevice, T> {\n+  void operator()(const CpuDevice& d, typename TTypes<T>::ConstVec input,\n                   const bool signed_input, const int num_bits,\n                   const bool range_given, Tensor* input_min_tensor,\n                   Tensor* input_max_tensor, QuantizerRoundMode round_mode,\n                   bool narrow_range, typename TTypes<T>::Vec out) {\n-    QuantizeAndDequantizeOneScaleImpl<CPUDevice, T>::Compute(\n+    QuantizeAndDequantizeOneScaleImpl<CpuDevice, T>::Compute(\n         d, input, signed_input, num_bits, range_given, input_min_tensor,\n         input_max_tensor, round_mode, narrow_range, out);\n   }\n };\n \n template <typename T>\n-struct QuantizeAndDequantizePerChannelFunctor<CPUDevice, T> {\n-  void operator()(const CPUDevice& d, typename TTypes<T, 3>::ConstTensor input,\n+struct QuantizeAndDequantizePerChannelFunctor<CpuDevice, T> {\n+  void operator()(const CpuDevice& d, typename TTypes<T, 3>::ConstTensor input,\n                   bool signed_input, int num_bits, bool range_given,\n                   Tensor* input_min_tensor, Tensor* input_max_tensor,\n                   QuantizerRoundMode round_mode, bool narrow_range,\n                   typename TTypes<T, 3>::Tensor out) {\n-    QuantizeAndDequantizePerChannelImpl<CPUDevice, T>::Compute(\n+    QuantizeAndDequantizePerChannelImpl<CpuDevice, T>::Compute(\n         d, input, signed_input, num_bits, range_given, input_min_tensor,\n         input_max_tensor, round_mode, narrow_range, out);\n   }\n };\n \n template <typename T>\n-struct QuantizeAndDequantizeOneScaleGradientFunctor<CPUDevice, T> {\n-  void operator()(const CPUDevice& d, typename TTypes<T>::ConstFlat gradient,\n+struct QuantizeAndDequantizeOneScaleGradientFunctor<CpuDevice, T> {\n+  void operator()(const CpuDevice& d, typename TTypes<T>::ConstFlat gradient,\n                   typename TTypes<T>::ConstFlat input,\n                   typename TTypes<T>::ConstScalar input_min_tensor,\n                   typename TTypes<T>::ConstScalar input_max_tensor,\n                   typename TTypes<T>::Flat input_backprop,\n                   typename TTypes<T>::Scalar input_min_backprop,\n                   typename TTypes<T>::Scalar input_max_backprop) {\n-    QuantizeAndDequantizeOneScaleGradientImpl<CPUDevice, T>::Compute(\n+    QuantizeAndDequantizeOneScaleGradientImpl<CpuDevice, T>::Compute(\n         d, gradient, input, input_min_tensor, input_max_tensor, input_backprop,\n         input_min_backprop, input_max_backprop);\n   }\n };\n \n template <typename T>\n-struct QuantizeAndDequantizePerChannelGradientFunctor<CPUDevice, T> {\n-  void operator()(const CPUDevice& d,\n+struct QuantizeAndDequantizePerChannelGradientFunctor<CpuDevice, T> {\n+  void operator()(const CpuDevice& d,\n                   typename TTypes<T, 3>::ConstTensor gradient,\n                   typename TTypes<T, 3>::ConstTensor input,\n                   const Tensor* input_min_tensor,\n@@ -425,16 +426,16 @@ struct QuantizeAndDequantizePerChannelGradientFunctor<CPUDevice, T> {\n                   typename TTypes<T, 3>::Tensor input_backprop,\n                   typename TTypes<T>::Flat input_min_backprop,\n                   typename TTypes<T>::Flat input_max_backprop) {\n-    QuantizeAndDequantizePerChannelGradientImpl<CPUDevice, T>::Compute(\n+    QuantizeAndDequantizePerChannelGradientImpl<CpuDevice, T>::Compute(\n         d, gradient, input, input_min_tensor, input_max_tensor, input_backprop,\n         input_min_backprop, input_max_backprop);\n   }\n };\n \n-template struct functor::QuantizeAndDequantizeOneScaleGradientFunctor<CPUDevice,\n+template struct functor::QuantizeAndDequantizeOneScaleGradientFunctor<CpuDevice,\n                                                                       float>;\n template struct functor::QuantizeAndDequantizePerChannelGradientFunctor<\n-    CPUDevice, double>;\n+    CpuDevice, double>;\n \n }  // namespace functor\n \n@@ -442,22 +443,22 @@ template struct functor::QuantizeAndDequantizePerChannelGradientFunctor<\n   REGISTER_KERNEL_BUILDER(Name(\"QuantizeAndDequantizeV2\")                      \\\n                               .Device(DEVICE_CPU)                              \\\n                               .TypeConstraint<T>(\"T\"),                         \\\n-                          QuantizeAndDequantizeV2Op<CPUDevice, T>);            \\\n+                          QuantizeAndDequantizeV2Op<CpuDevice, T>);            \\\n   REGISTER_KERNEL_BUILDER(Name(\"QuantizeAndDequantizeV3\")                      \\\n                               .Device(DEVICE_CPU)                              \\\n                               .TypeConstraint<T>(\"T\"),                         \\\n-                          QuantizeAndDequantizeV3Op<CPUDevice, T>);            \\\n+                          QuantizeAndDequantizeV3Op<CpuDevice, T>);            \\\n   REGISTER_KERNEL_BUILDER(Name(\"QuantizeAndDequantizeV4\")                      \\\n                               .Device(DEVICE_CPU)                              \\\n                               .TypeConstraint<T>(\"T\"),                         \\\n-                          QuantizeAndDequantizeV2Op<CPUDevice, T>);            \\\n+                          QuantizeAndDequantizeV2Op<CpuDevice, T>);            \\\n   REGISTER_KERNEL_BUILDER(Name(\"QuantizeAndDequantizeV4Grad\")                  \\\n                               .Device(DEVICE_CPU)                              \\\n                               .TypeConstraint<T>(\"T\"),                         \\\n-                          QuantizeAndDequantizeV4GradientOp<CPUDevice, T>);    \\\n+                          QuantizeAndDequantizeV4GradientOp<CpuDevice, T>);    \\\n   REGISTER_KERNEL_BUILDER(                                                     \\\n       Name(\"QuantizeAndDequantize\").Device(DEVICE_CPU).TypeConstraint<T>(\"T\"), \\\n-      QuantizeAndDequantizeOp<CPUDevice, T>);\n+      QuantizeAndDequantizeOp<CpuDevice, T>);\n TF_CALL_float(REGISTER_CPU_KERNEL);\n TF_CALL_double(REGISTER_CPU_KERNEL);\n #undef REGISTER_CPU_KERNEL\n@@ -470,29 +471,29 @@ TF_CALL_double(REGISTER_CPU_KERNEL);\n                               .HostMemory(\"input_min\")                         \\\n                               .HostMemory(\"input_max\")                         \\\n                               .TypeConstraint<T>(\"T\"),                         \\\n-                          QuantizeAndDequantizeV2Op<GPUDevice, T>);            \\\n+                          QuantizeAndDequantizeV2Op<GpuDevice, T>);            \\\n   REGISTER_KERNEL_BUILDER(Name(\"QuantizeAndDequantizeV3\")                      \\\n                               .Device(DEVICE_GPU)                              \\\n                               .HostMemory(\"input_min\")                         \\\n                               .HostMemory(\"input_max\")                         \\\n                               .HostMemory(\"num_bits\")                          \\\n                               .TypeConstraint<T>(\"T\"),                         \\\n-                          QuantizeAndDequantizeV3Op<GPUDevice, T>);            \\\n+                          QuantizeAndDequantizeV3Op<GpuDevice, T>);            \\\n   REGISTER_KERNEL_BUILDER(Name(\"QuantizeAndDequantizeV4\")                      \\\n                               .Device(DEVICE_GPU)                              \\\n                               .HostMemory(\"input_min\")                         \\\n                               .HostMemory(\"input_max\")                         \\\n                               .TypeConstraint<T>(\"T\"),                         \\\n-                          QuantizeAndDequantizeV2Op<GPUDevice, T>);            \\\n+                          QuantizeAndDequantizeV2Op<GpuDevice, T>);            \\\n   REGISTER_KERNEL_BUILDER(Name(\"QuantizeAndDequantizeV4Grad\")                  \\\n                               .Device(DEVICE_GPU)                              \\\n                               .HostMemory(\"input_min\")                         \\\n                               .HostMemory(\"input_max\")                         \\\n                               .TypeConstraint<T>(\"T\"),                         \\\n-                          QuantizeAndDequantizeV4GradientOp<GPUDevice, T>);    \\\n+                          QuantizeAndDequantizeV4GradientOp<GpuDevice, T>);    \\\n   REGISTER_KERNEL_BUILDER(                                                     \\\n       Name(\"QuantizeAndDequantize\").Device(DEVICE_GPU).TypeConstraint<T>(\"T\"), \\\n-      QuantizeAndDequantizeOp<GPUDevice, T>);\n+      QuantizeAndDequantizeOp<GpuDevice, T>);\n TF_CALL_float(REGISTER_GPU_KERNEL);\n TF_CALL_double(REGISTER_GPU_KERNEL);\n #undef REGISTER_GPU_KERNEL"