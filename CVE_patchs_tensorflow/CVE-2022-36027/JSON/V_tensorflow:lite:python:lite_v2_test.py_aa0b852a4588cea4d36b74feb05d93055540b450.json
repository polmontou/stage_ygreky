"# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for lite.py functionality related to TensorFlow 2.0.\"\"\"\n\nimport ctypes\nimport functools\nimport itertools\nimport os\nimport sys\n\nfrom absl.testing import parameterized\nimport numpy as np\nfrom six.moves import range\nfrom six.moves import zip\nimport tensorflow as tf\n\n# Force loaded shared object symbols to be globally visible. This is needed so\n# that the interpreter_wrapper, in one .so file, can see the test_registerer,\n# in a different .so file. Note that this may already be set by default.\n# pylint: disable=g-import-not-at-top\nif hasattr(sys, 'setdlopenflags') and hasattr(sys, 'getdlopenflags'):\n  sys.setdlopenflags(sys.getdlopenflags() | ctypes.RTLD_GLOBAL)\n\nfrom tensorflow.lite.python import conversion_metadata_schema_py_generated as metadata_fb\nfrom tensorflow.lite.python import convert\nfrom tensorflow.lite.python import lite\nfrom tensorflow.lite.python import lite_v2_test_util\nfrom tensorflow.lite.python import schema_py_generated as schema_fb\nfrom tensorflow.lite.python import test_util as tflite_test_util\nfrom tensorflow.lite.python import util\nfrom tensorflow.lite.python.convert import mlir_quantize\nfrom tensorflow.lite.python.interpreter import Interpreter\nfrom tensorflow.lite.python.interpreter import InterpreterWithCustomOps\nfrom tensorflow.lite.python.interpreter import OpResolverType\nfrom tensorflow.lite.python.testdata import _pywrap_test_registerer as test_registerer\nfrom tensorflow.lite.python.testdata import double_op\nfrom tensorflow.lite.python.util import get_conversion_metadata\nfrom tensorflow.lite.toco import types_pb2 as _types_pb2\nfrom tensorflow.lite.tools.flatbuffer_utils import convert_bytearray_to_object as _convert_bytearray_to_object\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.framework import versions\nfrom tensorflow.python.lib.io import file_io\nfrom tensorflow.python.ops import map_ops\nfrom tensorflow.python.ops import rnn\nfrom tensorflow.python.platform import resource_loader\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.saved_model import save_options\nfrom tensorflow.python.saved_model import saved_model\nfrom tensorflow.python.saved_model.loader_impl import parse_saved_model\nfrom tensorflow.python.saved_model.save import save\nfrom tensorflow.python.trackable import autotrackable\n\n# Only run jax related tests when we can import jax.\nDISABLE_JAX_TEST = False\ntry:\n  import jax\n  from jax import numpy as jnp\nexcept ImportError:\n  DISABLE_JAX_TEST = True\n# pylint: enable=g-import-not-at-top\n\n\nclass FromConcreteFunctionTest(lite_v2_test_util.ModelTest):\n\n  @test_util.run_v2_only\n  def testTypeInvalid(self):\n    root = self._getSimpleVariableModel()\n    with self.assertRaises(ValueError) as error:\n      _ = lite.TFLiteConverterV2.from_concrete_functions([root.f], root)\n    self.assertIn('call get_concrete_function', str(error.exception))\n\n  @test_util.run_v2_only\n  def testFloat(self):\n    root = self._getSimpleVariableModel()\n    input_data = tf.constant(1., shape=[1])\n    concrete_func = root.f.get_concrete_function(input_data)\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               root)\n    tflite_model = converter.convert()\n\n    # Check output value from converted model.\n    expected_value = root.f(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertEqual(expected_value.numpy(), actual_value)\n\n  @parameterized.named_parameters(('_INT8InputOutput', dtypes.int8),\n                                  ('_UINT8InputOutput', dtypes.uint8),\n                                  ('_INT16InputOutput', dtypes.int16))\n  @test_util.run_v2_only\n  def testInvalidFloat(self, inference_input_output_type):\n    root = self._getSimpleVariableModel()\n    input_data = tf.constant(1., shape=[1])\n    concrete_func = root.f.get_concrete_function(input_data)\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               root)\n    with self.assertRaises(ValueError) as error:\n      converter.inference_input_type = inference_input_output_type\n      converter.inference_output_type = inference_input_output_type\n      converter.convert()\n    self.assertEqual(\n        'The inference_input_type and inference_output_type '\n        'must be tf.float32.', str(error.exception))\n\n  @test_util.run_v2_only\n  def testScalarInput(self):\n    root = self._getSimpleVariableModel()\n    input_data = tf.constant(1., shape=[])\n    concrete_func = root.f.get_concrete_function(input_data)\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               root)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = root.f(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertEqual(expected_value.numpy(), actual_value)\n\n  @test_util.run_v2_only\n  def testModelWithoutInputs(self):\n\n    def _get_random_number_gen():\n      root = autotrackable.AutoTrackable()\n\n      @tf.function(input_signature=[])\n      def func():\n        return tf.random.uniform(shape=[1], dtype=tf.float32)\n\n      root.f = func\n      to_save = root.f.get_concrete_function()\n      return (root, to_save)\n\n    # Model with no input\n    root, concrete_func = _get_random_number_gen()\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               root)\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n  @test_util.run_v2_only\n  def testMultiFunctionModel(self):\n    \"\"\"Convert a single model in a multi-functional model.\"\"\"\n    root = self._getMultiFunctionModel()\n    input_data = tf.constant(1., shape=[1])\n    concrete_func = root.add.get_concrete_function(input_data)\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               root)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = root.add(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertEqual(expected_value.numpy(), actual_value)\n\n  @test_util.run_v2_only\n  def testConvertMultipleFunctions(self):\n    \"\"\"Convert multiple functions in a multi-functional model.\"\"\"\n    root = self._getMultiFunctionModel()\n    input_data = tf.constant(1., shape=[1])\n    add_func = root.add.get_concrete_function(input_data)\n    sub_func = root.sub.get_concrete_function(input_data)\n\n    # Try converting multiple functions.\n    converter = lite.TFLiteConverterV2.from_concrete_functions(\n        [add_func, sub_func], root)\n    tflite_model = converter.convert()\n\n    # Check signatures are valid from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    signature_defs = interpreter.get_signature_list()\n\n    # Verify the SignatureDef structure returned is as expected.\n    self.assertEqual(len(signature_defs), 2)\n    self.assertEqual(list(signature_defs.keys()), ['add', 'sub'])\n    self.assertEqual(len(signature_defs.values()), 2)\n    self.assertEqual(list(signature_defs['add'].keys()), ['inputs', 'outputs'])\n    self.assertCountEqual(signature_defs['add']['inputs'], ['x'])\n    self.assertEqual(list(signature_defs['add']['outputs']), ['output_0'])\n    self.assertEqual(list(signature_defs['sub'].keys()), ['inputs', 'outputs'])\n    self.assertCountEqual(signature_defs['sub']['inputs'], ['x'])\n    self.assertEqual(list(signature_defs['sub']['outputs']), ['output_0'])\n\n    # Verify the Signature runner executions.\n    add_signature_runner = interpreter.get_signature_runner('add')\n    add_output = add_signature_runner(x=input_data)\n    self.assertEqual(add_output['output_0'], 3)\n    input_details = add_signature_runner.get_input_details()\n    self.assertEqual(1, len(input_details))\n    self.assertEqual('add_x:0', input_details['x']['name'])\n    self.assertEqual(np.float32, input_details['x']['dtype'])\n    self.assertTrue(([1] == input_details['x']['shape']).all())\n    self.assertEqual((0.0, 0), input_details['x']['quantization'])\n\n    sub_signature_runner = interpreter.get_signature_runner('sub')\n    sub_output = sub_signature_runner(x=input_data)\n    self.assertEqual(sub_output['output_0'], -2)\n    output_details = sub_signature_runner.get_output_details()\n    self.assertEqual(1, len(output_details))\n    self.assertEqual('StatefulPartitionedCall:0',\n                     output_details['output_0']['name'])\n    self.assertEqual(np.float32, output_details['output_0']['dtype'])\n    self.assertTrue(([1] == output_details['output_0']['shape']).all())\n    self.assertEqual((0.0, 0), output_details['output_0']['quantization'])\n\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertEqual(metadata.environment.apiVersion, 2)\n    self.assertEqual(metadata.environment.modelType,\n                     metadata_fb.ModelType.TF_CONCRETE_FUNCTIONS)\n    self.assertAllEqual([], metadata.options.modelOptimizationModes)\n\n  def _getIntegerQuantizeModel(self, num_filters=16):\n    np.random.seed(0)\n\n    root = autotrackable.AutoTrackable()\n\n    @tf.function(\n        input_signature=[tf.TensorSpec(shape=[1, 5, 5, 3], dtype=tf.float32)])\n    def func(inp):\n      conv = tf.nn.conv2d(\n          inp,\n          tf.ones([3, 3, 3, num_filters]), strides=[1, 1, 1, 1], padding='SAME')\n      output = tf.nn.relu(conv, name='output')\n      return output\n\n    def calibration_gen():\n      for _ in range(5):\n        yield [np.random.uniform(-1, 1, size=(1, 5, 5, 3)).astype(np.float32)]\n\n    root.f = func\n    to_save = root.f.get_concrete_function()\n    return (root, to_save, calibration_gen)\n\n  @parameterized.named_parameters(\n      ('EnableMlirQuantizer', True),  # enable mlir quantizer\n      ('DisableMlirQuantizer', False))  # disable mlir quantizer\n  def testPostTrainingCalibrateAndQuantize(self, mlir_quantizer):\n    root, func, calibration_gen = self._getIntegerQuantizeModel()\n\n    # Convert float model.\n    float_converter = lite.TFLiteConverterV2.from_concrete_functions([func],\n                                                                     root)\n    float_tflite_model = float_converter.convert()\n    self.assertIsNotNone(float_tflite_model)\n\n    # Convert quantized model.\n    quantized_converter = lite.TFLiteConverterV2.from_concrete_functions([func],\n                                                                         root)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_converter.representative_dataset = calibration_gen\n    quantized_converter.experimental_new_quantizer = mlir_quantizer\n    quantized_tflite_model = quantized_converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(quantized_tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertEqual(\n        metadata.environment.tensorflowVersion.decode('utf-8'),\n        versions.__version__)\n    self.assertEqual(metadata.environment.apiVersion, 2)\n    self.assertEqual(metadata.environment.modelType,\n                     metadata_fb.ModelType.TF_CONCRETE_FUNCTIONS)\n    self.assertEqual(metadata.options.allowCustomOps, False)\n    self.assertEqual(metadata.options.enableSelectTfOps, False)\n    self.assertEqual(metadata.options.forceSelectTfOps, False)\n    self.assertAllEqual([metadata_fb.ModelOptimizationMode.PTQ_FULL_INTEGER],\n                        metadata.options.modelOptimizationModes)\n\n    # The default input and output types should be float.\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    self.assertLen(input_details, 1)\n    self.assertEqual(np.float32, input_details[0]['dtype'])\n    output_details = interpreter.get_output_details()\n    self.assertLen(output_details, 1)\n    self.assertEqual(np.float32, output_details[0]['dtype'])\n\n    # Ensure that the quantized weights tflite model is smaller.\n    self.assertLess(len(quantized_tflite_model), len(float_tflite_model))\n\n  @parameterized.named_parameters(('_INT8InputOutput', dtypes.int8),\n                                  ('_UINT8InputOutput', dtypes.uint8),\n                                  ('_INT16InputOutput', dtypes.int16))\n  @test_util.run_v2_only\n  def testInvalidPostTrainingDynamicRangeQuantization(\n      self, inference_input_output_type):\n    root, func, _ = self._getIntegerQuantizeModel()\n\n    # Convert float model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([func], root)\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n    # Convert quantized model.\n    quantized_converter = lite.TFLiteConverterV2.from_concrete_functions([func],\n                                                                         root)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    with self.assertRaises(ValueError) as error:\n      quantized_converter.inference_input_type = inference_input_output_type\n      quantized_converter.inference_output_type = inference_input_output_type\n      quantized_converter.convert()\n    self.assertEqual(\n        'The inference_input_type and inference_output_type '\n        'must be tf.float32.', str(error.exception))\n\n  def _createV2QATSavedModelWithFloatOpsAtEnd(self):\n    \"\"\"Create a simple QAT SavedModel that includes float ops at the end.\"\"\"\n    saved_model_dir = os.path.join(self.get_temp_dir(), 'qat_float_ops_at_end')\n    input_tensor = tf.keras.layers.Input((32, 32, 128))\n    x = tf.quantization.fake_quant_with_min_max_args(input_tensor, -3.0, 3.0)\n    x = tf.keras.layers.Conv2D(1, (3, 3))(x)\n    x = tf.quantization.fake_quant_with_min_max_args(x, -3.0, 3.0)\n    # Exclude the quantization of the following Dense layer by not putting\n    # fake quant layer after the dense layer.\n    output_tensor = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n    model = tf.keras.Model(input_tensor, output_tensor)\n    model.save(saved_model_dir)\n    return saved_model_dir\n\n  def testQuantizationRemovesQDQsForFloatIOInQAT(self):\n    saved_model_dir = self._createV2QATSavedModelWithFloatOpsAtEnd()\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_model = converter.convert()\n\n    # Because assertions on the model later, we opt out applying default TFLite\n    # delegates (i.e. the XNNPACK delegate).\n    interpreter = Interpreter(\n        model_content=quantized_model,\n        experimental_op_resolver_type=OpResolverType\n        .BUILTIN_WITHOUT_DEFAULT_DELEGATES)\n    interpreter.allocate_tensors()\n    # The model should have LOGISTIC op, instead of DEQUANTIZE op.\n    op_details = interpreter._get_ops_details()\n    self.assertEqual(op_details[len(op_details) - 1]['op_name'], 'LOGISTIC')\n\n  @parameterized.named_parameters(\n      ('EnableMlirQuantizer', True),  # enable mlir quantizer\n      ('DisableMlirQuantizer', False))  # disable mlir quantizer\n  def testQuantizationRemovesQDQsForFloatIO(self, mlir_quantizer):\n    func, calibration_gen = self._getSqrtModel()\n    converter = lite.TFLiteConverterV2.from_concrete_functions(\n        [func.get_concrete_function()])\n    converter.representative_dataset = calibration_gen\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    converter.experimental_new_quantizer = mlir_quantizer\n    quantized_model = converter.convert()\n\n    # Because assertions on the model later, we opt out applying default TFLite\n    # delegates (i.e. the XNNPACK delegate).\n    interpreter = Interpreter(\n        model_content=quantized_model,\n        experimental_op_resolver_type=OpResolverType\n        .BUILTIN_WITHOUT_DEFAULT_DELEGATES)\n    interpreter.allocate_tensors()\n    # The model should have only one sqrt op.\n    op_details = interpreter._get_ops_details()\n    self.assertLen(op_details, 1)\n    self.assertEqual(op_details[0]['op_name'], 'SQRT')\n\n  @parameterized.named_parameters(\n      ('_Default', False, False, dtypes.float32),\n      ('_INT8InputOutput', False, False, dtypes.int8),\n      ('_UINT8InputOutput', False, False, dtypes.uint8),\n      ('_INT16Quantize', False, True, dtypes.float32),\n      ('_INT16Quantize_INT16InputOutput', False, True, dtypes.int16),\n      ('_IntOnly', True, False, dtypes.float32),\n      ('_IntOnly_INT8InputOutput', True, False, dtypes.int8),\n      ('_IntOnly_UINT8InputOutput', True, False, dtypes.uint8),\n      ('_IntOnly_INT16Quantize', True, True, dtypes.float32),\n      ('_IntOnly_INT16Quantize_INT16InputOutput', True, True, dtypes.int16))\n  def testIntegerQuantization(self, is_int_only, is_int16_quantize,\n                              inference_input_output_type):\n    root, func, calibration_gen = self._getIntegerQuantizeModel()\n\n    # Convert float model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([func], root)\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n    # Convert quantized model.\n    quantized_converter = lite.TFLiteConverterV2.from_concrete_functions([func],\n                                                                         root)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_converter.representative_dataset = calibration_gen\n    if is_int_only:\n      if is_int16_quantize:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.\n            EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8\n        ]\n      else:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.TFLITE_BUILTINS_INT8\n        ]\n    else:\n      if is_int16_quantize:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.\n            EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,\n            lite.OpsSet.TFLITE_BUILTINS\n        ]\n    quantized_converter.inference_input_type = inference_input_output_type\n    quantized_converter.inference_output_type = inference_input_output_type\n    quantized_tflite_model = quantized_converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(quantized_tflite_model)\n    self.assertIsNotNone(metadata)\n    expected_opt_options = [metadata_fb.ModelOptimizationMode.PTQ_FULL_INTEGER]\n    if is_int16_quantize:\n      expected_opt_options = [metadata_fb.ModelOptimizationMode.PTQ_INT16]\n    self.assertAllEqual(expected_opt_options,\n                        metadata.options.modelOptimizationModes)\n\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    self.assertLen(input_details, 1)\n    self.assertEqual(inference_input_output_type.as_numpy_dtype,\n                     input_details[0]['dtype'])\n    output_details = interpreter.get_output_details()\n    self.assertLen(output_details, 1)\n    self.assertEqual(inference_input_output_type.as_numpy_dtype,\n                     output_details[0]['dtype'])\n\n    # Ensure that the quantized tflite model is smaller.\n    self.assertLess(len(quantized_tflite_model), len(tflite_model))\n\n  @parameterized.named_parameters(\n      ('_INT16Quantize_INT8InputOutput', True, dtypes.int8))\n  def testInvalidIntegerQuantization(self, is_int16_quantize,\n                                     inference_input_output_type):\n    root, func, calibration_gen = self._getIntegerQuantizeModel()\n\n    # Convert quantized model.\n    quantized_converter = lite.TFLiteConverterV2.from_concrete_functions([func],\n                                                                         root)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_converter.representative_dataset = calibration_gen\n    if is_int16_quantize:\n      quantized_converter.target_spec.supported_ops = [\n          lite.OpsSet.\n          EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,\n          lite.OpsSet.TFLITE_BUILTINS\n      ]\n    with self.assertRaises(ValueError) as error:\n      quantized_converter.inference_input_type = dtypes.int8\n      quantized_converter.inference_output_type = dtypes.int8\n      quantized_converter.convert()\n    self.assertEqual(\n        'The inference_input_type and inference_output_type '\n        \"must be in ['tf.float32', 'tf.int16'].\", str(error.exception))\n\n  def testCalibrateAndQuantizeBuiltinInt16(self):\n    root, func, calibration_gen = self._getIntegerQuantizeModel()\n\n    # Convert float model.\n    float_converter = lite.TFLiteConverterV2.from_concrete_functions([func],\n                                                                     root)\n    float_tflite_model = float_converter.convert()\n    self.assertIsNotNone(float_tflite_model)\n\n    converter = lite.TFLiteConverterV2.from_concrete_functions([func], root)\n    # TODO(b/156309549): We should add INT16 to the builtin types.\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    converter.target_spec.supported_ops = [lite.OpsSet.TFLITE_BUILTINS_INT8]\n    converter.representative_dataset = calibration_gen\n    converter._experimental_calibrate_only = True\n    calibrated_tflite = converter.convert()\n    quantized_tflite_model = mlir_quantize(\n        calibrated_tflite, inference_type=_types_pb2.QUANTIZED_INT16)\n\n    self.assertIsNotNone(quantized_tflite_model)\n\n    # The default input and output types should be float.\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    self.assertLen(input_details, 1)\n    self.assertEqual(np.float32, input_details[0]['dtype'])\n    output_details = interpreter.get_output_details()\n    self.assertLen(output_details, 1)\n    self.assertEqual(np.float32, output_details[0]['dtype'])\n\n    # Ensure that the quantized weights tflite model is smaller.\n    self.assertLess(len(quantized_tflite_model), len(float_tflite_model))\n\n  @test_util.run_v2_only\n  def testSignatureDefs(self):\n    \"\"\"Test converting SignatureDef is correct and uses SignatureDef API.\"\"\"\n    root = self._getMultiFunctionModel()\n    input_data = tf.constant(1., shape=[1])\n    add_func = root.add.get_concrete_function(input_data)\n\n    converter = lite.TFLiteConverterV2([add_func], trackable_obj=root)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = add_func(input_data)\n    interpreter = Interpreter(model_content=tflite_model)\n    signature_defs = interpreter.get_signature_list()\n    results = self._evaluateTFLiteModelUsingSignatureDef(\n        tflite_model, 'serving_default', {'x': input_data})\n    self.assertLen(list(results.keys()), 1)\n    self.assertStartsWith(list(results.keys())[0], 'output')\n    self.assertAllClose(\n        expected_value.numpy(),\n        results[signature_defs['serving_default']['outputs'][0]])\n\n    # Verify the SignatureDef structure returned is as expected.\n    self.assertEqual(len(signature_defs), 1)\n    self.assertEqual(list(signature_defs.keys()), ['serving_default'])\n    self.assertEqual(len(signature_defs.values()), 1)\n    self.assertEqual(\n        list(signature_defs['serving_default'].keys()), ['inputs', 'outputs'])\n    self.assertCountEqual(signature_defs['serving_default']['inputs'], ['x'])\n    self.assertLen(list(signature_defs['serving_default']['outputs']), 1)\n    self.assertStartsWith(\n        list(signature_defs['serving_default']['outputs'])[0], 'output')\n\n  @test_util.run_v2_only\n  def testNoSignatureDefsWhenTrackingObjIsNone(self):\n    \"\"\"Test converting SignatureDef is correct and uses SignatureDef API.\"\"\"\n    root = self._getSimpleVariableModel()\n    input_data = tf.constant(1., shape=[1])\n    concrete_func = root.f.get_concrete_function(input_data)\n\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               None)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    signature_defs = interpreter.get_signature_list()\n    # Verify that there is no SignatureDef structure found.\n    self.assertEqual(len(signature_defs), 0)\n\n  @test_util.run_v2_only\n  def testNoSignatureDefsWhenInvalidTrackingObjIsGiven(self):\n    \"\"\"Test converting SignatureDef is correct and uses SignatureDef API.\"\"\"\n    root = self._getSimpleVariableModel()\n    input_data = tf.constant(1., shape=[1])\n    concrete_func = root.f.get_concrete_function(input_data)\n\n    converter = lite.TFLiteConverterV2.from_concrete_functions(\n        [concrete_func], trackable_obj=autotrackable.AutoTrackable())\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    signature_defs = interpreter.get_signature_list()\n    # Verify that there is no SignatureDef structure found.\n    self.assertEqual(len(signature_defs), 0)\n\n  @test_util.run_v2_only\n  def testTrackbleObject(self):\n    \"\"\"Test converting with trackable objects.\"\"\"\n    root = self._getMultiFunctionModel()\n    input_data = tf.constant(1., shape=[1])\n    add_func = root.add.get_concrete_function(input_data)\n\n    converter = lite.TFLiteConverterV2.from_concrete_functions(\n        [add_func], trackable_obj=root)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = add_func(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertEqual(expected_value.numpy(), actual_value)\n\n  def _getTrainingTimeQuantizedModel(self):\n\n    class QLinear(tf.keras.layers.Layer):\n\n      def __init__(self, units=3, **kwargs):\n        super(QLinear, self).__init__(**kwargs)\n        self.units = units\n\n      def build(self, input_shape):\n        self.w = self.add_weight(\n            'weight',\n            shape=(input_shape[-1], self.units),\n            initializer='random_normal',\n            trainable=True)\n        self.min_var = self.add_weight(\n            'min',\n            initializer=tf.keras.initializers.Constant(-6.0),\n            trainable=False)\n        self.max_var = self.add_weight(\n            'max',\n            initializer=tf.keras.initializers.Constant(6.0),\n            trainable=False)\n\n      def call(self, inputs):\n        x = tf.quantization.fake_quant_with_min_max_vars(\n            inputs, self.min_var, self.max_var)\n\n        w_fq = tf.quantization.fake_quant_with_min_max_vars(\n            self.w, self.min_var, self.max_var)\n        x = tf.matmul(x, w_fq)\n\n        x = tf.quantization.fake_quant_with_min_max_vars(\n            x, self.min_var, self.max_var)\n\n        return x\n\n    return tf.keras.Sequential(QLinear(3, input_shape=(2,)))\n\n  @parameterized.named_parameters(\n      ('_DefaultFLOAT32InputOutput', dtypes.float32),\n      ('_INT8InputOutput', dtypes.int8), ('_UINT8InputOutput', dtypes.uint8))\n  @test_util.run_v2_only\n  def testTrainingTimeQuantization(self, inference_input_output_type):\n    model = self._getTrainingTimeQuantizedModel()\n\n    float_converter = lite.TFLiteConverterV2.from_keras_model(model)\n    float_tflite_model = float_converter.convert()\n    self.assertIsNotNone(float_tflite_model)\n\n    quantized_converter = lite.TFLiteConverterV2.from_keras_model(model)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_converter.inference_input_type = inference_input_output_type\n    quantized_converter.inference_output_type = inference_input_output_type\n    quantized_tflite_model = quantized_converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(quantized_tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertAllEqual(\n        [metadata_fb.ModelOptimizationMode.QUANTIZATION_AWARE_TRAINING],\n        metadata.options.modelOptimizationModes)\n\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    self.assertLen(input_details, 1)\n    self.assertEqual(inference_input_output_type.as_numpy_dtype,\n                     input_details[0]['dtype'])\n    output_details = interpreter.get_output_details()\n    self.assertLen(output_details, 1)\n    self.assertEqual(inference_input_output_type.as_numpy_dtype,\n                     output_details[0]['dtype'])\n\n    # Ensure that the quantized tflite model is smaller.\n    self.assertLess(len(quantized_tflite_model), len(float_tflite_model))\n\n  @test_util.run_v2_only\n  def testNewQuantizer(self):\n    \"\"\"Test the model quantized by the new converter.\"\"\"\n    root, func, calibration_gen = self._getIntegerQuantizeModel()\n\n    quantized_converter = lite.TFLiteConverterV2.from_concrete_functions([func],\n                                                                         root)\n    quantized_converter.target_spec.supported_ops = [\n        lite.OpsSet.TFLITE_BUILTINS_INT8\n    ]\n    quantized_converter.representative_dataset = calibration_gen\n\n    # default quantizer\n    quantized_converter.experimental_new_quantizer = False\n    old_tflite = quantized_converter.convert()\n\n    # new quantizer\n    quantized_converter.experimental_new_quantizer = True\n    new_tflite = quantized_converter.convert()\n\n    for _ in range(5):\n      input_data = tf.constant(\n          np.random.uniform(-1, 1, size=(1, 5, 5, 3)).astype(np.float32))\n      old_value = self._evaluateTFLiteModel(old_tflite, [input_data])\n      new_value = self._evaluateTFLiteModel(new_tflite, [input_data])\n      self.assertAllClose(old_value, new_value, atol=1e-01)\n\n  @test_util.run_v2_only\n  def testEmbeddings(self):\n    \"\"\"Test model with embeddings.\"\"\"\n    input_data = tf.constant(\n        np.array(np.random.random_sample((20)), dtype=np.int32))\n\n    class EmbeddingModel(tf.keras.Model):\n\n      def __init__(self):\n        super(EmbeddingModel, self).__init__()\n        self.shared_weights = self.add_weight(\n            'weights',\n            shape=(2000, 300),\n            dtype=tf.float32,\n            initializer=tf.random_normal_initializer(\n                mean=0.0, stddev=300**(-0.5)))\n\n      @tf.function(input_signature=[tf.TensorSpec(shape=(20), dtype=tf.int32)])\n      def func(self, x):\n        return tf.gather(self.shared_weights, x)\n\n    # Building the model.\n    root = EmbeddingModel()\n    concrete_func = root.func.get_concrete_function()\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               root)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = root.func(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertAllClose(expected_value.numpy(), actual_value[0], atol=1e-05)\n\n  @test_util.run_v2_only\n  def testGraphDebugInfo(self):\n    \"\"\"Test a concrete function has debug info captured.\"\"\"\n    root = autotrackable.AutoTrackable()\n    root.v1 = tf.Variable(3.)\n    root.f = tf.function(lambda x: root.v1 * x)\n    input_data = tf.constant(1., shape=[1])\n    concrete_func = root.f.get_concrete_function(input_data)\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               root)\n    converter.convert()\n    self._assertValidDebugInfo(converter._debug_info)\n\n  def _getIntegerQuantizationModelWithFlexOp(self):\n    np.random.seed(0)\n\n    root = autotrackable.AutoTrackable()\n\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[3, 3, 3, 3, 3], dtype=tf.float32)\n    ])\n    def func(inp):\n      tanh = tf.math.tanh(inp)\n      # Flex delegate will merge the consecutive conv3d and erf ops into one\n      # Delegate node.\n      conv3d = tf.nn.conv3d(\n          tanh,\n          tf.ones([3, 3, 3, 3, 3]),\n          strides=[1, 1, 1, 1, 1],\n          padding='SAME')\n      erf = tf.math.erf(conv3d)\n      output = tf.math.tanh(erf)\n      return output\n\n    def calibration_gen():\n      for _ in range(5):\n        yield [\n            np.random.uniform(-1, 1, size=(3, 3, 3, 3, 3)).astype(np.float32)\n        ]\n\n    root.f = func\n    return (root, root.f.get_concrete_function(), calibration_gen)\n\n  @parameterized.named_parameters(\n      ('_Default', False, False, dtypes.float32),\n      ('_INT8InputOutput', False, False, dtypes.int8),\n      ('_UINT8InputOutput', False, False, dtypes.uint8),\n      ('_INT16Quantize', False, True, dtypes.float32),\n      ('_INT16Quantize_INT16InputOutput', False, True, dtypes.int16),\n      ('_IntOnly', True, False, dtypes.float32),\n      ('_IntOnly_INT8InputOutput', True, False, dtypes.int8),\n      ('_IntOnly_UINT8InputOutput', True, False, dtypes.uint8),\n      ('_IntOnly_INT16Quantize', True, True, dtypes.float32),\n      ('_IntOnly_INT16Quantize_INT16InputOutput', True, True, dtypes.int16))\n  @test_util.run_v2_only\n  def testIntegerQuantizationWithFlexOp(self, is_int_only, is_int16_quantize,\n                                        inference_input_output_type):\n    root, func, calibration_gen = self._getIntegerQuantizationModelWithFlexOp()\n\n    quantized_converter = tf.lite.TFLiteConverter.from_concrete_functions(\n        [func], root)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_converter.representative_dataset = calibration_gen\n    if is_int_only:\n      if is_int16_quantize:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.\n            EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,\n            lite.OpsSet.SELECT_TF_OPS\n        ]\n      else:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.TFLITE_BUILTINS_INT8, lite.OpsSet.SELECT_TF_OPS\n        ]\n    else:\n      if is_int16_quantize:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.\n            EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,\n            lite.OpsSet.TFLITE_BUILTINS,\n            lite.OpsSet.SELECT_TF_OPS\n        ]\n      else:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.TFLITE_BUILTINS, lite.OpsSet.SELECT_TF_OPS\n        ]\n\n    quantized_converter.inference_input_type = inference_input_output_type\n    quantized_converter.inference_output_type = inference_input_output_type\n    quantized_tflite_model = quantized_converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(quantized_tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertEqual(metadata.options.enableSelectTfOps, True)\n    expected_opt_options = [metadata_fb.ModelOptimizationMode.PTQ_FULL_INTEGER]\n    if is_int16_quantize:\n      expected_opt_options = [metadata_fb.ModelOptimizationMode.PTQ_INT16]\n    self.assertAllEqual(expected_opt_options,\n                        metadata.options.modelOptimizationModes)\n\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    self.assertLen(input_details, 1)\n    self.assertEqual(inference_input_output_type.as_numpy_dtype,\n                     input_details[0]['dtype'])\n    output_details = interpreter.get_output_details()\n    self.assertLen(output_details, 1)\n    self.assertEqual(inference_input_output_type.as_numpy_dtype,\n                     output_details[0]['dtype'])\n\n  def _getIntegerQuantizationModelWithUnsupportedOps(self):\n    np.random.seed(0)\n\n    root = autotrackable.AutoTrackable()\n\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[3], dtype=tf.float32),\n        tf.TensorSpec(shape=[3], dtype=tf.float32)\n    ])\n    def func(a, b):\n      # ceil kernel does not support int8 nor int16 types neither.\n      left = tf.math.ceil(a)\n      right = tf.nn.tanh(b)\n      add = tf.math.add(left, right)\n      # ceil kernel does not support int8 nor int16 types neither.\n      output = tf.math.ceil(add)\n      return (output, right)\n\n    def calibration_gen():\n      for _ in range(5):\n        yield [\n            np.random.uniform(-1, 1, size=(3)).astype(np.float32),\n            np.random.uniform(-1, 1, size=(3)).astype(np.float32)\n        ]\n\n    root.f = func\n    return (root, root.f.get_concrete_function(), calibration_gen)\n\n  @parameterized.named_parameters(\n      ('_INT8InputOutput', False, False, dtypes.int8),\n      ('_UINT8InputOutput', False, False, dtypes.uint8),\n      ('_INT16Quantize_INT16InputOutput', False, True, dtypes.int16),\n      ('_IntOnly_INT8InputOutput', True, False, dtypes.int8),\n      ('_IntOnly_UINT8InputOutput', True, False, dtypes.uint8),\n      ('_IntOnly_INT16Quantize_INT16InputOutput', True, True, dtypes.int16),\n      ('_IntOnly_INT8InputOutputMlirQuant', True, False, dtypes.int8, True),\n      ('_IntOnly_UINT8InputOutputMlirQuant', True, False, dtypes.uint8, True))\n  @test_util.run_v2_only\n  def testIntegerQuantizationWithUnsupportedOps(self,\n                                                is_int_only,\n                                                is_int16_quantize,\n                                                inference_input_output_type,\n                                                enable_mlir_quantizer=False):\n    root, func, calib_gen = self._getIntegerQuantizationModelWithUnsupportedOps(\n    )\n\n    quantized_converter = tf.lite.TFLiteConverter.from_concrete_functions(\n        [func], root)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_converter.representative_dataset = calib_gen\n    if is_int_only:\n      if is_int16_quantize:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.\n            EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,\n            lite.OpsSet.TFLITE_BUILTINS\n        ]\n      else:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.TFLITE_BUILTINS_INT8, lite.OpsSet.TFLITE_BUILTINS\n        ]\n    else:\n      if is_int16_quantize:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.\n            EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,\n            lite.OpsSet.TFLITE_BUILTINS\n        ]\n      else:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.TFLITE_BUILTINS\n        ]\n\n    quantized_converter.inference_input_type = inference_input_output_type\n    quantized_converter.inference_output_type = inference_input_output_type\n    quantized_converter.experimental_new_quantizer = enable_mlir_quantizer\n    quantized_tflite_model = quantized_converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n\n    expected_dtype = inference_input_output_type.as_numpy_dtype\n    # Allow float32 for fallback on non-quantizable op.\n    expected_ceil_dtype = (\n        expected_dtype if enable_mlir_quantizer else dtypes.float32)\n\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    self.assertLen(input_details, 2)\n    self.assertEqual(input_details[0]['dtype'], expected_dtype)\n    self.assertEqual(input_details[1]['dtype'], expected_ceil_dtype)\n    output_details = interpreter.get_output_details()\n    self.assertLen(output_details, 2)\n    self.assertEqual(output_details[0]['dtype'], expected_dtype)\n    self.assertEqual(output_details[1]['dtype'], expected_ceil_dtype)\n\n  def _getIntegerQuantizationModelWithControlFlow(self):\n    def true_fn(x):\n      return x\n\n    def false_fn(x):\n      return x\n\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[1, 2], dtype=tf.float32),\n        tf.TensorSpec(shape=(), dtype=tf.bool)\n    ])\n    def model(x, b):\n      x = x + x\n      x = tf.cond(b, true_fn=lambda: true_fn(x), false_fn=lambda: false_fn(x))\n      return x + x\n\n    def calibration_gen():\n      for _ in range(5):\n        yield [\n            np.random.uniform(-1, 1, size=(\n                1,\n                2,\n            )).astype(np.float32),\n            tf.constant(True),\n        ]\n      for _ in range(5):\n        yield [\n            np.random.uniform(-1, 1, size=(\n                1,\n                2,\n            )).astype(np.float32),\n            tf.constant(False),\n        ]\n\n    return (model, model.get_concrete_function(), calibration_gen)\n\n  @parameterized.named_parameters(\n      ('_INT8InputOutput', False, False, dtypes.int8),\n      ('_UINT8InputOutput', False, False, dtypes.uint8),\n      ('_INT16Quantize_INT16InputOutput', False, True, dtypes.int16),\n      ('_IntOnly_INT8InputOutput', True, False, dtypes.int8),\n      ('_IntOnly_UINT8InputOutput', True, False, dtypes.uint8),\n      ('_IntOnly_INT16Quantize_INT16InputOutput', True, True, dtypes.int16),\n      # TODO(b/198231624): Support control flow ops in MLIR quantizer\n      # ('_IntOnly_INT8InputOutputMlirQuant', True, False, dtypes.int8, True),\n      # ('_IntOnly_UINT8InputOutputMlirQuant', True, False, dtypes.uint8, True),\n  )\n  @test_util.run_v2_only\n  def testIntegerQuantizationWithControlFlow(self,\n                                             is_int_only,\n                                             is_int16_quantize,\n                                             inference_input_output_type,\n                                             enable_mlir_quantizer=False):\n    root, func, calib_gen = self._getIntegerQuantizationModelWithControlFlow()\n\n    quantized_converter = tf.lite.TFLiteConverter.from_concrete_functions(\n        [func], root)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_converter.representative_dataset = calib_gen\n    if is_int_only:\n      if is_int16_quantize:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet\n            .EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,\n            lite.OpsSet.TFLITE_BUILTINS\n        ]\n      else:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.TFLITE_BUILTINS_INT8, lite.OpsSet.TFLITE_BUILTINS\n        ]\n    else:\n      if is_int16_quantize:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet\n            .EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,\n            lite.OpsSet.TFLITE_BUILTINS\n        ]\n      else:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.TFLITE_BUILTINS\n        ]\n\n    quantized_converter.inference_input_type = inference_input_output_type\n    quantized_converter.inference_output_type = inference_input_output_type\n    quantized_converter.experimental_new_quantizer = enable_mlir_quantizer\n\n    quantized_tflite_model = quantized_converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n\n    expected_dtype = inference_input_output_type.as_numpy_dtype\n\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    self.assertLen(input_details, 2)\n    self.assertEqual(input_details[0]['dtype'], expected_dtype)\n    self.assertEqual(input_details[1]['dtype'], dtypes.bool)\n    output_details = interpreter.get_output_details()\n    self.assertLen(output_details, 1)\n    self.assertEqual(output_details[0]['dtype'], expected_dtype)\n\n  @parameterized.named_parameters(\n      ('_BlocklistedNoneWithLowering', None, None, True),\n      ('_BlocklistedNoneWithoutLowering', None, None, False),\n      ('_BlocklistedOpsWithLowering', {'CONV_2D'}, None, True),\n      ('_BlocklistedOpsWithoutLowering', {'CONV_2D'}, None, False),\n      ('_BlocklistedNodesWithLowering', None, {'PartitionedCall:0'}, True),\n      ('_BlocklistedNodesWithoutLowering', None, {'Identity'}, False))\n  @test_util.run_v2_only\n  def testNewQuantizerBlocklistingArgs(self, denylisted_ops, denylisted_nodes,\n                                       lower_to_saved_model):\n    \"\"\"Test the model quantized by the new converter and denylisted options.\"\"\"\n    root, func, calibration_gen = self._getIntegerQuantizeModel()\n    quantized_converter = lite.TFLiteConverterV2.from_concrete_functions([func],\n                                                                         root)\n    quantized_converter.target_spec.supported_ops = [\n        lite.OpsSet.TFLITE_BUILTINS_INT8\n    ]\n    quantized_converter.representative_dataset = calibration_gen\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_converter.experimental_new_quantizer = True\n    quantized_converter._experimental_calibrate_only = True\n    quantized_converter.experimental_lower_to_saved_model = lower_to_saved_model\n    calibrated = quantized_converter.convert()\n    quantized_tflite_model = mlir_quantize(\n        calibrated,\n        denylisted_ops=denylisted_ops,\n        denylisted_nodes=denylisted_nodes)\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    details = interpreter.get_tensor_details()\n    num_quantized_tensors = sum(\n        [1 for detail in details\n         if len(detail['quantization_parameters']['scales'])])\n    if denylisted_nodes or denylisted_ops:\n      self.assertEqual(num_quantized_tensors, 0)\n      return\n    self.assertEqual(num_quantized_tensors, 4)  # quant, filter, bias, dequant\n\n  @parameterized.named_parameters(\n      ('_SingleLayer', False),\n      ('_WholeModel', True),\n  )\n  @test_util.run_v2_only\n  def testNewQuantizerNumericVerificationDebugMode(self, whole_model_verify):\n    \"\"\"Test the model quantized by the new converter with numeric verify ops.\"\"\"\n    root, func, calibration_gen = self._getIntegerQuantizeModel()\n\n    quantized_converter = lite.TFLiteConverterV2.from_concrete_functions([func],\n                                                                         root)\n    quantized_converter.target_spec.supported_ops = [\n        lite.OpsSet.TFLITE_BUILTINS_INT8\n    ]\n    quantized_converter.representative_dataset = calibration_gen\n\n    # Create a TFLite model with new quantizer.\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_converter.experimental_new_quantizer = True\n    production_tflite = quantized_converter.convert()\n    # Create a TFLite model with new quantizer and numeric verify ops.\n    quantized_converter._experimental_calibrate_only = True\n    calibrated = quantized_converter.convert()\n    debug_mode_tflite = mlir_quantize(\n        calibrated,\n        enable_numeric_verify=True,\n        enable_whole_model_verify=whole_model_verify)\n\n    # Check if adding debug mode should output a different flatbuffer.\n    self.assertNotEqual(production_tflite, debug_mode_tflite)\n\n    # Check if newly added ops are numeric verify ops.\n    input_data = tf.constant(\n        np.random.uniform(-1, 1, size=(1, 5, 5, 3)).astype(np.float32))\n\n    def examine_tflite_model(tflite_content, input_data):\n      interpreter = Interpreter(\n          model_content=tflite_content,\n          experimental_op_resolver_type=OpResolverType\n          .BUILTIN_WITHOUT_DEFAULT_DELEGATES)\n      interpreter.allocate_tensors()\n      input_details = interpreter.get_input_details()\n      interpreter.set_tensor(input_details[0]['index'], input_data.numpy())\n      interpreter.invoke()\n      tensor_details = interpreter.get_tensor_details()\n      return {\n          details['name']: interpreter.get_tensor(details['index'])\n          for details in interpreter.get_tensor_details()\n      }, tensor_details\n\n    tflite_result, _ = examine_tflite_model(production_tflite, input_data)\n    debug_mode_tflite_result, debug_tensor_details = examine_tflite_model(\n        debug_mode_tflite, input_data)\n\n    # MLIR-based quantizer should output flatbuffer model with `tfl.quantize`.\n    num_production_quantize_ops = len([\n        None for output_tensor_name in tflite_result\n        if 'tfl.quantize' in output_tensor_name\n    ])\n    self.assertEqual(num_production_quantize_ops, 1)\n    # MLIR-based quantizer should output flatbuffer model with `tfl.quantize`.\n    num_debug_quantize_ops = len([\n        None for output_tensor_name in debug_mode_tflite_result\n        if 'tfl.quantize' in output_tensor_name\n    ])\n    # Two numbers should be equal.\n    self.assertEqual(num_production_quantize_ops, num_debug_quantize_ops)\n    # DebugMode TFLite flatbuffer should have NumericVerifyOps more than zero.\n    # The name has the prefix \"NumericVerify/{name}:{id}\n    # where {name} is the tensor name of the original quantized op's activation,\n    # and {id} is its tensor id.\n    num_debug_ops = 0\n    for output_tensor_name in debug_mode_tflite_result:\n      if 'NumericVerify' in output_tensor_name:\n        pos_end_prefix = len('NumericVerify/')\n        pos_colon = output_tensor_name.rfind(':')\n        self.assertEqual('NumericVerify/', output_tensor_name[:pos_end_prefix])\n        tensor_id = int(output_tensor_name[pos_colon + 1:])\n        original_tensor_name = output_tensor_name[pos_end_prefix:pos_colon]\n        self.assertEqual(original_tensor_name,\n                         debug_tensor_details[tensor_id]['name'])\n        num_debug_ops += 1\n    self.assertEqual(num_debug_ops, 1)\n    # The number of debug ops should be equal to that of quantized ops.\n    self.assertEqual(num_debug_ops, num_debug_quantize_ops)\n\n  @parameterized.named_parameters(\n      ('_PerChannelQuant', False, False),\n      ('_PerChannelMlirQuant', False, True),\n      ('_PerTensorQuant', True, False),\n      ('_PerTensorMlirQuant', True, True),\n      ('_PerChannelDynamicRange', False, False, False),\n      ('_PerTensorDynamicRange', True, False, False))\n  @test_util.run_v2_only\n  def testDisablePerChannelQuantization(self, disable_per_channel=False,\n                                        enable_mlir_quantizer=False,\n                                        representative_dataset=True):\n    k_conv_name = 'Conv2D'\n    # Dynamic range quant requires total num elements of filters > 1024.\n    k_num_filters = 38\n    root, func, calib_gen = self._getIntegerQuantizeModel(k_num_filters)\n    quantized_converter = tf.lite.TFLiteConverter.from_concrete_functions(\n        [func], root)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_converter.representative_dataset = calib_gen\n    quantized_converter.target_spec.supported_ops = [\n        lite.OpsSet.TFLITE_BUILTINS\n    ]\n    quantized_converter.experimental_new_quantizer = enable_mlir_quantizer\n    if disable_per_channel:\n      quantized_converter._experimental_disable_per_channel = (\n          disable_per_channel)\n    quantized_tflite_model = quantized_converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    detail = next((d for d in interpreter.get_tensor_details()\n                   if d['name'].startswith(k_conv_name)))\n    quant_params = detail['quantization_parameters']\n    expected_num_params = 1 if disable_per_channel else k_num_filters\n    self.assertLen(quant_params['scales'], expected_num_params)\n    self.assertLen(quant_params['zero_points'], expected_num_params)\n\n  @parameterized.named_parameters(('MlirQuantize', True),\n                                  ('TocoQuantize', False))\n  @test_util.run_v2_only\n  def testQuantizeBiasOverflow(self, enable_mlir_quantizer):\n    \"\"\"Tests if the quantizer handles bias overflow by adjusting scales.\"\"\"\n    input_data = np.array([[-1e-3, 1e-3]], dtype=np.float32)\n\n    def calibration_gen():\n      yield {'x': input_data}\n\n    root = self._getMatMulModelWithSmallWeights()\n    input_data = tf.constant([-1e-3, 1e-3], shape=(1, 2))\n    concrete_func = root.matmul.get_concrete_function(input_data)\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               root)\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    converter.representative_dataset = calibration_gen\n    converter.experimental_new_quantizer = enable_mlir_quantizer\n    quantized_model = converter.convert()\n\n    interpreter = Interpreter(model_content=quantized_model)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    output_details = interpreter.get_output_details()\n    output = interpreter.get_tensor(output_details[0]['index'])\n    # the inputs and weights are far smaller than the biases, so the final\n    # result should be equal to the biases.\n    self.assertAllClose(root.bias, output.flatten())\n\n  @test_util.run_v2_only\n  def testOpVersion(self):\n    @tf.function(\n        input_signature=[tf.TensorSpec(shape=[5, 5], dtype=tf.float32)])\n    def custom_resize(image):\n      # Add \"batch\" and \"channels\" dimensions\n      image = image[tf.newaxis, ..., tf.newaxis]\n      # ResizeBilinear version 3.\n      resize1 = tf.compat.v1.image.resize_bilinear(\n          image, [2, 2], half_pixel_centers=True)\n      # ResizeBilinear version 1.\n      resize2 = tf.compat.v1.image.resize_bilinear(image, [2, 2])\n      return resize1 + resize2\n\n    concrete_func = custom_resize.get_concrete_function()\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               custom_resize)\n    tflite_model = converter.convert()\n    model_object = schema_fb.Model.GetRootAsModel(tflite_model, 0)\n    model = schema_fb.ModelT.InitFromObj(model_object)\n\n    for operator in model.operatorCodes:\n      if operator.builtinCode == schema_fb.BuiltinOperator.RESIZE_BILINEAR:\n        # half_pixel_centers is supported by ResizeBilinear version 3.\n        self.assertEqual(operator.version, 3)\n        break\n\n  @test_util.run_v2_only\n  def testForceSelectTFOps(self):\n    root = self._getSimpleVariableModel()\n    input_data = tf.constant(1., shape=[1])\n    concrete_func = root.f.get_concrete_function(input_data)\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               root)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertEqual(metadata.options.forceSelectTfOps, True)\n\n    # Check output value from converted model.\n    expected_value = root.f(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertEqual(expected_value.numpy(), actual_value)\n\n  def testExcludeConversionMetadata(self):\n    root = self._getSimpleVariableModel()\n    input_data = tf.constant(1., shape=[1])\n    concrete_func = root.f.get_concrete_function(input_data)\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               root)\n    converter.exclude_conversion_metadata = True\n    tflite_model = converter.convert()\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(tflite_model)\n    self.assertIsNone(metadata)\n\n  def testConversionMetadataForDynamicRange(self):\n    func, _ = self._getSqrtModel()\n    converter = lite.TFLiteConverterV2.from_concrete_functions(\n        [func.get_concrete_function()])\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_model = converter.convert()\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(quantized_model)\n    self.assertIsNotNone(metadata)\n    self.assertAllEqual([metadata_fb.ModelOptimizationMode.PTQ_DYNAMIC_RANGE],\n                        metadata.options.modelOptimizationModes)\n\n  def testConversionMetadataForFloat16(self):\n    root, func, calibration_gen = self._getIntegerQuantizeModel()\n    converter = lite.TFLiteConverterV2.from_concrete_functions([func], root)\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    converter.representative_dataset = calibration_gen\n    converter.target_spec.supported_types = [dtypes.float16]\n    quantized_model = converter.convert()\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(quantized_model)\n    self.assertIsNotNone(metadata)\n    self.assertAllEqual([metadata_fb.ModelOptimizationMode.PTQ_FLOAT16],\n                        metadata.options.modelOptimizationModes)\n\n\nclass FromSavedModelTest(lite_v2_test_util.ModelTest):\n\n  def _createV1SavedModel(self, shape):\n    \"\"\"Create a simple SavedModel.\"\"\"\n    saved_model_dir = os.path.join(self.get_temp_dir(), 'simple_savedmodel')\n    with tf.Graph().as_default():\n      with tf.compat.v1.Session() as sess:\n        in_tensor_1 = tf.compat.v1.placeholder(\n            shape=shape, dtype=tf.float32, name='inputB')\n        in_tensor_2 = tf.compat.v1.placeholder(\n            shape=shape, dtype=tf.float32, name='inputA')\n        variable_node = tf.Variable(1.0, name='variable_node')\n        out_tensor = in_tensor_1 + in_tensor_2 * variable_node\n        inputs = {'x': in_tensor_1, 'y': in_tensor_2}\n        outputs = {'z': out_tensor}\n        sess.run(tf.compat.v1.variables_initializer([variable_node]))\n        saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n    return saved_model_dir\n\n  def _createV2QATSavedModel(self, shape):\n    \"\"\"Create a simple QAT SavedModel in TF 2.\"\"\"\n    saved_model_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    input_name = 'input'\n    output_name = 'scores'\n\n    input_tensor = tf.keras.layers.Input((32, 32, 128), name=input_name)\n    x = tf.quantization.fake_quant_with_min_max_args(input_tensor, -3.0, 3.0)\n    x = tf.keras.layers.Conv2D(1, (3, 3))(x)\n    x = tf.quantization.fake_quant_with_min_max_args(x, -3.0, 3.0)\n    scores = tf.keras.layers.Reshape((-1,), name=output_name)(x)\n    model = tf.keras.Model(input_tensor, scores)\n    model.save(saved_model_dir)\n    return saved_model_dir, input_name, output_name\n\n  @test_util.run_v2_only\n  def testV1SimpleModel(self):\n    \"\"\"Test a SavedModel.\"\"\"\n    with tf.Graph().as_default():\n      saved_model_dir = self._createV1SavedModel(shape=[1, 16, 16, 3])\n\n      # Convert model and ensure model is not None.\n      converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n      tflite_model = converter.convert()\n      self.assertTrue(tflite_model)\n\n      interpreter = Interpreter(model_content=tflite_model)\n      interpreter.allocate_tensors()\n\n      input_details = interpreter.get_input_details()\n      self.assertLen(input_details, 2)\n      self.assertStartsWith(input_details[0]['name'], 'inputA')\n      self.assertEqual(np.float32, input_details[0]['dtype'])\n      self.assertAllEqual([1, 16, 16, 3], input_details[0]['shape'])\n      self.assertEqual((0., 0.), input_details[0]['quantization'])\n\n      self.assertStartsWith(\n          input_details[1]['name'],\n          'inputB',\n      )\n      self.assertEqual(np.float32, input_details[1]['dtype'])\n      self.assertTrue([1, 16, 16, 3], input_details[1]['shape'])\n      self.assertEqual((0., 0.), input_details[1]['quantization'])\n\n      output_details = interpreter.get_output_details()\n      self.assertLen(output_details, 1)\n      self.assertStartsWith(output_details[0]['name'], 'add')\n      self.assertEqual(np.float32, output_details[0]['dtype'])\n      self.assertTrue([1, 16, 16, 3], output_details[0]['shape'])\n      self.assertEqual((0., 0.), output_details[0]['quantization'])\n\n  @parameterized.named_parameters(\n      ('Default', False),\n      ('UnfoldLargeConstant', True),\n  )\n  @test_util.run_v2_only\n  def testUnfoldLargeConstant(self, unfold_large_constant):\n    \"\"\"Test unfolding large splat constant in a TF Lite model.\"\"\"\n    saved_model_dir = os.path.join(self.get_temp_dir(), 'simple_savedmodel')\n    with tf.Graph().as_default():\n      with tf.compat.v1.Session() as sess:\n        in_tensor = tf.compat.v1.placeholder(\n            shape=[1000, 1000], dtype=tf.float32, name='input')\n        constant = tf.constant(value=1, dtype=tf.float32, shape=[1000, 1000])\n        out_tensor = in_tensor + constant\n        inputs = {'x': in_tensor}\n        outputs = {'y': out_tensor}\n        saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter._experimental_unfold_large_splat_constant = unfold_large_constant\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n    model = util._convert_model_from_bytearray_to_object(tflite_model)\n    if unfold_large_constant:\n      self.assertEqual(model.operatorCodes[0].builtinCode,\n                       schema_fb.BuiltinOperator.FILL)\n      self.assertEqual(model.operatorCodes[1].builtinCode,\n                       schema_fb.BuiltinOperator.ADD)\n    else:\n      self.assertEqual(model.operatorCodes[0].builtinCode,\n                       schema_fb.BuiltinOperator.ADD)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    interpreter.allocate_tensors()\n\n    input_details = interpreter.get_input_details()\n    self.assertLen(input_details, 1)\n    self.assertEqual('input:0', input_details[0]['name'])\n    self.assertEqual(np.float32, input_details[0]['dtype'])\n    self.assertAllEqual([1000, 1000], input_details[0]['shape'])\n    self.assertEqual((0., 0.), input_details[0]['quantization'])\n\n    output_details = interpreter.get_output_details()\n    self.assertEqual('add:0', output_details[0]['name'])\n    self.assertEqual(np.float32, output_details[0]['dtype'])\n    self.assertAllEqual([1000, 1000], output_details[0]['shape'])\n    self.assertEqual((0., 0.), output_details[0]['quantization'])\n\n    interpreter.set_tensor(input_details[0]['index'],\n                           np.ones(shape=[1000, 1000], dtype=np.float32))\n    interpreter.invoke()\n    self.assertAllEqual(\n        np.full(shape=[1000, 1000], fill_value=2.0, dtype=np.float32),\n        interpreter.get_tensor(output_details[0]['index']))\n\n  @test_util.run_v2_only\n  def testPreserveAssert(self):\n    \"\"\"Test preserving AssertOp in a TF Lite model.\"\"\"\n    saved_model_dir = os.path.join(self.get_temp_dir(), 'simple_savedmodel')\n    with tf.Graph().as_default():\n      with tf.compat.v1.Session() as sess:\n        in_tensor = tf.compat.v1.placeholder(\n            shape=[10, 10], dtype=tf.float32, name='input')\n        constant = tf.constant(value=1, dtype=tf.float32, shape=[10, 10])\n        assert_op = tf.Assert(tf.less_equal(in_tensor, constant), [in_tensor])\n        with tf.control_dependencies([assert_op]):\n          out_tensor = in_tensor + constant\n        inputs = {'x': in_tensor}\n        outputs = {'y': out_tensor}\n        saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    converter._experimental_preserve_assert_op = True\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n    model = util._convert_model_from_bytearray_to_object(tflite_model)\n    has_assert = False\n    for op_code in model.operatorCodes:\n      if op_code.customCode == b'FlexAssert':\n        has_assert = True\n        break\n    self.assertTrue(has_assert)\n\n  @test_util.run_v2_only\n  def testTF1HubFormattedModel(self):\n    \"\"\"Test a TF1 hub formatted model.\"\"\"\n    saved_model_dir = self._createV1SavedModel(shape=[1, 16, 16, 3])\n\n    # TF1 hub model is based on V1 saved model and they omit the saved model\n    # schema version setting.\n    saved_model_proto = parse_saved_model(saved_model_dir)\n    saved_model_proto.saved_model_schema_version = 0\n\n    saved_model_pb_file_path = os.path.join(saved_model_dir, 'saved_model.pb')\n    with file_io.FileIO(saved_model_pb_file_path, 'wb') as writer:\n      writer.write(saved_model_proto.SerializeToString())\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n  def _createV1ModelWithHashTableInitializer(self):\n    # Create a v1 saved model with hash table initializers.\n    tf.compat.v1.disable_eager_execution()\n    saved_model_dir = os.path.join(self.get_temp_dir(),\n                                   'savedmodel_with_hashtable')\n\n    table_initializer = tf.lookup.KeyValueTensorInitializer(\n        keys=['a', 'b', 'c', 'd'],\n        values=[1, 2, 3, 4],\n        key_dtype=tf.string,\n        value_dtype=tf.int64)\n    table = tf.lookup.StaticHashTable(\n        table_initializer, default_value=tf.constant(-1, dtype=tf.int64))\n\n    x = tf.compat.v1.placeholder(tf.string, shape=(), name='input')\n    y = table.lookup(x)\n\n    tensor_info_x = tf.compat.v1.saved_model.utils.build_tensor_info(x)\n    tensor_info_y = tf.compat.v1.saved_model.utils.build_tensor_info(y)\n\n    signature_def_map, init_op, assets_collection = {\n        'serving_default':\n            (tf.compat.v1.saved_model.signature_def_utils.build_signature_def(\n                inputs={'x': tensor_info_x},\n                outputs={'y': tensor_info_y},\n                method_name='some_function'))\n    }, tf.compat.v1.tables_initializer(), None\n\n    sess = tf.compat.v1.Session()\n    sess.run(tf.compat.v1.initializers.global_variables())\n\n    builder = tf.compat.v1.saved_model.builder.SavedModelBuilder(\n        saved_model_dir)\n    builder.add_meta_graph_and_variables(\n        sess, [tf.compat.v1.saved_model.tag_constants.SERVING],\n        signature_def_map,\n        main_op=init_op,\n        assets_collection=assets_collection,\n        strip_default_attrs=True)\n    builder.save()\n\n    # Restore TF v2 behavior.\n    tf.compat.v1.reset_default_graph()\n    tf.compat.v1.enable_eager_execution()\n    return saved_model_dir\n\n  @test_util.run_v2_only\n  def testModelWithHashTableInitializer(self):\n    \"\"\"Test a model with saved_model's session initializer for hash tables.\"\"\"\n    saved_model_dir = self._createV1ModelWithHashTableInitializer()\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    input_data = np.array(['a', 'b', 'c', 'z'], dtype=np.string_)\n    interpreter.resize_tensor_input(\n        input_details[0]['index'], [4], strict=False)\n    interpreter.allocate_tensors()\n\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Invoke multiple times to ensure the initializer graph runs only once.\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual([1, 2, 3, -1], list(actual_value))\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual([1, 2, 3, -1], list(actual_value))\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual([1, 2, 3, -1], list(actual_value))\n\n  def _createV1ModelWithMutableHashTable(self):\n    # Create a v1 saved model with mutable hash table.\n    tf.compat.v1.disable_eager_execution()\n    saved_model_dir = os.path.join(self.get_temp_dir(),\n                                   'savedmodel_with_mutable_hashtable')\n\n    table = tf.raw_ops.MutableHashTableV2(\n        key_dtype=tf.string, value_dtype=tf.int64)\n    x = tf.compat.v1.placeholder(tf.string, shape=(), name='input')\n    keys = tf.constant(['a', 'b'], tf.string)\n    values = tf.constant([1, 5], tf.int64)\n    default_value = tf.constant(-1, tf.int64)\n    insert_call = tf.raw_ops.LookupTableInsertV2(\n        table_handle=table, keys=keys, values=values)\n    with tf.control_dependencies([insert_call]):\n      y = tf.raw_ops.LookupTableFindV2(\n          table_handle=table, keys=x, default_value=default_value)\n\n    tensor_info_x = tf.compat.v1.saved_model.utils.build_tensor_info(x)\n    tensor_info_y = tf.compat.v1.saved_model.utils.build_tensor_info(y)\n\n    signature_def_map, init_op, assets_collection = {\n        'serving_default':\n            (tf.compat.v1.saved_model.signature_def_utils.build_signature_def(\n                inputs={'x': tensor_info_x},\n                outputs={'y': tensor_info_y},\n                method_name='some_function'))\n    }, tf.compat.v1.tables_initializer(), None\n\n    sess = tf.compat.v1.Session()\n\n    builder = tf.compat.v1.saved_model.builder.SavedModelBuilder(\n        saved_model_dir)\n    builder.add_meta_graph_and_variables(\n        sess, [tf.compat.v1.saved_model.tag_constants.SERVING],\n        signature_def_map,\n        main_op=init_op,\n        assets_collection=assets_collection,\n        strip_default_attrs=True)\n    builder.save()\n\n    # Restore TF v2 behavior.\n    tf.compat.v1.reset_default_graph()\n    tf.compat.v1.enable_eager_execution()\n    return saved_model_dir\n\n  @test_util.run_v2_only\n  def testModelWithMutableHashTable(self):\n    \"\"\"Test a model with saved_model's session initializer for hash tables.\"\"\"\n    saved_model_dir = self._createV1ModelWithMutableHashTable()\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    input_data = np.array(['a', 'b', 'c'], dtype=np.string_)\n    interpreter.resize_tensor_input(\n        input_details[0]['index'], [3], strict=False)\n    interpreter.allocate_tensors()\n\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual([1, 5, -1], list(actual_value))\n\n  @test_util.run_v2_only\n  def testReduceSumWithInt16Quant(self):\n    \"\"\"Test a model with quantized int16 reduce sum op.\"\"\"\n    inp = tf.keras.Input([3, 3], 3, name='x')\n    m = tf.keras.Model(inp, tf.reduce_sum(inp, axis=-1))\n\n    converter = tf.lite.TFLiteConverter.from_keras_model(m)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet\n        .EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8\n    ]\n    converter.inference_input_type = tf.int16\n    converter.inference_output_type = tf.int16\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    inputs = {\n        i.name: np.random.normal(size=i.shape).astype(np.float32)\n        for i in m.inputs\n    }\n    converter.representative_dataset = lambda: [inputs]\n    content = converter.convert()\n\n    interpreter = tf.lite.Interpreter(model_content=content)\n    runner = interpreter.get_signature_runner('serving_default')\n    y = runner(x=np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]]).astype(np.int16))\n    self.assertEqual([3, 6, 9], list(list(y.values())[0]))\n\n  @test_util.run_v2_only\n  def testConstModel(self):\n    \"\"\"Test a basic model with functions to make sure functions are inlined.\"\"\"\n    input_data = tf.constant(1., shape=[1])\n    root = autotrackable.AutoTrackable()\n    root.f = tf.function(lambda x: 2. * x)\n    to_save = root.f.get_concrete_function(input_data)\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir, to_save)\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_saved_model(save_dir)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = root.f(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertEqual(expected_value.numpy(), actual_value)\n\n  @test_util.run_v2_only\n  def testVariableModel(self):\n    \"\"\"Test a basic model with Variables with saving/loading the SavedModel.\"\"\"\n    root = self._getSimpleVariableModel()\n    input_data = tf.constant(1., shape=[1])\n    to_save = root.f.get_concrete_function(input_data)\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir, to_save)\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_saved_model(save_dir)\n    tflite_model = converter.convert()\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertEqual(metadata.environment.modelType,\n                     metadata_fb.ModelType.TF_SAVED_MODEL)\n\n    # Check values from converted model.\n    expected_value = root.f(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertEqual(expected_value.numpy(), actual_value)\n\n  @parameterized.named_parameters(('EnableResourceVariables', True),\n                                  ('DisableResourceVariables', False))\n  @test_util.run_v2_only\n  def testNativeVariablesModel(self, enable_resource_variables):\n    \"\"\"Test a basic model with Variables with saving/loading the SavedModel.\"\"\"\n    root = self._getSimpleModelWithVariables()\n    input_data = tf.constant(1., shape=[1, 10])\n    to_save = root.assign_add.get_concrete_function(input_data)\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir, to_save)\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_saved_model(save_dir)\n    converter.experimental_enable_resource_variables = enable_resource_variables\n\n    if not enable_resource_variables:\n      with self.assertRaises(convert.ConverterError) as error:\n        tflite_model = converter.convert()\n      self.assertIn(\n          'Variable constant folding is failed. Please consider using enabling '\n          '`experimental_enable_resource_variables` flag in the TFLite '\n          'converter object.',\n          str(error.exception))\n      return\n\n    # Enable resource variables.\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = root.assign_add(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    for tf_result, tflite_result in zip(expected_value, actual_value[0]):\n      self.assertAllClose(tf_result, tflite_result, atol=1e-05)\n\n  @test_util.run_v2_only\n  def testSignatures(self):\n    \"\"\"Test values for `signature_keys` argument.\"\"\"\n    root = self._getSimpleVariableModel()\n    input_data = tf.constant(1., shape=[1])\n    to_save = root.f.get_concrete_function(input_data)\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir, to_save)\n\n    # Convert model with invalid `signature_keys`.\n    with self.assertRaises(ValueError) as error:\n      _ = lite.TFLiteConverterV2.from_saved_model(\n          save_dir, signature_keys=['INVALID'])\n    self.assertIn(\"Invalid signature key 'INVALID'\", str(error.exception))\n\n    # Convert model with empty `signature_keys`.\n    converter = lite.TFLiteConverterV2.from_saved_model(\n        save_dir, signature_keys=[])\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = root.f(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertEqual(expected_value.numpy(), actual_value)\n\n  @test_util.run_v2_only\n  def testSignatureDefsWithFullIntegerQuantization(self):\n    # SETUP\n    # 1. Define input shapes\n    tf_input_shape = (32, 32, 128)\n    tflite_input_shape = (1,) + tf_input_shape\n    # 2. Define model\n    tf_saved_model_dir, input_name, output_name = (\n        self._createV2QATSavedModel(tf_input_shape))\n\n    # MODEL 1: TFLite (float) model\n    # 1. Create TFLite model\n    converter = tf.lite.TFLiteConverter.from_saved_model(tf_saved_model_dir)\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    tflite_model = converter.convert()\n    # 2. Initialize the Intepreter\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()[0]\n    output_details = interpreter.get_output_details()[0]\n    interpreter.resize_tensor_input(input_details['index'], tflite_input_shape)\n    interpreter.allocate_tensors()\n    signature_list = interpreter._get_full_signature_list()['serving_default']\n    # 3. (Skip) Verify that signature def input/output tensors are in the model.\n    # 4. Evaluate the model\n    input_data = np.random.random(tflite_input_shape).astype(np.float32)\n    result = self._evaluateTFLiteModelUsingSignatureDef(\n        tflite_model, 'serving_default', {input_name: input_data})[output_name]\n\n    # MODEL 2: TFLite (full integer quantized) model\n    # 1. Create TFLite model\n    converter = tf.lite.TFLiteConverter.from_saved_model(tf_saved_model_dir)\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    converter.inference_input_type = tf.int8\n    converter.inference_output_type = tf.int8\n    tflite_model_quant = converter.convert()\n    # 2. Initialize the Intepreter\n    interpreter = Interpreter(model_content=tflite_model_quant)\n    input_details = interpreter.get_input_details()[0]\n    output_details = interpreter.get_output_details()[0]\n    interpreter.resize_tensor_input(input_details['index'], tflite_input_shape)\n    interpreter.allocate_tensors()\n    # 3. Verify that signature def input/output tensors are in the model.\n    all_indices = {item['index'] for item in interpreter.get_tensor_details()}\n    signature_list = interpreter._get_full_signature_list()['serving_default']\n    input_tensor_indices = set(signature_list['inputs'].values())\n    assert input_tensor_indices.issubset(all_indices)\n    output_tensor_indices = set(signature_list['outputs'].values())\n    assert output_tensor_indices.issubset(all_indices)\n\n    # 4. Evaluate the model\n    input_data = np.random.random(tflite_input_shape)\n    input_scale, input_zero_point = input_details['quantization']\n    if (input_scale, input_zero_point) != (0.0, 0):\n      input_data = input_data / input_scale + input_zero_point\n      input_data = input_data.astype(input_details['dtype'])\n    result_quant = self._evaluateTFLiteModelUsingSignatureDef(\n        tflite_model_quant, 'serving_default',\n        {input_name: input_data})[output_name]\n    output_scale, output_zero_point = output_details['quantization']\n    if (output_scale, output_zero_point) != (0.0, 0):\n      result_quant = result_quant.astype(np.float32)\n      result_quant = (result_quant - output_zero_point) * output_scale\n\n    # COMPARE: Validate that results from both models are approx. the same.\n    root_mean_squared = np.sqrt(np.mean((result-result_quant)**2))\n    assert root_mean_squared < 1.0\n\n  @test_util.run_v2_only\n  def testSignatureDefs(self):\n    \"\"\"Test converting SignatureDef is correct and uses SignatureDef API.\"\"\"\n    root = self._getMultiFunctionModel()\n    input_data_0 = tf.constant(1., shape=[1])\n    input_data_1 = tf.constant(3., shape=[1])\n    mul_add_func = root.mul_add.get_concrete_function(input_data_1,\n                                                      input_data_0)\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir, {'mul_add': mul_add_func})\n\n    converter = lite.TFLiteConverterV2.from_saved_model(\n        save_dir, signature_keys=['mul_add'])\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = root.mul_add(input_data_1, input_data_0)\n    interpreter = Interpreter(model_content=tflite_model)\n    signature_defs = interpreter.get_signature_list()\n    results = self._evaluateTFLiteModelUsingSignatureDef(\n        tflite_model, 'mul_add', {\n            'y': input_data_0,\n            'x': input_data_1\n        })\n    self.assertEqual(list(results.keys()), ['output_0'])\n    self.assertEqual(expected_value.numpy(), results['output_0'])\n\n    # Verify the SignatureDef structure returned is as expected.\n    self.assertEqual(len(signature_defs), 1)\n    self.assertEqual(list(signature_defs.keys()), ['mul_add'])\n    self.assertEqual(len(signature_defs.values()), 1)\n    self.assertEqual(\n        list(signature_defs['mul_add'].keys()), ['inputs', 'outputs'])\n    self.assertCountEqual(signature_defs['mul_add']['inputs'], ['x', 'y'])\n    self.assertEqual(list(signature_defs['mul_add']['outputs']), ['output_0'])\n\n  @test_util.run_v2_only\n  def testSignatureDefsWithDefaultValue(self):\n    \"\"\"Test converting SignatureDef is correct and uses SignatureDef API.\n\n    This test uses None as signature_key to test default behavior.\n    \"\"\"\n    root = self._getMultiFunctionModel()\n    input_data_0 = tf.constant(1., shape=[1])\n    input_data_1 = tf.constant(3., shape=[1])\n    mul_add_func = root.mul_add.get_concrete_function(input_data_1,\n                                                      input_data_0)\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir, {'mul_add': mul_add_func})\n\n    converter = lite.TFLiteConverterV2.from_saved_model(\n        save_dir, signature_keys=['mul_add'])\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = root.mul_add(input_data_1, input_data_0)\n    interpreter = Interpreter(model_content=tflite_model)\n    signature_defs = interpreter.get_signature_list()\n    results = self._evaluateTFLiteModelUsingSignatureDef(\n        tflite_model, None, {\n            'y': input_data_0,\n            'x': input_data_1\n        })\n    self.assertEqual(list(results.keys()), ['output_0'])\n    self.assertEqual(expected_value.numpy(), results['output_0'])\n\n    # Verify the SignatureDef structure returned is as expected.\n    self.assertEqual(len(signature_defs), 1)\n    self.assertEqual(list(signature_defs.keys()), ['mul_add'])\n    self.assertEqual(len(signature_defs.values()), 1)\n    self.assertEqual(\n        list(signature_defs['mul_add'].keys()), ['inputs', 'outputs'])\n    self.assertCountEqual(signature_defs['mul_add']['inputs'], ['x', 'y'])\n    self.assertEqual(list(signature_defs['mul_add']['outputs']), ['output_0'])\n\n  @test_util.run_v2_only\n  def testSignatureDefsQuantizedModel(self):\n    \"\"\"Test converting SignatureDef on quantized model.\"\"\"\n    root = self._getMultiFunctionModel()\n    input_data_0 = tf.constant(1., shape=[1])\n    input_data_1 = tf.constant(3., shape=[1])\n    mul_add_func = root.mul_add.get_concrete_function(input_data_1,\n                                                      input_data_0)\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir, {'mul_add': mul_add_func})\n\n    converter = lite.TFLiteConverterV2.from_saved_model(\n        save_dir, signature_keys=['mul_add'])\n\n    def representative_dataset_gen():\n      for _ in range(2):\n        yield {\n            'x':\n                np.random.uniform(low=0, high=1,\n                                  size=(1, 1)).astype(np.float32),\n            'y':\n                np.random.uniform(low=0, high=1, size=(1, 1)).astype(np.float32)\n        }\n\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    converter.representative_dataset = representative_dataset_gen\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n    tflite_model = converter.convert()\n\n    # Check signatures are valid from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    signature_defs = interpreter.get_signature_list()\n\n    # Verify the SignatureDef structure returned is as expected.\n    self.assertEqual(len(signature_defs), 1)\n    self.assertEqual(list(signature_defs.keys()), ['mul_add'])\n    self.assertEqual(len(signature_defs.values()), 1)\n    self.assertEqual(\n        list(signature_defs['mul_add'].keys()), ['inputs', 'outputs'])\n    self.assertCountEqual(signature_defs['mul_add']['inputs'], ['x', 'y'])\n    self.assertEqual(list(signature_defs['mul_add']['outputs']), ['output_0'])\n\n  @test_util.run_v2_only\n  def testMultipleFunctionModel(self):\n    \"\"\"Convert multiple functions in a multi-functional model.\"\"\"\n    root = self._getMultiFunctionModel()\n    input_data = tf.constant(1., shape=[1])\n    add_func = root.add.get_concrete_function(input_data)\n    sub_func = root.sub.get_concrete_function(input_data)\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir, {'add': add_func, 'sub': sub_func})\n\n    # Try converting multiple functions.\n    converter = lite.TFLiteConverterV2.from_saved_model(save_dir)\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    interpreter = tf.lite.Interpreter(model_content=tflite_model)\n    signature_defs = interpreter.get_signature_list()\n\n    # Verify the SignatureDef structure returned is as expected.\n    self.assertEqual(len(signature_defs), 2)\n    self.assertEqual(list(signature_defs.keys()), ['add', 'sub'])\n    self.assertEqual(len(signature_defs.values()), 2)\n    self.assertEqual(list(signature_defs['add'].keys()), ['inputs', 'outputs'])\n    self.assertCountEqual(signature_defs['add']['inputs'], ['x'])\n    self.assertEqual(list(signature_defs['add']['outputs']), ['output_0'])\n    self.assertEqual(list(signature_defs['sub'].keys()), ['inputs', 'outputs'])\n    self.assertCountEqual(signature_defs['sub']['inputs'], ['x'])\n    self.assertEqual(list(signature_defs['sub']['outputs']), ['output_0'])\n\n    # Verify the Signature runner executions.\n    add_signature_runner = interpreter.get_signature_runner('add')\n    add_output = add_signature_runner(x=input_data)\n    self.assertEqual(add_output['output_0'], 3)\n\n    sub_signature_runner = interpreter.get_signature_runner('sub')\n    sub_output = sub_signature_runner(x=input_data)\n    self.assertEqual(sub_output['output_0'], -2)\n\n  @parameterized.named_parameters(\n      ('_Default', False, False, dtypes.float32, False),\n      ('_DefaultMlirQuant', False, False, dtypes.float32, True),\n      ('_INT8InputOutput', False, False, dtypes.int8),\n      ('_UINT8InputOutput', False, False, dtypes.uint8),\n      ('_INT16Quantize_INT16InputOutput', False, True, dtypes.int16),\n      ('_IntOnly_INT8InputOutput', True, False, dtypes.int8),\n      ('_IntOnly_UINT8InputOutput', True, False, dtypes.uint8),\n      ('_IntOnly_INT16Quantize_INT16InputOutput', True, True, dtypes.int16),\n      ('_IntOnly_INT8InputOutputMlirQuant', True, False, dtypes.int8, True),\n      ('_IntOnly_UINT8InputOutputMlirQuant', True, False, dtypes.uint8, True))\n  @test_util.run_v2_only\n  def testMultipleFunctionQuantizedModel(self,\n                                         is_int_only,\n                                         is_int16_quantize,\n                                         inference_input_output_type,\n                                         enable_mlir_quantizer=False):\n    \"\"\"Convert multiple functions in a multi-functional model.\"\"\"\n    root = self._getMultiFunctionModel()\n    input_data = tf.constant(1., shape=[1])\n    add_func = root.add.get_concrete_function(input_data)\n    sub_func = root.sub.get_concrete_function(input_data)\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir, {'add': add_func, 'sub': sub_func})\n\n    # Try converting multiple functions.\n    converter = lite.TFLiteConverterV2.from_saved_model(save_dir)\n\n    def representative_dataset_gen():\n      for _ in range(2):\n        yield ('add', {\n            'x': np.random.uniform(low=0, high=1, size=(1,)).astype(np.float32),\n        })\n      for _ in range(2):\n        yield ('sub', {\n            'x': np.random.uniform(low=0, high=1, size=(1,)).astype(np.float32),\n        })\n\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    converter.representative_dataset = representative_dataset_gen\n    if is_int_only:\n      if is_int16_quantize:\n        converter.target_spec.supported_ops = [\n            lite.OpsSet\n            .EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8\n        ]\n      else:\n        converter.target_spec.supported_ops = [lite.OpsSet.TFLITE_BUILTINS_INT8]\n    else:\n      if is_int16_quantize:\n        converter.target_spec.supported_ops = [\n            lite.OpsSet\n            .EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8\n        ]\n      else:\n        converter.target_spec.supported_ops = [lite.OpsSet.TFLITE_BUILTINS]\n    converter.inference_input_type = inference_input_output_type\n    converter.inference_output_type = inference_input_output_type\n    converter.experimental_new_quantizer = enable_mlir_quantizer\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    interpreter = tf.lite.Interpreter(model_content=tflite_model)\n    signature_defs = interpreter.get_signature_list()\n\n    # Verify the SignatureDef structure returned is as expected.\n    self.assertEqual(len(signature_defs), 2)\n    self.assertEqual(list(signature_defs.keys()), ['add', 'sub'])\n    self.assertEqual(len(signature_defs.values()), 2)\n    self.assertEqual(list(signature_defs['add'].keys()), ['inputs', 'outputs'])\n    self.assertCountEqual(signature_defs['add']['inputs'], ['x'])\n    self.assertEqual(list(signature_defs['add']['outputs']), ['output_0'])\n    self.assertEqual(list(signature_defs['sub'].keys()), ['inputs', 'outputs'])\n    self.assertCountEqual(signature_defs['sub']['inputs'], ['x'])\n    self.assertEqual(list(signature_defs['sub']['outputs']), ['output_0'])\n\n    # Verify the Signature runner executions.\n    input_data = tf.constant(\n        np.random.uniform(-1, 1, size=(1,)).astype(\n            inference_input_output_type.as_numpy_dtype))\n    add_signature_runner = interpreter.get_signature_runner('add')\n    add_output = add_signature_runner(x=input_data)\n    self.assertIsNotNone(add_output['output_0'])\n    input_details = add_signature_runner.get_input_details()\n    self.assertLen(input_details, 1)\n    self.assertStartsWith(input_details['x']['name'], 'add_x:0')\n    self.assertEqual(inference_input_output_type.as_numpy_dtype,\n                     input_details['x']['dtype'])\n    self.assertTrue(([1] == input_details['x']['shape']).all())\n    if inference_input_output_type == dtypes.float32:\n      self.assertEqual((0.0, 0), input_details['x']['quantization'])\n\n    sub_signature_runner = interpreter.get_signature_runner('sub')\n    sub_output = sub_signature_runner(x=input_data)\n    self.assertIsNotNone(sub_output['output_0'])\n    output_details = sub_signature_runner.get_output_details()\n    self.assertLen(output_details, 1)\n    self.assertStartsWith(output_details['output_0']['name'],\n                          'StatefulPartitionedCall:0')\n    self.assertEqual(inference_input_output_type.as_numpy_dtype,\n                     output_details['output_0']['dtype'])\n    self.assertTrue(([1] == output_details['output_0']['shape']).all())\n    if inference_input_output_type == dtypes.float32:\n      self.assertEqual((0.0, 0), output_details['output_0']['quantization'])\n\n  @test_util.run_v2_only\n  def testMultipleFunctionModelWithSharedWeight(self):\n    \"\"\"Convert multiple functions with the shared weight.\"\"\"\n    root = self._getMultiFunctionModelWithSharedWeight()\n    input_data = tf.constant(1., shape=[1])\n    add_func = root.add.get_concrete_function(input_data)\n    sub_func = root.sub.get_concrete_function(input_data)\n    mul_func = root.mul.get_concrete_function(input_data)\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir, {'add': add_func, 'sub': sub_func, 'mul': mul_func})\n\n    # Try converting multiple functions.\n    converter = lite.TFLiteConverterV2.from_saved_model(save_dir)\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    # Make sure that the weight tensors are shared.\n    self.assertLess(len(tflite_model), 1100000)\n\n    # TODO(b/184696047): Write down the test codes for multiple signature\n    #                    runners once the Python API is ready to use.\n    interpreter = tf.lite.Interpreter(model_content=tflite_model)\n    signature_defs = interpreter.get_signature_list()\n    self.assertLen(signature_defs, 3)\n    add_signature_runner = interpreter.get_signature_runner('add')\n    sub_signature_runner = interpreter.get_signature_runner('sub')\n    mul_signature_runner = interpreter.get_signature_runner('mul')\n    self.assertIsNotNone(add_signature_runner)\n    self.assertIsNotNone(sub_signature_runner)\n    self.assertIsNotNone(mul_signature_runner)\n\n  @test_util.run_v2_only\n  def testNoConcreteFunctionModel(self):\n    root = self._getMultiFunctionModel()\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir)\n\n    with self.assertRaises(ValueError) as error:\n      _ = lite.TFLiteConverterV2.from_saved_model(save_dir)\n    self.assertIn('Only support at least one signature key.',\n                  str(error.exception))\n\n  @test_util.run_v2_only\n  def testKerasSequentialModel(self):\n    \"\"\"Test a simple sequential tf.Keras model.\"\"\"\n    input_data = tf.constant(1., shape=[1, 1])\n\n    x = np.array([[1.], [2.]])\n    y = np.array([[2.], [4.]])\n\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(1),\n    ])\n    model.compile(optimizer='sgd', loss='mean_squared_error')\n    model.fit(x, y, epochs=1)\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(model, save_dir)\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_saved_model(save_dir)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = model.predict(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertEqual(expected_value, actual_value)\n\n  @test_util.run_v2_only\n  def testGraphDebugInfo(self):\n    \"\"\"Test a SavedModel has debug info captured.\"\"\"\n    input_data = tf.constant(1., shape=[1])\n    root = autotrackable.AutoTrackable()\n    root.f = tf.function(lambda x: 2. * x)\n    to_save = root.f.get_concrete_function(input_data)\n    options = save_options.SaveOptions(save_debug_info=True)\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir, to_save, options)\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_saved_model(save_dir)\n    converter.convert()\n    self._assertValidDebugInfo(converter._debug_info)\n\n  @test_util.run_v2_only\n  def testNonStatefulConvLSTM2D(self):\n    \"\"\"Test saved model with non stateful ConvLSTM2D keras layer.\"\"\"\n    # Create keras model\n    model = tf.keras.Sequential([\n        tf.keras.layers.ConvLSTM2D(\n            32, (3, 3),\n            padding='same',\n            return_sequences=True,\n            stateful=False,\n            batch_input_shape=(1, 1, 10, 10, 1))\n    ])\n    model.compile()\n\n    # Export the keras model to saved model.\n    saved_model_dir = os.path.join(self.get_temp_dir(), 'conv_lstm_2d')\n    model.save(saved_model_dir, save_format='tf', include_optimizer=False)\n\n    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n  @test_util.run_v2_only\n  def testKerasConvLSTM2DWithMoreThanOneDilationRate(self):\n    input_tensor = tf.keras.layers.Input(\n        batch_size=8,\n        shape=[9, 10, 11, 12],\n        name='input_tensor',\n        dtype=tf.float32)\n\n    output = tf.keras.layers.ConvLSTM2D(\n        filters=3,\n        kernel_size=3,\n        strides=1,\n        padding='VALID',\n        dilation_rate=2,\n        use_bias=False,\n        bias_initializer='ones',\n        data_format='channels_last')(\n            input_tensor)\n\n    model = tf.keras.Model(inputs=[input_tensor], outputs=output)\n    model.compile(\n        optimizer='adam',\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy'])\n\n    # Export the keras model to saved model.\n    saved_model_dir = os.path.join(self.get_temp_dir(),\n                                   'conv_lstm_2d_with_dilation_rate')\n    model.save(saved_model_dir, save_format='tf', include_optimizer=False)\n\n    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n  @test_util.run_v2_only\n  def testKerasFullyConnectedOutputShape3D(self):\n    \"\"\"Create a simple FullyConnected Model with an output of three dimensions.\"\"\"\n    input_tensor = tf.keras.layers.Input(\n        batch_size=1, shape=[3, 3], name='input_tensor', dtype=tf.float32)\n\n    x = tf.quantization.fake_quant_with_min_max_args(input_tensor, -3.0, 3.0)\n    x = tf.keras.layers.Dense(3)(x)\n    x = tf.quantization.fake_quant_with_min_max_args(x, -3.0, 3.0)\n    model = tf.keras.Model(input_tensor, x)\n\n    model.compile(\n        optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n\n    # Export the keras model to saved model.\n    saved_model_dir = os.path.join(self.get_temp_dir(),\n                                   'fully_connected_output_3d')\n    model.save(saved_model_dir, save_format='tf', include_optimizer=False)\n    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n    interpreter = Interpreter(model_content=tflite_model)\n    output_details = interpreter.get_output_details()\n    input_details = interpreter.get_input_details()\n    interpreter.allocate_tensors()\n\n    input_data = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]], np.float32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    expected_value = model.predict(input_data)\n\n    self.assertLen(output_details[0]['shape_signature'], 3)\n    self.assertAllClose(expected_value, actual_value, atol=1e-1)\n    self.assertEqual(\n        list(output_details[0]['shape_signature']),\n        list(model.layers[-1].output_shape))\n\n  def _createModelWithInputShape(self, shape):\n    \"\"\"Create a simple SavedModel with a certain shape.\"\"\"\n    saved_model_dir = os.path.join(self.get_temp_dir(), 'input_shape_model')\n    with tf.Graph().as_default():\n      with tf.compat.v1.Session() as sess:\n        unknown_shape = tf.TensorShape(shape)\n        in_tensor = tf.compat.v1.placeholder(\n            shape=unknown_shape, dtype=tf.float32, name='input')\n        out_tensor = in_tensor + in_tensor\n        inputs = {'input': in_tensor}\n        outputs = {'output': out_tensor}\n        saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n    return saved_model_dir\n\n  @test_util.run_v2_only\n  def testUnknownInputShapeModel(self):\n    \"\"\"Test a SavedModel with an unknown input shape.\"\"\"\n    saved_model_dir = self._createModelWithInputShape(None)\n\n    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n    # Validate that tensors with unknown shape have unknown rank.\n    tflite_model_obj = _convert_bytearray_to_object(tflite_model)\n    for tensor in tflite_model_obj.subgraphs[0].tensors:\n      self.assertEqual(False, tensor.hasRank)\n      self.assertEqual([], tensor.shape.tolist())\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    input_data = np.array([1., 2., 3.], dtype=np.float32)\n    interpreter.resize_tensor_input(\n        input_details[0]['index'], [3], strict=False)\n    interpreter.allocate_tensors()\n\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual([2., 4., 6.], list(actual_value))\n\n  @test_util.run_v2_only\n  def testScalarInputShapeModel(self):\n    \"\"\"Test a SavedModel with a scalar input.\"\"\"\n    saved_model_dir = self._createModelWithInputShape([])\n\n    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n    # Validate that scalar tensors have a rank = 0.\n    tflite_model_obj = _convert_bytearray_to_object(tflite_model)\n    for tensor in tflite_model_obj.subgraphs[0].tensors:\n      self.assertEqual(True, tensor.hasRank)\n      self.assertEqual([], tensor.shape.tolist())\n\n  @test_util.run_v2_only\n  def testMatrixInputShapeModel(self):\n    \"\"\"Test a SavedModel with a matrix input.\"\"\"\n    saved_model_dir = self._createModelWithInputShape([2, 3])\n\n    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n    # Validate that matrix tensors have a rank = 2.\n    tflite_model_obj = _convert_bytearray_to_object(tflite_model)\n    for tensor in tflite_model_obj.subgraphs[0].tensors:\n      self.assertEqual(True, tensor.hasRank)\n      self.assertEqual([2, 3], tensor.shape.tolist())\n\n  @parameterized.named_parameters(\n      ('_PerChannelQuant', False, False),\n      ('_PerChannelMlirQuant', False, True),\n      ('_PerTensorQuant', True, False),\n      ('_PerTensorMlirQuant', True, True),\n      ('_PerChannelDynamicRange', False, False, True),\n      ('_PerTensorDynamicRange', True, False, True))\n  @test_util.run_v2_only\n  def testDisablePerChannelQuantization(self,\n                                        disable_per_channel=False,\n                                        enable_mlir_quantizer=False,\n                                        representative_dataset=True):\n    # Dynamic range quant requires total num elements of filters > 1024.\n    k_num_filters = 38\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Conv2D(k_num_filters, (3, 3), activation='relu')\n    ])\n    model.build(input_shape=(1, 5, 5, 3))\n    saved_model_dir = os.path.join(self.get_temp_dir(), 'conv_saved_model')\n    save(model, saved_model_dir)\n    k_conv_name = 'sequential/conv2d/Conv2D'\n    quantized_converter = tf.lite.TFLiteConverter.from_saved_model(\n        saved_model_dir)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    if representative_dataset:\n      def calib_gen():\n        for _ in range(5):\n          yield [np.random.uniform(-1, 1, size=(1, 5, 5, 3)).astype(np.float32)]\n      quantized_converter.representative_dataset = calib_gen\n    quantized_converter.target_spec.supported_ops = [\n        lite.OpsSet.TFLITE_BUILTINS\n    ]\n    quantized_converter.experimental_new_quantizer = enable_mlir_quantizer\n    if disable_per_channel:\n      quantized_converter._experimental_disable_per_channel = (\n          disable_per_channel)\n    quantized_tflite_model = quantized_converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    detail = next((d for d in interpreter.get_tensor_details()\n                   if d['name'].startswith(k_conv_name)))\n    quant_params = detail['quantization_parameters']\n    expected_num_params = k_num_filters\n    if disable_per_channel:\n      expected_num_params = 1\n    self.assertLen(quant_params['scales'], expected_num_params)\n    self.assertLen(quant_params['zero_points'], expected_num_params)\n\n  @parameterized.named_parameters(\n      ('_INT8Quant_INT32Bias', False, False, dtypes.int32, True),\n      ('_INT16Quant_INT64Bias', True, False, dtypes.int64, True),\n      ('_INT8Quant_INT32Bias_Set', False, True, dtypes.int32, True),\n      ('_INT8Quant_INT64Bias_Set', False, True, dtypes.int64, False),\n      ('_INT16Quant_INT32Bias_Set', True, True, dtypes.int32, True),\n      ('_INT16Quant_INT64Bias_Set', True, True, dtypes.int64, True),\n      ('_INT16Quant_FLOAT32Bias_Set', True, True, dtypes.float32, False),\n  )\n  @test_util.run_v2_only\n  def testBiasQuantization(self, is_int16_quantize, explicitly_set_bias,\n                           bias_type, is_valid_bias_type):\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Dense(\n            1024, input_shape=[1024], activation=None, bias_initializer='ones')\n    ])\n    saved_model_dir = os.path.join(self.get_temp_dir(), 'dense_saved_model')\n    save(model, saved_model_dir)\n    k_dense_bias_name = 'sequential/dense/BiasAdd/ReadVariableOp'\n    quantized_converter = tf.lite.TFLiteConverter.from_saved_model(\n        saved_model_dir)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n\n    if explicitly_set_bias:\n      quantized_converter._experimental_full_integer_quantization_bias_type = bias_type\n\n    if is_int16_quantize:\n      quantized_converter.target_spec.supported_ops = [\n          lite.OpsSet\n          .EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8\n      ]\n    else:\n      quantized_converter.target_spec.supported_ops = [\n          lite.OpsSet.TFLITE_BUILTINS_INT8\n      ]\n\n    def calibration_gen():\n      for _ in range(5):\n        yield [np.random.randn(1, 1024).astype(np.float32)]\n\n    quantized_converter.representative_dataset = calibration_gen\n\n    if not is_valid_bias_type:\n      with self.assertRaisesRegex(ValueError, 'Expected bias type to be'):\n        quantized_converter.convert()\n      return\n\n    quantized_tflite_model = quantized_converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    dense_bias = next((d for d in interpreter.get_tensor_details()\n                       if d['name'].startswith(k_dense_bias_name)))\n    self.assertEqual(bias_type, dense_bias['dtype'])\n\n  @parameterized.named_parameters(\n      ('_Int8PerChannelMlirDynamicRangeQuant', True, False, False),\n      ('_Int8PerChannelTocoDynamicRangeQuant', False, False, False),\n      ('_Int8PerTensorMlirDynamicRangeQuant', True, True, False),\n      ('_Int8PerTensorTocoDynamicRangeQuant', False, True, False),\n      ('_Float16DynamicRangeQuant', True, False, True))\n  @test_util.run_v2_only\n  def testMlirDynamicRangeQuantization(self, enable_new_dynamic_range_quantizer,\n                                       disable_per_channel,\n                                       enable_float16_quant):\n    num_filters = 1024\n    conv_name = 'sequential/conv2d/Conv2D'\n    model = tf.keras.models.Sequential(\n        [tf.keras.layers.Conv2D(num_filters, (3, 3), activation='relu')])\n    model.build(input_shape=(1, 32, 32, 3))\n    saved_model_dir = self.create_tempdir()\n    save(model, saved_model_dir.full_path)\n\n    converter = tf.lite.TFLiteConverter.from_saved_model(\n        saved_model_dir.full_path)\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    converter.experimental_new_dynamic_range_quantizer = (\n        enable_new_dynamic_range_quantizer)\n    converter._experimental_disable_per_channel = disable_per_channel\n    if enable_float16_quant:\n      converter.target_spec.supported_types = [tf.float16]\n    quantized_tflite_model = converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    quantized_weight = None\n    quantized_weight_with_one_postfix = None\n    quantized_weight_without_one_postfix = None\n    for d in interpreter.get_tensor_details():\n      if d['name'] == conv_name + '1':\n        quantized_weight = d\n        quantized_weight_with_one_postfix = d\n        break\n    for d in interpreter.get_tensor_details():\n      if d['name'].startswith(conv_name):\n        if quantized_weight is None:\n          quantized_weight = d\n        quantized_weight_without_one_postfix = d\n        break\n\n    self.assertIsNotNone(quantized_weight)\n    quant_params = quantized_weight['quantization_parameters']\n\n    if enable_float16_quant:\n      expected_num_params = 0\n    else:\n      expected_num_params = 1 if disable_per_channel else num_filters\n    self.assertLen(quant_params['scales'], expected_num_params)\n    self.assertLen(quant_params['zero_points'], expected_num_params)\n\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    self.assertEqual(np.float32, input_details[0]['dtype'])\n    self.assertEqual(np.float32, output_details[0]['dtype'])\n    if enable_float16_quant:\n      self.assertTrue(\n          (quantized_weight_with_one_postfix is not None and\n           np.float16 == quantized_weight_with_one_postfix['dtype']) or\n          (quantized_weight_without_one_postfix is not None and\n           np.float16 == quantized_weight_without_one_postfix['dtype']))\n    else:\n      self.assertEqual(np.int8, quantized_weight['dtype'])\n\n\nclass FromKerasModelTest(lite_v2_test_util.ModelTest):\n\n  @test_util.run_v2_only\n  def testSequentialModel(self):\n    \"\"\"Test a simple sequential tf.Keras model.\"\"\"\n    input_data = tf.constant(1., shape=[1, 1])\n\n    # Create a simple Keras model.\n    x = np.array([[1.], [2.]])\n    y = np.array([[2.], [4.]])\n\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(units=1, input_shape=[1])\n    ])\n    model.compile(optimizer='sgd', loss='mean_squared_error')\n    model.fit(x, y, epochs=1)\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_keras_model(model)\n    tflite_model = converter.convert()\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertEqual(metadata.environment.modelType,\n                     metadata_fb.ModelType.KERAS_MODEL)\n\n    # Check values from converted model.\n    expected_value = model.predict(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertEqual(expected_value, actual_value)\n\n  @test_util.run_v2_only\n  def testSequentialMultiInputOutputModel(self):\n    \"\"\"Test a tf.Keras model with multiple inputs and outputs.\"\"\"\n    left_input_data = tf.constant(1., shape=[1, 3])\n    right_input_data = tf.constant(1., shape=[1, 3])\n\n    # Create a simple Keras model.\n    input_a_np = np.random.random((10, 3))\n    input_b_np = np.random.random((10, 3))\n    output_c_np = np.random.random((10, 3))\n    output_d_np = np.random.random((10, 2))\n\n    input_a = tf.keras.layers.Input(shape=(3,), name='input_a')\n    input_b = tf.keras.layers.Input(shape=(3,), name='input_b')\n\n    dense = tf.keras.layers.Dense(8, name='dense_1')\n    interm_a = dense(input_a)\n    interm_b = dense(input_b)\n    merged = tf.keras.layers.concatenate([interm_a, interm_b], name='merge')\n\n    output_c = tf.keras.layers.Dense(\n        3, activation='softmax', name='dense_2')(\n            merged)\n    output_d = tf.keras.layers.Dense(\n        2, activation='softmax', name='dense_3')(\n            merged)\n\n    model = tf.keras.models.Model(\n        inputs=[input_a, input_b], outputs=[output_c, output_d])\n    model.compile(optimizer='sgd', loss='mean_squared_error')\n    model.fit([input_a_np, input_b_np], [output_c_np, output_d_np], epochs=1)\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_keras_model(model)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    input_data = [left_input_data, right_input_data]\n    expected_value = model.predict(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, input_data)\n    for tf_result, tflite_result in zip(expected_value, actual_value):\n      self.assertAllClose(tf_result, tflite_result, atol=1e-05)\n\n  @test_util.run_v2_only\n  def testGraphDebugInfo(self):\n    \"\"\"Test a tf.Keras model has debug info captured.\"\"\"\n    # Create a simple Keras model.\n    x = [-1, 0, 1, 2, 3, 4]\n    y = [-3, -1, 1, 3, 5, 7]\n    model = tf.keras.models.Sequential(\n        [tf.keras.layers.Dense(units=1, input_shape=[1])])\n    model.compile(optimizer='sgd', loss='mean_squared_error')\n    model.fit(x, y, epochs=1)\n    converter = lite.TFLiteConverterV2.from_keras_model(model)\n    converter.convert()\n    self._assertValidDebugInfo(converter._debug_info)\n\n  @test_util.run_v2_only\n  def testKerasFallbackPath(self):\n    \"\"\"Test keras model which failed when exporting to the saved model.\"\"\"\n    input_data = tf.constant(\n        np.array(np.random.random_sample((20)), dtype=np.float32))\n\n    class Model(tf.keras.Model):\n\n      def __init__(self):\n        super(Model, self).__init__()\n        # A None name will cause a failure in exporting to a saved model.\n        self.shared_weights = self.add_weight(\n            name=None,\n            shape=(20, 1),\n            dtype=tf.float32,\n            initializer=tf.random_normal_initializer(\n                mean=0.0, stddev=300**(-0.5)))\n\n      def call(self, x):\n        return tf.add(self.shared_weights, x)\n\n    # Building the model.\n    model = Model()\n    model.compile(optimizer='sgd', loss='mean_squared_error')\n    model.fit(input_data, input_data, epochs=1)\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_keras_model(model)\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n  @test_util.run_v2_only\n  def testSignatureDefs(self):\n    \"\"\"Test converting SignatureDef is correct and uses SignatureDef API.\"\"\"\n    keras_model = tf.keras.Sequential([\n        tf.keras.layers.Conv2D(\n            32,\n            kernel_size=3,\n            padding='same',\n            activation='relu',\n            input_shape=(32, 32, 3),\n            name='tensor'),\n        tf.keras.layers.Dense(10, name='output_tensor')\n    ])\n\n    converter = lite.TFLiteConverterV2.from_keras_model(keras_model)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    input_data = tf.constant(\n        np.random.uniform(-1, 1, size=(1, 32, 32, 3)).astype(np.float32))\n    expected_value = keras_model(input_data)\n    interpreter = Interpreter(model_content=tflite_model)\n    signature_defs = interpreter.get_signature_list()\n    results = self._evaluateTFLiteModelUsingSignatureDef(\n        tflite_model, 'serving_default', {'tensor_input': input_data})\n    self.assertEqual(list(results.keys()), ['output_tensor'])\n    self.assertAllClose(expected_value.numpy(), results['output_tensor'])\n\n    # Verify the SignatureDef structure returned is as expected.\n    self.assertEqual(len(signature_defs), 1)\n    self.assertEqual(list(signature_defs.keys()), ['serving_default'])\n    self.assertEqual(len(signature_defs.values()), 1)\n    self.assertEqual(\n        list(signature_defs['serving_default'].keys()), ['inputs', 'outputs'])\n    self.assertCountEqual(signature_defs['serving_default']['inputs'],\n                          ['tensor_input'])\n    self.assertEqual(\n        list(signature_defs['serving_default']['outputs']), ['output_tensor'])\n\n  @parameterized.named_parameters(\n      ('_PerChannelMlirDynamicRangeQuant', True, False, False),\n      ('_PerChannelTocoDynamicRangeQuant', False, False, False),\n      ('_PerTensorMlirDynamicRangeQuant', True, True, False),\n      ('_PerTensorTocoDynamicRangeQuant', False, True, False),\n      ('_Float16DynamicRangeQuant', True, False, True))\n  @test_util.run_v2_only\n  def testMlirDynamicRangeQuantization(self, enable_new_dynamic_range_quantizer,\n                                       disable_per_channel,\n                                       enable_float16_quant):\n    num_filters = 1024\n    conv_name = 'sequential/conv2d/Conv2D'\n    model = tf.keras.models.Sequential(\n        [tf.keras.Input(shape=(32, 32, 3)),\n         tf.keras.layers.Conv2D(num_filters, (3, 3), activation='relu')])\n    model.build()\n\n    converter = lite.TFLiteConverterV2.from_keras_model(model)\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    converter.experimental_new_dynamic_range_quantizer = (\n        enable_new_dynamic_range_quantizer)\n    converter._experimental_disable_per_channel = disable_per_channel\n    if enable_float16_quant:\n      converter.target_spec.supported_types = [tf.float16]\n    quantized_tflite_model = converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    quantized_weight = None\n    quantized_weight_with_one_postfix = None\n    quantized_weight_without_one_postfix = None\n    for d in interpreter.get_tensor_details():\n      if d['name'] == conv_name + '1':\n        quantized_weight = d\n        quantized_weight_with_one_postfix = d\n        break\n    for d in interpreter.get_tensor_details():\n      if d['name'].startswith(conv_name):\n        if quantized_weight is None:\n          quantized_weight = d\n        quantized_weight_without_one_postfix = d\n        break\n\n    self.assertIsNotNone(quantized_weight)\n    quant_params = quantized_weight['quantization_parameters']\n\n    if enable_float16_quant:\n      expected_num_params = 0\n    else:\n      expected_num_params = 1 if disable_per_channel else num_filters\n    self.assertLen(quant_params['scales'], expected_num_params)\n    self.assertLen(quant_params['zero_points'], expected_num_params)\n\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    self.assertEqual(np.float32, input_details[0]['dtype'])\n    self.assertEqual(np.float32, output_details[0]['dtype'])\n    if enable_float16_quant:\n      self.assertTrue(\n          (quantized_weight_with_one_postfix is not None and\n           np.float16 == quantized_weight_with_one_postfix['dtype']) or\n          (quantized_weight_without_one_postfix is not None and\n           np.float16 == quantized_weight_without_one_postfix['dtype']))\n    else:\n      self.assertEqual(np.int8, quantized_weight['dtype'])\n\n  @parameterized.named_parameters([\n      ('{}BitWeightOnly={}LowBit={}'.format(num_bits, weight_only, low_bit),\n       num_bits, weight_only, low_bit) for num_bits, weight_only, low_bit\n      in itertools.product((2, 4, 6), (True, False), (True, False))])\n  @test_util.run_v2_only\n  def testQATLowBitKerasModel(self, num_bits, weight_only, low_bit):\n    bit_max = (1 << (num_bits - 1)) - 1\n    bit_min = -bit_max\n    tf_input_shape = (5, 5, 3)\n    tflite_input_shape = (1,) + tf_input_shape\n    model, input_name, output_name = (self._createV2QATLowBitKerasModel(\n        tf_input_shape, weight_only, num_bits, bit_min, bit_max))\n    input_data = np.linspace(\n        0, 6, np.prod(tflite_input_shape)).reshape(tflite_input_shape)\n    tf_result = model(input_data)\n\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    if low_bit:\n      converter._experimental_low_bit_qat = True\n    tflite_model = converter.convert()\n    result = self._evaluateTFLiteModelUsingSignatureDef(\n        tflite_model, 'serving_default',\n        {input_name: input_data.astype(np.float32)})[output_name]\n    self.assertAllClose(\n        [np.linalg.norm(result - tf_result.numpy().astype(np.float32))], [0.0])\n    interpreter = tf.lite.Interpreter(model_content=tflite_model)\n    interpreter.allocate_tensors()\n    num_8bit_activations = 0\n    num_8bit_weights = 0\n    kernel_name = ('model/conv_wrapper/Conv2D;model/conv_wrapper/'\n                   'FakeQuantWithMinMaxVarsPerChannel')\n\n    for detail in interpreter.get_tensor_details():\n      if (detail['dtype'] == np.int8 and detail['name'] and\n          detail['name'] == kernel_name):\n        num_8bit_weights += 1\n        weights = interpreter.get_tensor(detail['index'])\n        if low_bit:\n          self.assertFalse((bit_min > weights).any() or\n                           (weights > bit_max).any())\n        else:\n          self.assertTrue((bit_min > weights).any() or\n                          (weights > bit_max).any())\n        self.assertIn('scales', detail['quantization_parameters'])\n        if low_bit and detail['quantization_parameters']['scales']:\n          self.assertAllClose(\n              detail['quantization_parameters']['scales'], [1.0])\n      elif detail['dtype'] == np.int8 and detail['name']:\n        self.assertFalse(weight_only)\n        self.assertIn('scales', detail['quantization_parameters'])\n        if detail['quantization_parameters']['scales']:\n          self.assertAllClose(\n              detail['quantization_parameters']['scales'], [6/255])\n        num_8bit_activations += 1\n\n    self.assertEqual(num_8bit_weights, 0 if weight_only and not low_bit else 1)\n    # 3 activations with full integer: conv_input, conv_output, reshape_output\n    self.assertEqual(num_8bit_activations, 0 if weight_only else 3)\n\n\nclass FromJaxModelTest(lite_v2_test_util.ModelTest):\n\n  @test_util.run_v2_only\n  def testInvalidInputsModel(self):\n    if DISABLE_JAX_TEST:\n      return\n\n    def simple_model(input1, input2):\n      return jnp.sin(input1) + jnp.cos(input2)\n\n    input_tensor = jnp.zeros([10, 10])\n    # Invalid case: not specify serving_func\n    converter = lite.TFLiteConverterV2.experimental_from_jax(\n        None, [{\n            'input1': input_tensor\n        }])\n    with self.assertRaisesRegex(ValueError, 'No serving func is specified.'):\n      converter.convert()\n\n    # Invalid case: not specify input\n    converter = lite.TFLiteConverterV2.experimental_from_jax([simple_model],\n                                                             None)\n    with self.assertRaisesRegex(ValueError, 'Input tensors are not specified.'):\n      converter.convert()\n\n    converter = lite.TFLiteConverterV2.experimental_from_jax([simple_model], [])\n    with self.assertRaisesRegex(ValueError, 'Input tensors are not specified.'):\n      converter.convert()\n\n    # Invalid case: not wrap input_tensor in a list.\n    converter = lite.TFLiteConverterV2.experimental_from_jax([simple_model],\n                                                             input_tensor)\n    with self.assertRaisesRegex(\n        ValueError,\n        'The truth value of an array with more than one element is ambiguous.'):\n      converter.convert()\n\n    # Invalid case: only partial inputs are provided.\n    converter = lite.TFLiteConverterV2.experimental_from_jax(\n        [simple_model], [[('input1', input_tensor)]])\n    with self.assertRaisesRegex(\n        ValueError, 'Failed to convert the given Jax function to hlo.'):\n      converter.convert()\n\n    # Invalid case: serving functions length does not match input mapping.\n    converter = lite.TFLiteConverterV2.experimental_from_jax(\n        [simple_model, simple_model], [[\n            ('input1', input_tensor),\n            ('input2', input_tensor),\n        ]])\n    with self.assertRaisesRegex(\n        ValueError,\n        'Input tensor mapping len 1 does not match serving func len 2.'):\n      converter.convert()\n\n    # Invalid case: multiple serving function is provided.\n    converter = lite.TFLiteConverterV2.experimental_from_jax(\n        [simple_model, simple_model], [[\n            ('input1', input_tensor),\n            ('input2', input_tensor),\n        ], [\n            ('input1', input_tensor),\n            ('input2', input_tensor),\n        ]])\n    with self.assertRaisesRegex(\n        ValueError, 'Currently only support single serving function.'):\n      converter.convert()\n\n  @test_util.run_v2_only\n  def testSingleInputModel(self):\n    if DISABLE_JAX_TEST:\n      return\n\n    def single_input(input_tensor):\n      return jnp.sin(input_tensor)\n\n    # Convert model.\n    input_tensor = jnp.zeros([10, 10])\n    converter = lite.TFLiteConverterV2.experimental_from_jax(\n        [single_input], [[('input_tensor', input_tensor)]])\n    tflite_model = converter.convert()\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertEqual(metadata.environment.modelType, metadata_fb.ModelType.JAX)\n\n    # Check values from converted_model\n    input_data = np.random.random_sample((10, 10))\n    tf_input_data = tf.constant(input_data, dtype=np.float32)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [tf_input_data])[0]\n    expected_value = single_input(input_data)\n    self.assertAllClose(expected_value, actual_value, atol=1e-05)\n\n  @test_util.run_v2_only\n  def testMultipleInputsModel(self):\n    if DISABLE_JAX_TEST:\n      return\n\n    def multiple_inputs(input1, input2):\n      return input1 + input2\n\n    # Convert model.\n    input1 = jnp.zeros([10, 10])\n    input2 = jnp.zeros([10, 1])\n    converter = lite.TFLiteConverterV2.experimental_from_jax(\n        [multiple_inputs], [[('input1', input1), ('input2', input2)]])\n    tflite_model = converter.convert()\n\n    # Check values from converted_model\n    input1_data = np.random.random_sample((10, 10))\n    tf_input1_data = tf.constant(input1_data, dtype=np.float32)\n    input2_data = np.random.random_sample((10, 1))\n    tf_input2_data = tf.constant(input2_data, dtype=np.float32)\n    actual_value = self._evaluateTFLiteModel(\n        tflite_model, [tf_input1_data, tf_input2_data])[0]\n    expected_value = multiple_inputs(input1_data, input2_data)\n    self.assertAllClose(expected_value, actual_value, atol=1e-05)\n\n  @test_util.run_v2_only\n  def testInputSignaturesModel(self):\n    if DISABLE_JAX_TEST:\n      return\n\n    def multiple_inputs(input1, input2):\n      return input1 + input2\n\n    # Convert model.\n    input1 = jnp.zeros([10, 10])\n    input2 = jnp.zeros([10, 1])\n    converter = lite.TFLiteConverterV2.experimental_from_jax(\n        [multiple_inputs], [[('input1', input1), ('input2', input2)]])\n    tflite_model = converter.convert()\n\n    # Check values from converted_model\n    input1_data = np.random.random_sample((10, 10))\n    tf_input1_data = tf.constant(input1_data, dtype=np.float32)\n    input2_data = np.random.random_sample((10, 1))\n    tf_input2_data = tf.constant(input2_data, dtype=np.float32)\n    actual_value = self._evaluateTFLiteModel(\n        tflite_model, [tf_input1_data, tf_input2_data])[0]\n    expected_value = multiple_inputs(input1_data, input2_data)\n    self.assertAllClose(expected_value, actual_value, atol=1e-05)\n\n  @test_util.run_v2_only\n  def testModelWithParams(self):\n    if DISABLE_JAX_TEST:\n      return\n\n    def model(inputs, weights):\n      return jnp.matmul(weights, inputs)\n\n    weights = np.random.random_sample((10, 10))\n    serving_func = functools.partial(model, weights=weights)\n\n    # Convert model\n    input_tensor = jnp.zeros([10, 10])\n    converter = lite.TFLiteConverterV2.experimental_from_jax(\n        [serving_func], [[('inputs', input_tensor)]])\n    tflite_model = converter.convert()\n\n    # Check values from converted_model\n    input_data = np.random.random_sample((10, 10))\n    tf_input_data = tf.constant(input_data, dtype=np.float32)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [tf_input_data])[0]\n    expected_value = serving_func(input_data)\n    self.assertAllClose(expected_value, actual_value, atol=1e-05)\n\n  @test_util.run_v2_only\n  def testWhileLoop(self):\n    if DISABLE_JAX_TEST:\n      return\n\n    def condition(x):\n      return jnp.sum(x, keepdims=False) < 100\n\n    def body(x):\n      return jnp.add(x, 2.0)\n\n    def model(x):\n      result = jax.lax.while_loop(condition, body, x)\n      return result[0]\n\n    # Convert model.\n    input_tensor = jnp.zeros([3, 3])\n    converter = lite.TFLiteConverterV2.experimental_from_jax(\n        [model], [[('x', input_tensor)]])\n    tflite_model = converter.convert()\n\n    # Check values from converted_model\n    input_data = np.random.random_sample((3, 3))\n    tf_input_data = tf.constant(input_data, dtype=np.float32)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [tf_input_data])[0]\n    expected_value = model(input_data)\n    self.assertAllClose(expected_value, actual_value, atol=1e-05)\n\n\nclass ControlFlowTest(lite_v2_test_util.ModelTest):\n\n  @test_util.run_v2_only\n  def testCond(self):\n    input_data = {\n        'x': tf.constant([1., 2.], shape=[1, 2]),\n        'b': tf.constant(True)\n    }\n\n    weights = tf.Variable([[0.1, 0.2], [0.3, 0.4]], dtype=tf.float32)\n\n    def true_fn(x):\n      return tf.matmul(x, weights)\n\n    def false_fn(x):\n      return tf.add(x, weights)\n\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[1, 2], dtype=tf.float32),\n        tf.TensorSpec(shape=(), dtype=tf.bool)\n    ])\n    def model(x, b):\n      return tf.cond(\n          b, true_fn=lambda: true_fn(x), false_fn=lambda: false_fn(x))\n\n    concrete_func = model.get_concrete_function()\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = concrete_func(**input_data)\n    actual_value = self._evaluateTFLiteModel(\n        tflite_model, [input_data['x'], input_data['b']])[0]\n    self.assertAllClose(expected_value, actual_value)\n\n  @test_util.run_v2_only\n  def testCondWithFullIntegerQuantization(self):\n    weights = tf.Variable([[0.1, 0.2], [0.3, 0.4]], dtype=tf.float32)\n\n    def true_fn(x):\n      return tf.matmul(x, weights)\n\n    def false_fn(x):\n      return tf.add(x, weights)\n\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[1, 2], dtype=tf.float32),\n        tf.TensorSpec(shape=(), dtype=tf.bool)\n    ])\n    def model(x, b):\n      return tf.cond(\n          b, true_fn=lambda: true_fn(x), false_fn=lambda: false_fn(x))\n\n    def calibration_gen():\n      for _ in range(5):\n        yield [\n            np.random.uniform(-1, 1, size=(1, 2)).astype(np.float32),\n            tf.constant(True)\n        ]\n      for _ in range(5):\n        yield [\n            np.random.uniform(-1, 1, size=(1, 2)).astype(np.float32),\n            tf.constant(False)\n        ]\n\n    concrete_func = model.get_concrete_function()\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    converter.representative_dataset = calibration_gen\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n  @test_util.run_v2_only\n  def testConverterErrorOnControlFlowV1Ops(self):\n    filename = resource_loader.get_path_to_datafile(\n        'testdata/control_flow_v1_saved_model')\n    converter = lite.TFLiteConverterV2.from_saved_model(filename)\n    with self.assertRaises(convert.ConverterError) as error:\n      converter.convert()\n    self.assertIn(\n        'Failed to functionalize Control Flow V1 ops. Consider using Control '\n        'Flow V2 ops instead. See https://www.tensorflow.org/api_docs/python/'\n        'tf/compat/v1/enable_control_flow_v2.', str(error.exception))\n\n  @test_util.run_v2_only\n  def testStaticRnn(self):\n    input_data = tf.constant(\n        np.array(np.random.random_sample((3, 10)), dtype=np.float32))\n\n    cell = tf.keras.layers.LSTMCell(10)\n\n    @tf.function(\n        input_signature=[tf.TensorSpec(shape=[3, 10], dtype=tf.float32)])\n    def model(x):\n      seq = tf.split(x, 3, 0)\n      return rnn.static_rnn(cell, seq, dtype=tf.float32, sequence_length=[1])\n\n    concrete_func = model.get_concrete_function()\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = concrete_func(input_data)[0]\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    for expected, actual in zip(expected_value, actual_value):\n      self.assertAllClose(expected, actual)\n\n  @test_util.run_v2_only\n  def testWhileLoop(self):\n    input_data = tf.constant([1., 2., 3., 4.], shape=[2, 2])\n\n    weights = tf.Variable([[0.1, 0.2], [0.3, 0.4]], dtype=tf.float32)\n\n    def condition(x):\n      return tf.reduce_sum(x) < 100\n\n    def body(x):\n      return tf.add(x, weights)\n\n    @tf.function(\n        input_signature=[tf.TensorSpec(shape=[2, 2], dtype=tf.float32)])\n    def model(x):\n      return tf.while_loop(condition, body, [x])\n\n    concrete_func = model.get_concrete_function()\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = concrete_func(input_data)[0]\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])[0]\n    self.assertAllClose(expected_value, actual_value)\n\n  @test_util.run_v2_only\n  def testDynamicRnn(self):\n    input_data = tf.constant(\n        np.array(np.random.random_sample((3, 10, 10)), dtype=np.float32))\n\n    cell = tf.keras.layers.LSTMCell(10)\n\n    @tf.function(\n        input_signature=[tf.TensorSpec(shape=[3, 10, 10], dtype=tf.float32)])\n    def model(x):\n      rnn_layer = tf.keras.layers.RNN([cell], return_sequences=True)\n      return rnn_layer(x)\n\n    concrete_func = model.get_concrete_function()\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = concrete_func(input_data)\n    lite_outputs = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertLen(lite_outputs, 1)\n    actual_value = lite_outputs[0]\n    for expected, actual in zip(expected_value, actual_value):\n      self.assertAllClose(expected, actual)\n\n  @parameterized.named_parameters(\n      ('LSTMBatchSizeOne', tf.keras.layers.LSTM, True),\n      ('LSTM', tf.keras.layers.LSTM, False),\n      ('SimpleRNNBatchSizeOne', tf.keras.layers.SimpleRNN, True),\n      ('SimpleRNN', tf.keras.layers.SimpleRNN, False),\n      ('GRUBatchSizeOne', tf.keras.layers.GRU, True),\n      ('GRU', tf.keras.layers.GRU, False))\n  @test_util.run_v2_only\n  def testKerasRNN(self, rnn_layer, default_to_single_batch):\n    input_data = tf.constant(\n        np.array(np.random.random_sample((1, 10, 10)), dtype=np.float32))\n    rnn_obj = rnn_layer(units=10, input_shape=(10, 10))\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Input(shape=(10, 10), name='input'),\n        rnn_obj,\n    ])\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_keras_model(model)\n    converter._experimental_default_to_single_batch_in_tensor_list_ops = default_to_single_batch\n    if not default_to_single_batch:\n      converter.target_spec.supported_ops = [\n          tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n      ]\n    tflite_model = converter.convert()\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])[0]\n\n    # Check values from converted model.\n    expected_value = model.predict(input_data)\n    self.assertAllClose(expected_value, actual_value, atol=1e-05)\n\n  @parameterized.named_parameters(('LSTM', tf.keras.layers.LSTM),\n                                  ('SimpleRNN', tf.keras.layers.SimpleRNN),\n                                  ('GRU', tf.keras.layers.GRU))\n  @test_util.run_v2_only\n  def testKerasRNNMultiBatches(self, rnn_layer):\n    input_data = tf.constant(\n        np.array(np.random.random_sample((4, 10, 10)), dtype=np.float32))\n    # Specify a fixed batch size(4) for the test model.\n    x = tf.keras.layers.Input(batch_shape=(4, 10, 10))\n    y = rnn_layer(units=10, input_shape=(10, 10))(x)\n    model = tf.keras.Model(inputs=[x], outputs=[y])\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_keras_model(model)\n    tflite_model = converter.convert()\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])[0]\n\n    # Check values from converted model.\n    expected_value = model.predict(input_data)\n    self.assertAllClose(expected_value, actual_value, atol=1e-05)\n\n  @parameterized.named_parameters(('ForceToUseBatchSizeOne', True),\n                                  ('DontForceToUseBatchSizeOne', False))\n  @test_util.run_v2_only\n  def testKerasBidirectionalRNNReturnSequence(self, default_to_single_batch):\n    input_data = tf.constant(\n        np.array(np.random.random_sample((1, 10, 10)), dtype=np.float32))\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Input(shape=(10, 10), name='input'))\n    model.add(\n        tf.keras.layers.Bidirectional(\n            tf.keras.layers.LSTM(units=10, return_sequences=True),\n            input_shape=(10, 10)))\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(5))\n    model.add(tf.keras.layers.Activation('softmax'))\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_keras_model(model)\n    converter._experimental_default_to_single_batch_in_tensor_list_ops = default_to_single_batch\n    if not default_to_single_batch:\n      converter.target_spec.supported_ops = [\n          tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n      ]\n    tflite_model = converter.convert()\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])[0]\n\n    # Check values from converted model.\n    expected_value = model.predict(input_data)\n    self.assertAllClose(expected_value, actual_value, atol=1e-05)\n\n  @parameterized.named_parameters(('ForceToUseBatchSizeOne', True),\n                                  ('DontForceToUseBatchSizeOne', False))\n  @test_util.run_v2_only\n  def testKerasBidirectionalRNN(self, default_to_single_batch):\n    input_data = tf.constant(\n        np.array(np.random.random_sample((1, 10, 10)), dtype=np.float32))\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Input(shape=(10, 10), name='input'))\n    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=10)))\n    model.add(tf.keras.layers.Dense(5))\n    model.add(tf.keras.layers.Activation('softmax'))\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_keras_model(model)\n    converter._experimental_default_to_single_batch_in_tensor_list_ops = default_to_single_batch\n    if not default_to_single_batch:\n      converter.target_spec.supported_ops = [\n          tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n      ]\n    tflite_model = converter.convert()\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])[0]\n\n    # Check values from converted model.\n    expected_value = model.predict(input_data)\n    self.assertAllClose(expected_value, actual_value, atol=1e-05)\n\n\nclass GrapplerTest(lite_v2_test_util.ModelTest):\n\n  @test_util.run_v2_only\n  def testConstantFolding(self):\n    # Constant folding handles the tf.broadcast_to operation which was not\n    # supported by the TFLite at the time this test was added.\n    input_data = tf.constant([1., 2., 3., 4., 5., 6., 7., 8., 9.], shape=[3, 3])\n\n    @tf.function\n    def func(x):\n      y_const = tf.constant([1., 2., 3.])\n      y_broadcast = tf.broadcast_to(y_const, [3, 3])\n      return tf.matmul(x, y_broadcast)\n\n    root = autotrackable.AutoTrackable()\n    root.f = func\n    concrete_func = root.f.get_concrete_function(input_data)\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               root)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = root.f(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])[0]\n    self.assertAllClose(expected_value, actual_value)\n\n    # Enable hybrid quantization, same result\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    tflite_model = converter.convert()\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])[0]\n    self.assertAllClose(expected_value, actual_value)\n\n\nclass UnknownShapes(lite_v2_test_util.ModelTest):\n\n  @test_util.run_v2_only\n  def testMatMul(self):\n    input_data = tf.constant(\n        np.array(np.random.random_sample((10, 4)), dtype=np.float32))\n\n    @tf.function(\n        input_signature=[tf.TensorSpec(shape=[None, 4], dtype=tf.float32)])\n    def model(in_tensor):\n      shape = tf.shape(in_tensor)\n      fill = tf.transpose(tf.fill(shape, 1.))\n      return tf.matmul(fill, in_tensor)\n\n    concrete_func = model.get_concrete_function()\n\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = concrete_func(input_data)\n    actual_value = self._evaluateTFLiteModel(\n        tflite_model, [input_data], input_shapes=[([-1, 4], [10, 4])])[0]\n    self.assertAllClose(expected_value, actual_value, atol=1e-06)\n\n  def _getIntegerQuantizeModelWithUnknownShapes(self):\n    np.random.seed(0)\n\n    @tf.function(\n        input_signature=[tf.TensorSpec(shape=[None, 33], dtype=tf.float32)])\n    def model(input_tensor):\n      \"\"\"Define a model with tf.MatMul and unknown shapes.\"\"\"\n      # We need the tensor to have more than 1024 elements for quantize_weights\n      # to kick in. Thus, the [33, 33] shape.\n      const_tensor = tf.constant(\n          np.random.uniform(low=-10., high=10., size=[33, 33]),\n          shape=[33, 33],\n          dtype=tf.float32,\n          name='inputB')\n\n      shape = tf.shape(input_tensor)\n      fill = tf.transpose(tf.fill(shape, 1.))\n      mult = tf.matmul(fill, input_tensor)\n      return tf.matmul(mult, const_tensor)\n\n    root = autotrackable.AutoTrackable()\n    root.f = model\n    concrete_func = root.f.get_concrete_function()\n\n    def calibration_gen():\n      for batch in range(5, 20, 5):\n        for _ in range(5):\n          yield [np.random.uniform(-1, 1, size=(batch, 33)).astype(np.float32)]\n\n    return root, concrete_func, calibration_gen\n\n  @test_util.run_v2_only\n  def testMatMulQuantize(self):\n    root, concrete_func, _ = self._getIntegerQuantizeModelWithUnknownShapes()\n    float_converter = lite.TFLiteConverterV2.from_concrete_functions(\n        [concrete_func], root)\n    float_tflite_model = float_converter.convert()\n\n    quantized_converter = lite.TFLiteConverterV2.from_concrete_functions(\n        [concrete_func], root)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_tflite_model = quantized_converter.convert()\n\n    # The default input and output types should be float.\n    quantized_interpreter = Interpreter(model_content=quantized_tflite_model)\n    quantized_interpreter.allocate_tensors()\n    input_details = quantized_interpreter.get_input_details()\n    self.assertLen(input_details, 1)\n    self.assertEqual(np.float32, input_details[0]['dtype'])\n    self.assertAllEqual([-1, 33], input_details[0]['shape_signature'])\n\n    # Ensure that the quantized weights tflite model is smaller.\n    self.assertLess(len(quantized_tflite_model), len(float_tflite_model))\n\n  @test_util.run_v2_only\n  def testMatMulCalibrateAndQuantize(self):\n    root, concrete_func, calibration_gen = (\n        self._getIntegerQuantizeModelWithUnknownShapes())\n    float_converter = lite.TFLiteConverterV2.from_concrete_functions(\n        [concrete_func], root)\n    float_tflite_model = float_converter.convert()\n\n    quantized_converter = lite.TFLiteConverterV2.from_concrete_functions(\n        [concrete_func], root)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_converter.representative_dataset = calibration_gen\n    quantized_tflite_model = quantized_converter.convert()\n\n    # The default input and output types should be float.\n    quantized_interpreter = Interpreter(model_content=quantized_tflite_model)\n    quantized_interpreter.allocate_tensors()\n    input_details = quantized_interpreter.get_input_details()\n    self.assertLen(input_details, 1)\n    self.assertEqual(np.float32, input_details[0]['dtype'])\n    self.assertAllEqual([-1, 33], input_details[0]['shape_signature'])\n\n    # Ensure that the quantized weights tflite model is smaller.\n    self.assertLess(len(quantized_tflite_model), len(float_tflite_model))\n\n  def testBatchMatMul(self):\n    input_data_1 = tf.constant(\n        np.array(np.random.random_sample((1, 256, 256)), dtype=np.float32))\n    input_data_2 = tf.constant(\n        np.array(np.random.random_sample((1, 256, 256)), dtype=np.float32))\n\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[None, 256, 256], dtype=tf.float32),\n        tf.TensorSpec(shape=[None, 256, 256], dtype=tf.float32)\n    ])\n    def model(in_tensor_1, in_tensor_2):\n      return tf.matmul(in_tensor_1, in_tensor_2)\n\n    concrete_func = model.get_concrete_function()\n\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = concrete_func(input_data_1, input_data_2)\n    actual_value = self._evaluateTFLiteModel(\n        tflite_model, [input_data_1, input_data_2],\n        input_shapes=[([-1, 256, 256], [1, 256, 256])])[0]\n    self.assertAllClose(expected_value, actual_value, atol=4)\n\n  def testBatchMatMulInputInt8Int8OutputInt32(self):\n    input_data_1 = tf.constant(\n        np.array(\n            np.random.random_integers(-128, high=127, size=(1, 20, 30)),\n            dtype=np.int8))\n    input_data_2 = tf.constant(\n        np.array(\n            np.random.random_integers(-128, high=127, size=(1, 30, 10)),\n            dtype=np.int8))\n\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[None, 20, 30], dtype=tf.int8),\n        tf.TensorSpec(shape=[None, 30, 10], dtype=tf.int8)\n    ])\n    def model(in_tensor_1, in_tensor_2):\n      return tf.matmul(in_tensor_1, in_tensor_2, output_type=tf.int32)\n\n    concrete_func = model.get_concrete_function()\n\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = concrete_func(input_data_1, input_data_2)\n    actual_value = self._evaluateTFLiteModel(\n        tflite_model, [input_data_1, input_data_2],\n        input_shapes=[([-1, 20, 30], [1, 20, 30]), ([-1, 30, 10], [1, 30,\n                                                                   10])])[0]\n    self.assertAllEqual(expected_value, actual_value)\n\n  def testBatchMatMulHybrid(self):\n    # Test model that does batch matmul of:\n    # lhs input (1, 256, 128), rhs const (1, 128, 256).\n    # For dynamic range quantization situation, this will result in hybrid batch\n    # matmul, where lhs type is float32 and rhs type is int8.\n\n    # Intentionally set lhs, rhs sizes to satisfy following conditions:\n    # 1. rhs const num_elements >= 1024, since dynamic range quantization\n    # requires const tensor num_elements to be larger than\n    # min_elements_for_weights (which defaults to 1024).\n    # (https://github.com/tensorflow/tensorflow/blob/25e649ac3688655547da998eba2715cf70b3e5c9/tensorflow/compiler/mlir/lite/transforms/prepare_quantize_dynamic_range.cc#L262)\n    # 2. batch_size (256) > accum_dim_size (128) and\n    # num_units (256) > accum_dim_size (128), to test if the sizes are set\n    # correctly according to dimensions. See HybridAsymmetricBatchMatMulOpTest\n    # tests in\n    # https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/batch_matmul_test.cc.\n    input_data = tf.constant(\n        np.array(np.random.random_sample((1, 256, 128)), dtype=np.float32))\n\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[None, 256, 128], dtype=tf.float32)\n    ])\n    def model(in_tensor):\n      rhs = tf.constant(\n          np.array(np.random.random_sample((1, 128, 256)), dtype=np.float32))\n      return tf.matmul(in_tensor, rhs)\n\n    concrete_func = model.get_concrete_function()\n\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = concrete_func(input_data)\n    actual_value = self._evaluateTFLiteModel(\n        tflite_model, [input_data],\n        input_shapes=[([-1, 256, 128], [1, 256, 128])])[0]\n    self.assertAllClose(expected_value, actual_value, atol=4)\n\n  def testSizeInvalid(self):\n\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[1, None, 16, 3], dtype=tf.float32)\n    ])\n    def model(in_tensor):\n      return in_tensor + in_tensor\n\n    concrete_func = model.get_concrete_function()\n\n    # Test invalid shape. None after 1st dimension. Run with TOCO in order to\n    # invoke shape checking code.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    converter.experimental_new_converter = False\n    with self.assertRaises(ValueError) as error:\n      converter.convert()\n    self.assertEqual(\n        'None is only supported in the 1st dimension. Tensor '\n        '\\'in_tensor\\' has invalid shape \\'[1, None, 16, 3]\\'.',\n        str(error.exception))\n\n\nclass ResourceAndVariantTypes(lite_v2_test_util.ModelTest):\n\n  @test_util.run_v2_only\n  def testVariants(self):\n\n    @tf.function(input_signature=[tf.TensorSpec(shape=[1], dtype=tf.float32)])\n    def model(v):\n      m = map_ops.empty_tensor_map()\n      k = tf.constant(1.0)\n      p = tf.add(k, v)\n      with ops.control_dependencies([m]):\n        m2 = map_ops.tensor_map_insert(m, p, v)\n        with ops.control_dependencies([m2]):\n          return map_ops.tensor_map_size(m2)\n\n    concrete_func = model.get_concrete_function()\n\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    interpreter.allocate_tensors()\n\n    input_data = np.array([1.0], dtype=np.float32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(1, actual_value)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(1, actual_value)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(1, actual_value)\n\n  @test_util.run_v2_only\n  def testVariantsWithCond(self):\n\n    def create_v1_saved_model():\n      saved_model_dir = os.path.join(self.get_temp_dir(), 'variants_with_cond')\n      with tf.Graph().as_default():\n        with tf.compat.v1.Session() as sess:\n          m = map_ops.empty_tensor_map()\n\n          def body(i, m):\n            m = map_ops.tensor_map_insert(m, i, i)\n            return i + 1, m\n\n          in_tensor = tf.compat.v1.placeholder(\n              shape=[1], dtype=tf.int32, name='input')\n          _, result_m = tf.cond(in_tensor < 10, lambda: body(in_tensor, m),\n                                lambda: body(in_tensor + 1, m))\n          out_tensor = in_tensor + map_ops.tensor_map_size(result_m)\n\n          inputs = {'x': in_tensor}\n          outputs = {'z': out_tensor}\n          saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n      return saved_model_dir\n\n    saved_model_dir = create_v1_saved_model()\n\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    interpreter.allocate_tensors()\n\n    input_data = np.array([0], dtype=np.int32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    interpreter.invoke()\n    expected_value = np.array([1], dtype=np.int32)\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(expected_value, actual_value)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(expected_value, actual_value)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(expected_value, actual_value)\n\n  @test_util.run_v2_only\n  def testVariantsWithWhile(self):\n\n    def create_v1_saved_model():\n      saved_model_dir = os.path.join(self.get_temp_dir(), 'variants_with_while')\n      with tf.Graph().as_default():\n        with tf.compat.v1.Session() as sess:\n          m = map_ops.empty_tensor_map()\n\n          def cond(i, m):\n            del m\n            return i < 10\n\n          def body(i, m):\n            m = map_ops.tensor_map_insert(m, i, i)\n            return i + 1, m\n\n          _, result_m = tf.while_loop(cond, body, [0, m])\n          in_tensor = tf.compat.v1.placeholder(\n              shape=[1], dtype=tf.int32, name='input')\n          out_tensor = in_tensor + map_ops.tensor_map_size(result_m)\n\n          inputs = {'x': in_tensor}\n          outputs = {'z': out_tensor}\n          saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n      return saved_model_dir\n\n    saved_model_dir = create_v1_saved_model()\n\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    interpreter.allocate_tensors()\n\n    input_data = np.array([0], dtype=np.int32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(10, actual_value)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(10, actual_value)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(10, actual_value)\n\n  @test_util.run_v2_only\n  def testResources(self):\n\n    def create_v1_saved_model():\n      saved_model_dir = os.path.join(self.get_temp_dir(), 'simple_resources')\n      with tf.Graph().as_default():\n        with tf.compat.v1.Session() as sess:\n          in_tensor = tf.compat.v1.placeholder(\n              shape=[1], dtype=tf.float32, name='input')\n\n          stack = tf.raw_ops.StackV2(max_size=10, elem_type=tf.float32)\n          w = tf.raw_ops.StackPushV2(handle=stack, elem=in_tensor)\n          with ops.control_dependencies([w]):\n            a = in_tensor + in_tensor\n            with ops.control_dependencies([a]):\n              out_tensor = a + tf.raw_ops.StackPopV2(\n                  handle=stack, elem_type=tf.float32)\n\n          inputs = {'x': in_tensor}\n          outputs = {'z': out_tensor}\n          saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n      return saved_model_dir\n\n    saved_model_dir = create_v1_saved_model()\n\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    interpreter.allocate_tensors()\n\n    input_data = np.array([1.0], dtype=np.float32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(3.0, actual_value)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(3.0, actual_value)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(3.0, actual_value)\n\n  @test_util.run_v2_only\n  def testResourcesWithCond(self):\n\n    def create_v1_saved_model():\n      saved_model_dir = os.path.join(self.get_temp_dir(), 'resources_with_cond')\n      with tf.Graph().as_default():\n        with tf.compat.v1.Session() as sess:\n          in_tensor = tf.compat.v1.placeholder(\n              shape=[1], dtype=tf.float32, name='input')\n\n          def body(i, arr):\n            n = tf.raw_ops.StackPushV2(\n                handle=arr, elem=tf.cast(i, dtype=tf.float32))\n            return n, arr\n\n          arr = tf.raw_ops.StackV2(max_size=10, elem_type=tf.float32)\n          n, result_arr = tf.cond(in_tensor < 10, lambda: body(0, arr),\n                                  lambda: body(1, arr))\n\n          with ops.control_dependencies([result_arr, n]):\n            out_tensor = tf.raw_ops.StackPopV2(\n                handle=result_arr, elem_type=tf.float32)\n\n          inputs = {'x': in_tensor}\n          outputs = {'a': out_tensor}\n          saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n      return saved_model_dir\n\n    saved_model_dir = create_v1_saved_model()\n\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    interpreter.allocate_tensors()\n\n    input_data = np.array([1.0], dtype=np.float32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(0.0, actual_value)\n\n  @test_util.run_v2_only\n  def testResourcesWithWhile(self):\n\n    def create_v1_saved_model():\n      saved_model_dir = os.path.join(self.get_temp_dir(),\n                                     'resources_with_while')\n      with tf.Graph().as_default():\n        with tf.compat.v1.Session() as sess:\n          in_tensor = tf.compat.v1.placeholder(\n              shape=[1], dtype=tf.float32, name='input')\n\n          def cond(i, arr, m):\n            del arr\n            del m\n            return i < 10\n\n          def body(i, arr, m):\n            del m\n            n = tf.raw_ops.StackPushV2(\n                handle=arr, elem=tf.cast(i, dtype=tf.float32))\n            return i + 1, arr, n\n\n          arr = tf.raw_ops.StackV2(max_size=10, elem_type=tf.float32)\n          _, result_arr, n = tf.while_loop(cond, body, [0, arr, 0.0])\n\n          with ops.control_dependencies([result_arr, n]):\n            out_tensor = tf.raw_ops.StackPopV2(\n                handle=result_arr, elem_type=tf.float32)\n\n          inputs = {'x': in_tensor}\n          outputs = {'a': out_tensor}\n          saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n      return saved_model_dir\n\n    saved_model_dir = create_v1_saved_model()\n\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    interpreter.allocate_tensors()\n\n    input_data = np.array([1.0], dtype=np.float32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(9.0, actual_value)\n\n  @parameterized.named_parameters(('EnableLoweringTensorListOps', True),\n                                  ('DisableLoweringTensorListOps', False))\n  @test_util.run_v2_only\n  def testTensorListWithStaticSize(self, lower_tensor_list_ops):\n\n    def create_v1_saved_model():\n      saved_model_dir = os.path.join(self.get_temp_dir(),\n                                     'simple_mutable_variable')\n      with tf.Graph().as_default():\n        with tf.compat.v1.Session() as sess:\n          in_tensor = tf.compat.v1.placeholder(\n              shape=[1], dtype=tf.float32, name='input')\n\n          ta = tf.TensorArray(\n              tf.float32, size=3, dynamic_size=False, clear_after_read=False)\n          ta = ta.write(0, 10.0)\n          ta = ta.write(1, 20.0)\n          ta = ta.write(2, 30.0)\n\n          out_tensor = ta.read(0) + ta.read(2)\n\n          inputs = {'x': in_tensor}\n          outputs = {'z': out_tensor}\n          saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n      return saved_model_dir\n\n    saved_model_dir = create_v1_saved_model()\n\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    if not lower_tensor_list_ops:\n      converter.target_spec.supported_ops = [\n          tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n      ]\n    converter._experimental_lower_tensor_list_ops = lower_tensor_list_ops\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    interpreter.allocate_tensors()\n\n    input_data = np.array([1.0], dtype=np.float32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(40.0, actual_value)\n\n  @parameterized.named_parameters(('EnableLoweringTensorListOps', True),\n                                  ('DisableLoweringTensorListOps', False))\n  @test_util.run_v2_only\n  def testTensorListWithDynamicSize(self, lower_tensor_list_ops):\n\n    def create_v1_saved_model():\n      saved_model_dir = os.path.join(self.get_temp_dir(),\n                                     'simple_mutable_variable')\n      with tf.Graph().as_default():\n        with tf.compat.v1.Session() as sess:\n          in_tensor = tf.compat.v1.placeholder(\n              shape=[1], dtype=tf.float32, name='input')\n\n          ta = tf.TensorArray(\n              tf.float32, size=0, dynamic_size=True, clear_after_read=False)\n          ta = ta.write(0, 10.0)\n          ta = ta.write(1, 20.0)\n          ta = ta.write(2, 30.0)\n\n          out_tensor = ta.read(0) + ta.read(2)\n\n          inputs = {'x': in_tensor}\n          outputs = {'z': out_tensor}\n          saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n      return saved_model_dir\n\n    saved_model_dir = create_v1_saved_model()\n\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    if lower_tensor_list_ops:\n      with self.assertRaises(convert.ConverterError) as error:\n        converter.convert()\n      self.assertIn(\n          'Lowering tensor list ops is failed. Please consider using Select '\n          'TF ops and disabling `_experimental_lower_tensor_list_ops` flag in '\n          'the TFLite converter object.', str(error.exception))\n\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    interpreter.allocate_tensors()\n\n    input_data = np.array([1.0], dtype=np.float32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(40.0, actual_value)\n\n\nclass CalibrateAndQuantizeWithCustomOpTest(lite_v2_test_util.ModelTest):\n\n  def _createGraphWithCustomOp(self):\n    # Create a graph that has one double op.\n    np.random.seed(0)\n\n    saved_model_dir = os.path.join(self.get_temp_dir(), 'double_model')\n    with ops.Graph().as_default():\n      with tf.compat.v1.Session() as sess:\n        in_tensor = tf.compat.v1.placeholder(\n            shape=[1, 4], dtype=dtypes.float32, name='input')\n        out_tensor = double_op.double(in_tensor)\n        inputs = {'x': in_tensor}\n        outputs = {'z': out_tensor}\n        saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n\n    def calibration_gen():\n      for _ in range(100):\n        yield [np.random.uniform(-1, 1, size=(1, 4)).astype(np.float32)]\n\n    return (saved_model_dir, calibration_gen)\n\n  def testCustomOpRegistererByName(self):\n    \"\"\"Test a calibration with custom op registered by name.\"\"\"\n    saved_model_dir, calibration_gen = self._createGraphWithCustomOp()\n\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    converter.representative_dataset = calibration_gen\n    converter.allow_custom_ops = True\n    converter.target_spec._experimental_custom_op_registerers = [\n        'TF_TestRegisterer'\n    ]\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n    self.assertGreater(test_registerer.get_num_test_registerer_calls(), 0)\n    self.assertIn('Double', tflite_test_util.get_ops_list(tflite_model))\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertEqual(metadata.options.allowCustomOps, True)\n\n    # Check the model works with custom ops.\n    interpreter = InterpreterWithCustomOps(\n        model_content=tflite_model, custom_op_registerers=['TF_TestRegisterer'])\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    test_input = np.array([[0.0, 0.1, 0.2, 0.3]], dtype=np.float32)\n    interpreter.set_tensor(input_details[0]['index'], test_input)\n    interpreter.invoke()\n\n    output_details = interpreter.get_output_details()\n    expected_output = np.array([[0.0, 0.2, 0.4, 0.6]], dtype=np.float32)\n    output_data = interpreter.get_tensor(output_details[0]['index'])\n    self.assertArrayNear(expected_output[0], output_data[0], err=1e-2)\n\n  def testCustomOpRegistererByFunc(self):\n    \"\"\"Test a calibration with custom op registered by function.\"\"\"\n    saved_model_dir, calibration_gen = self._createGraphWithCustomOp()\n\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    converter.representative_dataset = calibration_gen\n    converter.allow_custom_ops = True\n    converter.target_spec._experimental_custom_op_registerers = [\n        test_registerer.TF_TestRegisterer\n    ]\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n    self.assertGreater(test_registerer.get_num_test_registerer_calls(), 0)\n    self.assertIn('Double', tflite_test_util.get_ops_list(tflite_model))\n\n    # Check the model works with custom ops.\n    interpreter = InterpreterWithCustomOps(\n        model_content=tflite_model,\n        custom_op_registerers=[test_registerer.TF_TestRegisterer])\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    test_input = np.array([[0.0, 0.1, 0.2, 0.3]], dtype=np.float32)\n    interpreter.set_tensor(input_details[0]['index'], test_input)\n    interpreter.invoke()\n\n    output_details = interpreter.get_output_details()\n    expected_output = np.array([[0.0, 0.2, 0.4, 0.6]], dtype=np.float32)\n    output_data = interpreter.get_tensor(output_details[0]['index'])\n    self.assertArrayNear(expected_output[0], output_data[0], err=1e-2)\n\n  def testCustomOpRegistererFailure(self):\n    \"\"\"Test a calibration with wrong custom op registerer.\"\"\"\n    saved_model_dir, calibration_gen = self._createGraphWithCustomOp()\n\n    bogus_name = 'CompletelyBogusRegistererName'\n\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    converter.representative_dataset = calibration_gen\n    converter.allow_custom_ops = True\n    converter.target_spec._experimental_custom_op_registerers = [bogus_name]\n\n    with self.assertRaisesRegex(\n        ValueError, 'Looking up symbol \\'' + bogus_name + '\\' failed'):\n      converter.convert()\n\n\nclass IntermediatesTest(lite_v2_test_util.ModelTest):\n\n  def _run(self, experimental_preserve_all_tensors):\n\n    @tf.function\n    def f(x):\n      y = tf.add(x, x, name='y')\n      z = tf.add(y, y, name='z')\n      w = tf.add(z, z, name='w')\n      return w\n\n    # NOTE this is exactly representable as a float as are the intermeidates of\n    # f. So direct comparison is ok below.\n\n    input_data = np.array(2.0, np.float32)\n    concrete_func = f.get_concrete_function(input_data)\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               f)\n    tflite_model = converter.convert()\n    interpreter = Interpreter(\n        model_content=tflite_model,\n        experimental_preserve_all_tensors=experimental_preserve_all_tensors)\n    interpreter.allocate_tensors()\n    interpreter.set_tensor(interpreter.get_input_details()[0]['index'],\n                           input_data)\n    interpreter.invoke()\n    out = interpreter.get_tensor(interpreter.get_output_details()[0]['index'])\n    tensors = {}\n    for t in interpreter.get_tensor_details():\n      # With Tensorflow Lite default delegate applied to the model graph, the\n      # access to original tensors of a delegated op could cause a ValueError\n      # (i.e. 'Tensor data is null. Run allocate_tensors() first') to be thrown\n      # out because the tensor memory isn't allocated at all.\n      val = None\n      try:\n        val = interpreter.get_tensor(t['index'])\n      except ValueError:\n        pass\n      tensors.update({t['name']: val})\n    return (tensors, out)\n\n  def testPreserve(self):\n    tensors, result = self._run(experimental_preserve_all_tensors=True)\n    # All intermediates should be true and result be true.\n    self.assertAllClose(tensors['x'], 2.0)\n    self.assertAllClose(tensors['y'], 4.0)\n    self.assertAllClose(tensors['z'], 8.0)\n    self.assertAllClose(result, 16.0)\n\n  def testNoPreserve(self):\n    tensors, result = self._run(experimental_preserve_all_tensors=False)\n    # One of them should be wrong if preserve is not true, but result should be\n    # ok. Input should still be ok for repeated invocation.\n    self.assertAllClose(tensors['x'], 2.0)\n    self.assertTrue(tensors['y'] != 4.0 or tensors['z'] != 8.0)\n    self.assertAllClose(result, 16.0)\n\n\nclass DatasetOpsTest(lite_v2_test_util.ModelTest):\n\n  @test_util.run_v2_only\n  def testReduceDataset(self):\n\n    @tf.function\n    def model():\n      dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4])\n      output = dataset.reduce(np.int32(0), lambda x, y: x + y)\n      return output\n\n    concrete_func = model.get_concrete_function()\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    output_details = interpreter.get_output_details()\n\n    interpreter.allocate_tensors()\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(10, actual_value)\n\n\nclass SparsityTest(lite_v2_test_util.ModelTest):\n\n  def _getSparsificableModel(self, matrix_b_values):\n    np.random.seed(0)\n    root = autotrackable.AutoTrackable()\n\n    @tf.function(\n        input_signature=[tf.TensorSpec(shape=[16, 4], dtype=tf.float32)])\n    def func(inp):\n      matrix_b = tf.constant(matrix_b_values, dtype=tf.float32)\n      matrix_b = tf.reshape(matrix_b, [4, 8])\n      matmul = tf.matmul(inp, matrix_b, transpose_a=False, transpose_b=False)\n      output = tf.nn.relu(matmul, name='output')\n      return output\n\n    root.f = func\n    to_save = root.f.get_concrete_function()\n    return (root, to_save)\n\n  def testRandomSparsity(self):\n    matrix_b_values = [\n        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 1\n    ]\n    root, func = self._getSparsificableModel(matrix_b_values)\n    float_converter = lite.TFLiteConverterV2.from_concrete_functions([func],\n                                                                     root)\n    float_converter.optimizations = [lite.Optimize.EXPERIMENTAL_SPARSITY]\n    float_tflite_model = float_converter.convert()\n    self.assertIsNotNone(float_tflite_model)\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(float_tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertAllEqual([metadata_fb.ModelOptimizationMode.RANDOM_SPARSITY],\n                        metadata.options.modelOptimizationModes)\n\n  def testBlockSparsity(self):\n    matrix_b_values = [\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 1, 0\n    ]\n    root, func = self._getSparsificableModel(matrix_b_values)\n    float_converter = lite.TFLiteConverterV2.from_concrete_functions([func],\n                                                                     root)\n    float_converter.optimizations = [lite.Optimize.EXPERIMENTAL_SPARSITY]\n    float_tflite_model = float_converter.convert()\n    self.assertIsNotNone(float_tflite_model)\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(float_tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertAllEqual([metadata_fb.ModelOptimizationMode.BLOCK_SPARSITY],\n                        metadata.options.modelOptimizationModes)\n\n  def testQuantizedBlockSparsity(self):\n    weight_values = np.array([\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 2, 0, 0, 0, 0, 5, 0, 0, 0, 3, 0, 0, 0, 1, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [3, 0, 7, 0, 0, 0, -6, -2, 0, 0, 0, 0, 0, -2, 0, 6],\n    ])\n\n    custom_init = tf.constant_initializer(weight_values.transpose())\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Dense(\n            8, kernel_initializer=custom_init, input_shape=[16])\n    ])\n\n    def calibration_gen():\n      for _ in range(10):\n        yield [np.random.uniform(-1, 1, size=(1, 16)).astype(np.float32) * 16]\n\n    quantized_converter = lite.TFLiteConverterV2.from_keras_model(model)\n    quantized_converter.optimizations = [\n        lite.Optimize.EXPERIMENTAL_SPARSITY, lite.Optimize.DEFAULT\n    ]\n    quantized_converter.representative_dataset = calibration_gen\n    quantized_tflite_model = quantized_converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(quantized_tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertEqual(\n        metadata.environment.tensorflowVersion.decode('utf-8'),\n        versions.__version__)\n    self.assertEqual(metadata.environment.apiVersion, 2)\n    self.assertAllEqual([\n        metadata_fb.ModelOptimizationMode.PTQ_FULL_INTEGER,\n        metadata_fb.ModelOptimizationMode.BLOCK_SPARSITY,\n    ], metadata.options.modelOptimizationModes)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    interpreter.allocate_tensors()\n    input_data = np.array(\n        [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]],\n        dtype=np.float32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertArrayNear(\n        np.array([0, 87, 0, 0, 0, 0, 0, 34], dtype=np.float32),\n        actual_value.flatten(),\n        err=1)\n\n  def testQuantizedButNotEnoughBlockSparsity(self):\n    # Sparsity level is 25%, which is not enough to apply the sparse conversion.\n    weight_values = np.array(\n        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n         [4, 4, -3, 4, 4, 1, -2, -2, 1, 3, 4, 1, 1, 1, -4, -5],\n         [1, 1, 5, -1, 3, -1, 1, -3, 4, -3, 2, -3, 3, -1, 3, -4],\n         [0, -3, -2, 5, 4, 2, 1, 4, -4, 4, 1, -2, 3, -2, -2, -1]])\n\n    custom_init = tf.constant_initializer(weight_values.transpose())\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Dense(\n            4, kernel_initializer=custom_init, input_shape=[16])\n    ])\n\n    def calibration_gen():\n      for _ in range(10):\n        yield [np.random.uniform(-1, 1, size=(1, 16)).astype(np.float32) * 16]\n\n    quantized_converter = lite.TFLiteConverterV2.from_keras_model(model)\n    quantized_converter.optimizations = [\n        lite.Optimize.EXPERIMENTAL_SPARSITY, lite.Optimize.DEFAULT\n    ]\n    quantized_converter.representative_dataset = calibration_gen\n    quantized_tflite_model = quantized_converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(quantized_tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertEqual(\n        metadata.environment.tensorflowVersion.decode('utf-8'),\n        versions.__version__)\n    self.assertEqual(metadata.environment.apiVersion, 2)\n    self.assertAllEqual([\n        metadata_fb.ModelOptimizationMode.PTQ_FULL_INTEGER,\n    ], metadata.options.modelOptimizationModes)\n    self.assertNotIn(metadata_fb.ModelOptimizationMode.RANDOM_SPARSITY,\n                     metadata.options.modelOptimizationModes)\n    self.assertNotIn(metadata_fb.ModelOptimizationMode.BLOCK_SPARSITY,\n                     metadata.options.modelOptimizationModes)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    interpreter.allocate_tensors()\n    input_data = np.array(\n        [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]],\n        dtype=np.float32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertArrayNear(\n        np.array([0, -3, 4, 35], dtype=np.float32),\n        actual_value.flatten(),\n        err=1)\n\nif __name__ == '__main__':\n  test.main()"