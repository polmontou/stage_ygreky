"/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define EIGEN_USE_GPU\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/common_runtime/gpu/gpu_event_mgr.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor_types.h\"\n#include \"tensorflow/core/kernels/gpu_prim.h\"\n#include \"tensorflow/core/kernels/gpu_prim_helpers.h\"\n#include \"tensorflow/core/kernels/sparse_fill_empty_rows_op.h\"\n#include \"tensorflow/core/lib/core/bits.h\"\n#include \"tensorflow/core/platform/types.h\"\n#include \"tensorflow/core/util/gpu_device_functions.h\"\n#include \"tensorflow/core/util/gpu_kernel_helper.h\"\n#include \"tensorflow/core/util/gpu_solvers.h\"  // For ScratchSpace\n\n#if GOOGLE_CUDA\n#include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_activation.h\"\nusing stream_executor::cuda::ScopedActivateExecutorContext;\n#elif TENSORFLOW_USE_ROCM\n#include \"tensorflow/compiler/xla/stream_executor/rocm/rocm_activation.h\"\nusing stream_executor::rocm::ScopedActivateExecutorContext;\n#endif\n\nnamespace tensorflow {\n\ntypedef Eigen::GpuDevice GPUDevice;\n\nnamespace kernel_forward {\nbool to_pointers(bool x) { return x; }\nint32 to_pointers(int32 x) { return x; }\nint64 to_pointers(int64 x) { return x; }\ntemplate <class T>\nT* to_pointers(T* x) {\n  return x;\n}\ntemplate <class T>\ntypename T::PointerType to_pointers(T& x) {\n  return x.data();\n}\ntemplate <class T>\ntypename T::ConstPointerType to_pointers(const T& x) {\n  return x.data();\n}\n\ntemplate <typename Tindex, typename... CallerArgs, typename... KernelArgs>\nStatus wrap_kernel_call(void (*func)(KernelArgs...), const GPUDevice& device,\n                        Tindex size, CallerArgs... args) {\n  auto config = GetGpuLaunchConfig(size, device);\n  return GpuLaunchKernel(func, config.block_count, config.thread_per_block, 0,\n                         device.stream(), config, to_pointers(args)...);\n}\n};  // namespace kernel_forward\n\nusing kernel_forward::wrap_kernel_call;\n\nnamespace functor {\n\nnamespace {\ntemplate <typename To>\nstruct CastFunctor {\n  template <typename From>\n  __host__ __device__ To operator()(const From& value) const {\n    return static_cast<To>(value);\n  }\n};\n\n// Computes elements_per_row[0..dense_rows] and sets *rows_are_not_ordered to\n// true if the indices are not ordered by row.\ntemplate <typename Tindex>\n__global__ __launch_bounds__(1024) void CountElementsPerRowKernel(\n    GpuLaunchConfig cfg, Tindex dense_rows, int rank, const Tindex* indices,\n    Tindex* elements_per_row, int* rows_are_not_ordered,\n    int* first_invalid_index) {\n  GPU_1D_KERNEL_LOOP(i, cfg.virtual_thread_count) {\n    Tindex row = indices[i * rank];\n    if (row < 0 || row >= dense_rows) {\n      GpuAtomicMin(first_invalid_index, i);\n      continue;\n    }\n    GpuAtomicAdd(&elements_per_row[row], 1);\n    // Note that this only needs to compare rows, not columns, to satisfy the\n    // row-major order invariant.\n    if (i > 0 && row < indices[(i - 1) * rank]) {\n      // TODO(benbarsdell): Replace this with atomic_ref::store when available.\n      GpuAtomicMax(rows_are_not_ordered, 1);\n    }\n  }\n}\n\ntemplate <typename Tindex>\n__global__ __launch_bounds__(1024) void CopyRowIndicesKernel(\n    GpuLaunchConfig cfg, int rank, const Tindex* indices, Tindex* row_indices) {\n  GPU_1D_KERNEL_LOOP(i, cfg.virtual_thread_count) {\n    row_indices[i] = indices[i * rank];\n  }\n}\n\n// Sets empty_row_indicator[row] to whether the row is empty.\ntemplate <typename Tindex>\n__global__ __launch_bounds__(1024) void ComputeEmptyRowIndicatorKernel(\n    GpuLaunchConfig cfg, const Tindex* elements_per_row,\n    bool* empty_row_indicator) {\n  GPU_1D_KERNEL_LOOP(row, cfg.virtual_thread_count) {\n    empty_row_indicator[row] = elements_per_row[row] == 0;\n  }\n}\n\n// Copies indices and values to output_indices and output_values, leaving space\n// in the output for the new elements that will be inserted wherever there is an\n// empty row.\ntemplate <typename T, typename Tindex>\n__global__ __launch_bounds__(1024) void ScatterInputElementsKernel(\n    GpuLaunchConfig cfg, Tindex dense_rows, int rank,\n    const Tindex* input_index_map, const Tindex* indices, const T* values,\n    const Tindex* num_new_rows_before, Tindex* output_indices, T* output_values,\n    Tindex* reverse_index_map) {\n  GPU_1D_KERNEL_LOOP(i, cfg.virtual_thread_count) {\n    Tindex input_i = input_index_map ? input_index_map[i] : i;\n    Tindex row = indices[input_i * rank];\n    Tindex output_i = i + num_new_rows_before[row];\n    for (int dim = 0; dim < rank; ++dim) {\n      output_indices[output_i * rank + dim] = indices[input_i * rank + dim];\n    }\n    output_values[output_i] = values[input_i];\n    if (reverse_index_map) {\n      reverse_index_map[input_i] = output_i;\n    }\n  }\n}\n\n// Sets the new elements (which correspond to the empty rows in the\n// input) in output_indices and output_values.\ntemplate <typename T, typename Tindex>\n__global__ __launch_bounds__(1024) void ScatterNewElementsKernel(\n    GpuLaunchConfig cfg, int rank, const T* default_value,\n    const Tindex* num_new_rows_through, const Tindex* input_row_ends,\n    const bool* empty_row_indicator, Tindex* output_indices, T* output_values) {\n  GPU_1D_KERNEL_LOOP(row, cfg.virtual_thread_count) {\n    if (!empty_row_indicator[row]) continue;  // Only process empty rows\n    Tindex input_i = (row == 0 ? 0 : input_row_ends[row - 1]);\n    Tindex output_i = input_i + (row == 0 ? 0 : num_new_rows_through[row - 1]);\n    for (int dim = 0; dim < rank; ++dim) {\n      output_indices[output_i * rank + dim] = (dim == 0) ? row : 0;\n    }\n    output_values[output_i] = *default_value;\n  }\n}\n\n}  // namespace\n\ntemplate <typename T, typename Tindex>\nstruct SparseFillEmptyRows<GPUDevice, T, Tindex> {\n  Status operator()(OpKernelContext* context, const Tensor& default_value_t,\n                    const Tensor& indices_t, const Tensor& values_t,\n                    const Tensor& dense_shape_t,\n                    typename AsyncOpKernel::DoneCallback done) {\n    const int kEmptyRowIndicatorOutput = 2;\n\n    const auto default_value = default_value_t.scalar<T>();\n    const auto indices = indices_t.matrix<Tindex>();\n    const auto values = values_t.vec<T>();\n    const auto dense_shape = dense_shape_t.vec<Tindex>();\n\n    const Tindex N = indices_t.shape().dim_size(0);\n    const int rank = indices_t.shape().dim_size(1);\n    const Tindex dense_rows = dense_shape(0);  // Must be on the host\n    DataType index_type = DataTypeToEnum<Tindex>::value;\n    const GPUDevice& device = context->eigen_device<GPUDevice>();\n    se::Stream* stream = context->op_device_context()->stream();\n    if (!stream) return errors::Internal(\"No GPU stream available.\");\n\n    if (dense_rows == 0) {\n      Tindex* output_indices;\n      T* output_values;\n      Tindex* reverse_index_map;\n      TF_RETURN_IF_ERROR(AllocateOutputsExceptEmptyRowIndicator(\n          context, N, rank, /*num_empty_rows=*/0, &output_indices,\n          &output_values, &reverse_index_map));\n      if (context->output_required(kEmptyRowIndicatorOutput)) {\n        Tensor* unused = nullptr;\n        TF_RETURN_IF_ERROR(context->allocate_output(kEmptyRowIndicatorOutput,\n                                                    TensorShape({0}), &unused));\n      }\n      done();\n      return OkStatus();\n    }\n\n    // The algorithm as currently implemented is summarized as follows:\n    // 1) Compute elements_per_row (using GpuAtomicAdd).\n    // 2) Compute input_row_ends (the end index of each row) by computing the\n    //    prefix sum of elements_per_row.\n    // 3) Compute empty_row_indicator = (elements_per_row == 0).\n    // 4) Compute num_empty_rows_through (the no. empty rows up to and including\n    //    each row) by computing the prefix sum of empty_row_indicator.\n    // 5) Synchronize and allocate outputs (the sync is done implicitly by\n    //    enqueueing the remainder of the computation onto the stream as a host\n    //    callback).\n    // 6) If rows are not ordered:\n    //      Compute input_index_map by argsorting row indices.\n    // 7) Scatter input elements into output_indices and output_values using\n    //    input_index_map and num_empty_rows_through, leaving spaces for the\n    //    new values that will be inserted.\n    // 8) Scatter new default values into output_indices and output_values using\n    //    num_new_rows_through, input_row_ends, and empty_row_indicator.\n\n    // Summary of temporary allocations:\n    //   Tindex elements_per_row[dense_rows]\n    //   int rows_are_not_ordered[1]\n    //   Tindex row_indices[N]      (if rows_are_not_ordered)\n    //   Tindex input_index_map[N]  (if rows_are_not_ordered)\n    //   Tindex input_row_ends[dense_rows]\n    //   bool empty_row_indicator[dense_rows]\n    //   Tindex num_empty_rows_through[dense_rows]\n    //   Workspace for inclusive sums.\n    //   Workspace for radix sort.\n\n    Tensor elements_per_row_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(\n        index_type, TensorShape({dense_rows}), &elements_per_row_t));\n    auto elements_per_row = elements_per_row_t.flat<Tindex>();\n    se::DeviceMemoryBase elements_per_row_gpu_memory(\n        elements_per_row.data(), dense_rows * sizeof(Tindex));\n    if (!stream\n             ->ThenMemZero(&elements_per_row_gpu_memory,\n                           dense_rows * sizeof(Tindex))\n             .ok()) {\n      return errors::Internal(\"Failed to zero elements_per_row\");\n    }\n    Tensor rows_are_not_ordered_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(DT_INT32, TensorShape({1}),\n                                              &rows_are_not_ordered_t));\n    auto rows_are_not_ordered_gpu = rows_are_not_ordered_t.flat<int>();\n    se::DeviceMemoryBase rows_are_not_ordered_gpu_memory(\n        rows_are_not_ordered_gpu.data(), sizeof(int));\n    if (!stream->ThenMemZero(&rows_are_not_ordered_gpu_memory, sizeof(int))\n             .ok()) {\n      return errors::Internal(\"Failed to zero rows_are_not_ordered\");\n    }\n    Tensor first_invalid_index_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(DT_INT32, TensorShape({1}),\n                                              &first_invalid_index_t));\n    auto first_invalid_index_gpu = first_invalid_index_t.flat<int>();\n    constexpr const int kAllIndicesValid = std::numeric_limits<int>::max();\n    se::DeviceMemoryBase first_invalid_index_gpu_memory(\n        first_invalid_index_gpu.data(), sizeof(int));\n    if (!stream\n             ->ThenMemset32(&first_invalid_index_gpu_memory, kAllIndicesValid,\n                            sizeof(int))\n             .ok()) {\n      return errors::Internal(\"Failed to initialize first_invalid_index\");\n    }\n\n    if (N > 0) {\n      TF_RETURN_IF_ERROR(wrap_kernel_call(\n          CountElementsPerRowKernel<Tindex>, /*device=*/device, /*size=*/N,\n          dense_rows, rank, indices, elements_per_row, rows_are_not_ordered_gpu,\n          first_invalid_index_gpu));\n    }\n\n    Tensor input_row_ends_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(\n        index_type, TensorShape({dense_rows}), &input_row_ends_t));\n    auto input_row_ends = input_row_ends_t.flat<Tindex>();\n\n    TF_RETURN_IF_ERROR(GpuInclusivePrefixSum(context, /*size=*/dense_rows,\n                                             /*input=*/elements_per_row.data(),\n                                             /*output=*/input_row_ends.data()));\n\n    Tensor empty_row_indicator_t;\n    bool* empty_row_indicator;\n    if (context->output_required(kEmptyRowIndicatorOutput)) {\n      Tensor* empty_row_indicator_t_ptr = nullptr;\n      TF_RETURN_IF_ERROR(context->allocate_output(kEmptyRowIndicatorOutput,\n                                                  TensorShape({dense_rows}),\n                                                  &empty_row_indicator_t_ptr));\n      empty_row_indicator = empty_row_indicator_t_ptr->vec<bool>().data();\n    } else {\n      TF_RETURN_IF_ERROR(context->allocate_temp(\n          DT_BOOL, TensorShape({dense_rows}), &empty_row_indicator_t));\n      empty_row_indicator = empty_row_indicator_t.vec<bool>().data();\n    }\n\n    if (dense_rows > 0) {\n      TF_RETURN_IF_ERROR(\n          wrap_kernel_call(ComputeEmptyRowIndicatorKernel<Tindex>,\n                           /*device=*/device, /*size=*/dense_rows,\n                           elements_per_row, empty_row_indicator));\n    }\n\n    // For each row, the number of empty rows up to and including that row.\n    Tensor num_empty_rows_through_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(\n        index_type, TensorShape({dense_rows}), &num_empty_rows_through_t));\n    auto num_empty_rows_through = num_empty_rows_through_t.flat<Tindex>();\n\n    gpuprim::TransformInputIterator<Tindex, CastFunctor<Tindex>, bool*>\n        empty_row_indicator_cast(empty_row_indicator, {});\n\n    // The inclusive sum in CUB does not work do the right thing if\n    // `empty_row_indicator` is passed in as a `bool *`.\n    TF_RETURN_IF_ERROR(\n        GpuInclusivePrefixSum(context, /*size=*/dense_rows,\n                              /*input=*/empty_row_indicator_cast,\n                              /*output=*/num_empty_rows_through.data()));\n\n    ScratchSpace<Tindex> num_empty_rows_host(context, 1, /*on_host=*/true);\n    if (!stream\n             ->ThenMemcpy(num_empty_rows_host.mutable_data(),\n                          se::DeviceMemoryBase(\n                              num_empty_rows_through.data() + (dense_rows - 1),\n                              sizeof(*num_empty_rows_host.data())),\n                          sizeof(*num_empty_rows_host.data()))\n             .ok()) {\n      return errors::Internal(\"Failed to copy num_empty_rows to host\");\n    }\n\n    ScratchSpace<int> rows_are_not_ordered_host(context, 1, /*on_host=*/true);\n    if (!stream\n             ->ThenMemcpy(rows_are_not_ordered_host.mutable_data(),\n                          rows_are_not_ordered_gpu_memory,\n                          sizeof(*rows_are_not_ordered_host.data()))\n             .ok()) {\n      return errors::Internal(\"Failed to copy rows_are_not_ordered to host\");\n    }\n\n    ScratchSpace<int> first_invalid_index_host(context, 1, /*on_host=*/true);\n    if (!stream\n             ->ThenMemcpy(first_invalid_index_host.mutable_data(),\n                          first_invalid_index_gpu_memory,\n                          sizeof(*first_invalid_index_host.data()))\n             .ok()) {\n      return errors::Internal(\"Failed to copy first_invalid_index to host\");\n    }\n\n    // We must wait for num_empty_rows and rows_are_not_ordered to be copied to\n    // the host, so we enqueue the remainder of the computation onto the stream\n    // asynchronously to avoid stalling execution.\n    auto async_finish_computation =\n        [this, context, kAllIndicesValid, index_type, N, rank, indices, values,\n         default_value, dense_rows, num_empty_rows_host,\n         rows_are_not_ordered_host, first_invalid_index_host,\n         num_empty_rows_through_t, num_empty_rows_through, input_row_ends_t,\n         input_row_ends, empty_row_indicator_t, empty_row_indicator,\n         done]() -> void {\n      DCHECK(done);  // Crash OK\n\n      // Ensure that within the callback, the proper GPU settings are\n      // configured.\n      auto stream = context->op_device_context()->stream();\n      ScopedActivateExecutorContext scoped_activation{stream->parent()};\n\n      int first_invalid_index = *first_invalid_index_host.data();\n      OP_REQUIRES_ASYNC(context, first_invalid_index == kAllIndicesValid,\n                        errors::InvalidArgument(\"indices(\", first_invalid_index,\n                                                \", 0) is invalid.\"),\n                        done);\n\n      Tindex num_empty_rows = *num_empty_rows_host.data();\n\n      Tindex* output_indices;\n      T* output_values;\n      Tindex* reverse_index_map;\n      OP_REQUIRES_OK_ASYNC(\n          context,\n          AllocateOutputsExceptEmptyRowIndicator(\n              context, N, rank, num_empty_rows, &output_indices, &output_values,\n              &reverse_index_map),\n          done);\n\n      const GPUDevice& device = context->eigen_device<GPUDevice>();\n\n      Tindex* input_index_map = nullptr;\n      Tensor input_index_map_t;\n      int rows_are_not_ordered = *rows_are_not_ordered_host.data();\n      if (rows_are_not_ordered) {\n        OP_REQUIRES_OK_ASYNC(context,\n                             ArgSortByRows(context, device, N, rank, dense_rows,\n                                           indices, &input_index_map_t),\n                             done);\n        input_index_map = input_index_map_t.vec<Tindex>().data();\n      }\n\n      if (N > 0) {\n        OP_REQUIRES_OK_ASYNC(\n            context,\n            wrap_kernel_call(ScatterInputElementsKernel<T, Tindex>,\n                             /*device=*/device, /*size=*/N, dense_rows, rank,\n                             input_index_map, indices, values,\n                             num_empty_rows_through, output_indices,\n                             output_values, reverse_index_map),\n            done);\n      }\n\n      if (dense_rows > 0) {\n        OP_REQUIRES_OK_ASYNC(\n            context,\n            wrap_kernel_call(ScatterNewElementsKernel<T, Tindex>,\n                             /*device=*/device, /*size=*/dense_rows, rank,\n                             default_value, num_empty_rows_through,\n                             input_row_ends, empty_row_indicator,\n                             output_indices, output_values),\n            done);\n      }\n\n      done();\n    };\n\n    context->device()\n        ->tensorflow_accelerator_device_info()\n        ->event_mgr->ThenExecute(stream, async_finish_computation);\n    return OkStatus();\n  }\n\n private:\n  Status AllocateOutputsExceptEmptyRowIndicator(\n      OpKernelContext* context, Tindex N, int rank, Tindex num_empty_rows,\n      Tindex** output_indices, T** output_values, Tindex** reverse_index_map) {\n    Tensor* output_indices_t;\n    const Tindex N_full = N + num_empty_rows;\n    TensorShape output_indices_shape({N_full, rank});\n    TF_RETURN_IF_ERROR(context->allocate_output(\n        \"output_indices\", output_indices_shape, &output_indices_t));\n    *output_indices = output_indices_t->matrix<Tindex>().data();\n\n    Tensor* output_values_t;\n    TF_RETURN_IF_ERROR(context->allocate_output(\n        \"output_values\", TensorShape({N_full}), &output_values_t));\n    *output_values = output_values_t->vec<T>().data();\n\n    const int kReverseIndexMapOutput = 3;\n    if (context->output_required(kReverseIndexMapOutput)) {\n      Tensor* reverse_index_map_t = nullptr;\n      TF_RETURN_IF_ERROR(context->allocate_output(\n          kReverseIndexMapOutput, TensorShape({N}), &reverse_index_map_t));\n      *reverse_index_map = reverse_index_map_t->vec<Tindex>().data();\n    } else {\n      *reverse_index_map = nullptr;\n    }\n    return OkStatus();\n  }\n\n  Status ArgSortByRows(OpKernelContext* context, const GPUDevice& device,\n                       Tindex N, int rank, Tindex dense_rows,\n                       typename TTypes<Tindex>::ConstMatrix indices,\n                       Tensor* input_index_map_t) {\n    DataType index_type = DataTypeToEnum<Tindex>::value;\n    // Extract row indices into separate array for use as keys for sorting.\n    Tensor row_indices_t;\n    TF_RETURN_IF_ERROR(\n        context->allocate_temp(index_type, TensorShape({N}), &row_indices_t));\n    auto row_indices = row_indices_t.flat<Tindex>();\n    if (N > 0) {\n      TF_RETURN_IF_ERROR(wrap_kernel_call(CopyRowIndicesKernel<Tindex>,\n                                          /*device=*/device, /*size=*/N, rank,\n                                          indices, row_indices));\n    }\n    // Allocate input_index_map.\n    TF_RETURN_IF_ERROR(context->allocate_temp(index_type, TensorShape({N}),\n                                              input_index_map_t));\n    Tindex* input_index_map = input_index_map_t->flat<Tindex>().data();\n    return GpuRadixSort(context, /*size=*/N, /*keys_in=*/row_indices.data(),\n                        /*keys_out=*/static_cast<Tindex*>(nullptr),\n                        /*indices_in=*/static_cast<Tindex*>(nullptr),\n                        /*indices_out=*/input_index_map,\n                        /*num_bits=*/Log2Ceiling64(dense_rows));\n  }\n};\n\n}  // namespace functor\n\n#define DEFINE_INT64(T) \\\n  template struct functor::SparseFillEmptyRows<GPUDevice, T, int64>;\nTF_CALL_POD_TYPES(DEFINE_INT64)\n#undef DEFINE_INT64\n\nnamespace {\n\ntemplate <typename T, typename Tindex>\n__global__ __launch_bounds__(1024) void GatherOriginalGradValuesKernel(\n    GpuLaunchConfig cfg, const Tindex* reverse_index_map, const T* grad_values,\n    T* d_values, bool* visited) {\n  GPU_1D_KERNEL_LOOP(input_i, cfg.virtual_thread_count) {\n    Tindex output_i = reverse_index_map[input_i];\n    d_values[input_i] = grad_values[output_i];\n    visited[output_i] = true;\n  }\n}\n\ntemplate <typename T, typename Tindex>\nstruct ZeroMaskedValues {\n  ZeroMaskedValues(const bool* _mask, const T* _values)\n      : mask(_mask), values(_values) {}\n  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE T operator()(Tindex i) const {\n    return mask[i] ? T(0) : values[i];\n  }\n  const bool* mask;  // true means return zero instead of value\n  const T* values;\n};\n\n}  // namespace\n\nnamespace functor {\n\ntemplate <typename T, typename Tindex>\nstruct SparseFillEmptyRowsGrad<GPUDevice, T, Tindex> {\n  Status operator()(OpKernelContext* context,\n                    typename TTypes<Tindex>::ConstVec reverse_index_map,\n                    typename TTypes<T>::ConstVec grad_values,\n                    typename TTypes<T>::Vec d_values,\n                    typename TTypes<T>::Scalar d_default_value) {\n    const GPUDevice& device = context->eigen_device<GPUDevice>();\n    const Tindex N = reverse_index_map.dimension(0);\n    const Tindex N_full = grad_values.dimension(0);\n\n    Tensor visited_t;\n    TF_RETURN_IF_ERROR(\n        context->allocate_temp(DT_BOOL, TensorShape({N_full}), &visited_t));\n    auto visited = visited_t.vec<bool>();\n    visited.device(device) = visited.constant(false);\n\n    if (N > 0) {\n      TF_RETURN_IF_ERROR(wrap_kernel_call(\n          GatherOriginalGradValuesKernel<T, Tindex>, /*device=*/device,\n          /*size=*/N, reverse_index_map, grad_values, d_values, visited));\n    }\n\n    // Now we mask out the visited values and sum the remaining ones (which\n    // correspond to the empty rows in the forward input) to compute\n    // d_default_value.\n\n    gpuprim::CountingInputIterator<Tindex, Tindex> counting_iterator(Tindex(0));\n    ZeroMaskedValues<T, Tindex> mask_values_fn(visited.data(),\n                                               grad_values.data());\n    gpuprim::TransformInputIterator<T, decltype(mask_values_fn),\n                                    decltype(counting_iterator), Tindex>\n        transform_iterator(counting_iterator, mask_values_fn);\n\n    std::size_t temp_storage_bytes = 0;\n    auto gpuprim_status = gpuprim::DeviceReduce::Sum(\n        /*temp_storage=*/nullptr, temp_storage_bytes,\n        /*d_in=*/transform_iterator,\n        /*d_out=*/d_default_value.data(),\n        /*num_items=*/N_full,\n        /*stream=*/device.stream());\n\n    if (gpuprim_status != gpuSuccess) {\n      return errors::Internal(\n          \"SparseFillEmptyRowsGrad: Could not launch \"\n          \"gpuprim::DeviceReduce::Sum to calculate temp_storage_bytes, \"\n          \"status: \",\n          GpuGetErrorString(gpuprim_status));\n    }\n\n    Tensor temp_storage;\n    TF_RETURN_IF_ERROR(context->allocate_temp(\n        DT_INT8, TensorShape({static_cast<int64_t>(temp_storage_bytes)}),\n        &temp_storage));\n\n    gpuprim_status = gpuprim::DeviceReduce::Sum(\n        /*temp_storage=*/temp_storage.flat<int8>().data(), temp_storage_bytes,\n        /*d_in=*/transform_iterator,\n        /*d_out=*/d_default_value.data(),\n        /*num_items=*/N_full,\n        /*stream=*/device.stream());\n\n    if (gpuprim_status != gpuSuccess) {\n      return errors::Internal(\n          \"SparseFillEmptyRowsGrad: Could not launch \"\n          \"gpuprim::DeviceReduce::Sum to sum values from originally-empty \"\n          \"rows. temp_storage_bytes: \",\n          temp_storage_bytes, \", status: \", GpuGetErrorString(gpuprim_status));\n    }\n\n    return OkStatus();\n  }\n};\n\n}  // namespace functor\n\n#define DEFINE_INT64(T) \\\n  template struct functor::SparseFillEmptyRowsGrad<GPUDevice, T, int64>;\nTF_CALL_REAL_NUMBER_TYPES(DEFINE_INT64);\n#undef DEFINE_INT64\n\n}  // namespace tensorflow\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM"