"/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include <algorithm>\n#include <cmath>\n\n#include \"tensorflow/core/framework/common_shape_fns.h\"\n#include \"tensorflow/core/framework/kernel_shape_util.h\"\n#include \"tensorflow/core/framework/numeric_op.h\"\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/shape_inference.h\"\n#include \"tensorflow/core/lib/core/bits.h\"\n#include \"tensorflow/core/lib/math/math_util.h\"\n#include \"tensorflow/core/util/mirror_pad_mode.h\"\n#include \"tensorflow/core/util/padding.h\"\n#include \"tensorflow/core/util/tensor_format.h\"\n\nnamespace tensorflow {\n\nusing shape_inference::DimensionHandle;\nusing shape_inference::InferenceContext;\nusing shape_inference::ShapeHandle;\n\nnamespace {\n\nStatus FractionalPoolShapeFn(InferenceContext* c) {\n  ShapeHandle input;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input));\n\n  std::vector<float> pooling_ratio;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"pooling_ratio\", &pooling_ratio));\n  if (pooling_ratio.size() != 4) {\n    return errors::InvalidArgument(\n        \"pooling_ratio field must specify 4 dimensions\");\n  }\n  std::vector<DimensionHandle> output_dims;\n  for (int i = 0; i < 4; ++i) {\n    DimensionHandle d = c->Dim(input, i);\n    if (c->ValueKnown(d)) {\n      // This must match the same logic in the kernel function in\n      // core/kernels/fractional_max_pool_op.cc.\n      auto val =\n          static_cast<int64_t>(std::floor(c->Value(d) / pooling_ratio[i]));\n      if (val < 0) {\n        return errors::InvalidArgument(\"Size computed for dim \", i,\n                                       \" is negative: \", val);\n      }\n      output_dims.push_back(c->MakeDim(val));\n    } else {\n      output_dims.push_back(c->UnknownDim());\n    }\n  }\n\n  c->set_output(0, c->MakeShape(output_dims));\n  c->set_output(1, c->Vector(output_dims[1]));\n  c->set_output(2, c->Vector(output_dims[2]));\n  return OkStatus();\n}\n\n}  // namespace\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"AvgPool\")\n    .Input(\"value: T\")\n    .Output(\"output: T\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::AvgPoolShape);\n\nREGISTER_OP(\"AvgPoolGrad\")\n    .Input(\"orig_input_shape: int32\")\n    .Input(\"grad: T\")\n    .Output(\"output: T\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::AvgPoolGradShape);\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"BatchNormWithGlobalNormalization\")\n    .Input(\"t: T\")\n    .Input(\"m: T\")\n    .Input(\"v: T\")\n    .Input(\"beta: T\")\n    .Input(\"gamma: T\")\n    .Output(\"result: T\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"variance_epsilon: float\")\n    .Attr(\"scale_after_normalization: bool\")\n    .Deprecated(9, \"Use tf.nn.batch_normalization()\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input));\n\n      DimensionHandle last_dim = c->Dim(input, 3);\n      for (int i = 1; i < 5; ++i) {  // covers m, v, beta, gamma\n        ShapeHandle vec;\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(i), 1, &vec));\n        TF_RETURN_IF_ERROR(c->Merge(last_dim, c->Dim(vec, 0), &last_dim));\n      }\n\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(c->ReplaceDim(input, 3, last_dim, &out));\n      c->set_output(0, out);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"BatchNormWithGlobalNormalizationGrad\")\n    .Input(\"t: T\")\n    .Input(\"m: T\")\n    .Input(\"v: T\")\n    .Input(\"gamma: T\")\n    .Input(\"backprop: T\")\n    .Output(\"dx: T\")\n    .Output(\"dm: T\")\n    .Output(\"dv: T\")\n    .Output(\"db: T\")\n    .Output(\"dg: T\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"variance_epsilon: float\")\n    .Attr(\"scale_after_normalization: bool\")\n    .Deprecated(9, \"Use tf.nn.batch_normalization()\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input));\n      TF_RETURN_IF_ERROR(\n          c->Merge(input, c->input(4), &input));  // with backprop\n\n      DimensionHandle last_dim = c->Dim(input, 3);\n      for (int i = 1; i < 4; ++i) {  // covers m, v, gamma\n        ShapeHandle vec;\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(i), 1, &vec));\n        TF_RETURN_IF_ERROR(c->Merge(last_dim, c->Dim(vec, 0), &last_dim));\n      }\n\n      ShapeHandle dx;\n      TF_RETURN_IF_ERROR(c->ReplaceDim(input, 3, last_dim, &dx));\n      c->set_output(0, dx);\n\n      ShapeHandle vector_shape = c->Vector(last_dim);\n      c->set_output(1, vector_shape);\n      c->set_output(2, vector_shape);\n      c->set_output(3, vector_shape);\n      c->set_output(4, vector_shape);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"FusedBatchNorm\")\n    .Input(\"x: T\")\n    .Input(\"scale: T\")\n    .Input(\"offset: T\")\n    .Input(\"mean: T\")\n    .Input(\"variance: T\")\n    .Output(\"y: T\")\n    .Output(\"batch_mean: T\")\n    .Output(\"batch_variance: T\")\n    .Output(\"reserve_space_1: T\")\n    .Output(\"reserve_space_2: T\")\n    .Attr(\"T: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(\"exponential_avg_factor: float = 1.0\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormShape);\n\nREGISTER_OP(\"FusedBatchNormV2\")\n    .Input(\"x: T\")\n    .Input(\"scale: U\")\n    .Input(\"offset: U\")\n    .Input(\"mean: U\")\n    .Input(\"variance: U\")\n    .Output(\"y: T\")\n    .Output(\"batch_mean: U\")\n    .Output(\"batch_variance: U\")\n    .Output(\"reserve_space_1: U\")\n    .Output(\"reserve_space_2: U\")\n    .Attr(\"T: {half, bfloat16, float}\")\n    .Attr(\"U: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(\"exponential_avg_factor: float = 1.0\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormShape);\n\nREGISTER_OP(\"FusedBatchNormV3\")\n    .Input(\"x: T\")\n    .Input(\"scale: U\")\n    .Input(\"offset: U\")\n    .Input(\"mean: U\")\n    .Input(\"variance: U\")\n    .Output(\"y: T\")\n    .Output(\"batch_mean: U\")\n    .Output(\"batch_variance: U\")\n    .Output(\"reserve_space_1: U\")\n    .Output(\"reserve_space_2: U\")\n    .Output(\"reserve_space_3: U\")\n    .Attr(\"T: {half, bfloat16, float}\")\n    .Attr(\"U: {bfloat16, float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(\"exponential_avg_factor: float = 1.0\")\n    .Attr(GetConvnetDataFormat2D3DAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormV3Shape);\n\nREGISTER_OP(\"_FusedBatchNormEx\")\n    .Input(\"x: T\")\n    .Input(\"scale: U\")\n    .Input(\"offset: U\")\n    .Input(\"mean: U\")\n    .Input(\"variance: U\")\n    .Input(\"side_input: num_side_inputs * T\")\n    .Output(\"y: T\")\n    .Output(\"batch_mean: U\")\n    .Output(\"batch_variance: U\")\n    .Output(\"reserve_space_1: U\")\n    .Output(\"reserve_space_2: U\")\n    .Output(\"reserve_space_3: U\")\n    .Attr(\"T: {half, float, bfloat16}\")\n    .Attr(\"U: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(\"exponential_avg_factor: float = 1.0\")\n    .Attr(\"num_side_inputs: int >= 0 = 0\")\n    .Attr(\"activation_mode: string = \\\"Identity\\\"\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormExShape)\n    .Doc(R\"doc(\nInternal FusedBatchNorm operation: reserved for internal use.\n\nDo not invoke this operator directly in Python. A fusion optimization is\nexpected to create these operators.\n)doc\");\n\nREGISTER_OP(\"FusedBatchNormGrad\")\n    .Input(\"y_backprop: T\")\n    .Input(\"x: T\")\n    .Input(\"scale: T\")\n    .Input(\"reserve_space_1: T\")\n    .Input(\"reserve_space_2: T\")\n    .Output(\"x_backprop: T\")\n    .Output(\"scale_backprop: T\")\n    .Output(\"offset_backprop: T\")\n    .Output(\"reserve_space_3: T\")\n    .Output(\"reserve_space_4: T\")\n    .Attr(\"T: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormGradShape);\n\nREGISTER_OP(\"FusedBatchNormGradV2\")\n    .Input(\"y_backprop: T\")\n    .Input(\"x: T\")\n    .Input(\"scale: float\")\n    .Input(\"reserve_space_1: U\")\n    .Input(\"reserve_space_2: U\")\n    .Output(\"x_backprop: T\")\n    .Output(\"scale_backprop: U\")\n    .Output(\"offset_backprop: U\")\n    .Output(\"reserve_space_3: U\")\n    .Output(\"reserve_space_4: U\")\n    .Attr(\"T: {half, bfloat16, float}\")\n    .Attr(\"U: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormGradShape);\n\nREGISTER_OP(\"FusedBatchNormGradV3\")\n    .Input(\"y_backprop: T\")\n    .Input(\"x: T\")\n    .Input(\"scale: float\")\n    .Input(\"reserve_space_1: U\")\n    .Input(\"reserve_space_2: U\")\n    .Input(\"reserve_space_3: U\")\n    .Output(\"x_backprop: T\")\n    .Output(\"scale_backprop: U\")\n    .Output(\"offset_backprop: U\")\n    .Output(\"reserve_space_4: U\")\n    .Output(\"reserve_space_5: U\")\n    .Attr(\"T: {half, bfloat16, float}\")\n    .Attr(\"U: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(GetConvnetDataFormat2D3DAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormGradShape);\n\nREGISTER_OP(\"_FusedBatchNormGradEx\")\n    .Input(\"y_backprop: T\")\n    .Input(\"x: T\")\n    .Input(\"scale: float\")\n    .Input(\"reserve_space_1: U\")\n    .Input(\"reserve_space_2: U\")\n    .Input(\"reserve_space_3: U\")\n    .Input(\"offset: float\")\n    .Input(\"y: T\")\n    .Output(\"x_backprop: T\")\n    .Output(\"scale_backprop: U\")\n    .Output(\"offset_backprop: U\")\n    .Output(\"reserve_space_4: U\")\n    .Output(\"reserve_space_5: U\")\n    .Output(\"side_input_backprop: num_side_inputs * T\")\n    .Attr(\"T: {half, float}\")\n    .Attr(\"U: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(\"num_side_inputs: int >= 0 = 0\")\n    .Attr(\"activation_mode: string = \\\"Identity\\\"\")\n    .Attr(GetConvnetDataFormat2D3DAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormGradExShape)\n    .Doc(R\"doc(\nInternal FusedBatchNormGrad operation: reserved for internal use.\n\nDo not invoke this operator directly in Python. A fusion optimization is\nexpected to create these operators.\n)doc\");\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"BiasAdd\")\n    .Attr(\"T: numbertype\")\n    .Input(\"value: T\")\n    .Input(\"bias: T\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Output(\"output: T\")\n    .SetShapeFn(shape_inference::BiasAddShape);\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"BiasAddGrad\")\n    .Attr(\"T: numbertype\")\n    .Input(\"out_backprop: T\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Output(\"output: T\")\n    .SetShapeFn(shape_inference::BiasAddGradShape);\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"BiasAddV1\")\n    .Attr(\"T: numbertype\")\n    .Input(\"value: T\")\n    .Input(\"bias: T\")\n    .Output(\"output: T\")\n    .SetShapeFn(shape_inference::BiasAddShape);\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"Conv2D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double, int32}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv2DShapeWithExplicitPadding);\n\nREGISTER_OP(\"Conv2DBackpropInput\")\n    .Input(\"input_sizes: int32\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double, int32}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv2DBackpropInputShape);\n\n// TODO(jeff): Instead of 'use_cudnn_for_gpu', maybe we should have a\n// more general string attribute ('kernel_impl'?) that can be used to\n// select among several possible implementations.\nREGISTER_OP(\"Conv2DBackpropFilter\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 4, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"_FusedConv2D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"args: TArgs\")\n    .Input(\"host_args : num_host_args * float\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, float, double, int8, qint8}\")\n    .Attr(\"TArgs: list(type)\")\n    .Attr(\"num_args: int >= 0\")\n    .Attr(\"num_host_args: int >= 0 =0\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(\"data_format: { 'NHWC', 'NCHW', 'NCHW_VECT_C' } = 'NHWC'\")\n    .Attr(\"filter_format: {'HWIO', 'OIHW', 'OIHW_VECT_I'} = 'HWIO'\")\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(\"fused_ops: list(string) = []\")\n    // Attributes for the FusedBatchNorm ------------------------------------ //\n    .Attr(\"epsilon: float = 0.0001\")\n    // Attributes for the LeakyRelu ----------------------------------------- //\n    .Attr(\"leakyrelu_alpha: float = 0.2\")\n    // ---------------------------------------------------------------------- //\n    .SetShapeFn(shape_inference::Conv2DShapeWithExplicitPadding)\n    .Doc(R\"doc(\nPerforms a convolution followed by a specified series of operations.\n\nThe inputs to the convolution are `input` and `filter`. The series of operations\nthat follows is specified by the `fused_ops` attribute, which is a list of TF op\nnames specified as strings (e.g. \"Relu\"). They are performed in order, where the\n(first) input to each op is the output of the preceding op. The first input and\nthe output of each fused_op must be of type T.\n\nCurrently supported fused_op combinations are: [X] and [X,A], where X is one of\n{\"BiasAdd\",\"FusedBatchNorm\"} and A is one of {\"Elu\",\"Relu\",\"Relu6\"}.\n\n* The first input to op X is the Conv2D result, and the additional input(s) to X\nare specified by `args`.\n* If there is an op A specified, the output of op X is the input to op A, and op\nA produces the _FusedConv2D output. Otherwise, op X produces the _FusedConv2D\noutput.\n\n*NOTE*: Do not invoke this operator directly in Python. Grappler is expected to\ncreate these operators.\n)doc\");\n\nnamespace {\n\nStatus CommonFusedConvCalculations(InferenceContext* c, bool has_resize) {\n  ShapeHandle input;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input));\n\n  ShapeHandle resized = input;\n  int paddings_index = 1;\n  int filter_index = 2;\n  if (has_resize) {\n    paddings_index = 2;\n    filter_index = 3;\n\n    ShapeHandle unused_size;\n    TF_RETURN_IF_ERROR(c->Merge(c->input(1), c->Vector(2), &unused_size));\n\n    const Tensor* size = c->input_tensor(1);\n    DimensionHandle new_height = c->UnknownDim();\n    DimensionHandle new_width = c->UnknownDim();\n    if (size != nullptr) {\n      new_height = c->MakeDim(size->flat<int32>()(0));\n      new_width = c->MakeDim(size->flat<int32>()(1));\n    }\n    TF_RETURN_IF_ERROR(c->ReplaceDim(resized, 1, new_height, &resized));\n    TF_RETURN_IF_ERROR(c->ReplaceDim(resized, 2, new_width, &resized));\n  }\n\n  ShapeHandle paddings;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(paddings_index), 2, &paddings));\n  TF_RETURN_IF_ERROR(\n      c->WithRank(resized, c->Value(c->Dim(paddings, 0)), &resized));\n  TF_RETURN_IF_ERROR(\n      c->Merge(paddings, c->Matrix(c->Rank(resized), 2), &paddings));\n\n  const Tensor* paddings_t = c->input_tensor(paddings_index);\n  ShapeHandle padded;\n  if (paddings_t != nullptr) {\n    std::vector<DimensionHandle> output_dims;\n    for (int i = 0; i < 4; ++i) {\n      DimensionHandle dim = c->Dim(resized, i);\n      int64_t p0 = static_cast<int64_t>(paddings_t->matrix<int32>()(i, 0));\n      int64_t p1 = static_cast<int64_t>(paddings_t->matrix<int32>()(i, 1));\n      if (p0 < 0 || p1 < 0) {\n        return errors::InvalidArgument(\"Paddings must be non-negative\");\n      }\n\n      TF_RETURN_IF_ERROR(c->Add(dim, p0 + p1, &dim));\n      output_dims.push_back(dim);\n    }\n    padded = c->MakeShape(output_dims);\n  } else {\n    padded = c->UnknownShapeOfRank(4);\n  }\n\n  // Work out the convolution's effect with 'padded' as the input.\n  ShapeHandle filter;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(filter_index), 4, &filter));\n  std::vector<int32> strides;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"strides\", &strides));\n  if (strides.size() != 4) {\n    return errors::InvalidArgument(\n        \"Operation requires the stride attribute to contain 4 values, but \",\n        \"got: \", strides.size());\n  }\n\n  int32_t stride_rows = strides[1];\n  int32_t stride_cols = strides[2];\n\n  DimensionHandle batch_size_dim = c->Dim(padded, 0);\n  DimensionHandle in_rows_dim = c->Dim(padded, 1);\n  DimensionHandle in_cols_dim = c->Dim(padded, 2);\n  DimensionHandle filter_rows_dim = c->Dim(filter, 0);\n  DimensionHandle filter_cols_dim = c->Dim(filter, 1);\n  DimensionHandle output_depth_dim = c->Dim(filter, 3);\n\n  DimensionHandle unused;\n  TF_RETURN_IF_ERROR(c->Merge(c->Dim(padded, 3), c->Dim(filter, 2), &unused));\n\n  Padding padding;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"padding\", &padding));\n\n  DimensionHandle output_rows, output_cols;\n  TF_RETURN_IF_ERROR(GetWindowedOutputSizeFromDims(\n      c, in_rows_dim, filter_rows_dim, stride_rows, padding, &output_rows));\n  TF_RETURN_IF_ERROR(GetWindowedOutputSizeFromDims(\n      c, in_cols_dim, filter_cols_dim, stride_cols, padding, &output_cols));\n\n  ShapeHandle output_shape = c->MakeShape(\n      {batch_size_dim, output_rows, output_cols, output_depth_dim});\n  c->set_output(0, output_shape);\n  return OkStatus();\n}\n\n}  // namespace\n\nREGISTER_OP(\"DataFormatDimMap\")\n    .Input(\"x: T\")\n    .Output(\"y: T\")\n    .Attr(\"T: {int32, int64} = DT_INT32\")\n    .Attr(\"src_format: string = 'NHWC'\")\n    .Attr(\"dst_format: string = 'NCHW'\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"DataFormatVecPermute\")\n    .Input(\"x: T\")\n    .Output(\"y: T\")\n    .Attr(\"T: {int32, int64} = DT_INT32\")\n    .Attr(\"src_format: string = 'NHWC'\")\n    .Attr(\"dst_format: string = 'NCHW'\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"FusedResizeAndPadConv2D\")\n    .Input(\"input: T\")\n    .Input(\"size: int32\")\n    .Input(\"paddings: int32\")\n    .Input(\"filter: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, float, double}\")\n    .Attr(\"resize_align_corners: bool = false\")\n    .Attr(GetMirrorPadModeAttrString())\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      return CommonFusedConvCalculations(c, /*has_resize=*/true);\n    });\n\nREGISTER_OP(\"FusedPadConv2D\")\n    .Input(\"input: T\")\n    .Input(\"paddings: int32\")\n    .Input(\"filter: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, float, double}\")\n    .Attr(GetMirrorPadModeAttrString())\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      return CommonFusedConvCalculations(c, /*has_resize=*/false);\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"DepthwiseConv2dNative\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::DepthwiseConv2DNativeShapeWithExplicitPadding);\n\nREGISTER_OP(\"DepthwiseConv2dNativeBackpropInput\")\n    .Input(\"input_sizes: int32\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 4, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"DepthwiseConv2dNativeBackpropFilter\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 4, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"_FusedDepthwiseConv2dNative\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"args: num_args * T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"num_args: int >= 0\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"fused_ops: list(string) = []\")\n    // Attributes for the FusedBatchNorm ------------------------------------ //\n    .Attr(\"epsilon: float = 0.0001\")\n    // Attributes for the LeakyRelu ----------------------------------------- //\n    .Attr(\"leakyrelu_alpha: float = 0.2\")\n    // ---------------------------------------------------------------------- //\n    .SetShapeFn(shape_inference::DepthwiseConv2DNativeShape);\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"Conv3D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv3DShape);\n\nREGISTER_OP(\"Conv3DBackpropInput\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, float, double}\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Deprecated(10, \"Use Conv3DBackpropInputV2\")\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      return UnchangedShapeWithRank(c, 5);\n    });\n\nREGISTER_OP(\"Conv3DBackpropFilter\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, float, double}\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Deprecated(10, \"Use Conv3DBackpropFilterV2\")\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 5, &out));\n      c->set_output(0, out);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"Conv3DBackpropInputV2\")\n    .Input(\"input_sizes: Tshape\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .Attr(\"Tshape: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 5, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"Conv3DBackpropFilterV2\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 5, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"AvgPool3D\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::Pool3DShape);\n\nREGISTER_OP(\"AvgPool3DGrad\")\n    .Input(\"orig_input_shape: int32\")\n    .Input(\"grad: T\")\n    .Output(\"output: T\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::AvgPool3DGradShape);\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"MaxPool3D\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float}\")\n    .SetShapeFn(shape_inference::Pool3DShape);\n\nREGISTER_OP(\"MaxPool3DGrad\")\n    .Input(\"orig_input: TInput\")\n    .Input(\"orig_output: TInput\")\n    .Input(\"grad: T\")\n    .Output(\"output: T\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float} = DT_FLOAT\")\n    .Attr(\"TInput: {half, bfloat16, float} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MaxPool3DGradShape);\n\nREGISTER_OP(\"MaxPool3DGradGrad\")\n    .Input(\"orig_input: T\")\n    .Input(\"orig_output: T\")\n    .Input(\"grad: T\")\n    .Output(\"output: T\")\n    .Attr(\"ksize: list(int) >= 5 \")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Pool3DShape(c));\n      ShapeHandle unused;\n      // Validate 'orig_input' is the same shape as 'grad'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(0), c->input(2), &unused));\n      // Validate 'orig_output' is same shape as 'output'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(1), c->output(0), &unused));\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"L2Loss\")\n    .Input(\"t: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::ScalarShape);\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"LRN\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"depth_radius: int = 5\")\n    .Attr(\"bias: float = 1.0\")\n    .Attr(\"alpha: float = 1.0\")\n    .Attr(\"beta: float = 0.5\")\n    .Attr(\"T: {half, bfloat16, float} = DT_FLOAT\")\n    .SetShapeFn([](InferenceContext* c) {\n      return UnchangedShapeWithRank(c, 4);\n    });\n\nREGISTER_OP(\"LRNGrad\")\n    .Input(\"input_grads: T\")\n    .Input(\"input_image: T\")\n    .Input(\"output_image: T\")\n    .Output(\"output: T\")\n    .Attr(\"depth_radius: int = 5\")\n    .Attr(\"bias: float = 1.0\")\n    .Attr(\"alpha: float = 1.0\")\n    .Attr(\"beta: float = 0.5\")\n    .Attr(\"T: {half, bfloat16, float} = DT_FLOAT\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &s));  // input_grads\n      TF_RETURN_IF_ERROR(c->Merge(s, c->input(1), &s));     // input_image\n      TF_RETURN_IF_ERROR(c->Merge(s, c->input(2), &s));     // output_image\n      c->set_output(0, s);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"MaxPool\")\n    .Attr(\n        \"T: {half, bfloat16, float, double, int32, int64, uint8, int16, int8, \"\n        \"uint16, qint8} = DT_FLOAT\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(\"data_format: {'NHWC', 'NCHW', 'NCHW_VECT_C'} = 'NHWC'\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .SetShapeFn(shape_inference::MaxPoolShapeWithExplicitPadding);\n\nREGISTER_OP(\"MaxPoolV2\")\n    .Attr(\n        \"T: {half, bfloat16, float, double, int32, int64, uint8, int16, int8, \"\n        \"uint16, qint8} = DT_FLOAT\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"data_format: {'NHWC', 'NCHW', 'NCHW_VECT_C'} = 'NHWC'\")\n    .Input(\"input: T\")\n    .Input(\"ksize: int32\")\n    .Input(\"strides: int32\")\n    .Output(\"output: T\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MaxPoolV2Shape(c, 3));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"MaxPoolGrad\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Input(\"orig_input: T\")\n    .Input(\"orig_output: T\")\n    .Input(\"grad: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MaxPoolGradShape);\n\nREGISTER_OP(\"MaxPoolGradV2\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Input(\"orig_input: T\")\n    .Input(\"orig_output: T\")\n    .Input(\"grad: T\")\n    .Input(\"ksize: int32\")\n    .Input(\"strides: int32\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MaxPoolGradShape);\n\n// TODO(b/150813181): Implement explicit padding.\nREGISTER_OP(\"MaxPoolGradGrad\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Input(\"orig_input: T\")\n    .Input(\"orig_output: T\")\n    .Input(\"grad: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MaxPoolShape(c));\n      ShapeHandle unused;\n      // Validate 'orig_input' is the same shape as 'grad'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(0), c->input(2), &unused));\n      // Validate 'orig_output' is same shape as 'output'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(1), c->output(0), &unused));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"MaxPoolGradGradV2\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Input(\"orig_input: T\")\n    .Input(\"orig_output: T\")\n    .Input(\"grad: T\")\n    .Input(\"ksize: int32\")\n    .Input(\"strides: int32\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MaxPoolV2Shape(c, 5));\n      ShapeHandle unused;\n      // Validate 'orig_input' is the same shape as 'grad'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(0), c->input(2), &unused));\n      // Validate 'orig_output' is same shape as 'output'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(1), c->output(0), &unused));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"MaxPoolWithArgmax\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(\"Targmax: {int32, int64} = DT_INT64\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"include_batch_in_index: bool = false\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Output(\"argmax: Targmax\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MaxPoolShape(c));\n      c->set_output(1, c->output(0));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"MaxPoolGradWithArgmax\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"include_batch_in_index: bool = false\")\n    .Attr(\"Targmax: {int32, int64}\")\n    .Input(\"input: T\")\n    .Input(\"grad: T\")\n    .Input(\"argmax: Targmax\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      return UnchangedShapeWithRank(c, 4);\n    });\n\nREGISTER_OP(\"MaxPoolGradGradWithArgmax\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"include_batch_in_index: bool = false\")\n    .Attr(\"Targmax: {int32, int64}\")\n    .Input(\"input: T\")\n    .Input(\"grad: T\")\n    .Input(\"argmax: Targmax\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MaxPoolShape(c));\n      ShapeHandle unused;\n      // Validate 'orig_input' is the same shape as 'grad'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(0), c->input(1), &unused));\n      // Validate 'argmax' is same shape as 'output'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(2), c->output(0), &unused));\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"Dilation2D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(\"rates: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input_shape;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input_shape));\n      ShapeHandle filter_shape;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 3, &filter_shape));\n\n      std::vector<int32> strides;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"strides\", &strides));\n      if (strides.size() != 4) {\n        return errors::InvalidArgument(\n            \"Dilation2D requires the stride attribute to contain 4 values, but \"\n            \"got: \",\n            strides.size());\n      }\n\n      std::vector<int32> rates;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"rates\", &rates));\n      if (rates.size() != 4) {\n        return errors::InvalidArgument(\n            \"Dilation2D requires the rates attribute to contain 4 values, but \"\n            \"got: \",\n            rates.size());\n      }\n\n      int32_t stride_rows = strides[1];\n      int32_t stride_cols = strides[2];\n\n      int32_t rate_rows = rates[1];\n      int32_t rate_cols = rates[2];\n\n      DimensionHandle batch_size_dim = c->Dim(input_shape, 0);\n      DimensionHandle in_rows_dim = c->Dim(input_shape, 1);\n      DimensionHandle in_cols_dim = c->Dim(input_shape, 2);\n      DimensionHandle filter_rows_dim = c->Dim(filter_shape, 0);\n      DimensionHandle filter_cols_dim = c->Dim(filter_shape, 1);\n      DimensionHandle output_depth_dim = c->Dim(filter_shape, 2);\n\n      if (!c->ValueKnown(in_rows_dim) || !c->ValueKnown(in_cols_dim) ||\n          !c->ValueKnown(filter_rows_dim) || !c->ValueKnown(filter_cols_dim)) {\n        ShapeHandle output_shape =\n            c->MakeShape({batch_size_dim, InferenceContext::kUnknownDim,\n                          InferenceContext::kUnknownDim, output_depth_dim});\n        c->set_output(0, output_shape);\n        return OkStatus();\n      }\n      DimensionHandle unused;\n      TF_RETURN_IF_ERROR(\n          c->Merge(c->Dim(input_shape, 3), output_depth_dim, &unused));\n\n      auto in_rows = c->Value(in_rows_dim);\n      auto in_cols = c->Value(in_cols_dim);\n      auto filter_rows = c->Value(filter_rows_dim);\n      auto filter_cols = c->Value(filter_cols_dim);\n      auto filter_rows_eff = filter_rows + (filter_rows - 1) * (rate_rows - 1);\n      auto filter_cols_eff = filter_cols + (filter_cols - 1) * (rate_cols - 1);\n\n      Padding padding;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"padding\", &padding));\n\n      int64_t output_rows, output_cols;\n      int64_t padding_before, padding_after;\n      TF_RETURN_IF_ERROR(GetWindowedOutputSizeVerbose(\n          in_rows, filter_rows_eff, stride_rows, padding, &output_rows,\n          &padding_before, &padding_after));\n      TF_RETURN_IF_ERROR(GetWindowedOutputSizeVerbose(\n          in_cols, filter_cols_eff, stride_cols, padding, &output_cols,\n          &padding_before, &padding_after));\n\n      ShapeHandle output_shape = c->MakeShape(\n          {batch_size_dim, output_rows, output_cols, output_depth_dim});\n      c->set_output(0, output_shape);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"Dilation2DBackpropInput\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"in_backprop: T\")\n    .Attr(\"T: realnumbertype\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(\"rates: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"Dilation2DBackpropFilter\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"filter_backprop: T\")\n    .Attr(\"T: realnumbertype\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(\"rates: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      c->set_output(0, c->input(1));\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"Relu\")\n    .Input(\"features: T\")\n    .Output(\"activations: T\")\n    .Attr(\"T: {realnumbertype, qint8}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"ReluGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Output(\"backprops: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn);\n\nREGISTER_OP(\"Relu6\")\n    .Input(\"features: T\")\n    .Output(\"activations: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"Relu6Grad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Output(\"backprops: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn);\n\nREGISTER_OP(\"LeakyRelu\")\n    .Input(\"features: T\")\n    .Output(\"activations: T\")\n    .Attr(\"alpha: float = 0.2\")\n    .Attr(\"T: {half, bfloat16, float, double} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"LeakyReluGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Output(\"backprops: T\")\n    .Attr(\"alpha: float = 0.2\")\n    .Attr(\"T: {half, bfloat16, float, double} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn);\n\nREGISTER_OP(\"Elu\")\n    .Input(\"features: T\")\n    .Output(\"activations: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"EluGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"outputs: T\")\n    .Output(\"backprops: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn);\n\nREGISTER_OP(\"Selu\")\n    .Input(\"features: T\")\n    .Output(\"activations: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"SeluGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"outputs: T\")\n    .Output(\"backprops: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn);\n\nREGISTER_OP(\"Softplus\")\n    .Input(\"features: T\")\n    .Output(\"activations: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"SoftplusGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Output(\"backprops: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn);\n\nREGISTER_OP(\"Softsign\")\n    .Input(\"features: T\")\n    .Output(\"activations: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"SoftsignGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Output(\"backprops: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn);\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"Softmax\")\n    .Input(\"logits: T\")\n    .Output(\"softmax: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn([](InferenceContext* c) {\n      return shape_inference::UnchangedShapeWithRankAtLeast(c, 1);\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"LogSoftmax\")\n    .Input(\"logits: T\")\n    .Output(\"logsoftmax: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn([](InferenceContext* c) {\n      return shape_inference::UnchangedShapeWithRankAtLeast(c, 1);\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"SoftmaxCrossEntropyWithLogits\")\n    .Input(\"features: T\")\n    .Input(\"labels: T\")\n    .Output(\"loss: T\")\n    .Output(\"backprop: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input;\n      if (c->WithRank(c->input(0), 2, &input) == OkStatus() &&\n          c->Merge(input, c->input(1), &input) == OkStatus()) {\n        DimensionHandle batch_size = c->Dim(input, 0);\n        c->set_output(0, c->Vector(batch_size));\n        c->set_output(1, input);\n        return OkStatus();\n      }\n      TF_RETURN_IF_ERROR(BroadcastBinaryOpOutputShapeFn(c, 1));\n\n      if (!c->RankKnown(c->output(1))) {\n        return errors::InvalidArgument(\n            \"Shape must be broadcasted with rank 2, but is rank is unknown.\");\n      }\n\n      if (c->Rank(c->output(1)) != 2) {\n        return errors::InvalidArgument(\n            \"Shape must be broadcasted with rank 2, but is rank \",\n            c->Rank(c->output(1)));\n      }\n      DimensionHandle batch_size = c->Dim(c->output(1), 0);\n      c->set_output(0, c->Vector(batch_size));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"SparseSoftmaxCrossEntropyWithLogits\")\n    .Input(\"features: T\")\n    .Input(\"labels: Tlabels\")\n    .Output(\"loss: T\")\n    .Output(\"backprop: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"Tlabels: {int32, int64} = DT_INT64\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle features;\n      ShapeHandle labels;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 2, &features));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &labels));\n\n      DimensionHandle batch_size;\n      TF_RETURN_IF_ERROR(\n          c->Merge(c->Dim(features, 0), c->Dim(labels, 0), &batch_size));\n      TF_RETURN_IF_ERROR(c->ReplaceDim(features, 0, batch_size, &features));\n\n      c->set_output(0, c->Vector(batch_size));\n      c->set_output(1, features);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"InTopK\")\n    .Input(\"predictions: float\")\n    .Input(\"targets: T\")\n    .Output(\"precision: bool\")\n    .Attr(\"k: int\")\n    .Attr(\"T: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle predictions;\n      ShapeHandle targets;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 2, &predictions));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &targets));\n      DimensionHandle batch_size;\n      TF_RETURN_IF_ERROR(\n          c->Merge(c->Dim(predictions, 0), c->Dim(targets, 0), &batch_size));\n      c->set_output(0, c->Vector(batch_size));\n      return OkStatus();\n    });\n\n// This is the same as `InTopK`, but takes `k` as in input rather than an attr.\nREGISTER_OP(\"InTopKV2\")\n    .Input(\"predictions: float\")\n    .Input(\"targets: T\")\n    .Input(\"k: T\")\n    .Output(\"precision: bool\")\n    .Attr(\"T: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle predictions;\n      ShapeHandle targets;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 2, &predictions));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &targets));\n      DimensionHandle batch_size;\n      TF_RETURN_IF_ERROR(\n          c->Merge(c->Dim(predictions, 0), c->Dim(targets, 0), &batch_size));\n      c->set_output(0, c->Vector(batch_size));\n      return OkStatus();\n    });\n\nnamespace {\n\nStatus TopKShapeFn(InferenceContext* c) {\n  ShapeHandle input;\n  TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &input));\n\n  // Get the k value, either from input tensor or attribute.\n  DimensionHandle k_dim;\n  if (c->num_inputs() >= 2) {\n    TF_RETURN_IF_ERROR(c->MakeDimForScalarInput(1, &k_dim));\n  } else {\n    int32_t k;\n    TF_RETURN_IF_ERROR(c->GetAttr(\"k\", &k));\n    if (k < 0) {\n      return errors::InvalidArgument(\"Need k >= 0, got \", k);\n    }\n    k_dim = c->MakeDim(k);\n  }\n\n  DimensionHandle last_dim = c->Dim(input, -1);\n  if (c->ValueKnown(last_dim) && c->ValueKnown(k_dim) &&\n      c->Value(last_dim) < c->Value(k_dim)) {\n    return errors::InvalidArgument(\n        \"input must have last dimension >= k = \", c->Value(k_dim), \" but is \",\n        c->Value(last_dim));\n  }\n\n  // Replace last_dim with k_dim.\n  ShapeHandle s;\n  TF_RETURN_IF_ERROR(c->Subshape(input, 0, -1, &s));\n  TF_RETURN_IF_ERROR(c->Concatenate(s, c->Vector(k_dim), &s));\n  c->set_output(0, s);\n  c->set_output(1, s);\n  return OkStatus();\n}\n\n// Utility functions for ApproxTopKShape.\n// It is not easy to link xla/client/lib into the tensorflow core lib, so we\n// have to replicate the logic.\n// LINT.IfChange\ninline uint32_t log2_floor(uint64_t value) {\n  return value == 0 ? 0 : Log2Floor(value);\n}\n\ninline uint32_t log2_ceil(uint64_t value) {\n  return value == 0 ? 0 : Log2Ceiling(value);\n}\n\nStatus ApproxTopKShape(shape_inference::InferenceContext* c) {\n  int64_t k;\n  int64_t reduction_dimension;\n  float recall_target;\n  int64_t reduction_input_size_override;\n  bool aggregate_to_topk;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"k\", &k));\n  TF_RETURN_IF_ERROR(c->GetAttr(\"reduction_dimension\", &reduction_dimension));\n  TF_RETURN_IF_ERROR(c->GetAttr(\"recall_target\", &recall_target));\n  TF_RETURN_IF_ERROR(c->GetAttr(\"reduction_input_size_override\",\n                                &reduction_input_size_override));\n  TF_RETURN_IF_ERROR(c->GetAttr(\"aggregate_to_topk\", &aggregate_to_topk));\n  ShapeHandle input_shape;\n  TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &input_shape));\n  if (reduction_dimension < 0) {\n    // Reverse index\n    reduction_dimension += c->Rank(input_shape);\n  }\n  int64_t reduction_dim_value =\n      c->Value(c->Dim(input_shape, reduction_dimension));\n\n  if (reduction_dim_value < k) {\n    return errors::InvalidArgument(\"input must have last dimension >= k = \", k,\n                                   \" but was \", reduction_dim_value);\n  }\n\n  int64_t output_dim_value = [&] {\n    if (aggregate_to_topk) {\n      return k;\n    }\n    int64_t tpu_tiling = c->Rank(input_shape) == 1 ? 1024 : 128;\n    if (reduction_dim_value <= tpu_tiling || recall_target == 1.0) {\n      return reduction_dim_value;\n    }\n    if (k == 1) {\n      return tpu_tiling;\n    }\n    uint64_t logical_input_size = reduction_input_size_override >= 0\n                                      ? reduction_input_size_override\n                                      : reduction_dim_value;\n    uint64_t m = std::min<uint64_t>(\n        std::max<uint64_t>(\n            static_cast<uint64_t>((1.0 - k) /\n                                  std::log(static_cast<double>(recall_target))),\n            tpu_tiling),\n        reduction_dim_value);\n    uint32_t log2_reduction = log2_floor(logical_input_size / m);\n    if (log2_reduction == 0) {\n      return reduction_dim_value;\n    }\n    log2_reduction = std::min<uint32_t>(\n        log2_reduction, log2_ceil(reduction_dim_value / tpu_tiling));\n    return tensorflow::MathUtil::CeilOfRatio<int64_t>(\n               tensorflow::MathUtil::CeilOfRatio<int64_t>(reduction_dim_value,\n                                                          tpu_tiling),\n               (1 << log2_reduction)) *\n           tpu_tiling;\n  }();\n\n  auto output_dim = c->MakeDim(output_dim_value);\n\n  ShapeHandle output_shape;\n  TF_RETURN_IF_ERROR(c->ReplaceDim(input_shape, reduction_dimension, output_dim,\n                                   &output_shape));\n  c->set_output(0, output_shape);\n  c->set_output(1, output_shape);\n  return OkStatus();\n}\n// LINT.ThenChange(//tensorflow/compiler/xla/client/lib/approx_topk_shape.cc)\n\n}  // namespace\n\nREGISTER_OP(\"TopK\")\n    .Input(\"input: T\")\n    .Output(\"values: T\")\n    .Output(\"indices: int32\")\n    .Attr(\"k: int >= 0\")\n    .Attr(\"sorted: bool = true\")\n    .Attr(\"T: realnumbertype\")\n    .Deprecated(7, \"Use TopKV2 instead\")\n    .SetShapeFn(TopKShapeFn);\n\n// This is the same as `TopK`, but takes `k` as in input rather than an attr.\nREGISTER_OP(\"TopKV2\")\n    .Input(\"input: T\")\n    .Input(\"k: int32\")\n    .Output(\"values: T\")\n    .Output(\"indices: int32\")\n    .Attr(\"sorted: bool = true\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn(TopKShapeFn);\n\nREGISTER_OP(\"ApproxTopK\")\n    .Input(\"input: T\")\n    .Output(\"values: T\")\n    .Output(\"indices: int32\")\n    .Attr(\"k: int >= 0\")\n    .Attr(\"reduction_dimension: int = -1\")\n    .Attr(\"recall_target: float = 0.95\")\n    .Attr(\"is_max_k: bool = true\")\n    .Attr(\"reduction_input_size_override: int = -1\")\n    .Attr(\"aggregate_to_topk: bool = true\")\n    .Attr(\"T: {half, bfloat16, float}\")\n    .SetShapeFn(ApproxTopKShape);\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"NthElement\")\n    .Input(\"input: T\")\n    .Input(\"n: int32\")\n    .Output(\"values: T\")\n    .Attr(\"reverse: bool = false\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input;\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &input));\n\n      // Get the n value from input tensor, and make sure which is a scalar.\n      DimensionHandle n_dim;\n      TF_RETURN_IF_ERROR(c->MakeDimForScalarInput(1, &n_dim));\n\n      // The last dimension of input tensor must be greater than N.\n      DimensionHandle last_dim = c->Dim(input, -1);\n      if (c->ValueKnown(last_dim) && c->ValueKnown(n_dim) &&\n          c->Value(last_dim) <= c->Value(n_dim)) {\n        return errors::InvalidArgument(\n            \"Input must have last dimension > n = \", c->Value(n_dim),\n            \" but is \", c->Value(last_dim));\n      }\n\n      // Reduce last_dim for output tensor\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->Subshape(input, 0, -1, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"FractionalMaxPool\")\n    .Input(\"value: T\")\n    .Output(\"output: T\")\n    .Output(\"row_pooling_sequence: int64\")\n    .Output(\"col_pooling_sequence: int64\")\n    .Attr(\"pooling_ratio: list(float) >=4\")\n    .Attr(\"pseudo_random: bool = false\")\n    .Attr(\"overlapping: bool = false\")\n    .Attr(\"deterministic: bool = false\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .Attr(\"T: {float, double, int32, int64}\")\n    .SetShapeFn(FractionalPoolShapeFn);\n\nREGISTER_OP(\"FractionalMaxPoolGrad\")\n    .Input(\"orig_input: T\")\n    .Input(\"orig_output: T\")\n    .Input(\"out_backprop: T\")\n    .Input(\"row_pooling_sequence: int64\")\n    .Input(\"col_pooling_sequence: int64\")\n    .Output(\"output: T\")\n    .Attr(\"overlapping: bool = false\")\n    .Attr(\"T: {float, double, int32, int64}\")\n    .SetShapeFn([](InferenceContext* c) {\n      return shape_inference::UnchangedShapeWithRank(c, 4);\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"FractionalAvgPool\")\n    .Input(\"value: T\")\n    .Output(\"output: T\")\n    .Output(\"row_pooling_sequence: int64\")\n    .Output(\"col_pooling_sequence: int64\")\n    .Attr(\"pooling_ratio: list(float) >=4\")\n    .Attr(\"pseudo_random: bool = false\")\n    .Attr(\"overlapping: bool = false\")\n    .Attr(\"deterministic: bool = false\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .Attr(\"T: {float, double, int32, int64}\")\n    .SetShapeFn(FractionalPoolShapeFn);\n\nREGISTER_OP(\"FractionalAvgPoolGrad\")\n    .Input(\"orig_input_tensor_shape: int64\")\n    .Input(\"out_backprop: T\")\n    .Input(\"row_pooling_sequence: int64\")\n    .Input(\"col_pooling_sequence: int64\")\n    .Output(\"output: T\")\n    .Attr(\"overlapping: bool = false\")\n    .Attr(\"T: {float, double, int32, int64}\")\n    .SetShapeFn([](InferenceContext* c) {\n      if (c->input_tensor(0) != nullptr) {\n        ShapeHandle out;\n        TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &out));\n        c->set_output(0, out);\n      } else {\n        c->set_output(0, c->UnknownShapeOfRank(4));\n      }\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedAvgPool\")\n    .Input(\"input: T\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Output(\"output: T\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"T: quantizedtype\")\n    .Attr(\"ksize: list(int)\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn(shape_inference::QuantizedAvgPoolShape);\n\nREGISTER_OP(\"QuantizedBiasAdd\")\n    .Input(\"input: T1\")\n    .Input(\"bias: T2\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_bias: float\")\n    .Input(\"max_bias: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_out: float\")\n    .Output(\"max_out: float\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"out_type: quantizedtype\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::BiasAddShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedConv2D\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::QuantizedConv2DShape);\n\nREGISTER_OP(\"QuantizedMaxPool\")\n    .Input(\"input: T\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Output(\"output: T\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"T: quantizedtype\")\n    .Attr(\"ksize: list(int)\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MaxPoolShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedRelu\")\n    .Input(\"features: Tinput\")\n    .Input(\"min_features: float\")\n    .Input(\"max_features: float\")\n    .Output(\"activations: out_type\")\n    .Output(\"min_activations: float\")\n    .Output(\"max_activations: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::UnchangedShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedRelu6\")\n    .Input(\"features: Tinput\")\n    .Input(\"min_features: float\")\n    .Input(\"max_features: float\")\n    .Output(\"activations: out_type\")\n    .Output(\"min_activations: float\")\n    .Output(\"max_activations: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::UnchangedShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedReluX\")\n    .Input(\"features: Tinput\")\n    .Input(\"max_value: float\")\n    .Input(\"min_features: float\")\n    .Input(\"max_features: float\")\n    .Output(\"activations: out_type\")\n    .Output(\"min_activations: float\")\n    .Output(\"max_activations: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::UnchangedShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedBatchNormWithGlobalNormalization\")\n    .Input(\"t: Tinput\")\n    .Input(\"t_min: float\")\n    .Input(\"t_max: float\")\n    .Input(\"m: Tinput\")\n    .Input(\"m_min: float\")\n    .Input(\"m_max: float\")\n    .Input(\"v: Tinput\")\n    .Input(\"v_min: float\")\n    .Input(\"v_max: float\")\n    .Input(\"beta: Tinput\")\n    .Input(\"beta_min: float\")\n    .Input(\"beta_max: float\")\n    .Input(\"gamma: Tinput\")\n    .Input(\"gamma_min: float\")\n    .Input(\"gamma_max: float\")\n    .Output(\"result: out_type\")\n    .Output(\"result_min: float\")\n    .Output(\"result_max: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"out_type: quantizedtype\")\n    .Attr(\"variance_epsilon: float\")\n    .Attr(\"scale_after_normalization: bool\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input));\n\n      DimensionHandle last_dim = c->Dim(input, 3);\n      for (int i = 1; i < 5; ++i) {  // covers m, v, beta, gamma\n        ShapeHandle vec;\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(i * 3), 1, &vec));\n        TF_RETURN_IF_ERROR(c->Merge(last_dim, c->Dim(vec, 0), &last_dim));\n      }\n\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(c->ReplaceDim(input, 3, last_dim, &out));\n      c->set_output(0, out);\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n\n      return OkStatus();\n    });\n\n#ifdef INTEL_MKL\nREGISTER_OP(\"_MklDepthwiseConv2dNative\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter: uint8\")\n    .Output(\"output: T\")\n    .Output(\"filter_output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_filter_output: uint8\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"is_filter_const: bool = false\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::DepthwiseConv2DNativeShapeWithExplicitPadding);\n\nREGISTER_OP(\"_MklConv2D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter: uint8\")\n    .Output(\"output: T\")\n    .Output(\"filter_output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_filter_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(\"is_filter_const: bool = false\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv2DShapeWithExplicitPadding)\n    .Doc(R\"doc(\nMKL version of Conv2D operator. Uses MKL DNN APIs to perform 2D convolution.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklNativeConv2D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(\"is_filter_const: bool = false\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv2DShapeWithExplicitPadding)\n    .Doc(R\"doc(\n    MKL version of Conv2D operator for Eager mode. Uses MKL DNN APIs to perform 2D convolution.\n\n    NOTE Do not invoke this operator directly in Python. Eager Op rewrite is\n    expected to invoke these operators.\n    )doc\");\n\nREGISTER_OP(\"__MklDummyConv2DWithBias\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"bias: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(\"is_filter_const: bool = false\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv2DShapeWithExplicitPadding)\n    .Doc(R\"doc(\nDummy node that enables fusing Conv2D and BiasAdd operator for MKL. This node\ndoes not perform anything. It is just created as an intermediate output of\nmerging Conv2D and BiasAdd.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklConv2DWithBias\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"bias: T\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter: uint8\")\n    .Input(\"mkl_bias: uint8\")\n    .Output(\"output: T\")\n    .Output(\"filter_output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_filter_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(\"is_filter_const: bool = false\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv2DShapeWithExplicitPadding)\n    .Doc(R\"doc(\nMKL version of Conv2D and BiasAdd operator. Uses MKL DNN APIs to perform\n2D convolution and add Bias to the output of convolution.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"__MklDummyPadWithConv2D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"paddings: Tpaddings\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"Tpaddings: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::Conv2DShape)\n    .Doc(R\"doc(\nDummy node that enables fusing Pad and Conv2D operator for MKL. This node\ndoes not perform anything. It is just created as an intermediate output of\nmerging Pad and Conv2D.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklPadWithConv2D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"paddings: Tpaddings\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter: uint8\")\n    .Input(\"mkl_paddings: uint8\")\n    .Output(\"output: T\")\n    .Output(\"filter_output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_filter_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"is_filter_const: bool = false\")\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"Tpaddings: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::Conv2DShape)\n    .Doc(R\"doc(\nMKL version of Pad and Conv2D operator. Uses MKL DNN APIs to perform\nPad and 2D convolution to the output of convolution.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklConv2DBackpropFilter\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter_size: uint8\")\n    .Input(\"mkl_out_backprop: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 4, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nMKL version of Conv2DBackpropFilter. Uses MKL DNN APIs to compute the\ngradients of convolution with respect to the filter.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklNativeConv2DBackpropFilter\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 4, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nMKL version of Conv2DBackpropFilter for Eager mode. Uses MKL DNN APIs\nto compute the gradients of convolution with respect to the filter.\n\nNOTE Do not invoke this operator directly in Python. Eager Op rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"__MklDummyConv2DBackpropFilterWithBias\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Output(\"bias_grad: T\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input_shape;\n      // Fetch the data_format attribute, which may not exist.\n      string data_format;\n      Status s = c->GetAttr(\"data_format\", &data_format);\n\n      if (s.ok() && data_format == \"NCHW\") {\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input_shape));\n        c->set_output(1, c->Vector(c->Dim(input_shape, -3)));\n      } else {\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input_shape));\n        c->set_output(1, c->Vector(c->Dim(input_shape, -1)));\n      }\n      ShapeHandle sh;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &sh));\n      TF_RETURN_IF_ERROR(c->WithRank(sh, 4, &sh));\n      c->set_output(0, sh);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nDummy node that enables fusing Conv2DBackpropFilter and BiasAddGrad operator\nfor MKL. This node does not perform anything. It is just created as an\nintermediate output of merging Conv2DBackpropFilter and BiasAddGrad.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklConv2DBackpropFilterWithBias\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter_size: uint8\")\n    .Input(\"mkl_out_backprop: uint8\")\n    .Output(\"output: T\")\n    .Output(\"bias_grad: T\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_bias_grad: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv2DBackpropFilterWithBiasShape)\n    .Doc(R\"doc(\nMKL version of Conv2DBackpropFilterWithBias. Uses MKL DNN APIs to compute the\ngradients of convolution with respect to the filter.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\n#ifdef INTEL_MKL_ML_ONLY\nREGISTER_OP(\"_MklConv2DWithBiasBackpropBias\")\n    .Input(\"out_backprop: T\")\n    .Input(\"mkl_out_backprop: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"T: {half, float, double}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Doc(R\"doc(\nMKL version of Conv2DBackpropBias. Uses MKL DNN APIs to compute the\ngradients of convolution with respect to the bias.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n#endif\n\nREGISTER_OP(\"_MklConv2DBackpropInput\")\n    .Input(\"input_sizes: int32\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Input(\"mkl_input_sizes: uint8\")\n    .Input(\"mkl_filter: uint8\")\n    .Input(\"mkl_out_backprop: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 4, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nMKL version of Convolution2D backward input. Uses MKL DNN APIs to compute the\ngradients of convolution with respect to the input.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklNativeConv2DBackpropInput\")\n    .Input(\"input_sizes: int32\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 4, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nMKL version of Convolution2D backward input for Eager mode. Uses MKL DNN APIs\nto compute the gradients of convolution with respect to the input.\n\nNOTE Do not invoke this operator directly in Python. Eager op rewrite is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklConv3D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter: uint8\")\n    .Output(\"output: T\")\n    .Output(\"filter_output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_filter_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(\"is_filter_const: bool = false\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv3DShape)\n    .Doc(R\"doc(\nMKL version of Conv3D operator. Uses MKL DNN APIs to perform 3D convolution.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklConv3DBackpropInputV2\")\n    .Input(\"input_sizes: Tshape\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Input(\"mkl_input_sizes: uint8\")\n    .Input(\"mkl_filter: uint8\")\n    .Input(\"mkl_out_backprop: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .Attr(\"Tshape: {int32, int64} = DT_INT32\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 5, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nMKL version of Convolution3D backward input. Uses MKL DNN APIs to compute the\ngradients of convolution with respect to the input.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklConv3DBackpropFilterV2\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter_size: uint8\")\n    .Input(\"mkl_out_backprop: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 5, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nMKL version of Conv3DBackpropFilter. Uses MKL DNN APIs to compute the\ngradients of convolution with respect to the filter.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklRelu\")\n    .Input(\"features: T\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"activations: T\")\n    .Output(\"mkl_activations: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .Doc(R\"doc(\nMKL version of Relu operator. Uses MKL DNN APIs to implement Relu operator.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklReluGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Input(\"mkl_gradients: uint8\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"backprops: T\")\n    .Output(\"mkl_backprops: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn)\n    .Doc(R\"doc(\nMKL version of ReluGrad operator. Uses MKL DNN APIs to compute rectified\nlinear gradients for Relu operation.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklRelu6\")\n    .Input(\"features: T\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"activations: T\")\n    .Output(\"mkl_activations: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .Doc(R\"doc(\nMKL version of Relu6 operator. Uses MKL DNN APIs to implement Relu6 operator.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklRelu6Grad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Input(\"mkl_gradients: uint8\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"backprops: T\")\n    .Output(\"mkl_backprops: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn)\n    .Doc(R\"doc(\nMKL version of Relu6Grad operator. Uses MKL DNN APIs to compute rectified\nlinear gradients for Relu6 operation.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklLeakyRelu\")\n    .Input(\"features: T\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"activations: T\")\n    .Output(\"mkl_activations: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .Attr(\"alpha: float = 0.2\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .Doc(R\"doc(\nMKL version of LeakyRelu operator. Uses MKL DNN APIs to implement\nLeakyRelu operator.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklLeakyReluGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Input(\"mkl_gradients: uint8\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"backprops: T\")\n    .Output(\"mkl_backprops: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .Attr(\"alpha: float = 0.2\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn)\n    .Doc(R\"doc(\nMKL version of LeakyReluGrad operator. Uses MKL DNN APIs to compute rectified\nlinear gradients for LeakyReluGrad operation.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklElu\")\n    .Input(\"features: T\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"activations: T\")\n    .Output(\"mkl_activations: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .Doc(R\"doc(\nMKL version of Elu operator. Uses MKL DNN APIs to implement Elu operator.\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklEluGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Input(\"mkl_gradients: uint8\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"backprops: T\")\n    .Output(\"mkl_backprops: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn)\n    .Doc(R\"doc(\nMKL version of EluGrad operator. Uses MKL DNN APIs to compute Elu\ngradients for Elu operation.\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklSoftmax\")\n    .Input(\"logits: T\")\n    .Input(\"mkl_logits: uint8\")\n    .Output(\"softmax: T\")\n    .Output(\"mkl_softmax: uint8\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .SetShapeFn([](InferenceContext* c) {\n      return shape_inference::UnchangedShapeWithRankAtLeast(c, 1);\n    })\n    .Doc(R\"doc(\nMKL version of ReluGrad operator. Uses MKL DNN APIs to compute rectified\nlinear gradients for Relu operation.\n)doc\");\n\nREGISTER_OP(\"_MklTanh\")\n    .Input(\"features: T\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"activations: T\")\n    .Output(\"mkl_activations: uint8\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .Doc(R\"doc(\nMKL version of Tanh operator. Uses MKL DNN APIs to implement Tanh operator.\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklTanhGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Input(\"mkl_gradients: uint8\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"backprops: T\")\n    .Output(\"mkl_backprops: uint8\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn)\n    .Doc(R\"doc(\nMKL version of TanhGrad operator. Uses MKL DNN APIs to compute tanh\ngradients for Tanh operation.\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklMaxPool\")\n    .Attr(\"T: {float, half, bfloat16} = DT_FLOAT\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(\"workspace_enabled: bool = false\")\n    .Input(\"input: T\")\n    .Input(\"mkl_input: uint8\")\n    .Output(\"output: T\")\n#ifdef INTEL_MKL_ML_ONLY\n    .Output(\"workspace: T\")\n#else\n    .Output(\"workspace: uint8\")\n#endif\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_workspace: uint8\")\n    .SetShapeFn(shape_inference::MaxPoolShape)\n    .Doc(R\"doc(\nMKL version of MaxPool operator. Uses MKL DNN APIs to perform max pooling\non the input.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklMaxPoolGrad\")\n    .Attr(\"T: {float, half, bfloat16} = DT_FLOAT\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(\"workspace_enabled: bool = false\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Input(\"orig_input: T\")\n    .Input(\"orig_output: T\")\n    .Input(\"grad: T\")\n#ifdef INTEL_MKL_ML_ONLY\n    .Input(\"workspace: T\")\n#else\n    .Input(\"workspace: uint8\")\n#endif\n    .Input(\"mkl_orig_input: uint8\")\n    .Input(\"mkl_orig_output: uint8\")\n    .Input(\"mkl_grad: uint8\")\n    .Input(\"mkl_workspace: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .SetShapeFn(shape_inference::MaxPoolGradShape)\n    .Doc(R\"doc(\noneDNN version of MaxPoolGrad. Uses oneDNN APIs to compute gradients of\nMaxPool operator.\n\n*NOTE*: Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklAvgPool\")\n    .Input(\"value: T\")\n    .Input(\"mkl_input: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"T: {float, half, double, bfloat16}\")\n    .SetShapeFn(shape_inference::AvgPoolShape)\n    .Doc(R\"doc(\nMKL version of AvgPool operator. Uses MKL DNN APIs to perform average pooling\non the input.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklAvgPoolGrad\")\n    .Input(\"orig_input_shape: int32\")\n    .Input(\"grad: T\")\n    .Input(\"mkl_orig_input: uint8\")\n    .Input(\"mkl_grad: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"T: {float, half, double, bfloat16}\")\n    .SetShapeFn(shape_inference::AvgPoolGradShape)\n    .Doc(R\"doc(\noneDNN version of AvgPoolGrad operator. Uses oneDNN APIs to compute gradients\nof AvgPool function.\n\n*NOTE*: Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklAvgPool3D\")\n    .Input(\"value: T\")\n    .Input(\"mkl_input: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {float, half, double, bfloat16}\")\n    .SetShapeFn(shape_inference::Pool3DShape)\n    .Doc(R\"doc(\nMKL version of AvgPool3D operator. Uses MKL DNN APIs to perform average pooling\non the input.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklAvgPool3DGrad\")\n    .Input(\"orig_input_shape: int32\")\n    .Input(\"grad: T\")\n    .Input(\"mkl_orig_input: uint8\")\n    .Input(\"mkl_grad: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {float, half, double, bfloat16}\")\n    .SetShapeFn(shape_inference::AvgPool3DGradShape)\n    .Doc(R\"doc(\noneDNN version of AvgPool3DGrad operator. Uses oneDNN APIs to compute gradients\nof AvgPool function.\n\n*NOTE*: Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklMaxPool3D\")\n    .Input(\"input: T\")\n    .Input(\"mkl_input: uint8\")\n    .Output(\"output: T\")\n    .Output(\"workspace: uint8\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_workspace: uint8\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float}\")\n    .Attr(\"workspace_enabled: bool = false\")\n    .SetShapeFn(shape_inference::Pool3DShape)\n    .Doc(R\"doc(\nMKL version of MaxPool3D operator. Uses MKL DNN APIs to perform average pooling\non the input.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklMaxPool3DGrad\")\n    .Input(\"orig_input: TInput\")\n    .Input(\"orig_output: TInput\")\n    .Input(\"grad: T\")\n    .Input(\"workspace: uint8\")\n    .Input(\"mkl_orig_input: uint8\")\n    .Input(\"mkl_orig_output: uint8\")\n    .Input(\"mkl_grad: uint8\")\n    .Input(\"mkl_workspace: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float} = DT_FLOAT\")\n    .Attr(\"TInput: {half, bfloat16, float} = DT_FLOAT\")\n    .Attr(\"workspace_enabled: bool = false\")\n    .SetShapeFn(shape_inference::MaxPool3DGradShape)\n    .Doc(R\"doc(\noneDNN version of MaxPool3DGrad operator. Uses oneDNN APIs to compute gradients\nof MaxPool3D function.\n\n*NOTE*: Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklLRN\")\n    .Input(\"input: T\")\n    .Input(\"mkl_input: uint8\")\n    .Output(\"output: T\")\n    .Output(\"workspace: uint8\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_workspace: uint8\")\n    .Attr(\"depth_radius: int = 5\")\n    .Attr(\"bias: float = 1.0\")\n    .Attr(\"alpha: float = 1.0\")\n    .Attr(\"beta: float = 0.5\")\n    .Attr(\"workspace_enabled: bool = false\")\n    .Attr(\"T: {float, half} = DT_FLOAT\")\n    .SetShapeFn([](InferenceContext* c) {\n      return UnchangedShapeWithRank(c, 4);\n    })\n    .Doc(R\"doc(\nMKL version of LRN operator. Uses MKL DNN APIs to perform local response\nnormalization.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklLRNGrad\")\n    .Input(\"input_grads: T\")\n    .Input(\"input_image: T\")\n    .Input(\"output_image: T\")\n    .Input(\"workspace: uint8\")\n    .Input(\"mkl_input_grads: uint8\")\n    .Input(\"mkl_input_image: uint8\")\n    .Input(\"mkl_output_image: uint8\")\n    .Input(\"mkl_workspace: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"depth_radius: int = 5\")\n    .Attr(\"bias: float = 1.0\")\n    .Attr(\"alpha: float = 1.0\")\n    .Attr(\"beta: float = 0.5\")\n    .Attr(\"workspace_enabled: bool = false\")\n    .Attr(\"T: {float, half} = DT_FLOAT\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &s));  // input_grads\n      TF_RETURN_IF_ERROR(c->Merge(s, c->input(1), &s));     // input_image\n      TF_RETURN_IF_ERROR(c->Merge(s, c->input(2), &s));     // output_image\n      c->set_output(0, s);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nMKL version of LRNGrad operator. Uses MKL DNN APIs to compute gradient for\nlocal response normalization.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklFusedBatchNorm\")\n    .Input(\"x: T\")\n    .Input(\"scale: T\")\n    .Input(\"offset: T\")\n    .Input(\"mean: T\")\n    .Input(\"variance: T\")\n    .Input(\"mkl_x: uint8\")\n    .Input(\"mkl_scale: uint8\")\n    .Input(\"mkl_offset: uint8\")\n    .Input(\"mkl_mean: uint8\")\n    .Input(\"mkl_variance: uint8\")\n    .Output(\"y: T\")\n    .Output(\"batch_mean: T\")\n    .Output(\"batch_variance: T\")\n    .Output(\"reserve_space_1: T\")\n    .Output(\"reserve_space_2: T\")\n    .Output(\"mkl_y: uint8\")\n    .Output(\"mkl_batch_mean: uint8\")\n    .Output(\"mkl_batch_variance: uint8\")\n    .Output(\"mkl_reserve_space_1: uint8\")\n    .Output(\"mkl_reserve_space_2: uint8\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(\"data_format: string = 'NHWC'\")\n    .Attr(\"exponential_avg_factor: float = 1.0\")\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormShape)\n    .Doc(R\"doc(\noneDNN version of FusedBatchNorm operator. Uses oneDNN APIs to perform fused\nbatch normalization.\n\n*NOTE*: Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklFusedBatchNormGrad\")\n    .Input(\"y_backprop: T\")\n    .Input(\"x: T\")\n    .Input(\"scale: T\")\n    .Input(\"reserve_space_1: T\")\n    .Input(\"reserve_space_2: T\")\n    .Input(\"mkl_y_backprop: uint8\")\n    .Input(\"mkl_x: uint8\")\n    .Input(\"mkl_scale: uint8\")\n    .Input(\"mkl_reserve_space_1: uint8\")\n    .Input(\"mkl_reserve_space_2: uint8\")\n    .Output(\"x_backprop: T\")\n    .Output(\"scale_backprop: T\")\n    .Output(\"offset_backprop: T\")\n    .Output(\"reserve_space_3: T\")\n    .Output(\"reserve_space_4: T\")\n    .Output(\"mkl_x_backprop: uint8\")\n    .Output(\"mkl_scale_backprop: uint8\")\n    .Output(\"mkl_offset_backprop: uint8\")\n    .Output(\"mkl_reserve_space_3: uint8\")\n    .Output(\"mkl_reserve_space_4: uint8\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(\"data_format: string = 'NHWC'\")\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormGradShape)\n    .Doc(R\"doc(\noneDNN version of FusedBatchNormGrad operator. Uses oneDNN APIs to compute\ngradients for fused batch normalization.\n\n*NOTE*: Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklFusedBatchNormV2\")\n    .Input(\"x: T\")\n    .Input(\"scale: U\")\n    .Input(\"offset: U\")\n    .Input(\"mean: U\")\n    .Input(\"variance: U\")\n    .Input(\"mkl_x: uint8\")\n    .Input(\"mkl_scale: uint8\")\n    .Input(\"mkl_offset: uint8\")\n    .Input(\"mkl_mean: uint8\")\n    .Input(\"mkl_variance: uint8\")\n    .Output(\"y: T\")\n    .Output(\"batch_mean: U\")\n    .Output(\"batch_variance: U\")\n    .Output(\"reserve_space_1: U\")\n    .Output(\"reserve_space_2: U\")\n    .Output(\"mkl_y: uint8\")\n    .Output(\"mkl_batch_mean: uint8\")\n    .Output(\"mkl_batch_variance: uint8\")\n    .Output(\"mkl_reserve_space_1: uint8\")\n    .Output(\"mkl_reserve_space_2: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"U: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"exponential_avg_factor: float = 1.0\")\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormShape);\n\nREGISTER_OP(\"_MklFusedBatchNormGradV2\")\n    .Input(\"y_backprop: T\")\n    .Input(\"x: T\")\n    .Input(\"scale: float\")\n    .Input(\"reserve_space_1: U\")\n    .Input(\"reserve_space_2: U\")\n    .Input(\"mkl_y_backprop: uint8\")\n    .Input(\"mkl_x: uint8\")\n    .Input(\"mkl_scale: uint8\")\n    .Input(\"mkl_reserve_space_1: uint8\")\n    .Input(\"mkl_reserve_space_2: uint8\")\n    .Output(\"x_backprop: T\")\n    .Output(\"scale_backprop: U\")\n    .Output(\"offset_backprop: U\")\n    .Output(\"reserve_space_3: U\")\n    .Output(\"reserve_space_4: U\")\n    .Output(\"mkl_x_backprop: uint8\")\n    .Output(\"mkl_scale_backprop: uint8\")\n    .Output(\"mkl_offset_backprop: uint8\")\n    .Output(\"mkl_reserve_space_3: uint8\")\n    .Output(\"mkl_reserve_space_4: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"U: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormGradShape);\n\nREGISTER_OP(\"_MklToTf\")\n    .Input(\"input: T\")\n    .Input(\"mkl_input: uint8\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, float, double, bfloat16, qint8, quint8, qint32}\")\n    .Attr(GetConvnetDataFormat2D3DAttrString())\n    .SetShapeFn(shape_inference::UnknownShape)\n    .Doc(R\"doc(\nMKL operator to convert a tensor from MKL layout to TensorFlow layout.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklInputConversion\")\n    .Input(\"input_0: T\")\n    .Input(\"input_1: T\")\n    .Input(\"mkl_input_0: uint8\")\n    .Input(\"mkl_input_1: uint8\")\n    .Output(\"output_0: T\")\n    .Output(\"output_1: T\")\n    .Output(\"mkl_output_0: uint8\")\n    .Output(\"mkl_output_1: uint8\")\n    // All datatypes supported by element-wise ops\n    .Attr(\n        \"T: {half, float, bfloat16, double, uint8, int8, uint16, int16, int32, \"\n        \"int64, complex64, complex128}\")\n    .Attr(GetConvnetDataFormat2D3DAttrString())\n    .SetShapeFn(shape_inference::UnknownShape)\n    .Doc(R\"doc(\nMKL operator to process the inputs to an elementwise MKL op. Both inputs\nneed to be either in TF or in MKL format. This op is added before every\nelement-wise MKL op.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\n#endif  // INTEL_MKL\nREGISTER_OP(\"QuantizedConv2DAndRequantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT8\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\n// Fusion of Quantized Conv2D and BiasAdd.\nREGISTER_OP(\"QuantizedConv2DWithBias\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: float\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(6), 1, &channel));\n      c->set_output(1, channel);\n      c->set_output(2, channel);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedConv2DWithBiasAndRequantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"out_type: quantizedtype = DT_QINT8\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(6), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\n// Fusion of Quantized Conv2D and Relu.\nREGISTER_OP(\"QuantizedConv2DAndRelu\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(4), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      c->set_output(1, channel);\n      c->set_output(2, channel);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedConv2DAndReluAndRequantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(4), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\n// Fusion of Quantized Conv2D, BiasAdd and Relu.\nREGISTER_OP(\"QuantizedConv2DWithBiasAndRelu\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: float\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(6), 1, &channel));\n      c->set_output(1, channel);\n      c->set_output(2, channel);\n      return OkStatus();\n    });\n\n// Fusion of Quantized Conv2D, BiasAdd, Relu, and Requantize.\nREGISTER_OP(\"QuantizedConv2DWithBiasAndReluAndRequantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(6), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\n// Fusion of Quantized Conv2D, BiasAdd, Sum, and Relu.\nREGISTER_OP(\"QuantizedConv2DWithBiasSumAndRelu\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: float\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"summand: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(6), 1, &channel));\n      c->set_output(1, channel);\n      c->set_output(2, channel);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedConv2DWithBiasSumAndReluAndRequantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Input(\"summand: Tsummand\")\n    .Input(\"min_summand: float\")\n    .Input(\"max_summand: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"Tsummand: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(6), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedConv2DWithBiasSignedSumAndReluAndRequantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Input(\"summand: Tsummand\")\n    .Input(\"min_summand: float\")\n    .Input(\"max_summand: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"Tsummand: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(6), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 0, &unused));\n      // Since activations are not requantized per channel, `min_output`\n      // and `max_output` are scalars.\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\n// Fusion of Quantized MatMul and BiasAdd.\nREGISTER_OP(\"QuantizedMatMulWithBias\")\n    .Input(\"a: T1\")\n    .Input(\"b: T2\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_a: float\")\n    .Input(\"max_a: float\")\n    .Input(\"min_b: float\")\n    .Input(\"max_b: float\")\n    .Output(\"out: Toutput\")\n    .Output(\"min_out: float\")\n    .Output(\"max_out: float\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"Toutput: quantizedtype = DT_QINT32\")\n    .Attr(\"transpose_a: bool = false\")\n    .Attr(\"transpose_b: bool = false\")\n    .Attr(\"input_quant_mode: {'MIN_FIRST', 'SCALED'} = 'MIN_FIRST'\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MatMulShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedMatMulWithBiasAndRelu\")\n    .Input(\"a: T1\")\n    .Input(\"b: T2\")\n    .Input(\"bias: float\")\n    .Input(\"min_a: float\")\n    .Input(\"max_a: float\")\n    .Input(\"min_b: float\")\n    .Input(\"max_b: float\")\n    .Output(\"out: Toutput\")\n    .Output(\"min_out: float\")\n    .Output(\"max_out: float\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"Toutput: quantizedtype = DT_QINT32\")\n    .Attr(\"transpose_a: bool = false\")\n    .Attr(\"transpose_b: bool = false\")\n    .Attr(\"input_quant_mode: {'MIN_FIRST', 'SCALED'} = 'MIN_FIRST'\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MatMulShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedMatMulWithBiasAndReluAndRequantize\")\n    .Input(\"a: T1\")\n    .Input(\"b: T2\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_a: float\")\n    .Input(\"max_a: float\")\n    .Input(\"min_b: float\")\n    .Input(\"max_b: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"out: Toutput\")\n    .Output(\"min_out: float\")\n    .Output(\"max_out: float\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"Toutput: quantizedtype = DT_QUINT8\")\n    .Attr(\"transpose_a: bool = false\")\n    .Attr(\"transpose_b: bool = false\")\n    .Attr(\"input_quant_mode: {'MIN_FIRST', 'SCALED'} = 'MIN_FIRST'\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MatMulShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedMatMulWithBiasAndDequantize\")\n    .Input(\"a: T1\")\n    .Input(\"b: T2\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_a: float\")\n    .Input(\"max_a: float\")\n    .Input(\"min_b: float\")\n    .Input(\"max_b: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"out: Toutput\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"Toutput: {float}\")\n    .Attr(\"transpose_a: bool = false\")\n    .Attr(\"transpose_b: bool = false\")\n    .Attr(\"input_quant_mode: {'MIN_FIRST', 'SCALED'} = 'MIN_FIRST'\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MatMulShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 0, &unused));\n\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedMatMulWithBiasAndRequantize\")\n    .Input(\"a: T1\")\n    .Input(\"b: T2\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_a: float\")\n    .Input(\"max_a: float\")\n    .Input(\"min_b: float\")\n    .Input(\"max_b: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"out: Toutput\")\n    .Output(\"min_out: float\")\n    .Output(\"max_out: float\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"Toutput: quantizedtype = DT_QUINT8\")\n    .Attr(\"transpose_a: bool = false\")\n    .Attr(\"transpose_b: bool = false\")\n    .Attr(\"input_quant_mode: {'MIN_FIRST', 'SCALED'} = 'MIN_FIRST'\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MatMulShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedConv2DPerChannel\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(4), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      c->set_output(1, channel);\n      c->set_output(2, channel);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedDepthwiseConv2D\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::DepthwiseConv2DNativeShape);\n\nREGISTER_OP(\"QuantizedDepthwiseConv2DWithBias\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: float\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::DepthwiseConv2DNativeShape);\n\nREGISTER_OP(\"QuantizedDepthwiseConv2DWithBiasAndRelu\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: float\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn(shape_inference::DepthwiseConv2DNativeShape);\n\nREGISTER_OP(\"QuantizedDepthwiseConv2DWithBiasAndReluAndRequantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn(shape_inference::DepthwiseConv2DNativeShape);\n\nREGISTER_OP(\"IsotonicRegression\")\n    .Input(\"input: T\")\n    .Output(\"output: output_dtype\")\n    .Output(\"segments: int32\")\n    .Attr(\"T: realnumbertype\")\n    .Attr(\"output_dtype: {half, bfloat16, float, double} = DT_FLOAT\")\n    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* context) {\n      context->set_output(0, context->input(0));\n      context->set_output(1, context->input(0));\n      return OkStatus();\n    });\n\n}  // namespace tensorflow"