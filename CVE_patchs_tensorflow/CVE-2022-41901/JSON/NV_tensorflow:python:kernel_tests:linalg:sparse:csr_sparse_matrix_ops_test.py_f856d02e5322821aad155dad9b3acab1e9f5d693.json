"# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"CSR sparse matrix tests.\"\"\"\n\nimport numpy as np\nfrom scipy import sparse\n\nfrom tensorflow.core.framework import tensor_pb2\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.python.client import session\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import random_seed\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_util\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import linalg_ops\nfrom tensorflow.python.ops import map_fn\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import sparse_ops\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops.linalg.sparse import sparse_csr_matrix_ops\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.platform import tf_logging\n\nCPU = \"/device:CPU:0\"\nGPU = \"/device:GPU:0\"\n\n\ndef dense_to_csr_sparse_matrix(dense):\n  dense_t = ops.convert_to_tensor(dense)\n  locs = array_ops.stop_gradient(array_ops.where(math_ops.abs(dense_t) > 0))\n  return sparse_csr_matrix_ops.dense_to_csr_sparse_matrix(dense_t, locs)\n\n\ndef _swap(a, i, j):\n  a[i], a[j] = a[j], a[i]\n\n\ndef twist_matrix(matrix, permutation_indices):\n  \"\"\"Permute the rows and columns of a 2D or (batched) 3D Tensor.\"\"\"\n  # Shuffle the rows and columns with the same permutation.\n  if matrix.shape.ndims == 2:\n    # Invert the permutation since `tf.gather` and `tf.gather_nd` need the\n    # mapping from each index `i` to the index that maps to `i`.\n    permutation_indices_inv = array_ops.invert_permutation(permutation_indices)\n    matrix = array_ops.gather(matrix, permutation_indices_inv, axis=0)\n    matrix = array_ops.gather(matrix, permutation_indices_inv, axis=1)\n  elif matrix.shape.ndims == 3:\n    permutation_indices_inv = map_fn.map_fn(array_ops.invert_permutation,\n                                            permutation_indices)\n    # For 3D Tensors, it's easy to shuffle the rows but not the columns. We\n    # permute the rows, transpose, permute the rows again, and transpose back.\n    batch_size = matrix.shape[0]\n    batch_indices = array_ops.broadcast_to(\n        math_ops.range(batch_size)[:, None], permutation_indices.shape)\n    for _ in range(2):\n      matrix = array_ops.gather_nd(\n          matrix,\n          array_ops.stack([batch_indices, permutation_indices_inv], axis=-1))\n      # Transpose the matrix, or equivalently, swap dimensions 1 and 2.\n      matrix = array_ops.transpose(matrix, perm=[0, 2, 1])\n  else:\n    raise ValueError(\"Input matrix must have rank 2 or 3. Got: {}\".format(\n        matrix.shape.ndims))\n\n  return matrix\n\n\nclass CSRSparseMatrixOpsTest(test.TestCase):\n\n  @classmethod\n  def setUpClass(cls):  # pylint: disable=g-missing-super-call\n    cls._gpu_available = test_util.is_gpu_available()\n\n  # TODO(ebrevdo): This will work once we find a way to get rendezvous\n  # working for CSRSparseMatrix and can remove the HostMemory\n  # annotations for the other ops.\n  @test_util.run_in_graph_and_eager_modes\n  def DISABLEDtestFromProto(self):\n    if not self._gpu_available:\n      return\n\n    a_indices = np.array([[0, 0], [2, 3]])\n    a_values = np.asarray([1.0, 5.0], dtype=np.float32)\n    a_dense_shape = np.asarray([5, 6], dtype=np.int64)\n    a_sparse_mat = sparse.coo_matrix(\n        (a_values, (a_indices[:, 0], a_indices[:, 1])), shape=a_dense_shape)\n    a_csr_mat = a_sparse_mat.tocsr()\n    a_col_inds = a_csr_mat.indices\n    a_row_ptrs = a_csr_mat.indptr\n\n    # Format of SparseMatrix:\n    #  type_name == \"tensorflow::CSRSparseMatrix\"\n    #  metadata == b (validated)\n    #  tensors == [dense_shape, row_ptrs, col_indices, values]\n    dense_shape_proto = tensor_util.make_tensor_proto(a_dense_shape)\n    row_ptrs_proto = tensor_util.make_tensor_proto(a_row_ptrs)\n    col_inds_proto = tensor_util.make_tensor_proto(a_col_inds)\n    values_proto = tensor_util.make_tensor_proto(a_values)\n    variant_tensor_data = tensor_pb2.VariantTensorDataProto(\n        type_name=\"tensorflow::CSRSparseMatrix\",\n        metadata=np.asarray(True).tobytes(),\n        tensors=[\n            dense_shape_proto, row_ptrs_proto, col_inds_proto, values_proto\n        ])\n    tensor_proto = tensor_pb2.TensorProto(\n        dtype=dtypes.variant.as_datatype_enum,\n        tensor_shape=tensor_shape.TensorShape([]).as_proto())\n    tensor_proto.variant_val.extend([variant_tensor_data])\n    a_sm = constant_op.constant(tensor_proto)\n    a_rt = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n        a_sm, type=dtypes.float32)\n    self.evaluate(a_rt)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSparseTensorConversion(self):\n    a_indices = np.array([[0, 0], [2, 3], [2, 4], [3, 0]])\n    a_values = [1.0, 5.0, -1.0, -2.0]\n    a_dense_shape = [5, 6]\n    a_sparse_mat = sparse.coo_matrix(\n        (a_values, (a_indices[:, 0], a_indices[:, 1])), shape=a_dense_shape)\n    a_csr_mat = a_sparse_mat.tocsr()\n\n    # Convert 2D SparseTensor to CSR Matrix\n    a_st = sparse_tensor.SparseTensor(a_indices, a_values, a_dense_shape)\n    a_st = math_ops.cast(a_st, dtypes.float32)\n    a_sm = sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(\n        a_st.indices, a_st.values, a_st.dense_shape)\n\n    # Get row indices and columns for batch 0.\n    a_sm_row_ptrs, a_sm_col_inds, a_sm_values = (\n        sparse_csr_matrix_ops.csr_sparse_matrix_components(\n            a_sm, 0, type=a_st.dtype))\n\n    a_sm_row_ptrs_values, a_sm_col_inds_values, a_sm_values_values = (\n        self.evaluate((a_sm_row_ptrs, a_sm_col_inds, a_sm_values)))\n\n    self.assertAllEqual(a_csr_mat.indices, a_sm_col_inds_values)\n    self.assertAllEqual(a_csr_mat.indptr, a_sm_row_ptrs_values)\n    self.assertAllClose(a_values, a_sm_values_values)\n\n    # Convert CSR Matrix to 2D SparseTensor\n    a_st_rt = sparse_csr_matrix_ops.csr_sparse_matrix_to_sparse_tensor(\n        a_sm, type=a_st.dtype)\n    a_st_rt_value = self.evaluate(a_st_rt)\n\n    self.assertAllEqual(a_indices, a_st_rt_value.indices)\n    self.assertAllClose(a_values, a_st_rt_value.values)\n    self.assertAllEqual(a_dense_shape, a_st_rt_value.dense_shape)\n\n  def testSparseTensorConversionInvalidInputShapes(self):\n    values = constant_op.constant(\n        0.554979503, shape=[5], dtype=dtypes.float32)\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 1\"):\n      indices = constant_op.constant(0, shape=[5, 2], dtype=dtypes.int64)\n      dense_shape = constant_op.constant(53, shape=[], dtype=dtypes.int64)\n      csr = sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(\n          indices=indices, values=values, dense_shape=dense_shape)\n      self.evaluate(csr)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 2\"):\n      indices = constant_op.constant(0, shape=[5], dtype=dtypes.int64)\n      dense_shape = constant_op.constant(53, shape=[1], dtype=dtypes.int64)\n      csr = sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(\n          indices=indices, values=values, dense_shape=dense_shape)\n      self.evaluate(csr)\n\n  # TODO(b/139491352): Add handle_data propagation to array_ops.identity.\n  @test_util.run_deprecated_v1\n  def testCSRSparseMatrixResourceVariable(self):\n    if not self._gpu_available:\n      return\n\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [53, 65, 127]\n    a_mats = sparsify(np.random.randn(*dense_shape)).astype(np.float32)\n\n    a_sm = dense_to_csr_sparse_matrix(a_mats)\n    with ops.device(\"/gpu:0\"):\n      v = variable_scope.get_variable(\"sm\", initializer=a_sm, use_resource=True)\n      v_id = array_ops.identity(v)\n      self.assertEqual(\n          sparse_csr_matrix_ops.dense_shape_and_type(v_id).shape, a_mats.shape)\n      a_rt = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n          v, type=dtypes.float32)\n    v_reassign = state_ops.assign(v, v_id).op\n    with self.assertRaisesOpError(\"uninitialized\"):\n      self.evaluate(a_rt)\n    self.evaluate(v.initializer)\n    a_rt_value = self.evaluate(a_rt)\n    self.assertAllClose(a_mats, a_rt_value)\n    self.evaluate(v_reassign)\n    a_rt_reassigned_value = self.evaluate(a_rt)\n    self.assertAllClose(a_mats, a_rt_reassigned_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testBatchSparseTensorConversion(self):\n    a_indices = np.array([[0, 0, 0], [0, 2, 3], [2, 0, 1]])\n    a_values = [1.0, 5.0, 6.0]\n    a_dense_shape = [3, 5, 6]\n    a_sparse_mats = [\n        sparse.coo_matrix(([1.0, 5.0], ([0, 2], [0, 3])),\n                          shape=a_dense_shape[1:]),\n        sparse.coo_matrix(([], ([], [])), shape=a_dense_shape[1:]),\n        sparse.coo_matrix(([6.0], ([0], [1])), shape=a_dense_shape[1:])\n    ]\n    a_csr_mats = [m.tocsr() for m in a_sparse_mats]\n\n    # Convert 3D SparseTensor to CSR Matrix\n    a_st = sparse_tensor.SparseTensor(a_indices, a_values, a_dense_shape)\n    a_st = math_ops.cast(a_st, dtypes.float32)\n    a_sm = sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(\n        a_st.indices, a_st.values, a_st.dense_shape)\n\n    # Get row indices and columns for batches.\n    a_sm_components = [\n        sparse_csr_matrix_ops.csr_sparse_matrix_components(\n            a_sm, i, type=a_st.dtype) for i in range(3)\n    ]\n\n    a_sm_values = self.evaluate(a_sm_components)\n\n    for i, (a_sm_val, a_csr_mat) in enumerate(zip(a_sm_values, a_csr_mats)):\n      tf_logging.info(\"Comparing batch %d\" % i)\n      self.assertAllEqual(a_csr_mat.indptr, a_sm_val.row_ptrs)\n      self.assertAllEqual(a_csr_mat.indices, a_sm_val.col_inds)\n      self.assertAllClose(a_csr_mat.data, a_sm_val.values)\n\n    # Convert CSR batched Matrix to 3D SparseTensor\n    a_st_rt = sparse_csr_matrix_ops.csr_sparse_matrix_to_sparse_tensor(\n        a_sm, type=a_st.dtype)\n    a_st_rt_value = self.evaluate(a_st_rt)\n\n    self.assertAllEqual(a_indices, a_st_rt_value.indices)\n    self.assertAllClose(a_values, a_st_rt_value.values)\n    self.assertAllEqual(a_dense_shape, a_st_rt_value.dense_shape)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSparseTensorConversion(self):\n    # Test two sets of conversions to check behavior of the ops in a\n    # concurrent environment (parallel executions of the ST -> SM ops).\n\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [53, 65, 127]\n\n    mats = [\n        sparsify(np.random.randn(*dense_shape)).astype(np.float32)\n        for _ in range(2)\n    ]\n    csr_mats = [list(map(sparse.csr_matrix, mat)) for mat in mats]\n    mats_t = [ops.convert_to_tensor(mat) for mat in mats]\n    mats_locs = [array_ops.where(mat_t > 0) for mat_t in mats_t]\n    sparse_tensors = list()\n    for mat_t, mat_loc in zip(mats_t, mats_locs):\n      sparse_tensors.append(\n          sparse_tensor.SparseTensor(mat_loc,\n                                     array_ops.gather_nd(mat_t,\n                                                         mat_loc), dense_shape))\n    sparse_matrices = [\n        sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(\n            st.indices, st.values, st.dense_shape) for st in sparse_tensors\n    ]\n    sm_nnz = [\n        sparse_csr_matrix_ops.sparse_matrix_nnz(sm) for sm in sparse_matrices\n    ]\n\n    # Get row indices and columns for batches.\n    sm_components = list()\n    for sm in sparse_matrices:\n      sm_components.append([\n          sparse_csr_matrix_ops.csr_sparse_matrix_components(\n              sm, i, type=dtypes.float32) for i in range(dense_shape[0])\n      ])\n\n    sm_nnz_values, sm_values = self.evaluate((sm_nnz, sm_components))\n\n    for i, (sm_values_i, csr_mats_i) in enumerate(zip(sm_values, csr_mats)):\n      for b, (sm_val, csr_mat) in enumerate(zip(sm_values_i, csr_mats_i)):\n        tf_logging.info(\"Comparing matrix %d batch %d\" % (i, b))\n        self.assertEqual(csr_mat.nnz, sm_nnz_values[i][b])\n        self.assertAllEqual(csr_mat.indptr, sm_val.row_ptrs)\n        self.assertAllEqual(csr_mat.indices, sm_val.col_inds)\n        self.assertAllClose(csr_mat.data, sm_val.values)\n\n    # Convert CSR batched Matrix to 3D SparseTensor\n    st_rt = [\n        sparse_csr_matrix_ops.csr_sparse_matrix_to_sparse_tensor(\n            sm, type=dtypes.float32) for sm in sparse_matrices\n    ]\n\n    st_values, st_rt_values = self.evaluate((sparse_tensors, st_rt))\n\n    for (st_value, st_rt_value) in zip(st_values, st_rt_values):\n      self.assertAllEqual(st_value.indices, st_rt_value.indices)\n      self.assertAllClose(st_value.values, st_rt_value.values)\n      self.assertAllEqual(dense_shape, st_rt_value.dense_shape)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testDenseConversion(self):\n    a_indices = np.array([[0, 0], [2, 3], [2, 4], [3, 0]])\n    a_values = np.array([1.0, 5.0, -1.0, -2.0]).astype(np.float32)\n    a_dense_shape = [5, 6]\n    a_sparse_mat = sparse.coo_matrix(\n        (a_values, (a_indices[:, 0], a_indices[:, 1])), shape=a_dense_shape)\n    a_csr_mat = a_sparse_mat.tocsr()\n    a_dense = a_sparse_mat.todense()\n\n    # Convert 2D SparseTensor to CSR Matrix\n    a_sm = dense_to_csr_sparse_matrix(a_dense)\n\n    # Get row indices and columns for batch 0.\n    a_sm_row_ptrs, a_sm_col_inds, a_sm_values = (\n        sparse_csr_matrix_ops.csr_sparse_matrix_components(\n            a_sm, 0, type=dtypes.float32))\n\n    a_sm_row_ptrs_values, a_sm_col_inds_values, a_sm_values_values = (\n        self.evaluate((a_sm_row_ptrs, a_sm_col_inds, a_sm_values)))\n\n    self.assertAllEqual(a_csr_mat.indices, a_sm_col_inds_values)\n    self.assertAllEqual(a_csr_mat.indptr, a_sm_row_ptrs_values)\n    self.assertAllClose(a_values, a_sm_values_values)\n\n    # Convert CSR Matrix to 2D dense matrix\n    a_rt = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n        a_sm, dtypes.float32)\n    a_rt_value = self.evaluate(a_rt)\n\n    self.assertAllEqual(a_dense, a_rt_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testBatchDenseConversion(self):\n    a_dense_shape = [4, 5, 6]\n    a_sparse_mats = [\n        sparse.coo_matrix(([1.0, 5.0], ([0, 2], [0, 3])),\n                          shape=a_dense_shape[1:]),\n        sparse.coo_matrix(([], ([], [])), shape=a_dense_shape[1:]),\n        sparse.coo_matrix(([6.0], ([0], [1])), shape=a_dense_shape[1:]),\n        sparse.coo_matrix(([], ([], [])), shape=a_dense_shape[1:]),\n    ]\n    a_csr_mats = [m.tocsr() for m in a_sparse_mats]\n    a_dense = np.asarray([m.todense() for m in a_sparse_mats], dtype=np.float32)\n\n    # Convert 3D SparseTensor to CSR Matrix\n    a_sm = dense_to_csr_sparse_matrix(a_dense)\n\n    # Get row indices and columns for batches.\n    a_sm_components = [\n        sparse_csr_matrix_ops.csr_sparse_matrix_components(\n            a_sm, i, type=dtypes.float32) for i in range(3)\n    ]\n\n    a_sm_values = self.evaluate(a_sm_components)\n\n    for i, (a_sm_val, a_csr_mat) in enumerate(zip(a_sm_values, a_csr_mats)):\n      tf_logging.info(\"Comparing batch %d\" % i)\n      self.assertAllEqual(a_csr_mat.indptr, a_sm_val.row_ptrs)\n      self.assertAllEqual(a_csr_mat.indices, a_sm_val.col_inds)\n      self.assertAllClose(a_csr_mat.data, a_sm_val.values)\n\n    # Convert CSR batched Matrix to 3D SparseTensor\n    a_rt = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n        a_sm, type=dtypes.float32)\n    a_rt_value = self.evaluate(a_rt)\n\n    self.assertAllEqual(a_dense, a_rt_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchDenseConversion(self):\n    # Test two sets of conversions to check behavior of the ops in a\n    # concurrent environment (parallel executions of the ST -> SM\n    # ops).\n\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [53, 65, 127]\n\n    mats = [\n        sparsify(np.random.randn(*dense_shape)).astype(np.float32)\n        for _ in range(2)\n    ]\n    csr_mats = [[sparse.csr_matrix(m) for m in mat] for mat in mats]\n    mats_t = [ops.convert_to_tensor(mat) for mat in mats]\n    mats_locs = [array_ops.where(mat_t > 0) for mat_t in mats_t]\n    sparse_matrices = [\n        sparse_csr_matrix_ops.dense_to_csr_sparse_matrix(mat, mat_loc)\n        for (mat, mat_loc) in zip(mats_t, mats_locs)\n    ]\n    sm_nnz = [\n        sparse_csr_matrix_ops.sparse_matrix_nnz(sm) for sm in sparse_matrices\n    ]\n\n    # Get row indices and columns for batches.\n    sm_components = []\n    for sm in sparse_matrices:\n      sm_components.append([\n          sparse_csr_matrix_ops.csr_sparse_matrix_components(\n              sm, i, type=dtypes.float32) for i in range(dense_shape[0])\n      ])\n\n    sm_nnz_values, sm_values = self.evaluate((sm_nnz, sm_components))\n\n    for i, (sm_values_i, csr_mats_i) in enumerate(zip(sm_values, csr_mats)):\n      for b, (sm_val, csr_mat) in enumerate(zip(sm_values_i, csr_mats_i)):\n        tf_logging.info(\"Comparing matrix %d batch %d\" % (i, b))\n        self.assertEqual(csr_mat.nnz, sm_nnz_values[i][b])\n        self.assertAllEqual(csr_mat.indptr, sm_val.row_ptrs)\n        self.assertAllEqual(csr_mat.indices, sm_val.col_inds)\n        self.assertAllClose(csr_mat.data, sm_val.values)\n\n    # Convert CSR batched Matrix to 3D dense tensor\n    sm_rt = [\n        sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n            sm, type=dtypes.float32) for sm in sparse_matrices\n    ]\n\n    sm_rt_values = self.evaluate(sm_rt)\n\n    for (mat, sm_rt_value) in zip(mats, sm_rt_values):\n      self.assertAllEqual(mat, sm_rt_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSparseMatrixAdd(self):\n    if not self._gpu_available:\n      return\n\n    a_indices = np.array([[0, 0], [2, 3]])\n    a_values = np.array([1.0, 5.0]).astype(np.float32)\n    a_dense_shape = [5, 6]\n    a_sparse_mat = sparse.coo_matrix(\n        (a_values, (a_indices[:, 0], a_indices[:, 1])), shape=a_dense_shape)\n    a_dense = a_sparse_mat.todense()\n\n    b_indices = np.array([[1, 0], [1, 4], [2, 3], [4, 1]])\n    b_values = np.array([1.0, 0.5, -5.0, 2.0]).astype(np.float32)\n    b_dense_shape = [5, 6]\n    b_sparse_mat = sparse.coo_matrix(\n        (b_values, (b_indices[:, 0], b_indices[:, 1])), shape=b_dense_shape)\n    b_dense = b_sparse_mat.todense()\n\n    for (alpha, beta) in [(1.0, 1.0), (1.0, -1.0), (0.25, 0.5)]:\n      a_sum_b_sparse_mat = alpha * a_sparse_mat + beta * b_sparse_mat\n\n      # Convert 2D SparseTensor to CSR Matrix\n      a_sm = dense_to_csr_sparse_matrix(a_dense)\n      b_sm = dense_to_csr_sparse_matrix(b_dense)\n      alpha = np.float32(alpha)\n      beta = np.float32(beta)\n      c_sm = sparse_csr_matrix_ops.sparse_matrix_add(\n          a_sm, b_sm, alpha=alpha, beta=beta)\n      c_dense = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n          c_sm, dtypes.float32)\n      c_dense_value = self.evaluate(c_dense)\n\n      self.assertAllClose(a_sum_b_sparse_mat.todense(), c_dense_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSparseMatrixAdd(self):\n    if not self._gpu_available:\n      return\n\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [53, 65, 127]\n    a_mats = sparsify(np.random.randn(*dense_shape)).astype(np.float32)\n    b_mats = sparsify(np.random.randn(*dense_shape)).astype(np.float32)\n    for (alpha, beta) in [(1.0, 1.0), (1.0, -1.0), (0.25, 0.5)]:\n      tf_logging.info(\"testLargeBatchSparseMatrixAdd, comparing \"\n                      \"alpha, beta (%d, %d)\" % (alpha, beta))\n      a_sm = dense_to_csr_sparse_matrix(a_mats)\n      b_sm = dense_to_csr_sparse_matrix(b_mats)\n      alpha = np.float32(alpha)\n      beta = np.float32(beta)\n      c_sm = sparse_csr_matrix_ops.sparse_matrix_add(\n          a_sm, b_sm, alpha=alpha, beta=beta)\n      c_dense = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n          c_sm, dtypes.float32)\n      c_dense_value = self.evaluate(c_dense)\n\n      self.assertAllClose(c_dense_value, alpha * a_mats + beta * b_mats)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSparseMatrixMatMul(self):\n    for shapes in [[(5, 6), (6, 1)], [(5, 6), (6, 2)]]:\n      a_indices = np.array([[0, 0], [2, 3]])\n      a_values = np.array([1.0, 5.0]).astype(np.float32)\n      a_dense_shape = shapes[0]\n      a_sparse_mat = sparse.coo_matrix(\n          (a_values, (a_indices[:, 0], a_indices[:, 1])), shape=a_dense_shape)\n      a_dense = a_sparse_mat.todense()\n\n      # Will multiply sparse a (shape=shapes[0]) by dense b (shape=shapes[1]).\n      b = np.random.randn(*shapes[1]).astype(np.float32)\n\n      a_sm = dense_to_csr_sparse_matrix(a_dense)\n      c = sparse_csr_matrix_ops.sparse_matrix_mat_mul(a=a_sm, b=b)\n      c_value = self.evaluate(c)\n\n      expected_c_value = a_sparse_mat.dot(b)\n      self.assertAllClose(expected_c_value, c_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSparseMatrixMatMulConjugateOutput(self):\n    for shapes in [[(5, 6), (6, 1)], [(5, 6), (6, 2)]]:\n      a_indices = np.array([[0, 0], [2, 3]])\n      a_values = np.array([1.0 + 1.j, 5.0 - 2.j]).astype(np.complex64)\n      a_dense_shape = shapes[0]\n      a_sparse_mat = sparse.coo_matrix(\n          (a_values, (a_indices[:, 0], a_indices[:, 1])), shape=a_dense_shape)\n      a_dense = a_sparse_mat.todense()\n\n      # Will multiply sparse a (shape=shapes[0]) by dense b (shape=shapes[1]).\n      b = np.random.randn(*shapes[1]).astype(np.complex64)\n\n      a_sm = dense_to_csr_sparse_matrix(a_dense)\n      c = sparse_csr_matrix_ops.sparse_matrix_mat_mul(\n          a=a_sm, b=b, conjugate_output=True)\n      c_value = self.evaluate(c)\n\n      expected_c_value = self.evaluate(\n          math_ops.conj(test_util.matmul_without_tf32(a_dense, b)))\n      self.assertAllClose(expected_c_value, c_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSparseMatrixMatMul(self):\n    dtypes_to_test = [np.float32, np.complex64]\n    sparsify = lambda m: m * (m > 0)\n    for dtype in dtypes_to_test:\n      for (transpose_a, transpose_b) in ((False, False), (False, True),\n                                         (True, False), (True, True)):\n        for (adjoint_a, adjoint_b) in ((False, False), (False, True),\n                                       (True, False), (True, True)):\n          if (transpose_a and adjoint_a) or (transpose_b and adjoint_b):\n            continue\n          for shapes in [[[53, 127, 65], [53, 65, 1]],\n                         [[53, 127, 1], [53, 1, 65]],\n                         [[53, 127, 65], [53, 65, 127]]]:\n            a_dense_shape = shapes[0]\n            b_dense_shape = shapes[1]\n            if transpose_a or adjoint_a:\n              _swap(a_dense_shape, -2, -1)\n            if transpose_b or adjoint_b:\n              _swap(b_dense_shape, -2, -1)\n            a_mats = sparsify(\n                (np.random.randn(*a_dense_shape) +\n                 1.j * np.random.randn(*a_dense_shape))).astype(dtype)\n            b_mats = (np.random.randn(*b_dense_shape) +\n                      1.j * np.random.randn(*b_dense_shape)).astype(dtype)\n            tf_logging.info(\n                \"testLargeBatchSparseMatrixMatMul transpose_a %s transpose_b \"\n                \"%s adjoint_a %s adjoint_b %s\" %\n                (transpose_a, transpose_b, adjoint_a, adjoint_b))\n            a_sm = dense_to_csr_sparse_matrix(a_mats)\n            c_t = sparse_csr_matrix_ops.sparse_matrix_mat_mul(\n                a_sm,\n                b_mats,\n                transpose_output=False,\n                conjugate_output=False,\n                transpose_a=transpose_a,\n                transpose_b=transpose_b,\n                adjoint_a=adjoint_a,\n                adjoint_b=adjoint_b)\n            c_dense_t = test_util.matmul_without_tf32(\n                a_mats,\n                b_mats,\n                transpose_a=transpose_a,\n                transpose_b=transpose_b,\n                adjoint_a=adjoint_a,\n                adjoint_b=adjoint_b)\n            self.assertAllEqual(c_dense_t.shape, c_t.shape)\n            c_t_value, c_dense_t_value = self.evaluate((c_t, c_dense_t))\n\n            self.assertAllClose(\n                c_t_value, c_dense_t_value, rtol=1e-6, atol=2e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSparseMatrixMatMulTransposed(self):\n    dtypes_to_test = [np.float32, np.complex64]\n\n    sparsify = lambda m: m * (m > 0)\n    for dtype in dtypes_to_test:\n      for (transpose_a, transpose_b) in ((False, False), (False, True),\n                                         (True, False), (True, True)):\n        for (adjoint_a, adjoint_b) in ((False, False), (False, True),\n                                       (True, False), (True, True)):\n          if (transpose_a and adjoint_a) or (transpose_b and adjoint_b):\n            continue\n          for shapes in [[[53, 127, 65], [53, 65, 1]],\n                         [[53, 127, 1], [53, 1, 65]],\n                         [[53, 127, 65], [53, 65, 127]]]:\n            a_dense_shape = shapes[0]\n            b_dense_shape = shapes[1]\n            if transpose_a or adjoint_a:\n              _swap(a_dense_shape, -2, -1)\n            if transpose_b or adjoint_b:\n              _swap(b_dense_shape, -2, -1)\n            a_mats = sparsify(\n                (np.random.randn(*a_dense_shape) +\n                 1.j * np.random.randn(*a_dense_shape))).astype(dtype)\n            b_mats = (np.random.randn(*b_dense_shape) +\n                      1.j * np.random.randn(*b_dense_shape)).astype(dtype)\n            tf_logging.info(\n                \"testLargeBatchSparseMatrixMatMul transpose_a %s transpose_b \"\n                \"%s adjoint_a %s adjoint_b %s\" %\n                (transpose_a, transpose_b, adjoint_a, adjoint_b))\n            a_sm = dense_to_csr_sparse_matrix(a_mats)\n            c_t = sparse_csr_matrix_ops.sparse_matrix_mat_mul(\n                a_sm,\n                b_mats,\n                transpose_output=True,\n                conjugate_output=False,\n                transpose_a=transpose_a,\n                transpose_b=transpose_b,\n                adjoint_a=adjoint_a,\n                adjoint_b=adjoint_b)\n\n            # Example: t(adj(a) . b) = t(b) . conj(a)\n            c_dense_t = test_util.matmul_without_tf32(\n                math_ops.conj(b_mats) if adjoint_b else b_mats,\n                math_ops.conj(a_mats) if adjoint_a else a_mats,\n                transpose_a=not (transpose_b or adjoint_b),\n                transpose_b=not (transpose_a or adjoint_a),\n                adjoint_a=False,\n                adjoint_b=False)\n            self.assertAllEqual(c_t.shape, c_dense_t.shape)\n            c_t_value, c_dense_t_value = self.evaluate((c_t, c_dense_t))\n            self.assertAllClose(\n                c_t_value, c_dense_t_value, rtol=1e-6, atol=2e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSparseMatrixMatMulConjugate(self):\n    sparsify = lambda m: m * (m > 0)\n    a_dense_shape = [53, 65, 127]\n    b_dense_shape = [53, 127, 67]\n    a_mats = sparsify(\n        (np.random.randn(*a_dense_shape) +\n         1.j * np.random.randn(*a_dense_shape))).astype(np.complex64)\n    b_mats = (np.random.randn(*b_dense_shape) +\n              1.j * np.random.randn(*b_dense_shape)).astype(np.complex64)\n    a_sm = dense_to_csr_sparse_matrix(a_mats)\n    c_t = sparse_csr_matrix_ops.sparse_matrix_mat_mul(\n        a_sm, b_mats, conjugate_output=True)\n\n    c_dense_t = math_ops.conj(test_util.matmul_without_tf32(a_mats, b_mats))\n    self.assertAllEqual(c_t.shape, c_dense_t.shape)\n    c_t_value, c_dense_t_value = self.evaluate((c_t, c_dense_t))\n\n    self.assertAllClose(c_t_value, c_dense_t_value, atol=1e-5, rtol=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSparseMatrixSparseMatMul(self):\n    a_indices = np.array([[0, 0], [2, 3]])\n    a_values = np.array([1.0, 5.0]).astype(np.float32)\n    a_dense_shape = [5, 6]\n    a_sparse_mat = sparse.coo_matrix(\n        (a_values, (a_indices[:, 0], a_indices[:, 1])), shape=a_dense_shape)\n    a_dense = a_sparse_mat.todense()\n\n    b_indices = np.array([[0, 0], [3, 0], [3, 1]])\n    b_values = np.array([2.0, 7.0, 8.0]).astype(np.float32)\n    b_dense_shape = [6, 7]\n    b_sparse_mat = sparse.coo_matrix(\n        (b_values, (b_indices[:, 0], b_indices[:, 1])), shape=b_dense_shape)\n    b_dense = b_sparse_mat.todense()\n\n    a_sm = dense_to_csr_sparse_matrix(a_dense)\n    b_sm = dense_to_csr_sparse_matrix(b_dense)\n    c_sm = sparse_csr_matrix_ops.sparse_matrix_sparse_mat_mul(\n        a=a_sm, b=b_sm, type=dtypes.float32)\n\n    c_sm_dense = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n        c_sm, dtypes.float32)\n    c_sm_dense_value = self.evaluate(c_sm_dense)\n\n    expected_c_value = a_sparse_mat.dot(b_sparse_mat).todense()\n    self.assertAllClose(expected_c_value, c_sm_dense_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSparseMatrixSparseMatMul_NumericZerosNotPruned(self):\n    # Tests that numeric zeros appearing from the sparse-sparse matrix\n    # multiplication are not pruned from the sparse structural\n    a_indices = np.array([[0, 0], [0, 2]])\n    a_values = np.array([2.0, -1.0]).astype(np.float32)\n    a_dense_shape = [2, 3]\n    a_sparse_mat = sparse.coo_matrix(\n        (a_values, (a_indices[:, 0], a_indices[:, 1])), shape=a_dense_shape)\n    a_dense = a_sparse_mat.todense()\n\n    b_indices = np.array([[0, 1], [2, 1]])\n    b_values = np.array([3.0, 6.0]).astype(np.float32)\n    b_dense_shape = [3, 2]\n    b_sparse_mat = sparse.coo_matrix(\n        (b_values, (b_indices[:, 0], b_indices[:, 1])), shape=b_dense_shape)\n    b_dense = b_sparse_mat.todense()\n\n    # Convert to CSRSparseMatrix while removing numeric zeros from the\n    # structural representation.\n    a_sm = dense_to_csr_sparse_matrix(a_dense)\n    b_sm = dense_to_csr_sparse_matrix(b_dense)\n\n    # Compute the matmul.\n    c_sm = sparse_csr_matrix_ops.sparse_matrix_sparse_mat_mul(\n        a=a_sm, b=b_sm, type=dtypes.float32)\n    c_nnz = sparse_csr_matrix_ops.sparse_matrix_nnz(c_sm)\n    c_nnz_value = self.evaluate(c_nnz)\n\n    # Expect that there is a single numeric zero at index (0, 1) if zeros are\n    # not pruned, since 2.0 * 3.0 + (-1.0) * 6.0 = 0.0.\n    self.assertAllClose(1, c_nnz_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSparseMatrixSparseMatMul(self):\n    sparsify = lambda m: m * (m > 0)\n\n    for (transpose_a, transpose_b) in ((False, False), (False, True),\n                                       (True, False), (True, True)):\n      for (adjoint_a, adjoint_b) in ((False, False), (False, True),\n                                     (True, False), (True, True)):\n        if (transpose_a and adjoint_a) or (transpose_b and adjoint_b):\n          continue\n\n        a_dense_shape = ([53, 127, 65]\n                         if transpose_a or adjoint_a else [53, 65, 127])\n        b_dense_shape = ([53, 67, 127]\n                         if transpose_b or adjoint_b else [53, 127, 67])\n\n        a_mats = sparsify(np.random.randn(*a_dense_shape)).astype(np.float32)\n        b_mats = sparsify(np.random.randn(*b_dense_shape).astype(np.float32))\n\n        a_sm = dense_to_csr_sparse_matrix(a_mats)\n        b_sm = dense_to_csr_sparse_matrix(b_mats)\n        c_sm = sparse_csr_matrix_ops.sparse_matrix_sparse_mat_mul(\n            a_sm,\n            b_sm,\n            type=dtypes.float32,\n            transpose_a=transpose_a,\n            adjoint_a=adjoint_a,\n            transpose_b=transpose_b,\n            adjoint_b=adjoint_b)\n        c_sm_dense = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n            c_sm, dtypes.float32)\n        c_dense_t = test_util.matmul_without_tf32(\n            a_mats,\n            b_mats,\n            transpose_a=transpose_a,\n            adjoint_a=adjoint_a,\n            transpose_b=transpose_b,\n            adjoint_b=adjoint_b)\n        c_dense_t_value, c_sm_dense_value = self.evaluate(\n            (c_dense_t, c_sm_dense))\n\n        self.assertAllClose(c_sm_dense_value, c_dense_t_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchRegisteredAddN(self):\n    if not self._gpu_available:\n      return\n\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [53, 65, 127]\n    matrices = [\n        sparsify(np.random.randn(*dense_shape)).astype(np.float32)\n        for _ in range(16)\n    ]\n    sparse_matrices = [dense_to_csr_sparse_matrix(mat) for mat in matrices]\n    sparse_matrices_sum = math_ops.add_n(sparse_matrices)\n    sparse_matrices_sum_dense = \\\n        sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n            sparse_matrices_sum, dtypes.float32)\n    sparse_matrices_sum_dense_value = self.evaluate(sparse_matrices_sum_dense)\n\n    # Ensure that the dense (numpy) sum across all batches matches the result\n    # of add_n converted back to dense.\n    expected_sum = np.sum(matrices, axis=0)\n    self.assertAllClose(expected_sum, sparse_matrices_sum_dense_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testCSRZeros(self):\n    if not self._gpu_available:\n      return\n    a_dense_shape = [65, 127]\n    b_dense_shape = [53, 127, 67]\n    data_types = [\n        dtypes.float32, dtypes.float64, dtypes.complex64, dtypes.complex128\n    ]\n    for dtype in data_types:\n      # Check both rank-2 and rank-3 tensors.\n      a_sm = sparse_csr_matrix_ops.sparse_matrix_zeros(\n          a_dense_shape, type=dtype)\n      b_sm = sparse_csr_matrix_ops.sparse_matrix_zeros(\n          b_dense_shape, type=dtype)\n      a_rt = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(a_sm, type=dtype)\n      b_rt = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(b_sm, type=dtype)\n      a_rt_value, b_rt_value = self.evaluate((a_rt, b_rt))\n\n      self.assertAllEqual(a_rt_value, np.zeros(a_dense_shape))\n      self.assertAllEqual(b_rt_value, np.zeros(b_dense_shape))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchZerosLike(self):\n    if not self._gpu_available:\n      return\n\n    batch_size = 53\n    rows = 128\n    cols = 67\n    dense_shape = [batch_size, rows, cols]\n    data_types = [\n        dtypes.float32, dtypes.float64, dtypes.complex64, dtypes.complex128\n    ]\n    for dtype in data_types:\n      sparse_matrices = sparse_csr_matrix_ops.sparse_matrix_zeros(\n          dense_shape, type=dtype)\n      zeros_like_sparse_matrices = array_ops.zeros_like(sparse_matrices)\n      zeros_like_components = [\n          sparse_csr_matrix_ops.csr_sparse_matrix_components(\n              zeros_like_sparse_matrices, i, type=dtype)\n          for i in range(batch_size)\n      ]\n      zeros_like_components_values = self.evaluate(zeros_like_components)\n      for component in zeros_like_components_values:\n        self.assertAllEqual(component.row_ptrs, np.zeros(rows + 1, np.int32))\n        self.assertAllEqual(component.col_inds, np.empty([0], np.int32))\n        self.assertAllEqual(component.values, np.empty([0],\n                                                       dtype.as_numpy_dtype))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testTranspose(self):\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [127, 65]\n    data_types = [\n        dtypes.float32, dtypes.float64, dtypes.complex64, dtypes.complex128\n    ]\n    for dtype in data_types:\n      mats = sparsify(\n          (np.random.randn(*dense_shape) +\n           1.j * np.random.randn(*dense_shape))).astype(dtype.as_numpy_dtype)\n      for conjugate in False, True:\n        expected = np.transpose(mats)\n        if conjugate:\n          expected = np.conj(expected)\n        matrices = math_ops.cast(mats, dtype)\n        sparse_matrices = dense_to_csr_sparse_matrix(matrices)\n        transpose_sparse_matrices = \\\n            sparse_csr_matrix_ops.sparse_matrix_transpose(\n                sparse_matrices, conjugate=conjugate, type=dtype)\n        dense_transposed = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n            transpose_sparse_matrices, dtype)\n        dense_transposed_values = self.evaluate(dense_transposed)\n        self.assertAllClose(expected, dense_transposed_values)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchTranspose(self):\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [53, 65, 127]\n    data_types = [\n        dtypes.float32, dtypes.float64, dtypes.complex64, dtypes.complex128\n    ]\n    for dtype in data_types:\n      mats = sparsify(\n          (np.random.randn(*dense_shape) +\n           1.j * np.random.randn(*dense_shape))).astype(dtype.as_numpy_dtype)\n      expected = np.transpose(mats, (0, 2, 1))\n      for conjugate in False, True:\n        if conjugate:\n          expected = np.conj(expected)\n        matrices = math_ops.cast(mats, dtype)\n        sparse_matrices = dense_to_csr_sparse_matrix(matrices)\n        transpose_sparse_matrices = \\\n            sparse_csr_matrix_ops.sparse_matrix_transpose(\n                sparse_matrices, conjugate=conjugate, type=dtype)\n        dense_transposed = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n            transpose_sparse_matrices, dtype)\n        dense_transposed_values = self.evaluate(dense_transposed)\n        self.assertAllClose(expected, dense_transposed_values)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSoftmax(self):\n    if not self._gpu_available:\n      return\n\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [127, 65]\n    logits = sparsify(np.random.randn(*dense_shape))\n    logits_with_ninf = np.copy(logits)\n    logits_with_ninf[logits == 0] = -np.inf\n    data_types = [dtypes.float32, dtypes.float64]\n    for dtype in data_types:\n      logits_t = math_ops.cast(logits, dtype)\n      logits_t_with_ninf = math_ops.cast(logits_with_ninf, dtype)\n      expected = nn_ops.softmax(logits_t_with_ninf)\n      sparse_logits_t = dense_to_csr_sparse_matrix(logits_t)\n      softmax_sparse_logits_t = sparse_csr_matrix_ops.sparse_matrix_softmax(\n          sparse_logits_t, type=dtype)\n      dense_softmax = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n          softmax_sparse_logits_t, dtype)\n      dense_softmax_values, expected_values = self.evaluate(\n          (dense_softmax, expected))\n      self.assertAllClose(expected_values, dense_softmax_values)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSoftmax(self):\n    if not self._gpu_available:\n      return\n\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [53, 65, 127]\n    logits = sparsify(np.random.randn(*dense_shape))\n    logits_with_ninf = np.copy(logits)\n    logits_with_ninf[logits == 0] = -np.inf\n    data_types = [dtypes.float32, dtypes.float64]\n    for dtype in data_types:\n      logits_t = math_ops.cast(logits, dtype)\n      logits_t_with_ninf = math_ops.cast(logits_with_ninf, dtype)\n      expected = nn_ops.softmax(logits_t_with_ninf)\n      sparse_logits_t = dense_to_csr_sparse_matrix(logits_t)\n      softmax_sparse_logits_t = sparse_csr_matrix_ops.sparse_matrix_softmax(\n          sparse_logits_t, type=dtype)\n      dense_softmax = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n          softmax_sparse_logits_t, dtype)\n      dense_softmax_values, expected_values = self.evaluate(\n          (dense_softmax, expected))\n      self.assertAllClose(expected_values, dense_softmax_values)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSoftmaxEmpty(self):\n    if not self._gpu_available:\n      return\n\n    dense_shape = [53, 65, 127]\n    sparse_logits_t = sparse_csr_matrix_ops.sparse_matrix_zeros(\n        dense_shape, type=dtypes.float32)\n    softmax_sparse_logits_t = sparse_csr_matrix_ops.sparse_matrix_softmax(\n        sparse_logits_t, type=dtypes.float32)\n    dense_softmax = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n        softmax_sparse_logits_t, dtypes.float32)\n    dense_softmax_values = self.evaluate(dense_softmax)\n    self.assertAllEqual(\n        np.zeros_like(dense_softmax_values), dense_softmax_values)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSoftmaxGrad(self):\n    if not self._gpu_available:\n      return\n\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [127, 65]\n    softmax = sparsify(np.random.randn(*dense_shape))\n    grad_softmax = sparsify(np.random.randn(*dense_shape))\n    expected = (\n        (grad_softmax - np.sum(grad_softmax * softmax, -1, keepdims=True)) *\n        softmax)\n    data_types = [dtypes.float32, dtypes.float64]\n    for dtype in data_types:\n      softmax_t = math_ops.cast(softmax, dtype)\n      grad_softmax_t = math_ops.cast(grad_softmax, dtype)\n      softmax_sparse = dense_to_csr_sparse_matrix(softmax_t)\n      grad_softmax_sparse = dense_to_csr_sparse_matrix(grad_softmax_t)\n      gradients_sparse = sparse_csr_matrix_ops.sparse_matrix_softmax_grad(\n          softmax_sparse, grad_softmax_sparse, dtype)\n      dense_gradients = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n          gradients_sparse, dtype)\n      dense_gradients_values = self.evaluate((dense_gradients))\n      self.assertAllClose(expected, dense_gradients_values)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSoftmaxGrad(self):\n    if not self._gpu_available:\n      return\n\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [53, 65, 127]\n    softmax = sparsify(np.random.randn(*dense_shape))\n    grad_softmax = sparsify(np.random.randn(*dense_shape))\n    expected = (\n        (grad_softmax - np.sum(grad_softmax * softmax, -1, keepdims=True)) *\n        softmax)\n    data_types = [dtypes.float32, dtypes.float64]\n    for dtype in data_types:\n      softmax_t = math_ops.cast(softmax, dtype)\n      grad_softmax_t = math_ops.cast(grad_softmax, dtype)\n      softmax_sparse = dense_to_csr_sparse_matrix(softmax_t)\n      grad_softmax_sparse = dense_to_csr_sparse_matrix(grad_softmax_t)\n      gradients_sparse = sparse_csr_matrix_ops.sparse_matrix_softmax_grad(\n          softmax_sparse, grad_softmax_sparse, dtype)\n      dense_gradients = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n          gradients_sparse, dtype)\n      dense_gradients_values = self.evaluate((dense_gradients))\n      self.assertAllClose(expected, dense_gradients_values)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSoftmaxGradEmpty(self):\n    if not self._gpu_available:\n      return\n\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [53, 65, 127]\n    not_empty = sparsify(np.random.randn(*dense_shape)).astype(np.float32)\n    sparse_empty = sparse_csr_matrix_ops.sparse_matrix_zeros(\n        dense_shape, type=dtypes.float32)\n    sparse_not_empty = dense_to_csr_sparse_matrix(not_empty)\n    gradients_empty_softmax = sparse_csr_matrix_ops.sparse_matrix_softmax_grad(\n        sparse_empty, sparse_not_empty, dtypes.float32)\n    gradients_empty_grad_softmax = (\n        sparse_csr_matrix_ops.sparse_matrix_softmax_grad(\n            sparse_not_empty, sparse_empty, dtypes.float32))\n    gradients_empty_both = sparse_csr_matrix_ops.sparse_matrix_softmax_grad(\n        sparse_empty, sparse_empty, dtypes.float32)\n    ges = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n        gradients_empty_softmax, dtypes.float32)\n    gegs = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n        gradients_empty_grad_softmax, dtypes.float32)\n    geb = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n        gradients_empty_both, dtypes.float32)\n    ges_v, gegs_v, geb_v = self.evaluate((ges, gegs, geb))\n    for v in (ges_v, gegs_v, geb_v):\n      self.assertAllEqual(np.zeros(dense_shape), v)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchConj(self):\n    if not self._gpu_available:\n      return\n\n    sparsify = lambda m: m * (np.real(m) > 0)\n    dense_shape = [53, 65, 127]\n    matrices = (\n        sparsify(np.random.randn(*dense_shape)) +\n        1j * np.random.randn(*dense_shape))\n    data_types = [\n        dtypes.float32, dtypes.float64, dtypes.complex64, dtypes.complex128\n    ]\n    for dtype in data_types:\n      matrices_t = matrices.astype(dtype.as_numpy_dtype)\n      expected = np.conj(matrices_t)\n      sparse_matrices = dense_to_csr_sparse_matrix(matrices_t)\n      conj_sparse_matrices = math_ops.conj(sparse_matrices)\n      dense_conj_matrices = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n          conj_sparse_matrices, dtype)\n      conj_values = self.evaluate(dense_conj_matrices)\n      self.assertAllClose(expected, conj_values)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSparseMatrixMulScalar(self):\n    if not self._gpu_available:\n      return\n\n    sparsify = lambda m: m * (m > 0)\n    a_dense_shape = [53, 65, 127]\n    a_mats = sparsify(np.random.randn(*a_dense_shape)).astype(np.float32)\n    b = np.float32(3.5)\n    expected = a_mats * b\n    a_sm = dense_to_csr_sparse_matrix(a_mats)\n    c_t = sparse_csr_matrix_ops.sparse_matrix_mul(a_sm, b)\n    c_dense_t = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n        c_t, dtypes.float32)\n    c_dense_t_value = self.evaluate(c_dense_t)\n\n    self.assertAllClose(expected, c_dense_t_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSparseMatrixMulVec(self):\n    if not self._gpu_available:\n      return\n\n    sparsify = lambda m: m * (m > 0)\n    a_dense_shape = [53, 65, 127]\n    a_mats = sparsify(np.random.randn(*a_dense_shape)).astype(np.float32)\n    b = np.random.randn(53, 1, 1).astype(np.float32)\n    expected = a_mats * b\n    a_sm = dense_to_csr_sparse_matrix(a_mats)\n    c_t = sparse_csr_matrix_ops.sparse_matrix_mul(a_sm, b)\n    c_dense_t = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n        c_t, dtypes.float32)\n    c_dense_t_value = self.evaluate(c_dense_t)\n\n    self.assertAllClose(expected, c_dense_t_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSparseCholesky(self):\n    dense_matrix = np.array([\n        [2, 0, 0, 0, 0, 0],\n        [0, 3, 0, 0, 0, 0],\n        [1, 1, 7, 0, 0, 0],\n        [0, 0, 0, 4, 0, 0],\n        [0, 0, 1, 0, 5, 0],\n        [0, 0, 2, 0, 1, 6],\n    ]).astype(np.complex128)\n\n    data_types = [\n        dtypes.float32, dtypes.float64, dtypes.complex64, dtypes.complex128\n    ]\n    for dtype in data_types:\n      with test_util.force_cpu():\n        if dtype.is_complex:\n          dense_matrix += 0.5j * np.tril(dense_matrix, -1)\n\n        sparse_matrix = dense_to_csr_sparse_matrix(\n            math_ops.cast(dense_matrix, dtype))\n        # Obtain the Sparse Cholesky factor using AMD Ordering for reducing\n        # fill-in.\n        ordering_amd = sparse_csr_matrix_ops.sparse_matrix_ordering_amd(\n            sparse_matrix)\n        cholesky_sparse_matrices = (\n            sparse_csr_matrix_ops.sparse_matrix_sparse_cholesky(\n                sparse_matrix, ordering_amd, type=dtype))\n        dense_cholesky = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n            cholesky_sparse_matrices, dtype)\n        # Compute L * Lh where L is the Sparse Cholesky factor.\n        verification = test_util.matmul_without_tf32(\n            dense_cholesky, array_ops.transpose(dense_cholesky, conjugate=True))\n        verification = twist_matrix(verification, ordering_amd)\n        # Assert that input matrix A satisfies A = L * Lh.\n        verification_values = self.evaluate(verification)\n        full_dense_matrix = (\n            dense_matrix +\n            np.conjugate(np.transpose(np.tril(dense_matrix, -1))))\n        self.assertAllClose(full_dense_matrix, verification_values)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testBatchSparseCholesky(self):\n    dense_mat = np.array([\n        # A diagonal matrix.\n        [\n            [1, 0, 0, 0],  #\n            [0, 2, 0, 0],  #\n            [0, 0, 3, 0],  #\n            [0, 0, 0, 4],\n        ],  #\n        # A tridiagonal hermitian matrix.\n        [\n            [5 + 0j, 1 + 0j, 0 + 0j, 0 + 0j],  #\n            [1 + 0j, 4 + 0j, 1 + 2j, 0 + 0j],  #\n            [0 + 0j, 1 - 2j, 9 + 0j, 3 - 3j],  #\n            [0 + 0j, 0 + 0j, 3 + 3j, 7 + 0j],\n        ],  #\n        # A diagonal matrix with a corner element; for which\n        # OrderingAMD returns a non-identity permutation.\n        [\n            [1, 0, 0, 1.],  #\n            [0, 2, 0, 0.],  #\n            [0, 0, 3, 0.],  #\n            [1, 0, 0, 4.],\n        ]  #\n    ]).astype(np.complex128)\n\n    data_types = [\n        dtypes.float32, dtypes.float64, dtypes.complex64, dtypes.complex128\n    ]\n    for dtype in data_types:\n      sparse_matrix = dense_to_csr_sparse_matrix(\n          math_ops.cast(dense_mat, dtype))\n      ordering_amd = sparse_csr_matrix_ops.sparse_matrix_ordering_amd(\n          sparse_matrix)\n\n      cholesky_sparse_matrix = (\n          sparse_csr_matrix_ops.sparse_matrix_sparse_cholesky(\n              sparse_matrix, ordering_amd, type=dtype))\n      dense_cholesky = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n          cholesky_sparse_matrix, dtype)\n\n      # Compute L * Lh.\n      verification = test_util.matmul_without_tf32(\n          dense_cholesky,\n          array_ops.transpose(dense_cholesky, perm=[0, 2, 1], conjugate=True))\n      verification = twist_matrix(verification, ordering_amd)\n\n      verification_values = self.evaluate(verification)\n      self.assertAllClose(\n          dense_mat.astype(dtype.as_numpy_dtype), verification_values)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSparseCholesky(self):\n    sparsity = 0.1\n    sparsify = lambda m: m * (m > 1 - sparsity)\n\n    batch_size = 53\n    num_rows = 147\n    dense_shape = [batch_size, num_rows, num_rows]\n\n    dense_matrix = sparsify(np.random.uniform(size=dense_shape)).astype(\n        np.float32)\n\n    # Create a \"random\" SPD matrix, by choosing each entry of A between\n    # 0 and 1 at the specified density, and computing 0.5(A + At) + n*I.\n    # This ensures diagonal dominance which implies positive-definiteness.\n    dense_matrix = (\n        0.5 *\n        (dense_matrix + array_ops.transpose(dense_matrix, perm=[0, 2, 1])) +\n        num_rows * linalg_ops.eye(dense_shape[-1], batch_shape=[batch_size]))\n    # Compute the fill-in reducing permutation and use it to perform\n    # the Sparse Cholesky factorization.\n    sparse_matrix = dense_to_csr_sparse_matrix(dense_matrix)\n    ordering_amd = sparse_csr_matrix_ops.sparse_matrix_ordering_amd(\n        sparse_matrix)\n\n    cholesky_sparse_matrix = \\\n        sparse_csr_matrix_ops.sparse_matrix_sparse_cholesky(\n            sparse_matrix, ordering_amd, type=dtypes.float32)\n    dense_cholesky = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n        cholesky_sparse_matrix, dtypes.float32)\n\n    # Compute L * Lh.\n    verification = test_util.matmul_without_tf32(\n        dense_cholesky, array_ops.transpose(dense_cholesky, perm=[0, 2, 1]))\n    verification = twist_matrix(verification, ordering_amd)\n    verification_values = self.evaluate(verification)\n    self.assertAllClose(dense_matrix, verification_values, atol=1e-5, rtol=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSparseCholesky_InvalidMatrix(self):\n    # Verify that non-SPD matrices result in an Invalid Argument error.\n    invalid_matrices = [\n        # zero matrix.\n        np.array([\n            [0., 0., 0., 0.],  #\n            [0., 0., 0., 0.],  #\n            [0., 0., 0., 0.],  #\n            [0., 0., 0., 0.]  #\n        ]),\n        # zero diagonal entry.\n        np.array([\n            [9., 0., 5., 0.],  #\n            [0., 0., 0., 1.],  #\n            [5., 0., 8., 0.],  #\n            [0., 1., 0., 7.]  #\n        ]),\n        # not positive definite.\n        np.array([\n            [2., -2., 0., 0.],  #\n            [-2., 2., 0., 0.],  #\n            [0., 0., 3., -3.],  #\n            [0., 0., -3., 3.]  #\n        ]),\n    ]\n\n    with test_util.force_cpu():\n      for invalid_matrix in invalid_matrices:\n        with self.assertRaises(errors.InvalidArgumentError):\n          sparse_matrix = dense_to_csr_sparse_matrix(\n              invalid_matrix.astype(np.float32))\n          # Compute the fill-in reducing permutation and use it to perform\n          # the Sparse Cholesky factorization.\n          ordering_amd = sparse_csr_matrix_ops.sparse_matrix_ordering_amd(\n              sparse_matrix)\n          cholesky_sparse_matrices = (\n              sparse_csr_matrix_ops.sparse_matrix_sparse_cholesky(\n                  sparse_matrix, ordering_amd, type=dtypes.float32))\n          # Convert the Cholesky factor to a dense matrix to be evaluated.\n          dense_cholesky = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n              cholesky_sparse_matrices, type=dtypes.float32)\n          self.evaluate(dense_cholesky)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testOrderingAMD(self):\n    num_rows = 6\n    # An SPD matrix where AMD ordering can reduce fill-in for Cholesky factor.\n    dense_matrix = np.array([\n        [7, 0, 0, 0, 0, 0],\n        [1, 4, 0, 0, 0, 0],\n        [1, 1, 3, 0, 0, 0],\n        [0, 0, 0, 4, 0, 0],\n        [2, 0, 0, 0, 5, 0],\n        [1, 2, 2, 0, 0, 6],\n    ]).astype(np.float32)\n\n    with test_util.force_cpu():\n      sparse_matrix = dense_to_csr_sparse_matrix(dense_matrix)\n\n      # Obtain the Sparse Cholesky factor with the identity permutation as the\n      # fill-in reducing ordering.\n      cholesky_without_ordering = (\n          sparse_csr_matrix_ops.sparse_matrix_sparse_cholesky(\n              sparse_matrix, math_ops.range(num_rows), type=dtypes.float32))\n      cholesky_without_ordering_nnz = sparse_csr_matrix_ops.sparse_matrix_nnz(\n          cholesky_without_ordering)\n\n      # Obtain the Sparse Cholesky factor using AMD Ordering for reducing\n      # fill-in.\n      ordering_amd = sparse_csr_matrix_ops.sparse_matrix_ordering_amd(\n          sparse_matrix)\n      cholesky_with_amd = sparse_csr_matrix_ops.sparse_matrix_sparse_cholesky(\n          sparse_matrix, ordering_amd, type=dtypes.float32)\n      cholesky_with_amd_nnz = sparse_csr_matrix_ops.sparse_matrix_nnz(\n          cholesky_with_amd)\n\n      (ordering_amd_value, cholesky_with_amd_nnz_value,\n       cholesky_without_ordering_nnz_value) = self.evaluate(\n           [ordering_amd, cholesky_with_amd_nnz, cholesky_without_ordering_nnz])\n\n      # AMD ordering should return a valid permutation.\n      self.assertAllClose(np.arange(num_rows), np.sort(ordering_amd_value))\n      # Check that cholesky with AMD ordering has a strictly lower nonzero count\n      # for this matrix.\n      self.assertLess(cholesky_with_amd_nnz_value,\n                      cholesky_without_ordering_nnz_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testNoMatrixNoCrash(self):\n    # Round-about way of creating an empty variant tensor that works in both\n    # graph and eager modes.\n    no_matrix = array_ops.reshape(dense_to_csr_sparse_matrix([[0.0]]), [1])[0:0]\n    with self.assertRaisesRegex(\n        (ValueError, errors.InvalidArgumentError),\n        \"(Invalid input matrix)|(Shape must be rank 0)\"):\n      sparse_csr_matrix_ops.sparse_matrix_nnz(no_matrix)\n\n\nclass CSRSparseMatrixOpsBenchmark(test.Benchmark):\n\n  def benchmark_sparse_matrix_mat_mul_gpu(self):\n    if not test_util.is_gpu_available():\n      return\n\n    sparsify = lambda m: array_ops.where(m > 2, m, array_ops.zeros_like(m))\n\n    # XW, X dense and W sparse\n    # X is shaped [{1, 8, 16}, 2000]\n    # W is shaped [2000, 4000]\n\n    for batch_size in [1, 8, 16]:\n      x_dense_shape = [batch_size, 2000]\n      w_dense_shape = [2000, 4000]\n\n      with ops.Graph().as_default(), ops.device(\"/gpu:0\"):\n        x_mats = random_ops.random_normal(x_dense_shape, dtype=dtypes.float32)\n        w_mats = sparsify(\n            random_ops.random_normal(w_dense_shape, dtype=dtypes.float32))\n        nnz = array_ops.shape(array_ops.where(w_mats))[0]\n        ratio = math_ops.cast(nnz, dtypes.float32) / np.prod(w_dense_shape)\n        w_sm = dense_to_csr_sparse_matrix(w_mats)\n        with ops.name_scope(\"w_sm_var\"):\n          w_sm_var = variable_scope.get_variable(\n              \"sm\", initializer=w_sm, use_resource=True)\n          w_sm_var_v = w_sm_var.read_value()\n        with ops.name_scope(\"w_var\"):\n          w_var = variable_scope.get_variable(\n              \"sm_dense\", initializer=w_mats, use_resource=True)\n          w_var_v = w_var.read_value()\n        with ops.name_scope(\"b\"):\n          x = variable_scope.get_variable(\n              \"b\", initializer=x_mats, use_resource=True)\n          x_v = x.read_value()\n        # X*W = (W'*X')'\n        xw_sparse = sparse_csr_matrix_ops.sparse_matrix_mat_mul(\n            w_sm_var_v,\n            x_v,\n            transpose_a=True,\n            transpose_b=True,\n            transpose_output=True)\n        xw_dense = math_ops.matmul(x_v, w_var_v)\n\n        with session.Session() as sess:\n          self.evaluate(\n              [w_var.initializer, w_sm_var.initializer, x.initializer])\n          nnz_value, ratio_value = self.evaluate((nnz, ratio))\n          name_template = (\n              \"sparse_matrix_mat_mul_gpu_%s_W_2000x4000_batch_size_%d\")\n          self.run_op_benchmark(\n              sess,\n              xw_sparse.op,\n              name=name_template % (\"sparse\", batch_size),\n              extras={\n                  \"percentage_nonzero\": ratio_value,\n                  \"num_nonzero\": nnz_value\n              },\n              min_iters=50)\n          self.run_op_benchmark(\n              sess,\n              xw_dense.op,\n              name=name_template % (\"dense\", batch_size),\n              extras={\n                  \"percentage_nonzero\": ratio_value,\n                  \"num_nonzero\": nnz_value\n              },\n              min_iters=50)\n\n  def benchmark_sparse_matrix_mat_vec_mul(self):\n    # num_rows, device, transpose.\n    cases = [\n        [2000, CPU, False],\n        [8000, CPU, False],\n        [12000, CPU, False],\n        [2000, CPU, True],\n        [8000, CPU, True],\n        [12000, CPU, True],\n    ]\n    seed = 42\n\n    for num_rows, device, transpose in cases:\n      if device == GPU and not test_util.is_gpu_available():\n        continue\n      for num_threads in [1, 2, 4, 6, 8, 10]:\n        device_str = \"cpu\" if device == CPU else \"gpu\"\n        w_dense_shape = [num_rows, num_rows]\n        x_dense_shape = [num_rows, 1]\n\n        with ops.Graph().as_default(), ops.device(device):\n          random_seed.set_random_seed(seed)\n          x = random_ops.random_normal(x_dense_shape, dtype=dtypes.float32)\n          w_np = sparse.rand(\n              w_dense_shape[0],\n              w_dense_shape[1],\n              density=0.01,\n              dtype=np.float32,\n              random_state=np.random.RandomState(seed))\n          w_st = sparse_tensor.SparseTensor(\n              zip(w_np.row, w_np.col), w_np.data, w_np.shape)\n          w_st = sparse_ops.sparse_reorder(w_st)\n\n          nnz = array_ops.shape(w_st.values)[0]\n          ratio = math_ops.cast(nnz, dtypes.float32) / np.prod(w_np.shape)\n\n          w_sm = sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(\n              w_st.indices, w_st.values, w_st.dense_shape)\n          xw_sparse_matrix = sparse_csr_matrix_ops.sparse_matrix_mat_mul(\n              w_sm,\n              x,\n              transpose_a=transpose,\n              transpose_b=False,\n              transpose_output=False)\n          xw_sparse_tensor = sparse_ops.sparse_tensor_dense_matmul(\n              w_st, x, adjoint_a=transpose, adjoint_b=False)\n\n          with session.Session(\n              config=config_pb2.ConfigProto(\n                  intra_op_parallelism_threads=num_threads)) as sess:\n            nnz_value, ratio_value = sess.run((nnz, ratio))\n            name_template = (\"mat_vec_mul_%s_%s_W_%d_transpose_%s_threads_%d\")\n            self.run_op_benchmark(\n                sess,\n                xw_sparse_matrix.op,\n                name=name_template %\n                (device_str, \"sparse_matrix\", num_rows, transpose, num_threads),\n                extras={\n                    \"percentage_nonzero\": ratio_value,\n                    \"num_nonzero\": nnz_value,\n                },\n                min_iters=10)\n            self.run_op_benchmark(\n                sess,\n                xw_sparse_tensor.op,\n                name=name_template %\n                (device_str, \"sparse_tensor\", num_rows, transpose, num_threads),\n                extras={\n                    \"percentage_nonzero\": ratio_value,\n                    \"num_nonzero\": nnz_value,\n                },\n                min_iters=10)\n\n  def benchmark_sparse_matrix_sparse_matmul(self):\n    density = 0.05\n    # pylint: disable=g-long-lambda\n    sparsify = lambda m: array_ops.where(m > 1. - density, m,\n                                         array_ops.zeros_like(m))\n    # pylint: enable=g-long-lambda\n\n    for batch_size in [1, 16]:\n      for num_threads in [1, 4, 12]:\n        dense_shape = [batch_size, 250, 250]\n\n        for device in [CPU, GPU]:\n          if device == GPU and not test_util.is_gpu_available():\n            continue\n\n          with ops.Graph().as_default(), ops.device(device):\n            x_mats = sparsify(\n                random_ops.random_uniform(dense_shape, dtype=dtypes.float32))\n            y_mats = sparsify(\n                random_ops.random_uniform(dense_shape, dtype=dtypes.float32))\n\n            nnz = array_ops.shape(array_ops.where(x_mats))[0] + array_ops.shape(\n                array_ops.where(y_mats))[0]\n            ratio = math_ops.cast(nnz,\n                                  dtypes.float32) / (2 * np.prod(dense_shape))\n\n            x_sm = dense_to_csr_sparse_matrix(x_mats)\n            y_sm = dense_to_csr_sparse_matrix(y_mats)\n\n            xy_sparse = sparse_csr_matrix_ops.sparse_matrix_sparse_mat_mul(\n                x_sm, y_sm, type=dtypes.float32)\n\n            with session.Session(\n                config=config_pb2.ConfigProto(\n                    intra_op_parallelism_threads=num_threads)) as sess:\n              nnz_value, ratio_value = self.evaluate((nnz, ratio))\n              name_template = (\n                  \"sparse_matrix_sparse_matmul_%s_N_%d_batch_size_%d_threads_%d\"\n              )\n              device_str = \"cpu\" if device == CPU else \"gpu\"\n              self.run_op_benchmark(\n                  sess,\n                  xy_sparse.op,\n                  name=name_template %\n                  (device_str, dense_shape[-1], batch_size, num_threads),\n                  extras={\n                      \"percentage_nonzero\": ratio_value,\n                      \"num_nonzero\": nnz_value\n                  },\n                  min_iters=50)\n\n  def benchmark_sparse_dense_conversion(self):\n    sparsity = 0.05\n\n    for batch_size in [1, 16]:\n      for num_threads in [1, 4, 12]:\n        dense_shape = [batch_size, 750, 750]\n\n        for device in [CPU, GPU]:\n          if device == GPU and not test_util.is_gpu_available():\n            continue\n\n          with ops.Graph().as_default(), ops.device(device):\n            mats = random_ops.random_uniform(dense_shape, dtype=dtypes.float32)\n            mats_locs = array_ops.where(mats > 1.0 - sparsity)\n\n            sparse_matrices = sparse_csr_matrix_ops.dense_to_csr_sparse_matrix(\n                mats, mats_locs)\n            dense_matrices = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n                sparse_matrices, type=dtypes.float32)\n            nnz = math_ops.reduce_sum(\n                sparse_csr_matrix_ops.sparse_matrix_nnz(sparse_matrices))\n            ratio = math_ops.cast(nnz, dtypes.float32) / np.prod(dense_shape)\n\n            with session.Session(\n                config=config_pb2.ConfigProto(\n                    intra_op_parallelism_threads=num_threads)) as sess:\n              nnz_value, ratio_value = self.evaluate((nnz, ratio))\n              device_str = \"cpu\" if device == CPU else \"gpu\"\n              name_template = (\n                  \"dense_to_sparse_matrix_%s_N_%d_batch_size_%d_num_threads_%d\")\n              self.run_op_benchmark(\n                  sess,\n                  sparse_matrices.op,\n                  name=name_template %\n                  (device_str, dense_shape[-1], batch_size, num_threads),\n                  extras={\n                      \"percentage_nonzero\": ratio_value,\n                      \"num_nonzero\": nnz_value,\n                  },\n                  min_iters=50)\n              name_template = (\n                  \"sparse_matrix_to_dense_%s_N_%d_batch_size_%d_num_threads_%d\")\n              self.run_op_benchmark(\n                  sess,\n                  dense_matrices.op,\n                  name=name_template %\n                  (device_str, dense_shape[-1], batch_size, num_threads),\n                  extras={\n                      \"percentage_nonzero\": ratio_value,\n                      \"num_nonzero\": nnz_value,\n                  },\n                  min_iters=50)\n\n  def benchmark_sparse_cholesky(self):\n    # TODO(anudhyan): Use conversions from SparseTensor instead of to get this\n    # benchmark working for larger matrices. For this to work without GPU, we\n    # need to write CPU kernels for SparseTensor conversions.\n    num_rows = 500\n    density = 0.01\n    # pylint: disable=g-long-lambda\n    sparsify = lambda m: array_ops.where(m > 1. - density, m,\n                                         array_ops.zeros_like(m))\n    # pylint: enable=g-long-lambda\n\n    for batch_size in [1, 16]:\n      for num_threads in [1, 4, 12]:\n        dense_shape = [batch_size, num_rows, num_rows]\n\n        with ops.Graph().as_default(), ops.device(CPU):\n          # Create a \"random\" SPD matrix, by choosing each entry of A between\n          # 0 and 1 at the specified density, and computing 0.5(A + At) + n*I.\n          # This ensures diagonal dominance which implies positive-definiteness.\n          dense_matrix = sparsify(\n              random_ops.random_uniform(dense_shape, dtype=dtypes.float32))\n          spd_dense_matrix = (\n              0.5 *\n              (dense_matrix + array_ops.transpose(dense_matrix, perm=[0, 2, 1]))\n              + num_rows *\n              linalg_ops.eye(dense_shape[-1], batch_shape=[batch_size]))\n\n          # Convert to SparseMatrix and invoke Sparse Cholesky factorization\n          # with AMD Ordering.\n          sparse_matrix = dense_to_csr_sparse_matrix(spd_dense_matrix)\n          ordering_amd = sparse_csr_matrix_ops.sparse_matrix_ordering_amd(\n              sparse_matrix)\n          cholesky_sparse_matrix = (\n              sparse_csr_matrix_ops.sparse_matrix_sparse_cholesky(\n                  sparse_matrix, ordering_amd, type=dtypes.float32))\n\n          nnz = math_ops.reduce_sum(\n              sparse_csr_matrix_ops.sparse_matrix_nnz(sparse_matrix))\n          ratio = math_ops.cast(nnz, dtypes.float32) / np.prod(dense_shape)\n          ordering_amd_name_template = (\n              \"sparse_matrix_ordering_amd_cpu_N_%d_batch_size_%d_threads_%d\")\n          sparse_cholesky_name_template = (\n              \"sparse_matrix_sparse_cholesky_cpu_N_%d_batch_size_%d_threads_%d\")\n          with session.Session(\n              config=config_pb2.ConfigProto(\n                  intra_op_parallelism_threads=num_threads)) as sess:\n            nnz_value, ratio_value = self.evaluate((nnz, ratio))\n            self.run_op_benchmark(\n                sess,\n                ordering_amd.op,\n                name=ordering_amd_name_template %\n                (dense_shape[-1], batch_size, num_threads),\n                extras={\n                    \"percentage_nonzero\": ratio_value,\n                    \"num_nonzero\": nnz_value\n                },\n                min_iters=25)\n            self.run_op_benchmark(\n                sess,\n                cholesky_sparse_matrix.op,\n                name=sparse_cholesky_name_template %\n                (dense_shape[-1], batch_size, num_threads),\n                extras={\n                    \"percentage_nonzero\": ratio_value,\n                    \"num_nonzero\": nnz_value\n                },\n                min_iters=25)\n\n\nif __name__ == \"__main__\":\n  test.main()"