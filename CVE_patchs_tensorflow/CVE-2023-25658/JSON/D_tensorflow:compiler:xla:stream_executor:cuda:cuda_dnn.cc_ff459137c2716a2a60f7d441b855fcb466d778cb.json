"diff --git a/tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc b/tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc\nindex fb0ca7fcdf7..c331220602b 100644\n--- a/tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc\n+++ b/tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc\n@@ -36,8 +36,7 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_stream.h\"\n #include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_timer.h\"\n #include \"tensorflow/compiler/xla/stream_executor/dnn.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/error.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/initialize.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/logging.h\"\n #include \"tensorflow/compiler/xla/stream_executor/plugin_registry.h\"\n #include \"tensorflow/compiler/xla/stream_executor/scratch_allocator.h\"\n@@ -85,7 +84,7 @@ static_assert(CUDNN_VERSION >= 7300, \"cuDNN needs to be version 7.3 or higher\");\n       std::ostringstream oss;                                           \\\n       oss << CudnnStatusToString(_status) << \"\\nin \" << __FILE__ << \"(\" \\\n           << __LINE__ << \"): '\" << #expr << \"'\";                        \\\n-      return tsl::Status(port::error::UNKNOWN, oss.str());              \\\n+      return tsl::Status(tsl::error::UNKNOWN, oss.str());               \\\n     }                                                                   \\\n   } while (false)\n \n@@ -96,7 +95,7 @@ static_assert(CUDNN_VERSION >= 7300, \"cuDNN needs to be version 7.3 or higher\");\n       std::ostringstream oss;                                           \\\n       oss << CudnnStatusToString(_status) << \"\\nin \" << __FILE__ << \"(\" \\\n           << __LINE__ << \"): '\" << #expr << \"' \" << (expr).get_error(); \\\n-      return tsl::Status(port::error::UNKNOWN, oss.str());              \\\n+      return tsl::Status(tsl::error::UNKNOWN, oss.str());               \\\n     }                                                                   \\\n   } while (false)\n \n@@ -417,7 +416,7 @@ tsl::Status CudnnSupport::Init() {\n           \"configuration.\");\n       LOG(ERROR) << error;\n       cudnnDestroy(cudnn_handle);\n-      return tsl::Status(port::error::INTERNAL, error);\n+      return tsl::Status(tsl::error::INTERNAL, error);\n     }\n \n     cudnn_.reset(new CudnnAccess(cudnn_handle));\n@@ -441,7 +440,7 @@ tsl::Status CudnnSupport::Init() {\n     }\n   }\n \n-  return tsl::Status(port::error::INTERNAL,\n+  return tsl::Status(tsl::error::INTERNAL,\n                      absl::StrCat(\"cudnn library could not create a handle: \",\n                                   CudnnStatusToString(status)));\n }\n@@ -1299,7 +1298,7 @@ class CudnnRnnDescriptor : public dnn::RnnDescriptor {\n             ? algorithm_config.algorithm()->tensor_ops_enabled()\n             : allow_tensor_ops;\n     if (use_tensor_ops && !allow_tensor_ops) {\n-      return tsl::Status(port::error::INVALID_ARGUMENT,\n+      return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                          \"Algo requests disallowed tensor op evaluation.\");\n     }\n \n@@ -1658,7 +1657,7 @@ class CudnnRnnSequenceTensorDescriptor\n       GpuExecutor* parent, int max_seq_length, int batch_size, int data_size,\n       cudnnDataType_t data_type) {\n     if (max_seq_length <= 0) {\n-      return tsl::Status(port::error::INVALID_ARGUMENT, \"max_seq_length <= 0\");\n+      return tsl::Status(tsl::error::INVALID_ARGUMENT, \"max_seq_length <= 0\");\n     }\n     int dims[] = {batch_size, data_size, 1};\n     int strides[] = {dims[1] * dims[2], dims[2], 1};\n@@ -1677,7 +1676,7 @@ class CudnnRnnSequenceTensorDescriptor\n       const absl::Span<const int>& seq_lengths, bool time_major,\n       cudnnDataType_t data_type) {\n     if (max_seq_length <= 0) {\n-      return tsl::Status(port::error::INVALID_ARGUMENT, \"max_seq_length <= 0\");\n+      return tsl::Status(tsl::error::INVALID_ARGUMENT, \"max_seq_length <= 0\");\n     }\n     int dims[] = {batch_size, data_size, 1};\n     int strides[] = {dims[1] * dims[2], dims[2], 1};\n@@ -1804,30 +1803,30 @@ tsl::StatusOr<RnnModelDims> ExtractAndCheckRnnForward(\n             model_dims.num_layers * model_dims.dir_count &&\n         input_h_desc.batch_size() == model_dims.batch_size &&\n         input_h_desc.data_size() == model_dims.hidden_size)) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT, \"Invalid input_h shape\");\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT, \"Invalid input_h shape\");\n   }\n   // The LSTM projection will be used if input_h_desc.data_size() <\n   // input_c_desc.data_size()\n   if (!(input_h_desc.num_layers() == input_c_desc.num_layers() &&\n         input_h_desc.batch_size() == input_c_desc.batch_size() &&\n         input_h_desc.data_size() <= input_c_desc.data_size())) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT, \"Invalid input_c shape\");\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT, \"Invalid input_c shape\");\n   }\n   if (!(output_desc.max_seq_length() == model_dims.max_seq_length &&\n         output_desc.batch_size() == model_dims.batch_size &&\n         output_desc.data_size() ==\n             model_dims.hidden_size * model_dims.dir_count)) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT, \"Invalid output shape\");\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT, \"Invalid output shape\");\n   }\n   if (!(input_h_desc.num_layers() == output_h_desc.num_layers() &&\n         input_h_desc.batch_size() == output_h_desc.batch_size() &&\n         input_h_desc.data_size() == output_h_desc.data_size())) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT, \"Invalid output_h shape\");\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT, \"Invalid output_h shape\");\n   }\n   if (!(input_h_desc.num_layers() == output_c_desc.num_layers() &&\n         input_h_desc.batch_size() == output_c_desc.batch_size() &&\n         input_h_desc.data_size() <= output_c_desc.data_size())) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT, \"Invalid output_c shape\");\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT, \"Invalid output_c shape\");\n   }\n \n   return model_dims;\n@@ -1849,7 +1848,7 @@ tsl::Status CheckRNNParameterSize(\n #endif\n   if (static_cast<int64_t>(params_size_in_bytes) !=\n       rnn_desc.ParamsSizeInBytes()) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT,\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                        \"Mismatching RNN parameter size\");\n   }\n   return ::tsl::OkStatus();\n@@ -1997,7 +1996,7 @@ tsl::Status CudnnSupport::DoRnnForwardImpl(\n       // possible. It is still possible for other threads to issue workload on\n       // to this stream. So it could take multiple profiling measurements.\n       if (!timer->Init() || !timer->Start(AsGpuStream(stream))) {\n-        return tsl::Status(port::error::INTERNAL, \"Failed to start timer\");\n+        return tsl::Status(tsl::error::INTERNAL, \"Failed to start timer\");\n       }\n     }\n \n@@ -2020,7 +2019,7 @@ tsl::Status CudnnSupport::DoRnnForwardImpl(\n \n     if (is_profiling) {\n       if (!timer->Stop(AsGpuStream(stream))) {\n-        return tsl::Status(port::error::INTERNAL, \"Failed to stop timer\");\n+        return tsl::Status(tsl::error::INTERNAL, \"Failed to stop timer\");\n       }\n       auto algo_desc = *rnn_desc.algorithm_config().algorithm();\n       output_profile_result->set_algorithm(algo_desc);\n@@ -2058,7 +2057,7 @@ tsl::Status CudnnSupport::DoRnnForwardImpl(\n     // possible. It is still possible for other threads to issue workload on\n     // to this stream. So it could take multiple profiling measurements.\n     if (!timer->Init() || !timer->Start(AsGpuStream(stream))) {\n-      return tsl::Status(port::error::INTERNAL, \"Failed to start timer\");\n+      return tsl::Status(tsl::error::INTERNAL, \"Failed to start timer\");\n     }\n   }\n \n@@ -2130,7 +2129,7 @@ tsl::Status CudnnSupport::DoRnnForwardImpl(\n \n   if (is_profiling) {\n     if (!timer->Stop(AsGpuStream(stream))) {\n-      return tsl::Status(port::error::INTERNAL, \"Failed to stop timer\");\n+      return tsl::Status(tsl::error::INTERNAL, \"Failed to stop timer\");\n     }\n     auto algo_desc = *rnn_desc.algorithm_config().algorithm();\n     output_profile_result->set_algorithm(algo_desc);\n@@ -2204,7 +2203,7 @@ tsl::Status CudnnSupport::DoRnnBackwardImpl(\n       // possible. It is still possible for other threads to issue workload on\n       // to this stream. So it could take multiple profiling measurements.\n       if (!timer->Init() || !timer->Start(AsGpuStream(stream))) {\n-        return tsl::Status(port::error::INTERNAL, \"Failed to start timer\");\n+        return tsl::Status(tsl::error::INTERNAL, \"Failed to start timer\");\n       }\n     }\n \n@@ -2253,7 +2252,7 @@ tsl::Status CudnnSupport::DoRnnBackwardImpl(\n \n     if (is_profiling) {\n       if (!timer->Stop(AsGpuStream(stream))) {\n-        return tsl::Status(port::error::INTERNAL, \"Failed to stop timer\");\n+        return tsl::Status(tsl::error::INTERNAL, \"Failed to stop timer\");\n       }\n       auto algo_desc = *rnn_desc.algorithm_config().algorithm();\n       output_profile_result->set_algorithm(algo_desc);\n@@ -2275,7 +2274,7 @@ tsl::Status CudnnSupport::DoRnnBackwardImpl(\n     // possible. It is still possible for other threads to issue workload on\n     // to this stream. So it could take multiple profiling measurements.\n     if (!timer->Init() || !timer->Start(AsGpuStream(stream))) {\n-      return tsl::Status(port::error::INTERNAL, \"Failed to start timer\");\n+      return tsl::Status(tsl::error::INTERNAL, \"Failed to start timer\");\n     }\n   }\n \n@@ -2362,7 +2361,7 @@ tsl::Status CudnnSupport::DoRnnBackwardImpl(\n \n   if (is_profiling) {\n     if (!timer->Stop(AsGpuStream(stream))) {\n-      return tsl::Status(port::error::INTERNAL, \"Failed to stop timer\");\n+      return tsl::Status(tsl::error::INTERNAL, \"Failed to stop timer\");\n     }\n     auto algo_desc = *rnn_desc.algorithm_config().algorithm();\n     output_profile_result->set_algorithm(algo_desc);\n@@ -2404,7 +2403,7 @@ tsl::Status CudnnSupport::DoCtcLossImpl(\n       /*workspace=*/scratch_memory.opaque(),\n       /*workSpaceSizeInBytes=*/scratch_memory.size()));\n #else\n-  return tsl::Status(port::error::INVALID_ARGUMENT,\n+  return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                      \"No supported cudnnCTCLoss when \"\n                      \"CUDNN_VERSION < 7.6.3\");\n #endif\n@@ -2786,7 +2785,7 @@ tsl::StatusOr<cudnnConvolutionFwdAlgo_t> GetCudnnConvolutionForwardAlgo(\n       return perf_results[r].algo;\n     }\n   }\n-  return tsl::Status(port::error::INTERNAL,\n+  return tsl::Status(tsl::error::INTERNAL,\n                      \"cudnnGetConvolutionForwardAlgorithm_v7 returned \"\n                      \"no suitable algorithms. This could be a cudnn bug.\");\n #else\n@@ -2828,7 +2827,7 @@ GetCudnnConvolutionBackwardDataAlgo(const CudnnHandle& cudnn,\n       return perf_results[r].algo;\n     }\n   }\n-  return tsl::Status(port::error::INTERNAL,\n+  return tsl::Status(tsl::error::INTERNAL,\n                      \"cudnnGetConvolutionBackwardDataAlgorithm_v7 returned \"\n                      \"no suitable algorithms. This could be a cudnn bug.\");\n #else\n@@ -2870,7 +2869,7 @@ GetCudnnConvolutionBackwardFilterAlgo(const CudnnHandle& cudnn,\n       return perf_results[r].algo;\n     }\n   }\n-  return tsl::Status(port::error::INTERNAL,\n+  return tsl::Status(tsl::error::INTERNAL,\n                      \"cudnnGetConvolutionBackwardFilterAlgorithm_v7 returned \"\n                      \"no suitable algorithms. This could be a cudnn bug.\");\n #else\n@@ -2895,7 +2894,7 @@ tsl::StatusOr<DeviceMemory<uint8_t>> AllocateCudnnConvolutionForwardWorkspace(\n     ScratchAllocator* scratch_allocator) {\n   if (IsTensorMathOpSet(conv) != algorithm_desc.tensor_ops_enabled()) {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         \"Mismatch between cudnn conv and algorithm descriptors.\");\n   }\n \n@@ -2917,7 +2916,7 @@ tsl::StatusOr<DeviceMemory<uint8_t>> AllocateCudnnConvolutionForwardWorkspace(\n \n   if (ABSL_PREDICT_FALSE(size_in_bytes_int64_t < 0)) {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         \"cudnnGetConvolutionForwardWorkspaceSize() returned \"\n         \"negative sizeInBytes value. This could be a cudnn bug.\");\n   }\n@@ -2927,7 +2926,7 @@ tsl::StatusOr<DeviceMemory<uint8_t>> AllocateCudnnConvolutionForwardWorkspace(\n   }\n \n   if (ABSL_PREDICT_FALSE(!scratch_allocator)) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT,\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                        \"No scratch allocator provided\");\n   }\n \n@@ -2944,7 +2943,7 @@ AllocateCudnnConvolutionBackwardDataWorkspace(\n     ScratchAllocator* scratch_allocator) {\n   if (IsTensorMathOpSet(conv) != algorithm_desc.tensor_ops_enabled()) {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         \"Mismatch between cudnn conv and algorithm descriptors.\");\n   }\n \n@@ -2967,7 +2966,7 @@ AllocateCudnnConvolutionBackwardDataWorkspace(\n \n   if (ABSL_PREDICT_FALSE(size_in_bytes_int64_t < 0)) {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         \"cudnnGetConvolutionBackwardDataWorkspaceSize() returned \"\n         \"negative sizeInBytes value. This could be a cudnn bug.\");\n   }\n@@ -2977,7 +2976,7 @@ AllocateCudnnConvolutionBackwardDataWorkspace(\n   }\n \n   if (ABSL_PREDICT_FALSE(!scratch_allocator)) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT,\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                        \"No scratch allocator provided\");\n   }\n \n@@ -2994,7 +2993,7 @@ AllocateCudnnConvolutionBackwardFilterWorkspace(\n     ScratchAllocator* scratch_allocator) {\n   if (IsTensorMathOpSet(conv) != algorithm_desc.tensor_ops_enabled()) {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         \"Mismatch between cudnn conv and algorithm descriptors.\");\n   }\n \n@@ -3017,7 +3016,7 @@ AllocateCudnnConvolutionBackwardFilterWorkspace(\n \n   if (ABSL_PREDICT_FALSE(size_in_bytes_int64_t < 0)) {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         \"cudnnGetConvolutionBackwardFilterWorkspaceSize() returned \"\n         \"negative sizeInBytes value. This could be a cudnn bug.\");\n   }\n@@ -3027,7 +3026,7 @@ AllocateCudnnConvolutionBackwardFilterWorkspace(\n   }\n \n   if (ABSL_PREDICT_FALSE(!scratch_allocator)) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT,\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                        \"No scratch allocator provided\");\n   }\n \n@@ -3040,7 +3039,7 @@ tsl::StatusOr<bool> UseTensorOps(Stream* stream, dnn::DataType type,\n   if (desc.has_value()) {\n     use_tensor_ops = desc->tensor_ops_enabled();\n     if (use_tensor_ops && !IsTensorMathEnabled(stream, type)) {\n-      return tsl::Status(port::error::INVALID_ARGUMENT,\n+      return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                          \"Algo requests disabled tensor op evaluation.\");\n     }\n   } else {\n@@ -3162,7 +3161,7 @@ tsl::StatusOr<dnn::AlgorithmDesc> GetCudnnConvolutionBackwardDataAlgorithm(\n   // no_scratch algorithm.\n   if (!algo_desc.has_value()) {\n     return tsl::Status(\n-        port::error::INVALID_ARGUMENT,\n+        tsl::error::INVALID_ARGUMENT,\n         \"The primary convolution algorithm failed memory allocation, \"\n         \"while a secondary algorithm is not provided.\");\n   }\n@@ -3224,7 +3223,7 @@ tsl::StatusOr<dnn::AlgorithmDesc> GetCudnnConvolutionBackwardFilterAlgorithm(\n   // no_scratch algorithm.\n   if (!algo_desc.has_value()) {\n     return tsl::Status(\n-        port::error::INVALID_ARGUMENT,\n+        tsl::error::INVALID_ARGUMENT,\n         absl::StrCat(\n             \"The primary convolution algorithm failed memory allocation, \"\n             \"while a secondary algorithm is not provided. Actual error: \",\n@@ -4254,7 +4253,7 @@ class CudnnLegacyConvRunner : public dnn::ConvRunner {\n       // possible. It is still possible for other threads to issue workload on\n       // to this stream. So it could take multiple profiling measurements.\n       if (!timer->Init() || !timer->Start(AsGpuStream(stream))) {\n-        return tsl::Status(port::error::INTERNAL, \"Failed to start timer\");\n+        return tsl::Status(tsl::error::INTERNAL, \"Failed to start timer\");\n       }\n     }\n \n@@ -4264,7 +4263,7 @@ class CudnnLegacyConvRunner : public dnn::ConvRunner {\n           ToCudnnDataType(input_type_) == CUDNN_DATA_INT8 &&\n           ToCudnnDataType(output_type_) == CUDNN_DATA_FLOAT) {\n         return tsl::Status(\n-            port::error::FAILED_PRECONDITION,\n+            tsl::error::FAILED_PRECONDITION,\n             \"This configuration potentially produces incorrect results.\");\n       }\n #else\n@@ -4336,7 +4335,7 @@ class CudnnLegacyConvRunner : public dnn::ConvRunner {\n \n     if (is_profiling) {\n       if (!timer->Stop(AsGpuStream(stream))) {\n-        return tsl::Status(port::error::INTERNAL, \"Failed to stop timer\");\n+        return tsl::Status(tsl::error::INTERNAL, \"Failed to stop timer\");\n       }\n       profile_result->set_algorithm(algo);\n       profile_result->set_elapsed_time_in_ms(timer->GetElapsedMilliseconds());\n@@ -4631,7 +4630,7 @@ class CudnnExecutionPlanRunner<void(Args...)>\n       // possible. It is still possible for other threads to issue workload on\n       // to this stream. So it could take multiple profiling measurements.\n       if (!timer->Init() || !timer->Start(AsGpuStream(stream))) {\n-        return tsl::Status(port::error::INTERNAL, \"Failed to start timer\");\n+        return tsl::Status(tsl::error::INTERNAL, \"Failed to start timer\");\n       }\n     }\n \n@@ -4641,7 +4640,7 @@ class CudnnExecutionPlanRunner<void(Args...)>\n \n     if (is_profiling) {\n       if (!timer->Stop(AsGpuStream(stream))) {\n-        return tsl::Status(port::error::INTERNAL, \"Failed to stop timer\");\n+        return tsl::Status(tsl::error::INTERNAL, \"Failed to stop timer\");\n       }\n       TF_ASSIGN_OR_RETURN(auto desc, ToAlgorithmDesc());\n       profile_result->set_algorithm(desc);\n@@ -4868,7 +4867,7 @@ tsl::Status CudnnSupport::GetConvolveRunners(\n     }\n     if (!got_algos) {\n       return tsl::Status(\n-          port::error::UNKNOWN,\n+          tsl::error::UNKNOWN,\n           absl::StrFormat(\"Listing algorithms failed for kind %d\", kind));\n     }\n \n@@ -5037,7 +5036,7 @@ class CudnnLegacyFusedConvRunner : public dnn::FusedConvRunner {\n       // possible. It is still possible for other threads to issue workload on\n       // to this stream. So it could take multiple profiling measurements.\n       if (!timer->Init() || !timer->Start(AsGpuStream(stream))) {\n-        return tsl::Status(port::error::INTERNAL, \"Failed to start timer\");\n+        return tsl::Status(tsl::error::INTERNAL, \"Failed to start timer\");\n       }\n     }\n     auto side_input_data_ptr = (side_input_scale_ == 0)\n@@ -5065,7 +5064,7 @@ class CudnnLegacyFusedConvRunner : public dnn::FusedConvRunner {\n             << \"\\noutput_data.opaque() = \" << output_data.opaque();\n \n     if (IsTensorMathOpSet(conv_) != tensor_ops_enabled_) {\n-      return tsl::Status(port::error::FAILED_PRECONDITION,\n+      return tsl::Status(tsl::error::FAILED_PRECONDITION,\n                          \"Tensor op math type in dnn::AlgorithmDesc does not \"\n                          \"match that of the CudnnConvolutionDescriptor\");\n     }\n@@ -5095,7 +5094,7 @@ class CudnnLegacyFusedConvRunner : public dnn::FusedConvRunner {\n \n     if (profile_result) {\n       if (!timer->Stop(AsGpuStream(stream))) {\n-        return tsl::Status(port::error::INTERNAL, \"Failed to stop timer\");\n+        return tsl::Status(tsl::error::INTERNAL, \"Failed to stop timer\");\n       }\n       profile_result->set_algorithm(algo);\n       profile_result->set_elapsed_time_in_ms(timer->GetElapsedMilliseconds());\n@@ -5308,7 +5307,7 @@ tsl::Status CudnnSupport::GetFusedConvolveRunners(\n       activation_mode != dnn::ActivationMode::kElu &&\n       activation_mode != dnn::ActivationMode::kLeakyRelu &&\n       activation_mode != dnn::ActivationMode::kNone) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT,\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                        \"CuDNN fusion only supports activations of \"\n                        \"{Relu, Relu6, Elu, <None>}.\");\n   }\n@@ -5319,7 +5318,7 @@ tsl::Status CudnnSupport::GetFusedConvolveRunners(\n     auto cuda_compute_capability = stream->GetCudaComputeCapability();\n     if (!GetConvolveAlgorithms(cuda_compute_capability, input_type,\n                                &algorithms)) {\n-      return tsl::Status(port::error::UNKNOWN,\n+      return tsl::Status(tsl::error::UNKNOWN,\n                          \"Listing fused convolve algorithms failed.\");\n     }\n \n@@ -5354,7 +5353,7 @@ tsl::Status CudnnSupport::GetFusedConvolveRunners(\n       leakyrelu_alpha, input_descriptor, filter_descriptor, bias_descriptor,\n       output_descriptor, convolution_descriptor, activation_mode, cudnn);\n   if (!op_graph_status.status().ok()) {\n-    return tsl::Status(port::error::INTERNAL,\n+    return tsl::Status(tsl::error::INTERNAL,\n                        absl::StrCat(\"Cudnn graph failed to build: \",\n                                     op_graph_status.status().ToString()));\n   }\n@@ -5391,7 +5390,7 @@ tsl::Status CudnnSupport::GetFusedMatmulRunners(\n       input_type, bias_type, output_type, trans_a, trans_b, m, n, k, lda, ldb,\n       ldc, activation_mode, cudnn);\n   if (!op_graph_status.status().ok()) {\n-    return tsl::Status(port::error::INTERNAL,\n+    return tsl::Status(tsl::error::INTERNAL,\n                        absl::StrCat(\"Cudnn graph failed to build: \",\n                                     op_graph_status.status().ToString()));\n   }\n@@ -5685,7 +5684,7 @@ tsl::Status CudnnSupport::DoBatchNormalizationForwardImpl(\n     if (activation_mode != dnn::ActivationMode::kNone ||\n         !side_input.is_null()) {\n       return tsl::Status(\n-          port::error::INTERNAL,\n+          tsl::error::INTERNAL,\n           absl::StrCat(\n               \"Side input and activation are not supported by cuDNN version: \",\n               CUDNN_VERSION));\n@@ -5968,7 +5967,7 @@ tsl::Status CudnnSupport::DoFusedConvolve(\n \n   if (activation_mode != dnn::ActivationMode::kRelu &&\n       activation_mode != dnn::ActivationMode::kNone) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT,\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                        \"cudnnConvolutionBiasActivationForward() only supports \"\n                        \"Relu or None activation.\");\n   }\n@@ -6070,7 +6069,7 @@ tsl::Status CudnnSupport::DoPrepareForCtcLoss(\n   }\n   *ctc_loss_algo_id = algo;\n #else\n-  return tsl::Status(port::error::INVALID_ARGUMENT,\n+  return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                      \"No supported cudnnGetCTCLossWorkspaceSize when \"\n                      \"CUDNN_VERSION < 7.6.3\");\n #endif\n@@ -6100,7 +6099,7 @@ tsl::Status CudnnSupport::DoCtcLoss(\n     int ctc_loss_algo_id) {\n   // Current cuDNN CTC Loss only supports the float datatype\n   if (CUDNN_VERSION < 7603 || element_type != dnn::DataType::kFloat) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT,\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                        \"CudnnCtcLossDescriptor is supported only when the \"\n                        \"CUDNN_VERSION >= 7.6.3 and DataType is float\");\n   }\n@@ -6382,7 +6381,7 @@ tsl::StatusOr<std::vector<PoolingSplitsSpec>> GetTensorSplits(\n \n   if (max_batches_per_split == 0) {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         absl::StrCat(\n             \"Tensor has too many elements for int32 indexing: batches=\",\n             num_batches, \" elements_per_batch=\", elements_per_batch_input,\n@@ -6442,7 +6441,7 @@ tsl::Status CudnnSupport::DoPoolForward(\n   auto splits_or =\n       GetTensorSplits(input_dimensions, output_dimensions, element_type);\n   if (!splits_or.ok()) {\n-    return tsl::Status(port::error::INTERNAL, \"Cudnn pooling failed to split\");\n+    return tsl::Status(tsl::error::INTERNAL, \"Cudnn pooling failed to split\");\n   }\n   auto splits = std::move(splits_or.value());\n \n@@ -6511,7 +6510,7 @@ tsl::Status CudnnSupport::DoPoolBackward(\n   auto splits_or =\n       GetTensorSplits(input_dimensions, output_dimensions, element_type);\n   if (!splits_or.ok()) {\n-    return tsl::Status(port::error::INTERNAL, \"Cudnn pooling failed to split\");\n+    return tsl::Status(tsl::error::INTERNAL, \"Cudnn pooling failed to split\");\n   }\n   auto splits = std::move(splits_or.value());\n "