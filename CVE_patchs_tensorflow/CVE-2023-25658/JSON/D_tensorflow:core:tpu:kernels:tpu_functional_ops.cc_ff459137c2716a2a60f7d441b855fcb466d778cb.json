"diff --git a/tensorflow/core/tpu/kernels/tpu_functional_ops.cc b/tensorflow/core/tpu/kernels/tpu_functional_ops.cc\nindex 3132238dbdc..4cf5538ae9c 100644\n--- a/tensorflow/core/tpu/kernels/tpu_functional_ops.cc\n+++ b/tensorflow/core/tpu/kernels/tpu_functional_ops.cc\n@@ -1410,7 +1410,7 @@ Status TPUPartitionedCallOp::InitializeVarOnTPU(\n   TF_RETURN_IF_ERROR(\n       InstantiatePartition(*init_graph, fname, device, &fhandle, nullptr));\n \n-  FunctionLibraryRuntime::Options opts;\n+  FunctionLibraryRuntime::Options opts(ctx->step_id());\n   opts.step_container = ctx->step_container();\n   opts.cancellation_manager = ctx->cancellation_manager();\n   opts.stats_collector = ctx->stats_collector();\n@@ -1569,7 +1569,7 @@ Status TPUPartitionedCallOp::InitializeShardedVarOnTPU(\n     functions.push_back(DeviceAndFHandle{.device = target, .handle = handle});\n   }\n \n-  FunctionLibraryRuntime::Options opts;\n+  FunctionLibraryRuntime::Options opts(ctx->step_id());\n \n   // Blocking on threads in the same thread pool is disallowed because\n   // concurrent warm-up requests can exhaust the default thread pool.\n@@ -2702,7 +2702,7 @@ void TPUPartitionedCallOp::ExecuteFunctions(\n     const std::vector<DeviceAndFHandle>& functions, OpKernelContext* ctx,\n     int device_ordinal, int64_t ordinal_selector_req_id, DoneCallback done) {\n   profiler::TraceMe trace_me(\"TPUPartitionedCallOp-ExecuteFunctions\");\n-  FunctionLibraryRuntime::Options opts;\n+  FunctionLibraryRuntime::Options opts(ctx->step_id());\n   opts.step_container = ctx->step_container();\n   opts.stats_collector = ctx->stats_collector();\n   // TODO(akshayka): Consider selecting a runner on a per-device basis,"