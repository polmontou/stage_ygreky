"diff --git a/tensorflow/lite/delegates/xnnpack/README.md b/tensorflow/lite/delegates/xnnpack/README.md\nindex e5e8458a70d..130dd6fe890 100644\n--- a/tensorflow/lite/delegates/xnnpack/README.md\n+++ b/tensorflow/lite/delegates/xnnpack/README.md\n@@ -454,23 +454,34 @@ Below is the list of currently supported floating-point operators:\n * Output size, filter and bias (if present) must be static (use\n   `kTfLiteMmapRo` allocation type).\n \n-### Floating-Point (IEEE FP16) Operators (experimental)\n+### Floating-Point (IEEE FP16) Operators\n \n XNNPACK supports half-precision (using IEEE FP16 format) inference for a subset\n of floating-point operators. XNNPACK automatically enables half-precision\n inference when the following conditions are met:\n \n * XNNPACK runs on hardware that natively supports computations in IEEE FP16\n-format. Currently, this hardware is limited to ARM64 devices with ARMv8.2 FP16\n-arithmetics extension, and includes Android phones starting with Pixel 3,\n-Galaxy S9 (Snapdragon SoC), Galaxy S10 (Exynos SoC), iOS devices with A11 or\n-newer SoCs, and all Apple Silicon Macs.\n+format. Currently, this hardware is limited to ARM & ARM64 devices with\n+ARMv8.2 FP16 arithmetics extension, and includes Android phones starting with\n+Pixel 3, Galaxy S9 (Snapdragon SoC), Galaxy S10 (Exynos SoC), iOS devices with\n+A11 or newer SoCs, all Apple Silicon Macs, and Windows ARM64 laptops based with\n+Snapdragon 850 SoC or newer.\n \n * IEEE FP16 inference is supported for every floating-point operator in the\n model.\n \n * The model's \"reduced_precision_support\" metadata indicates that the model\n-is compatible with FP16 inference.\n+is compatible with FP16 inference. The metadata can be added during model\n+conversion using the `_experimental_supported_accumulation_type` attribute\n+of the [tf.lite.TargetSpec](https://www.tensorflow.org/api_docs/python/tf/lite/TargetSpec)\n+object:\n+\n+```python\n+converter.optimizations = [tf.lite.Optimize.DEFAULT]\n+...\n+converter.target_spec.supported_types = [tf.float16]\n+converter.target_spec._experimental_supported_accumulation_type = tf.dtypes.float16\n+```\n \n When the above conditions are met, XNNPACK replace FP32 operators with their\n FP16 equivalents, and insert additional operators to convert model inputs\n@@ -486,7 +497,7 @@ is used. Forcing FP16 inference has several effects:\n * Besides ARM64 devices with ARMv8.2 FP16 arithmetics extension, forced FP16\n inference is supported on x86/x86-64 devices with AVX2 extension in emulation\n mode: all elementary floating-point operations are computed in FP32, then\n-converted to FP16 and back to FP32. Note that such simulation is not exactly\n+converted to FP16 and back to FP32. Note that such simulation is not bit-exact\n equivalent to native FP16 inference, but simulates the effects of restricted\n mantissa precision and exponent range in the native FP16 arithmetics.\n \n@@ -512,171 +523,10 @@ TfLiteDelegate* xnnpack_delegate =\n     TfLiteXNNPackDelegateCreate(&xnnpack_options);\n ```\n \n-Below is the list of operators supported in IEEE FP16 inference:\n-\n-#### `ABS`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `ADD`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-* Neither of the inputs can be static (use `kTfLiteMmapRo` allocation type).\n-\n-#### `AVERAGE_POOL_2D`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `CEIL`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `CONV_2D`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `CONCATENATION`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-* Neither of the inputs can be static (use `kTfLiteMmapRo` allocation type).\n-\n-#### `DEPTH_TO_SPACE`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `DEPTHWISE_CONV_2D`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `DIV`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-* Neither of the inputs can be static (use `kTfLiteMmapRo` allocation type).\n-\n-#### `FLOOR`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `FULLY_CONNECTED`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `HARD_SWISH`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `LEAKY_RELU`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `LOGISTIC`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `MAX_POOL_2D`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `MAXIMUM`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-* Neither of the inputs can be static (use `kTfLiteMmapRo` allocation type).\n-\n-#### `MEAN`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `MINIMUM`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-* Neither of the inputs can be static (use `kTfLiteMmapRo` allocation type).\n-\n-#### `MUL`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-* Neither of the inputs can be static (use `kTfLiteMmapRo` allocation type).\n-\n-#### `NEG`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `PAD`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `PRELU`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `RELU`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `RELU6`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `RELU_N1_TO_1`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `RESHAPE`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `RESIZE_BILINEAR`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `ROUND`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `SLICE`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `SOFTMAX`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `SPACE_TO_DEPTH`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `SPLIT`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `SQRT`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `SQUARE`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `SQUARED_DIFFERENCE`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-* Neither of the inputs can be static (use `kTfLiteMmapRo` allocation type).\n-\n-#### `STRIDED_SLICE`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `SUB`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-* Neither of the inputs can be static (use `kTfLiteMmapRo` allocation type).\n-\n-#### `TRANSPOSE`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `TRANSPOSE_CONV`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n+XNNPACK has full feature parity between FP32 and FP16 operators: all operators\n+that are supported for FP32 inference are also supported for FP16 inference,\n+and vice versa. In particular, sparse inference operators are supported for FP16\n+inference on ARM processors.\n \n ### Quantized Operators\n \n@@ -855,7 +705,8 @@ Below is the list of currently supported quantized operators:\n \n XNNPACK backend supports sparse inference for CNN models described in the\n [Fast Sparse ConvNets](https://arxiv.org/abs/1911.09723) paper. Sparse\n-inference is restricted to subgraphs with the following operators:\n+inference is restricted to subgraphs with the following floating-point\n+operators:\n \n * Sparse subgraph must store its weights in sparse representation (using\n   `DENSIFY` operators in the TensorFlow Lite schema)."