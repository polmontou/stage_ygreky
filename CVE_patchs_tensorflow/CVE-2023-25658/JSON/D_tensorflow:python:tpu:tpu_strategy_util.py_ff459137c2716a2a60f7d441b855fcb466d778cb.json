"diff --git a/tensorflow/python/tpu/tpu_strategy_util.py b/tensorflow/python/tpu/tpu_strategy_util.py\nindex 9daf746b996..dd74c23f5c7 100644\n--- a/tensorflow/python/tpu/tpu_strategy_util.py\n+++ b/tensorflow/python/tpu/tpu_strategy_util.py\n@@ -22,6 +22,8 @@ from tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver import T\n from tensorflow.python.eager import context\n from tensorflow.python.eager import monitoring\n from tensorflow.python.eager.def_function import function\n+from tensorflow.python.eager.def_function import functions_run_eagerly\n+from tensorflow.python.eager.def_function import run_functions_eagerly\n from tensorflow.python.framework import device\n from tensorflow.python.framework import errors\n from tensorflow.python.framework import ops\n@@ -111,6 +113,15 @@ def initialize_tpu_system(cluster_resolver=None):\n     # The TPU_SYSTEM device must match the device used in tpu.initialize_system\n     # exactly, otherwise you can get errors if there are multiple TPU_SYSTEM\n     # devices available.\n+    run_eagerly = functions_run_eagerly()\n+    if run_eagerly:\n+      logging.warning(\n+          \"It looks like tf.function behavior was disabled, perhaps using\"\n+          \" tf.config.run_functions_eagerly.\"\n+          \" tf.tpu.experimental.initialize_tpu_system requires tf.function to\"\n+          \" work. This primitive will override the disable.\"\n+      )\n+    run_functions_eagerly(False)\n     try:\n       with ops.device(tpu._tpu_system_device_name(job)):  # pylint: disable=protected-access\n         output = _tpu_init_fn()\n@@ -120,7 +131,9 @@ def initialize_tpu_system(cluster_resolver=None):\n           None, None,\n           \"TPUs not found in the cluster. Failed in initialization: \"\n           + str(e))\n-\n+    finally:\n+      if run_eagerly is not None:\n+        run_functions_eagerly(run_eagerly)\n     # Clear out the eager context caches since the memory is invalid now.\n     context.context()._initialize_logical_devices()  # pylint: disable=protected-access\n \n@@ -221,8 +234,21 @@ def shutdown_tpu_system(cluster_resolver=None):\n     # The TPU_SYSTEM device must match the device used in tpu.shutdown_system\n     # exactly, otherwise you can get errors if there are multiple TPU_SYSTEM\n     # devices available.\n-    with ops.device(tpu._tpu_system_device_name(job)):  # pylint: disable=protected-access\n-      _tpu_shutdown_fn()\n+    run_eagerly = functions_run_eagerly()\n+    if run_eagerly:\n+      logging.warning(\n+          \"It looks like tf.function behavior was disabled, perhaps using\"\n+          \" tf.config.run_functions_eagerly.\"\n+          \" tf.tpu.experimental.shutdown_tpu_system requires tf.function to\"\n+          \" work. This primitive will override the disable.\"\n+      )\n+    run_functions_eagerly(False)\n+    try:\n+      with ops.device(tpu._tpu_system_device_name(job)):  # pylint: disable=protected-access\n+        _tpu_shutdown_fn()\n+    finally:\n+      if run_eagerly is not None:\n+        run_functions_eagerly(run_eagerly)\n \n     # Clear out the eager context caches since the memory is invalid now.\n     logging.info(\"Clearing out eager caches\")"