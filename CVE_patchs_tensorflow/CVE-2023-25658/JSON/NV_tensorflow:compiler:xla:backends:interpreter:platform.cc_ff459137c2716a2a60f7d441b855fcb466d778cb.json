"/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/compiler/xla/backends/interpreter/platform.h\"\n\n#include <memory>\n#include <utility>\n\n#include \"absl/strings/str_format.h\"\n#include \"tensorflow/compiler/xla/backends/interpreter/executor.h\"\n#include \"tensorflow/compiler/xla/stream_executor/device_options.h\"\n#include \"tensorflow/compiler/xla/stream_executor/multi_platform_manager.h\"\n#include \"tensorflow/compiler/xla/stream_executor/platform.h\"\n#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n#include \"tensorflow/tsl/platform/status.h\"\n\nnamespace stream_executor {\nnamespace interpreter {\n\nXlaInterpreterPlatform::XlaInterpreterPlatform(const std::string& name,\n                                               const Platform::Id& id)\n    : name_(name), id_(id) {}\n\nXlaInterpreterPlatform::~XlaInterpreterPlatform() {}\n\nPlatform::Id XlaInterpreterPlatform::id() const { return id_; }\n\nint XlaInterpreterPlatform::VisibleDeviceCount() const { return 1; }\n\nconst std::string& XlaInterpreterPlatform::Name() const { return name_; }\n\ntsl::StatusOr<std::unique_ptr<DeviceDescription>>\nXlaInterpreterPlatform::DescriptionForDevice(int ordinal) const {\n  return XlaInterpreterExecutor::CreateDeviceDescription(ordinal);\n}\n\ntsl::StatusOr<StreamExecutor*> XlaInterpreterPlatform::ExecutorForDevice(\n    int ordinal) {\n  StreamExecutorConfig config;\n  config.ordinal = ordinal;\n  config.plugin_config = PluginConfig();\n  config.device_options = DeviceOptions::Default();\n  return GetExecutor(config);\n}\n\ntsl::StatusOr<StreamExecutor*>\nXlaInterpreterPlatform::ExecutorForDeviceWithPluginConfig(\n    int device_ordinal, const PluginConfig& plugin_config) {\n  StreamExecutorConfig config;\n  config.ordinal = device_ordinal;\n  config.plugin_config = plugin_config;\n  config.device_options = DeviceOptions::Default();\n  return GetExecutor(config);\n}\n\ntsl::StatusOr<StreamExecutor*> XlaInterpreterPlatform::GetExecutor(\n    const StreamExecutorConfig& config) {\n  return executor_cache_.GetOrCreate(\n      config, [&]() { return GetUncachedExecutor(config); });\n}\n\ntsl::StatusOr<std::unique_ptr<StreamExecutor>>\nXlaInterpreterPlatform::GetUncachedExecutor(\n    const StreamExecutorConfig& config) {\n  auto executor = std::make_unique<StreamExecutor>(\n      this, std::make_unique<XlaInterpreterExecutor>(config.plugin_config),\n      config.ordinal);\n  auto init_status = executor->Init(config.device_options);\n  if (!init_status.ok()) {\n    return tsl::Status{\n        tsl::error::INTERNAL,\n        absl::StrFormat(\n            \"failed initializing StreamExecutor for device ordinal %d: %s\",\n            config.ordinal, init_status.ToString())};\n  }\n\n  return std::move(executor);\n}\n\nvoid XlaInterpreterPlatform::RegisterTraceListener(\n    std::unique_ptr<TraceListener> listener) {\n  LOG(FATAL) << \"not yet implemented: register executor trace listener\";\n}\n\nvoid XlaInterpreterPlatform::UnregisterTraceListener(TraceListener* listener) {\n  LOG(FATAL) << \"not yet implemented: unregister executor trace listener\";\n}\n\nstatic void InitializeXlaInterpreterPlatform() {\n  std::unique_ptr<Platform> platform(new XlaInterpreterPlatform);\n  TF_CHECK_OK(MultiPlatformManager::RegisterPlatform(std::move(platform)));\n}\n\n}  // namespace interpreter\n}  // namespace stream_executor\n\nREGISTER_MODULE_INITIALIZER(\n    interpreter_platform,\n    stream_executor::interpreter::InitializeXlaInterpreterPlatform());\n\n// Note that module initialization sequencing is not supported in the\n// open-source project, so this will be a no-op there.\nREGISTER_MODULE_INITIALIZER_SEQUENCE(interpreter_platform,\n                                     multi_platform_manager);\nREGISTER_MODULE_INITIALIZER_SEQUENCE(multi_platform_manager_listener,\n                                     interpreter_platform);"