"/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// Implements the StreamExecutor interface by passing through to its\n// implementation_ value (in pointer-to-implementation style), which\n// implements StreamExecutorInterface.\n\n#include \"tensorflow/compiler/xla/stream_executor/stream_executor_pimpl.h\"\n\n#include <atomic>\n#include <cstdint>\n#include <memory>\n#include <utility>\n\n#include \"absl/base/const_init.h\"\n#include \"absl/functional/any_invocable.h\"\n#include \"absl/strings/ascii.h\"\n#include \"absl/strings/str_cat.h\"\n#include \"absl/strings/str_format.h\"\n#include \"absl/synchronization/notification.h\"\n#include \"tensorflow/compiler/xla/stream_executor/blas.h\"\n#include \"tensorflow/compiler/xla/stream_executor/fft.h\"\n#include \"tensorflow/compiler/xla/stream_executor/platform/port.h\"\n#include \"tensorflow/compiler/xla/stream_executor/rng.h\"\n#include \"tensorflow/compiler/xla/stream_executor/stream.h\"\n#include \"tensorflow/compiler/xla/stream_executor/stream_executor_internal.h\"\n#include \"tensorflow/tsl/platform/errors.h\"\n#include \"tensorflow/tsl/platform/stacktrace.h\"\n#include \"tensorflow/tsl/platform/statusor.h\"\n#include \"tensorflow/tsl/platform/threadpool.h\"\n#include \"tensorflow/tsl/util/env_var.h\"\n\nnamespace {\nbool FLAGS_check_device_leaks = false;\n}  // namespace\n\nnamespace stream_executor {\nnamespace {\n\nstd::string StackTraceIfVLOG10() {\n  if (VLOG_IS_ON(10)) {\n    return absl::StrCat(\" \", tsl::CurrentStackTrace(), \"\\n\");\n  } else {\n    return \"\";\n  }\n}\n\n// Make sure the executor is done with its work; we know (because this isn't\n// publicly visible) that all enqueued work is quick.\nvoid BlockOnThreadExecutor(tsl::thread::ThreadPool* executor) {\n  absl::Notification n;\n  executor->Schedule([&n]() { n.Notify(); });\n  n.WaitForNotification();\n}\n\nstd::atomic_int_fast64_t correlation_id_generator(0);\n\n}  // namespace\n\ntemplate <typename BeginCallT, typename CompleteCallT, typename ReturnT,\n          typename... BeginArgsT>\nclass ScopedTracer {\n public:\n  ScopedTracer(StreamExecutor* stream_exec, BeginCallT begin_call,\n               CompleteCallT complete_call, const ReturnT* result,\n               BeginArgsT... begin_args)\n      : stream_exec_(stream_exec),\n        complete_call_(complete_call),\n        result_(result) {\n    if (stream_exec_->tracing_enabled_) {\n      correlation_id_ =\n          correlation_id_generator.fetch_add(1, std::memory_order_relaxed) - 1;\n      Trace(begin_call, begin_args...);\n    }\n  }\n\n  ~ScopedTracer() {\n    if (stream_exec_->tracing_enabled_) {\n      Trace(complete_call_, result_);\n    }\n  }\n\n private:\n  template <typename CallbackT, typename... TraceArgsT>\n  void Trace(CallbackT callback, TraceArgsT... args) {\n    {\n      // Instance tracers held in a block to limit the lock lifetime.\n      absl::ReaderMutexLock lock{&stream_exec_->mu_};\n      for (TraceListener* listener : stream_exec_->listeners_) {\n        (listener->*callback)(correlation_id_,\n                              std::forward<TraceArgsT>(args)...);\n      }\n    }\n  }\n\n  StreamExecutor* stream_exec_;\n  CompleteCallT complete_call_;\n  const ReturnT* result_;\n  int64_t correlation_id_;\n};\n\ntemplate <typename BeginCallT, typename CompleteCallT, typename ReturnT,\n          typename... BeginArgsT>\nScopedTracer<BeginCallT, CompleteCallT, ReturnT, BeginArgsT...>\nMakeScopedTracer(StreamExecutor* stream_exec, BeginCallT begin_call,\n                 CompleteCallT complete_call, ReturnT* result,\n                 BeginArgsT... begin_args) {\n  return ScopedTracer<BeginCallT, CompleteCallT, ReturnT, BeginArgsT...>(\n      stream_exec, begin_call, complete_call, result,\n      std::forward<BeginArgsT>(begin_args)...);\n}\n\n#define SCOPED_TRACE(LOC, ...) \\\n  auto tracer =                \\\n      MakeScopedTracer(this, &LOC##Begin, &LOC##Complete, ##__VA_ARGS__);\n\n/* static */ absl::Mutex StreamExecutor::static_mu_{absl::kConstInit};\n\n// Get per-device memory limit in bytes. Returns 0 if\n// TF_PER_DEVICE_MEMORY_LIMIT_MB environment variable is not set.\nstatic int64_t GetMemoryLimitBytes() {\n  int64_t value;\n  TF_CHECK_OK(\n      tsl::ReadInt64FromEnvVar(\"TF_PER_DEVICE_MEMORY_LIMIT_MB\", 0, &value));\n  return value * (1ll << 20);\n}\n\nStreamExecutor::StreamExecutor(\n    const Platform* platform,\n    std::unique_ptr<internal::StreamExecutorInterface> implementation,\n    int device_ordinal)\n    : platform_(platform),\n      implementation_(std::move(implementation)),\n      device_ordinal_(device_ordinal),\n      background_threads_(new tsl::thread::ThreadPool(\n          tsl::Env::Default(), \"stream_executor\", kNumBackgroundThreads)),\n      live_stream_count_(0),\n      tracing_enabled_(false),\n      mem_alloc_bytes_(0),\n      memory_limit_bytes_(GetMemoryLimitBytes()),\n      allocator_(this) {\n  std::string name = absl::AsciiStrToLower(platform_->Name());\n  if (name == \"cuda\") {\n    platform_kind_ = PlatformKind::kCuda;\n  } else if (name == \"rocm\") {\n    platform_kind_ = PlatformKind::kROCm;\n  } else if (name == \"opencl\") {\n    platform_kind_ = PlatformKind::kOpenCL;\n  } else if (name == \"host\") {\n    platform_kind_ = PlatformKind::kHost;\n  } else {\n    platform_kind_ = PlatformKind::kInvalid;\n  }\n}\n\nStreamExecutor::~StreamExecutor() {\n  BlockOnThreadExecutor(background_threads_.get());\n\n  if (live_stream_count_.load() != 0) {\n    LOG(WARNING) << \"Not all streams were deallocated at executor destruction \"\n                 << \"time. This may lead to unexpected/bad behavior - \"\n                 << \"especially if any stream is still active!\";\n  }\n\n  if (FLAGS_check_device_leaks) {\n    for (const auto& it : mem_allocs_) {\n      LOG(INFO) << \"Memory alloced at executor exit: addr: \"\n                << absl::StrFormat(\"%p\", it.first)\n                << \", bytes: \" << it.second.bytes << \", trace: \\n\"\n                << it.second.stack_trace;\n    }\n  }\n}\n\ntsl::Status StreamExecutor::Init(DeviceOptions device_options) {\n  TF_RETURN_IF_ERROR(\n      implementation_->Init(device_ordinal_, std::move(device_options)));\n  return ::tsl::OkStatus();\n}\n\ntsl::Status StreamExecutor::Init() { return Init(DeviceOptions::Default()); }\n\ntsl::Status StreamExecutor::GetKernel(const MultiKernelLoaderSpec& spec,\n                                      KernelBase* kernel) {\n  return implementation_->GetKernel(spec, kernel);\n}\n\nvoid StreamExecutor::UnloadKernel(const KernelBase* kernel) {\n  implementation_->UnloadKernel(kernel);\n}\n\ntsl::Status StreamExecutor::LoadModule(const MultiModuleLoaderSpec& spec,\n                                       ModuleHandle* module_handle) {\n  return implementation_->LoadModule(spec, module_handle);\n}\n\nbool StreamExecutor::UnloadModule(ModuleHandle module_handle) {\n  return implementation_->UnloadModule(module_handle);\n}\n\ntsl::StatusOr<std::shared_ptr<DeviceMemoryBase>>\nStreamExecutor::CreateOrShareConstant(Stream* stream,\n                                      const std::vector<uint8_t>& content) {\n  return implementation_->CreateOrShareConstant(stream, std::move(content));\n}\n\nvoid StreamExecutor::Deallocate(DeviceMemoryBase* mem) {\n  VLOG(1) << \"Called StreamExecutor::Deallocate(mem=\" << mem->opaque()\n          << \") mem->size()=\" << mem->size() << StackTraceIfVLOG10();\n\n  if (mem->opaque() != nullptr) {\n    EraseAllocRecord(mem->opaque());\n  }\n  implementation_->Deallocate(mem);\n  mem->Reset(nullptr, 0);\n}\n\nvoid StreamExecutor::GetMemAllocs(std::map<void*, AllocRecord>* records_out) {\n  absl::ReaderMutexLock lock(&mu_);\n  *records_out = mem_allocs_;\n}\n\nbool StreamExecutor::CanEnablePeerAccessTo(StreamExecutor* other) {\n  return implementation_->CanEnablePeerAccessTo(other->implementation_.get());\n}\n\ntsl::Status StreamExecutor::EnablePeerAccessTo(StreamExecutor* other) {\n  return implementation_->EnablePeerAccessTo(other->implementation_.get());\n}\n\nconst DeviceDescription& StreamExecutor::GetDeviceDescription() const {\n  absl::MutexLock lock(&mu_);\n  if (device_description_ != nullptr) {\n    return *device_description_;\n  }\n\n  device_description_ = CreateDeviceDescription();\n  return *device_description_;\n}\n\nint64_t StreamExecutor::GetDeviceLoad() const {\n  return implementation_->GetDeviceLoad();\n}\n\nint StreamExecutor::PlatformDeviceCount() const {\n  return implementation_->PlatformDeviceCount();\n}\n\nbool StreamExecutor::SupportsBlas() const {\n  return implementation_->SupportsBlas();\n}\n\nbool StreamExecutor::SupportsRng() const {\n  return implementation_->SupportsRng();\n}\n\nbool StreamExecutor::SupportsDnn() const {\n  return implementation_->SupportsDnn();\n}\n\nbool StreamExecutor::GetConvolveAlgorithms(\n    dnn::ConvolutionKind kind, dnn::DataType input_type,\n    std::vector<dnn::AlgorithmDesc>* out_algorithms) {\n  dnn::DnnSupport* dnn_support = AsDnn();\n  if (!dnn_support) {\n    return false;\n  }\n  switch (kind) {\n    default:\n      return false;\n    case dnn::ConvolutionKind::FORWARD:\n    case dnn::ConvolutionKind::FORWARD_BIAS_ACTIVATION:\n      return dnn_support->GetConvolveAlgorithms(\n          GetDeviceDescription().cuda_compute_capability(), input_type,\n          out_algorithms);\n    case dnn::ConvolutionKind::BACKWARD_DATA:\n      return dnn_support->GetConvolveBackwardDataAlgorithms(\n          GetDeviceDescription().cuda_compute_capability(), input_type,\n          out_algorithms);\n    case dnn::ConvolutionKind::BACKWARD_FILTER:\n      return dnn_support->GetConvolveBackwardFilterAlgorithms(\n          GetDeviceDescription().cuda_compute_capability(), input_type,\n          out_algorithms);\n  }\n}\n\ntsl::Status StreamExecutor::GetConvolveRunners(\n    bool use_cudnn_frontend, dnn::ConvolutionKind kind,\n    dnn::DataType input_type, dnn::DataType output_type, Stream* stream,\n    const dnn::BatchDescriptor& input_descriptor, DeviceMemoryBase input_data,\n    const dnn::FilterDescriptor& filter_descriptor,\n    DeviceMemoryBase filter_data, const dnn::BatchDescriptor& output_descriptor,\n    DeviceMemoryBase output_data,\n    const dnn::ConvolutionDescriptor& convolution_descriptor, bool use_fallback,\n    ScratchAllocator* scratch_allocator,\n    std::vector<std::unique_ptr<const dnn::ConvRunner>>* out_exec_plans) {\n  dnn::DnnSupport* dnn_support = AsDnn();\n  if (!dnn_support) {\n    return tsl::errors::Unimplemented(\"DNN library is not found.\");\n  }\n  return dnn_support->GetConvolveRunners(\n      use_cudnn_frontend, kind, input_type, output_type, stream,\n      input_descriptor, input_data, filter_descriptor, filter_data,\n      output_descriptor, output_data, convolution_descriptor, use_fallback,\n      scratch_allocator, out_exec_plans);\n}\n\ntsl::Status StreamExecutor::GetFusedConvolveRunners(\n    bool use_cudnn_frontend, dnn::ConvolutionKind kind,\n    dnn::DataType input_type, dnn::DataType bias_type,\n    dnn::DataType output_type, double conv_input_scale, double side_input_scale,\n    double leakyrelu_alpha, Stream* stream,\n    const dnn::BatchDescriptor& input_descriptor,\n    const dnn::FilterDescriptor& filter_descriptor,\n    const dnn::BatchDescriptor& bias_descriptor,\n    const dnn::BatchDescriptor& output_descriptor,\n    const dnn::ConvolutionDescriptor& convolution_descriptor, bool use_fallback,\n    dnn::ActivationMode activation_mode,\n    std::vector<std::unique_ptr<const dnn::FusedConvRunner>>* out_exec_plans) {\n  dnn::DnnSupport* dnn_support = AsDnn();\n  if (!dnn_support) {\n    return tsl::errors::Unimplemented(\"DNN library is not found.\");\n  }\n  return dnn_support->GetFusedConvolveRunners(\n      use_cudnn_frontend, kind, input_type, bias_type, output_type,\n      conv_input_scale, side_input_scale, leakyrelu_alpha, stream,\n      input_descriptor, filter_descriptor, bias_descriptor, output_descriptor,\n      convolution_descriptor, use_fallback, activation_mode, out_exec_plans);\n}\n\ntsl::Status StreamExecutor::GetFusedMatmulRunners(\n    bool use_cudnn_frontend, dnn::DataType input_type, dnn::DataType bias_type,\n    dnn::DataType output_type, Stream* stream, bool trans_a, bool trans_b,\n    uint64_t m, uint64_t n, uint64_t k, int64_t lda, int64_t ldb, int64_t ldc,\n    dnn::ActivationMode activation_mode, bool use_fallback,\n    std::vector<std::unique_ptr<const dnn::FusedMatmulRunner>>*\n        out_exec_plans) {\n  dnn::DnnSupport* dnn_support = AsDnn();\n  if (!dnn_support) {\n    return tsl::errors::Unimplemented(\"DNN library is not found.\");\n  }\n\n  return dnn_support->GetFusedMatmulRunners(\n      use_cudnn_frontend, input_type, bias_type, output_type, stream, trans_a,\n      trans_b, m, n, k, lda, ldb, ldc, activation_mode, use_fallback,\n      out_exec_plans);\n}\n\nbool StreamExecutor::GetMIOpenConvolveAlgorithms(\n    dnn::ConvolutionKind kind, dnn::DataType element_type, Stream* stream,\n    const dnn::BatchDescriptor& input_descriptor, DeviceMemoryBase input_data,\n    const dnn::FilterDescriptor& filter_descriptor,\n    DeviceMemoryBase filter_data, const dnn::BatchDescriptor& output_descriptor,\n    DeviceMemoryBase output_data,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    ScratchAllocator* scratch_allocator,\n    std::vector<dnn::ProfileResult>* out_algorithms) {\n  dnn::DnnSupport* dnn_support = AsDnn();\n  if (!dnn_support) {\n    return false;\n  }\n  return dnn_support->GetMIOpenConvolveAlgorithms(\n      kind, element_type, stream, input_descriptor, input_data,\n      filter_descriptor, filter_data, output_descriptor, output_data,\n      convolution_descriptor, scratch_allocator, out_algorithms);\n}\n\nbool StreamExecutor::GetRnnAlgorithms(\n    std::vector<dnn::AlgorithmDesc>* out_algorithms) {\n  dnn::DnnSupport* dnn_support = AsDnn();\n  if (!dnn_support) {\n    return false;\n  }\n  return dnn_support->GetRnnAlgorithms(out_algorithms);\n}\n\nbool StreamExecutor::GetBlasGemmAlgorithms(\n    Stream* stream, std::vector<blas::AlgorithmType>* out_algorithms) {\n  blas::BlasSupport* blas_support = AsBlas();\n  if (!blas_support) {\n    return false;\n  }\n  return blas_support->GetBlasGemmAlgorithms(stream, out_algorithms);\n}\n\ntsl::StatusOr<std::unique_ptr<dnn::RnnDescriptor>>\nStreamExecutor::createRnnDescriptor(\n    int num_layers, int hidden_size, int input_size, int cell_size,\n    int batch_size, dnn::RnnInputMode input_mode,\n    dnn::RnnDirectionMode direction_mode, dnn::RnnMode rnn_mode,\n    dnn::DataType data_type, const dnn::AlgorithmConfig& algorithm_config,\n    float dropout, uint64_t seed, ScratchAllocator* state_allocator,\n    bool use_padded_io) {\n  dnn::DnnSupport* dnn_support = AsDnn();\n  if (!dnn_support) {\n    return tsl::Status(tsl::error::UNKNOWN,\n                       \"Fail to find the dnn implementation.\");\n  }\n  return dnn_support->createRnnDescriptor(\n      num_layers, hidden_size, input_size, cell_size, batch_size, input_mode,\n      direction_mode, rnn_mode, data_type, algorithm_config, dropout, seed,\n      state_allocator, use_padded_io);\n}\n\ntsl::StatusOr<std::unique_ptr<dnn::RnnSequenceTensorDescriptor>>\nStreamExecutor::createRnnSequenceTensorDescriptor(int max_seq_length,\n                                                  int batch_size, int data_size,\n                                                  dnn::DataType data_type) {\n  dnn::DnnSupport* dnn_support = AsDnn();\n  if (!dnn_support) {\n    return tsl::Status(tsl::error::UNKNOWN,\n                       \"Fail to find the dnn implementation.\");\n  }\n  return dnn_support->createRnnSequenceTensorDescriptor(\n      max_seq_length, batch_size, data_size, data_type);\n}\n\ntsl::StatusOr<std::unique_ptr<dnn::RnnSequenceTensorDescriptor>>\nStreamExecutor::createRnnSequenceTensorDescriptor(\n    int max_seq_length, int batch_size, int data_size,\n    const absl::Span<const int>& seq_lengths, bool time_major,\n    dnn::DataType data_type) {\n  dnn::DnnSupport* dnn_support = AsDnn();\n  if (!dnn_support) {\n    return tsl::Status(tsl::error::UNKNOWN,\n                       \"Fail to find the dnn implementation.\");\n  }\n  return dnn_support->createRnnSequenceTensorDescriptor(\n      max_seq_length, batch_size, data_size, seq_lengths, time_major,\n      data_type);\n}\n\ntsl::StatusOr<std::unique_ptr<dnn::RnnStateTensorDescriptor>>\nStreamExecutor::createRnnStateTensorDescriptor(int num_layer, int batch_size,\n                                               int data_size,\n                                               dnn::DataType data_type) {\n  dnn::DnnSupport* dnn_support = AsDnn();\n  if (!dnn_support) {\n    return tsl::Status(tsl::error::UNKNOWN,\n                       \"Fail to find the dnn implementation.\");\n  }\n  return dnn_support->createRnnStateTensorDescriptor(num_layer, batch_size,\n                                                     data_size, data_type);\n}\n\ndnn::DnnSupport* StreamExecutor::AsDnn() {\n  absl::MutexLock lock(&mu_);\n  if (dnn_ != nullptr) {\n    return dnn_.get();\n  }\n\n  dnn_.reset(implementation_->CreateDnn());\n  return dnn_.get();\n}\n\nblas::BlasSupport* StreamExecutor::AsBlas() {\n  absl::MutexLock lock(&mu_);\n  if (blas_ != nullptr) {\n    return blas_.get();\n  }\n\n  blas_.reset(implementation_->CreateBlas());\n  return blas_.get();\n}\n\nfft::FftSupport* StreamExecutor::AsFft() {\n  absl::MutexLock lock(&mu_);\n  if (fft_ != nullptr) {\n    return fft_.get();\n  }\n\n  fft_.reset(implementation_->CreateFft());\n  return fft_.get();\n}\n\nrng::RngSupport* StreamExecutor::AsRng() {\n  absl::MutexLock lock(&mu_);\n  if (rng_ != nullptr) {\n    return rng_.get();\n  }\n\n  rng_.reset(implementation_->CreateRng());\n  return rng_.get();\n}\n\ntsl::Status StreamExecutor::Launch(Stream* stream, const ThreadDim& thread_dims,\n                                   const BlockDim& block_dims,\n                                   const KernelBase& kernel,\n                                   const KernelArgsArrayBase& args) {\n  SubmitTrace(&TraceListener::LaunchSubmit, stream, thread_dims, block_dims,\n              kernel, args);\n\n  return implementation_->Launch(stream, thread_dims, block_dims, kernel, args);\n}\n\ntsl::Status StreamExecutor::BlockHostUntilDone(Stream* stream) {\n  tsl::Status result;\n  SCOPED_TRACE(TraceListener::BlockHostUntilDone, &result, stream);\n\n  result = implementation_->BlockHostUntilDone(stream);\n  return result;\n}\n\ntsl::Status StreamExecutor::GetStatus(Stream* stream) {\n  return implementation_->GetStatus(stream);\n}\n\nDeviceMemoryBase StreamExecutor::Allocate(uint64_t size, int64_t memory_space) {\n  if (memory_limit_bytes_ > 0 &&\n      static_cast<int64_t>(mem_alloc_bytes_ + size) > memory_limit_bytes_) {\n    LOG(WARNING) << \"Not enough memory to allocate \" << size << \" on device \"\n                 << device_ordinal_\n                 << \" within provided limit. [used=\" << mem_alloc_bytes_\n                 << \", limit=\" << memory_limit_bytes_ << \"]\";\n    return DeviceMemoryBase();\n  }\n  DeviceMemoryBase buf = implementation_->Allocate(size, memory_space);\n  VLOG(1) << \"Called StreamExecutor::Allocate(size=\" << size\n          << \", memory_space=\" << memory_space << \") returns \" << buf.opaque()\n          << StackTraceIfVLOG10();\n  CreateAllocRecord(buf.opaque(), size);\n\n  return buf;\n}\n\ntsl::StatusOr<DeviceMemoryBase> StreamExecutor::GetUntypedSymbol(\n    const std::string& symbol_name, ModuleHandle module_handle) {\n  // If failed to get the symbol, opaque/bytes are unchanged. Initialize them to\n  // be nullptr/0 for consistency with DeviceMemory semantics.\n  void* opaque = nullptr;\n  size_t bytes = 0;\n  if (GetSymbol(symbol_name, module_handle, &opaque, &bytes)) {\n    return DeviceMemoryBase(opaque, bytes);\n  }\n\n  return tsl::Status(\n      tsl::error::NOT_FOUND,\n      absl::StrCat(\"Check if module containing symbol \", symbol_name,\n                   \" is loaded (module_handle = \",\n                   reinterpret_cast<uintptr_t>(module_handle.id()), \")\"));\n}\n\nbool StreamExecutor::GetSymbol(const std::string& symbol_name,\n                               ModuleHandle module_handle, void** mem,\n                               size_t* bytes) {\n  return implementation_->GetSymbol(symbol_name, module_handle, mem, bytes);\n}\n\nvoid* StreamExecutor::UnifiedMemoryAllocate(uint64_t bytes) {\n  void* buffer = implementation_->UnifiedMemoryAllocate(bytes);\n  VLOG(1) << \"Called StreamExecutor::UnifiedMemoryAllocate(size=\" << bytes\n          << \") returns \" << buffer << StackTraceIfVLOG10();\n  return buffer;\n}\n\nvoid StreamExecutor::UnifiedMemoryDeallocate(void* location) {\n  VLOG(1) << \"Called StreamExecutor::UnifiedMemoryDeallocate(location=\"\n          << location << \")\" << StackTraceIfVLOG10();\n\n  return implementation_->UnifiedMemoryDeallocate(location);\n}\n\nvoid* StreamExecutor::HostMemoryAllocate(uint64_t size) {\n  void* buffer = implementation_->HostMemoryAllocate(size);\n  VLOG(1) << \"Called StreamExecutor::HostMemoryAllocate(size=\" << size\n          << \") returns \" << buffer << StackTraceIfVLOG10();\n  return buffer;\n}\n\nvoid StreamExecutor::HostMemoryDeallocate(void* location) {\n  VLOG(1) << \"Called StreamExecutor::HostMemoryDeallocate(location=\" << location\n          << \")\" << StackTraceIfVLOG10();\n\n  return implementation_->HostMemoryDeallocate(location);\n}\n\nbool StreamExecutor::HostMemoryRegister(void* location, uint64_t size) {\n  VLOG(1) << \"Called StreamExecutor::HostMemoryRegister(location=\" << location\n          << \", size=\" << size << \")\" << StackTraceIfVLOG10();\n  if (location == nullptr || size == 0) {\n    LOG(WARNING) << \"attempting to register null or zero-sized memory: \"\n                 << location << \"; size \" << size;\n  }\n  return implementation_->HostMemoryRegister(location, size);\n}\n\nbool StreamExecutor::HostMemoryUnregister(void* location) {\n  VLOG(1) << \"Called StreamExecutor::HostMemoryUnregister(location=\" << location\n          << \")\" << StackTraceIfVLOG10();\n  return implementation_->HostMemoryUnregister(location);\n}\n\nbool StreamExecutor::SynchronizeAllActivity() {\n  VLOG(1) << \"Called StreamExecutor::SynchronizeAllActivity()\"\n          << StackTraceIfVLOG10();\n  bool ok = implementation_->SynchronizeAllActivity();\n\n  // This should all be quick and infallible work, so we can perform the\n  // synchronization even in the case of failure.\n  BlockOnThreadExecutor(background_threads_.get());\n\n  return ok;\n}\n\ntsl::Status StreamExecutor::SynchronousMemZero(DeviceMemoryBase* location,\n                                               uint64_t size) {\n  VLOG(1) << \"Called StreamExecutor::SynchronousMemZero(location=\" << location\n          << \", size=\" << size << \")\" << StackTraceIfVLOG10();\n\n  return implementation_->SynchronousMemZero(location, size);\n}\n\ntsl::Status StreamExecutor::SynchronousMemSet(DeviceMemoryBase* location,\n                                              int value, uint64_t size) {\n  VLOG(1) << \"Called StreamExecutor::SynchronousMemSet(location=\" << location\n          << \", value=\" << value << \", size=\" << size << \")\"\n          << StackTraceIfVLOG10();\n\n  return implementation_->SynchronousMemSet(location, value, size);\n}\n\nbool StreamExecutor::SynchronousMemcpy(DeviceMemoryBase* device_dst,\n                                       const void* host_src, uint64_t size) {\n  VLOG(1) << \"Called StreamExecutor::SynchronousMemcpy(device_dst=\"\n          << device_dst->opaque() << \", host_src=\" << host_src\n          << \", size=\" << size << \") H2D\" << StackTraceIfVLOG10();\n\n  // Tracing overloaded methods is very difficult due to issues with type\n  // inference on template args. Since use of these overloaded methods is\n  // discouraged anyway, this isn't a huge deal.\n  tsl::Status status =\n      implementation_->SynchronousMemcpy(device_dst, host_src, size);\n  if (!status.ok()) {\n    LOG(ERROR) << \"synchronous memcpy: \" << status;\n  }\n  return status.ok();\n}\n\nbool StreamExecutor::SynchronousMemcpy(void* host_dst,\n                                       const DeviceMemoryBase& device_src,\n                                       uint64_t size) {\n  VLOG(1) << \"Called StreamExecutor::SynchronousMemcpy(host_dst=\" << host_dst\n          << \", device_src=\" << device_src.opaque() << \", size=\" << size\n          << \") D2H\" << StackTraceIfVLOG10();\n\n  tsl::Status status =\n      implementation_->SynchronousMemcpy(host_dst, device_src, size);\n  if (!status.ok()) {\n    LOG(ERROR) << \"synchronous memcpy: \" << status;\n  }\n  return status.ok();\n}\n\nbool StreamExecutor::SynchronousMemcpy(DeviceMemoryBase* device_dst,\n                                       const DeviceMemoryBase& device_src,\n                                       uint64_t size) {\n  VLOG(1) << \"Called StreamExecutor::SynchronousMemcpy(device_dst=\"\n          << device_dst->opaque() << \", device_src=\" << device_src.opaque()\n          << \", size=\" << size << \") D2D\" << StackTraceIfVLOG10();\n\n  tsl::Status status = implementation_->SynchronousMemcpyDeviceToDevice(\n      device_dst, device_src, size);\n  if (!status.ok()) {\n    LOG(ERROR) << \"synchronous memcpy: \" << status;\n  }\n  return status.ok();\n}\n\ntsl::Status StreamExecutor::SynchronousMemcpyD2H(\n    const DeviceMemoryBase& device_src, int64_t size, void* host_dst) {\n  VLOG(1) << \"Called StreamExecutor::SynchronousMemcpyD2H(device_src=\"\n          << device_src.opaque() << \", size=\" << size\n          << \", host_dst=\" << host_dst << \")\" << StackTraceIfVLOG10();\n\n  tsl::Status result;\n  SCOPED_TRACE(TraceListener::SynchronousMemcpyD2H, &result, device_src, size,\n               host_dst);\n\n  result = implementation_->SynchronousMemcpy(host_dst, device_src, size);\n  if (!result.ok()) {\n    result = tsl::Status(\n        tsl::error::INTERNAL,\n        absl::StrFormat(\"failed to synchronously memcpy device-to-host: device \"\n                        \"%p to host %p size %d: %s\",\n                        device_src.opaque(), host_dst, size,\n                        result.ToString()));\n  }\n\n  return result;\n}\n\ntsl::Status StreamExecutor::SynchronousMemcpyH2D(const void* host_src,\n                                                 int64_t size,\n                                                 DeviceMemoryBase* device_dst) {\n  VLOG(1) << \"Called StreamExecutor::SynchronousMemcpyH2D(host_src=\" << host_src\n          << \", size=\" << size << \", device_dst=\" << device_dst->opaque() << \")\"\n          << StackTraceIfVLOG10();\n\n  tsl::Status result;\n  SCOPED_TRACE(TraceListener::SynchronousMemcpyH2D, &result, host_src, size,\n               device_dst);\n\n  result = implementation_->SynchronousMemcpy(device_dst, host_src, size);\n  if (!result.ok()) {\n    result = tsl::Status(\n        tsl::error::INTERNAL,\n        absl::StrFormat(\"failed to synchronously memcpy host-to-device: host \"\n                        \"%p to device %p size %d: %s\",\n                        host_src, device_dst->opaque(), size,\n                        result.ToString()));\n  }\n\n  return result;\n}\n\nbool StreamExecutor::Memcpy(Stream* stream, void* host_dst,\n                            const DeviceMemoryBase& device_src, uint64_t size) {\n  return implementation_->Memcpy(stream, host_dst, device_src, size);\n}\n\nbool StreamExecutor::Memcpy(Stream* stream, DeviceMemoryBase* device_dst,\n                            const void* host_src, uint64_t size) {\n  return implementation_->Memcpy(stream, device_dst, host_src, size);\n}\n\nbool StreamExecutor::MemcpyDeviceToDevice(Stream* stream,\n                                          DeviceMemoryBase* device_dst,\n                                          const DeviceMemoryBase& device_src,\n                                          uint64_t size) {\n  return implementation_->MemcpyDeviceToDevice(stream, device_dst, device_src,\n                                               size);\n}\n\ntsl::Status StreamExecutor::MemZero(Stream* stream, DeviceMemoryBase* location,\n                                    uint64_t size) {\n  return implementation_->MemZero(stream, location, size);\n}\n\ntsl::Status StreamExecutor::Memset32(Stream* stream, DeviceMemoryBase* location,\n                                     uint32_t pattern, uint64_t size) {\n  CHECK_EQ(0, size % 4)\n      << \"need 32-bit multiple size to fill with 32-bit pattern\";\n  return implementation_->Memset32(stream, location, pattern, size);\n}\n\nbool StreamExecutor::HostCallback(\n    Stream* stream, absl::AnyInvocable<tsl::Status() &&> callback) {\n  return implementation_->HostCallback(stream, std::move(callback));\n}\n\ntsl::Status StreamExecutor::AllocateEvent(Event* event) {\n  return implementation_->AllocateEvent(event);\n}\n\ntsl::Status StreamExecutor::DeallocateEvent(Event* event) {\n  return implementation_->DeallocateEvent(event);\n}\n\ntsl::Status StreamExecutor::RecordEvent(Stream* stream, Event* event) {\n  return implementation_->RecordEvent(stream, event);\n}\n\ntsl::Status StreamExecutor::WaitForEvent(Stream* stream, Event* event) {\n  return implementation_->WaitForEvent(stream, event);\n}\n\nEvent::Status StreamExecutor::PollForEventStatus(Event* event) {\n  return implementation_->PollForEventStatus(event);\n}\n\nbool StreamExecutor::AllocateStream(Stream* stream) {\n  live_stream_count_.fetch_add(1, std::memory_order_relaxed);\n  if (!implementation_->AllocateStream(stream)) {\n    auto count = live_stream_count_.fetch_sub(1);\n    CHECK_GE(count, 0) << \"live stream count should not dip below zero\";\n    LOG(INFO) << \"failed to allocate stream; live stream count: \" << count;\n    return false;\n  }\n\n  return true;\n}\n\nvoid StreamExecutor::DeallocateStream(Stream* stream) {\n  dnn::DnnSupport* dnn;\n  {\n    absl::MutexLock lock(&mu_);\n    dnn = dnn_.get();\n  }\n  if (dnn) {\n    dnn->NotifyStreamDestroyed(stream);\n  }\n  implementation_->DeallocateStream(stream);\n  CHECK_GE(live_stream_count_.fetch_sub(1), 0)\n      << \"live stream count should not dip below zero\";\n}\n\nbool StreamExecutor::CreateStreamDependency(Stream* dependent, Stream* other) {\n  return implementation_->CreateStreamDependency(dependent, other);\n}\n\nbool StreamExecutor::AllocateTimer(Timer* timer) {\n  return implementation_->AllocateTimer(timer);\n}\n\nvoid StreamExecutor::DeallocateTimer(Timer* timer) {\n  return implementation_->DeallocateTimer(timer);\n}\n\nbool StreamExecutor::StartTimer(Stream* stream, Timer* timer) {\n  return implementation_->StartTimer(stream, timer);\n}\n\nbool StreamExecutor::StopTimer(Stream* stream, Timer* timer) {\n  return implementation_->StopTimer(stream, timer);\n}\n\nstd::unique_ptr<DeviceDescription> StreamExecutor::CreateDeviceDescription()\n    const {\n  return implementation_->CreateDeviceDescription().value();\n}\n\nbool StreamExecutor::DeviceMemoryUsage(int64_t* free, int64_t* total) const {\n  return implementation_->DeviceMemoryUsage(free, total);\n}\n\nvoid StreamExecutor::EnqueueOnBackgroundThread(std::function<void()> task) {\n  background_threads_->Schedule(std::move(task));\n}\n\nvoid StreamExecutor::CreateAllocRecord(void* opaque, uint64_t bytes) {\n  if (FLAGS_check_device_leaks && opaque != nullptr && bytes != 0) {\n    absl::MutexLock lock(&mu_);\n    mem_allocs_[opaque] = AllocRecord{bytes, \"\"};\n    mem_alloc_bytes_ += bytes;\n  }\n}\n\nvoid StreamExecutor::EraseAllocRecord(void* opaque) {\n  if (FLAGS_check_device_leaks && opaque != nullptr) {\n    absl::MutexLock lock(&mu_);\n    if (mem_allocs_.find(opaque) == mem_allocs_.end()) {\n      LOG(ERROR) << \"Deallocating unknown pointer: \" << opaque;\n    } else {\n      mem_alloc_bytes_ -= mem_allocs_[opaque].bytes;\n      mem_allocs_.erase(opaque);\n    }\n  }\n}\n\nvoid StreamExecutor::EnableTracing(bool enabled) { tracing_enabled_ = enabled; }\n\nvoid StreamExecutor::RegisterTraceListener(TraceListener* listener) {\n  {\n    absl::MutexLock lock(&mu_);\n    if (listeners_.find(listener) != listeners_.end()) {\n      LOG(INFO) << \"Attempt to register already-registered listener, \"\n                << listener;\n    } else {\n      listeners_.insert(listener);\n    }\n  }\n\n  implementation_->RegisterTraceListener(listener);\n}\n\nbool StreamExecutor::UnregisterTraceListener(TraceListener* listener) {\n  {\n    absl::MutexLock lock(&mu_);\n    if (listeners_.find(listener) == listeners_.end()) {\n      LOG(INFO) << \"Attempt to unregister unknown listener, \" << listener;\n      return false;\n    }\n    listeners_.erase(listener);\n  }\n\n  implementation_->UnregisterTraceListener(listener);\n  return true;\n}\n\nstd::optional<AllocatorStats> StreamExecutor::GetAllocatorStats() {\n  return implementation_->GetAllocatorStats();\n}\n\nbool StreamExecutor::ClearAllocatorStats() {\n  return implementation_->ClearAllocatorStats();\n}\n\ntemplate <typename TraceCallT, typename... ArgsT>\nvoid StreamExecutor::SubmitTrace(TraceCallT trace_call, ArgsT&&... args) {\n  if (tracing_enabled_) {\n    {\n      // instance tracers held in a block to limit the lock lifetime.\n      absl::ReaderMutexLock lock(&mu_);\n      for (TraceListener* listener : listeners_) {\n        (listener->*trace_call)(std::forward<ArgsT>(args)...);\n      }\n    }\n  }\n}\n\ninternal::StreamExecutorInterface* StreamExecutor::implementation() {\n  return implementation_->GetUnderlyingExecutor();\n}\n\nStreamExecutorMemoryAllocator::StreamExecutorMemoryAllocator(\n    StreamExecutor* executor)\n    : DeviceMemoryAllocator(executor->platform()) {\n  stream_executors_ = {executor};\n}\n\nStreamExecutorMemoryAllocator::StreamExecutorMemoryAllocator(\n    const Platform* platform,\n    absl::Span<StreamExecutor* const> stream_executors)\n    : DeviceMemoryAllocator(platform),\n      stream_executors_(stream_executors.begin(), stream_executors.end()) {}\n\ntsl::StatusOr<OwningDeviceMemory> StreamExecutorMemoryAllocator::Allocate(\n    int device_ordinal, uint64_t size, bool retry_on_failure,\n    int64_t memory_space) {\n  TF_ASSIGN_OR_RETURN(StreamExecutor * executor,\n                      GetStreamExecutor(device_ordinal));\n  DeviceMemoryBase result =\n      executor->AllocateArray<uint8_t>(size, memory_space);\n  if (size > 0 && result == nullptr) {\n    return tsl::errors::ResourceExhausted(absl::StrFormat(\n        \"Failed to allocate request for %s (%uB) on device ordinal %d\",\n        tsl::strings::HumanReadableNumBytes(size), size, device_ordinal));\n  }\n  VLOG(3) << absl::StreamFormat(\"Allocated %s (%uB) on device ordinal %d: %p\",\n                                tsl::strings::HumanReadableNumBytes(size), size,\n                                device_ordinal, result.opaque());\n  return OwningDeviceMemory(result, device_ordinal, this);\n}\n\ntsl::Status StreamExecutorMemoryAllocator::Deallocate(int device_ordinal,\n                                                      DeviceMemoryBase mem) {\n  if (!mem.is_null()) {\n    TF_ASSIGN_OR_RETURN(StreamExecutor * executor,\n                        GetStreamExecutor(device_ordinal));\n    VLOG(3) << absl::StreamFormat(\"Freeing %p on device ordinal %d\",\n                                  mem.opaque(), device_ordinal);\n    executor->Deallocate(&mem);\n  }\n  return ::tsl::OkStatus();\n}\n\ntsl::StatusOr<StreamExecutor*> StreamExecutorMemoryAllocator::GetStreamExecutor(\n    int device_ordinal) const {\n  if (device_ordinal < 0) {\n    return tsl::errors::InvalidArgument(absl::StrFormat(\n        \"device ordinal value (%d) must be non-negative\", device_ordinal));\n  }\n  for (StreamExecutor* se : stream_executors_) {\n    if (se->device_ordinal() == device_ordinal) {\n      return se;\n    }\n  }\n  return tsl::errors::NotFound(\n      absl::StrFormat(\"Device %s:%d present but not supported\",\n                      platform()->Name(), device_ordinal));\n}\n\nbool StreamExecutorMemoryAllocator::AllowsAsynchronousDeallocation() const {\n  return false;\n}\n\ntsl::StatusOr<Stream*> StreamExecutorMemoryAllocator::GetStream(\n    int device_ordinal) {\n  CHECK(!AllowsAsynchronousDeallocation())\n      << \"The logic below only works for synchronous allocators\";\n  TF_ASSIGN_OR_RETURN(StreamExecutor * executor,\n                      GetStreamExecutor(device_ordinal));\n  Stream* out = [&] {\n    absl::MutexLock lock(&mutex_);\n    if (!streams_.count(device_ordinal)) {\n      auto p = streams_.emplace(std::piecewise_construct,\n                                std::forward_as_tuple(device_ordinal),\n                                std::forward_as_tuple(executor));\n      p.first->second.Init();\n      return &p.first->second;\n    }\n    return &streams_.at(device_ordinal);\n  }();\n  return out;\n}\n\n}  // namespace stream_executor"