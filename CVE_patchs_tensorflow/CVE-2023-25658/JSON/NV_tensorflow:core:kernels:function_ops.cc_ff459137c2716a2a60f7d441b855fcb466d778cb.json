"/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/core/kernels/function_ops.h\"\n\n#include <deque>\n#include <vector>\n\n#include \"tensorflow/core/common_runtime/device.h\"\n#include \"tensorflow/core/common_runtime/executor.h\"\n#include \"tensorflow/core/common_runtime/function.h\"\n#include \"tensorflow/core/common_runtime/gradients.h\"\n#include \"tensorflow/core/common_runtime/graph_constructor.h\"\n#include \"tensorflow/core/common_runtime/memory_types.h\"\n#include \"tensorflow/core/framework/cancellation.h\"\n#include \"tensorflow/core/framework/full_type.pb.h\"\n#include \"tensorflow/core/framework/full_type_util.h\"\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/graph/algorithm.h\"\n#include \"tensorflow/core/platform/macros.h\"\n#include \"tensorflow/core/platform/tracing.h\"\n#include \"tensorflow/core/profiler/lib/traceme.h\"\n#include \"tensorflow/core/util/device_name_utils.h\"\n\nnamespace tensorflow {\n\nstatic constexpr const char* const kGradientOp =\n    FunctionLibraryDefinition::kGradientOp;\n\nArgOp::ArgOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n  OP_REQUIRES_OK(ctx, ctx->GetAttr(\"T\", &dtype_));\n  OP_REQUIRES_OK(ctx, ctx->GetAttr(\"index\", &index_));\n}\n\nvoid ArgOp::Compute(OpKernelContext* ctx) {\n  auto frame = ctx->call_frame();\n  OP_REQUIRES(ctx, frame != nullptr, errors::Internal(\"no call frame\"));\n  const Tensor* val;\n\n  auto validate_type = [this](const Tensor& val) {\n    if (val.dtype() == dtype_) {\n      return OkStatus();\n    } else {\n      return errors::InvalidArgument(\"Type mismatch: actual \",\n                                     DataTypeString(val.dtype()),\n                                     \" vs. expect \", DataTypeString(dtype_));\n    }\n  };\n\n  if (frame->CanConsumeArg(index_)) {\n    Tensor val;\n    frame->ConsumeArg(index_, &val);\n    OP_REQUIRES_OK(ctx, validate_type(val));\n    ctx->set_output(0, std::move(val));\n  } else {\n    OP_REQUIRES_OK(ctx, frame->GetArg(index_, &val));\n    OP_REQUIRES_OK(ctx, validate_type(*val));\n    ctx->set_output(0, *val);\n  }\n}\n\nRetvalOp::RetvalOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n  OP_REQUIRES_OK(ctx, ctx->GetAttr(\"T\", &dtype_));\n  OP_REQUIRES_OK(ctx, ctx->GetAttr(\"index\", &index_));\n}\n\nvoid RetvalOp::Compute(OpKernelContext* ctx) {\n  const Tensor& val = ctx->input(0);\n  OP_REQUIRES(ctx, val.dtype() == dtype_,\n              errors::InvalidArgument(\"Type mismatch: actual \",\n                                      DataTypeString(val.dtype()),\n                                      \" vs. expect \", DataTypeString(dtype_)));\n  auto frame = ctx->call_frame();\n  OP_REQUIRES(ctx, frame != nullptr, errors::Internal(\"no call frame\"));\n  OP_REQUIRES_OK(ctx, frame->SetRetval(index_, val));\n}\n\nREGISTER_SYSTEM_KERNEL_BUILDER(Name(kArgOp).Device(DEVICE_CPU), ArgOp);\nREGISTER_SYSTEM_KERNEL_BUILDER(Name(kDeviceArgOp).Device(DEVICE_CPU), ArgOp);\nREGISTER_SYSTEM_KERNEL_BUILDER(Name(kRetOp).Device(DEVICE_CPU), RetvalOp);\nREGISTER_SYSTEM_KERNEL_BUILDER(Name(kDeviceRetOp).Device(DEVICE_CPU), RetvalOp);\n\n// TPU ops are only registered when they are required as part of the larger\n// TPU runtime, and does not need to be registered when selective registration\n// is turned on.\nREGISTER_KERNEL_BUILDER(Name(kRetOp).Device(DEVICE_TPU_SYSTEM), RetvalOp);\n\n#define REGISTER(type)     \\\n  REGISTER_KERNEL_BUILDER( \\\n      Name(kArgOp).Device(DEVICE_DEFAULT).TypeConstraint<type>(\"T\"), ArgOp);\nTF_CALL_NUMBER_TYPES_NO_INT32(REGISTER);\nTF_CALL_QUANTIZED_TYPES(REGISTER);\nTF_CALL_bool(REGISTER);\nTF_CALL_float8_e5m2(REGISTER);\nTF_CALL_float8_e4m3fn(REGISTER);\n\nREGISTER_KERNEL_BUILDER(\n    Name(kDeviceArgOp).Device(DEVICE_DEFAULT).TypeConstraint<int32>(\"T\"),\n    ArgOp);\n\nREGISTER_KERNEL_BUILDER(Name(kArgOp)\n                            .Device(DEVICE_DEFAULT)\n                            .HostMemory(\"output\")\n                            .TypeConstraint<int32>(\"T\"),\n                        ArgOp);\n#undef REGISTER\n\nREGISTER_KERNEL_BUILDER(Name(kArgOp)\n                            .Device(DEVICE_DEFAULT)\n                            .HostMemory(\"output\")\n                            .TypeConstraint<ResourceHandle>(\"T\"),\n                        ArgOp);\n\nREGISTER_KERNEL_BUILDER(Name(kArgOp)\n                            .Device(DEVICE_DEFAULT)\n                            .HostMemory(\"output\")\n                            .TypeConstraint<tstring>(\"T\"),\n                        ArgOp);\n\nREGISTER_KERNEL_BUILDER(\n    Name(kArgOp).Device(DEVICE_DEFAULT).TypeConstraint<Variant>(\"T\"), ArgOp);\n\n#define REGISTER(type)                                               \\\n  REGISTER_KERNEL_BUILDER(                                           \\\n      Name(kRetOp).Device(DEVICE_DEFAULT).TypeConstraint<type>(\"T\"), \\\n      RetvalOp);\n\nTF_CALL_NUMBER_TYPES_NO_INT32(REGISTER);\nTF_CALL_QUANTIZED_TYPES(REGISTER);\nTF_CALL_qint16(REGISTER);\nTF_CALL_quint16(REGISTER);\nREGISTER(Variant);\nTF_CALL_bool(REGISTER);\nTF_CALL_float8_e5m2(REGISTER);\nTF_CALL_float8_e4m3fn(REGISTER);\n\nREGISTER_KERNEL_BUILDER(Name(kRetOp)\n                            .Device(DEVICE_DEFAULT)\n                            .HostMemory(\"input\")\n                            .TypeConstraint<int32>(\"T\"),\n                        RetvalOp);\nREGISTER_KERNEL_BUILDER(\n    Name(kDeviceRetOp).Device(DEVICE_DEFAULT).TypeConstraint<int32>(\"T\"),\n    RetvalOp);\n\nREGISTER_KERNEL_BUILDER(Name(kRetOp)\n                            .Device(DEVICE_DEFAULT)\n                            .TypeConstraint<ResourceHandle>(\"T\")\n                            .HostMemory(\"input\"),\n                        RetvalOp);\n\nREGISTER_KERNEL_BUILDER(Name(kRetOp)\n                            .Device(DEVICE_DEFAULT)\n                            .TypeConstraint<tstring>(\"T\")\n                            .HostMemory(\"input\"),\n                        RetvalOp);\n\n#undef REGISTER\n\nclass PassOn : public OpKernel {\n public:\n  explicit PassOn(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES(ctx, ctx->num_inputs() == ctx->num_outputs(),\n                errors::Internal(\"#inputs != #outputs : \", ctx->num_inputs(),\n                                 \" vs. \", ctx->num_outputs()));\n    for (int i = 0; i < ctx->num_inputs(); ++i) {\n      OP_REQUIRES(\n          ctx, input_type(i) == output_type(i),\n          errors::Internal(\"Input and output types for position \", i,\n                           \" do not match: \", DataTypeString(input_type(i)),\n                           \" vs. \", DataTypeString(output_type(i))));\n    }\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    for (int i = 0; i < ctx->num_inputs(); ++i) {\n      ctx->set_output(i, ctx->input(i));\n    }\n  }\n};\n\nREGISTER_SYSTEM_KERNEL_BUILDER(Name(\"_ListToArray\").Device(DEVICE_CPU), PassOn);\nREGISTER_SYSTEM_KERNEL_BUILDER(Name(\"_ArrayToList\").Device(DEVICE_CPU), PassOn);\n\n#define REGISTER_DEFAULT_KERNELS(type)                                       \\\n  REGISTER_KERNEL_BUILDER(                                                   \\\n      Name(\"_ListToArray\").Device(DEVICE_DEFAULT).TypeConstraint<type>(\"T\"), \\\n      PassOn);                                                               \\\n  REGISTER_KERNEL_BUILDER(                                                   \\\n      Name(\"_ArrayToList\").Device(DEVICE_DEFAULT).TypeConstraint<type>(\"T\"), \\\n      PassOn);\n\nREGISTER_DEFAULT_KERNELS(Eigen::half);\nREGISTER_DEFAULT_KERNELS(float);\nREGISTER_DEFAULT_KERNELS(double);\n\n#undef REGISTER_DEFAULT_KERNELS\n\nREGISTER_KERNEL_BUILDER(Name(\"_ListToArray\")\n                            .Device(DEVICE_DEFAULT)\n                            .HostMemory(\"input\")\n                            .HostMemory(\"output\")\n                            .TypeConstraint<int32>(\"T\"),\n                        PassOn);\nREGISTER_KERNEL_BUILDER(Name(\"_ArrayToList\")\n                            .Device(DEVICE_DEFAULT)\n                            .HostMemory(\"input\")\n                            .HostMemory(\"output\")\n                            .TypeConstraint<int32>(\"T\"),\n                        PassOn);\n\nclass SymbolicGradientOp : public AsyncOpKernel {\n public:\n  explicit SymbolicGradientOp(OpKernelConstruction* ctx) : AsyncOpKernel(ctx) {}\n\n  ~SymbolicGradientOp() override {}\n\n  void ComputeAsync(OpKernelContext* ctx, DoneCallback done) override {\n    FunctionLibraryRuntime* lib = ctx->function_library();\n    OP_REQUIRES_ASYNC(ctx, lib != nullptr,\n                      errors::Internal(\"No function library is provided.\"),\n                      done);\n\n    FunctionLibraryRuntime::Handle handle;\n    OP_REQUIRES_OK_ASYNC(\n        ctx, lib->Instantiate(kGradientOp, AttrSlice(def()), &handle), done);\n\n    FunctionLibraryRuntime::Options opts(ctx->step_id());\n    opts.rendezvous = ctx->rendezvous();\n    opts.cancellation_manager = ctx->cancellation_manager();\n    opts.collective_executor = ctx->collective_executor();\n    opts.runner = ctx->runner();\n    opts.run_all_kernels_inline = ctx->run_all_kernels_inline();\n    opts.stats_collector = ctx->stats_collector();\n    opts.step_container = ctx->step_container();\n    std::vector<Tensor> args;\n    args.reserve(ctx->num_inputs());\n    for (int i = 0; i < ctx->num_inputs(); ++i) {\n      args.push_back(ctx->input(i));\n    }\n    std::vector<Tensor>* rets = new std::vector<Tensor>;\n    profiler::TraceMe trace_me(\"SymbolicGradientOp\");\n    lib->Run(opts, handle, args, rets, [ctx, done, rets](const Status& status) {\n      if (!status.ok()) {\n        ctx->SetStatus(status);\n      } else if (rets->size() != ctx->num_outputs()) {\n        ctx->SetStatus(errors::InvalidArgument(\n            \"SymGrad expects to return \", ctx->num_outputs(),\n            \" tensor(s), but get \", rets->size(), \" tensor(s) instead.\"));\n      } else {\n        for (size_t i = 0; i < rets->size(); ++i) {\n          ctx->set_output(i, std::move((*rets)[i]));\n        }\n      }\n      delete rets;\n      done();\n    });\n  }\n\n private:\n  TF_DISALLOW_COPY_AND_ASSIGN(SymbolicGradientOp);\n};\n\nREGISTER_KERNEL_BUILDER(Name(kGradientOp).Device(DEVICE_CPU),\n                        SymbolicGradientOp);\nREGISTER_KERNEL_BUILDER(Name(kGradientOp).Device(DEVICE_DEFAULT),\n                        SymbolicGradientOp);\n\nRemoteCallOp::RemoteCallOp(OpKernelConstruction* ctx)\n    : AsyncOpKernel(ctx), return_type_(ctx->def().experimental_type()) {\n  OP_REQUIRES_OK(ctx,\n                 ctx->GetAttr(FunctionLibraryDefinition::kFuncAttr, &func_));\n  OP_REQUIRES_OK(ctx, ctx->GetAttr(\"Tin\", &input_dtypes_));\n  OP_REQUIRES_OK(ctx, ctx->GetAttr(\"Tout\", &output_dtypes_));\n}\n\nvoid RemoteCallOp::ComputeAsync(OpKernelContext* ctx, DoneCallback done) {\n  FunctionLibraryRuntime* lib = ctx->function_library();\n  OP_REQUIRES_ASYNC(ctx, lib != nullptr,\n                    errors::Internal(\"No function library is provided.\"), done);\n\n  const string& source_device = lib->device()->name();\n  const Tensor* target;\n  OP_REQUIRES_OK_ASYNC(ctx, ctx->input(\"target\", &target), done);\n\n  FunctionTarget function_target;\n  OP_REQUIRES_OK_ASYNC(\n      ctx,\n      DeviceNameUtils::CanonicalizeDeviceName(\n          target->scalar<tstring>()(), source_device, &function_target.first),\n      done);\n  function_target.second = lib;\n\n  const string& target_device = function_target.first;\n  const string& func_name = func_.name();\n\n  FunctionLibraryRuntime::Handle handle;\n  {\n    mutex_lock l(mu_);\n    auto cached_entry = handle_cache_.find(function_target);\n    if (cached_entry != handle_cache_.end()) {\n      handle = cached_entry->second;\n    } else {\n      VLOG(1) << \"Instantiating \" << func_name << \" on \" << target_device;\n      profiler::TraceMe activity(\n          [&] {\n            return strings::StrCat(\"RemoteCall: Instantiate: \", func_name,\n                                   \" on \", target_device);\n          },\n          profiler::TraceMeLevel::kInfo);\n      FunctionLibraryRuntime::InstantiateOptions instantiate_opts;\n      const auto* config = (ctx->function_library())\n                               ? ctx->function_library()->config_proto()\n                               : nullptr;\n      if (config) {\n        instantiate_opts.config_proto = *config;\n      }\n      instantiate_opts.target = target_device;\n      OP_REQUIRES_OK_ASYNC(ctx,\n                           lib->Instantiate(func_name, AttrSlice(&func_.attr()),\n                                            instantiate_opts, &handle),\n                           done);\n      auto insert_result = handle_cache_.insert({function_target, handle});\n      CHECK(insert_result.second) << \"Insert unsuccessful.\";\n      VLOG(1) << \"Instantiated \" << func_name << \" on \" << target_device\n              << \", resulting in handle: \" << handle << \" flr: \" << lib;\n    }\n  }\n\n  OpInputList arguments;\n  OP_REQUIRES_OK_ASYNC(ctx, ctx->input_list(\"args\", &arguments), done);\n\n  FunctionLibraryRuntime::Options opts;\n  opts.runner = nullptr;  // Use default runner at remote device.\n  opts.run_all_kernels_inline = ctx->run_all_kernels_inline();\n  opts.source_device = source_device;\n  if (opts.source_device != target_device) {\n    opts.remote_execution = true;\n  }\n  opts.create_rendezvous = true;\n  CancellationManager* cancel_mgr = nullptr;\n  if (ctx->cancellation_manager() != nullptr) {\n    cancel_mgr = new CancellationManager(ctx->cancellation_manager());\n  }\n  opts.cancellation_manager = cancel_mgr;\n  opts.collective_executor = ctx->collective_executor();\n  std::vector<Tensor> args(arguments.begin(), arguments.end());\n  opts.args_alloc_attrs.reserve(input_dtypes_.size());\n  for (const auto& dtype : input_dtypes_) {\n    AllocatorAttributes arg_alloc_attrs;\n    arg_alloc_attrs.set_on_host(DataTypeAlwaysOnHost(dtype));\n    opts.args_alloc_attrs.push_back(arg_alloc_attrs);\n  }\n  opts.rets_alloc_attrs.reserve(output_dtypes_.size());\n  DCHECK(!return_type_.IsInitialized() ||\n         (return_type_.type_id() == TFT_UNSET) ||\n         (output_dtypes_.size() == return_type_.args_size()))\n      << \"RemoteCall op has a full type information for \"\n      << return_type_.args_size() << \" outputs but the number of outputs is \"\n      << output_dtypes_.size();\n  for (const auto& dtype : output_dtypes_) {\n    AllocatorAttributes ret_alloc_attrs;\n    bool on_host = DataTypeAlwaysOnHost(dtype);\n    if (return_type_.IsInitialized() && (return_type_.type_id() != TFT_UNSET)) {\n      DCHECK(return_type_.type_id() == TFT_PRODUCT)\n          << return_type_.DebugString();\n      FullTypeDef ftd = full_type::GetArgDefaultUnset(\n          return_type_, opts.rets_alloc_attrs.size());\n      if (full_type::IsHostMemoryType(ftd)) {\n        on_host = true;\n      }\n      VLOG(5) << \"FulltypeDef for RemoteCall output=\"\n              << opts.rets_alloc_attrs.size()\n              << \", IsHostMemoryType=\" << full_type::IsHostMemoryType(ftd)\n              << \":\\n\"\n              << ftd.DebugString();\n    }\n    ret_alloc_attrs.set_on_host(on_host);\n    opts.rets_alloc_attrs.push_back(ret_alloc_attrs);\n  }\n  auto* rets = new std::vector<Tensor>;\n  VLOG(1) << \"Running \" << func_name << \" on \" << target_device\n          << \" with handle: \" << handle;\n  profiler::TraceMe trace_me(\n      [&] {\n        return profiler::TraceMeEncode(\n            \"RemoteCallOp\",\n            {{\"func_name\", func_name}, {\"device\", target_device}});\n      },\n      profiler::TraceMeLevel::kInfo);\n  lib->Run(\n      opts, handle, args, rets,\n      [rets, done = std::move(done), func_name, ctx, cancel_mgr,\n       target_device = std::move(function_target.first)](const Status& status) {\n        profiler::TraceMe activity(\n            [&] {\n              return profiler::TraceMeEncode(\n                  \"RemoteCallOpDone\",\n                  {{\"func_name\", func_name}, {\"device\", target_device}});\n            },\n            profiler::TraceMeLevel::kInfo);\n        if (!status.ok()) {\n          ctx->SetStatus(status);\n        } else {\n          for (size_t i = 0; i < rets->size(); ++i) {\n            ctx->set_output(i, std::move((*rets)[i]));\n          }\n        }\n        delete cancel_mgr;\n        delete rets;\n        done();\n      });\n}\n\nstring RemoteCallOp::TraceString(const OpKernelContext& ctx,\n                                 bool verbose) const {\n  string trace_string = profiler::TraceMeOp(\n      strings::StrCat(name_view(), \"__\", func_.name()), type_string_view());\n  if (verbose) {\n    string shape = ShapeTraceString(ctx);\n    if (!shape.empty()) {\n      trace_string =\n          profiler::TraceMeEncode(std::move(trace_string), {{\"shape\", shape}});\n    }\n  }\n  return trace_string;\n}\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"RemoteCall\").Device(DEVICE_CPU).HostMemory(\"target\"), RemoteCallOp);\nREGISTER_KERNEL_BUILDER(\n    Name(\"RemoteCall\").Device(DEVICE_DEFAULT).HostMemory(\"target\"),\n    RemoteCallOp);\n}  // namespace tensorflow"