"/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#include \"tensorflow/core/framework/types.h\"\n#define EIGEN_USE_THREADS\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/common_runtime/device.h\"\n#include \"tensorflow/core/framework/device_base.h\"\n#include \"tensorflow/core/framework/function.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/lib/core/threadpool.h\"\n#include \"tensorflow/core/platform/casts.h\"\n#include \"tensorflow/core/platform/errors.h\"\n#include \"tensorflow/core/platform/macros.h\"\n#include \"tensorflow/core/profiler/lib/traceme.h\"\n\nnamespace tensorflow {\ntypedef Eigen::GpuDevice GPUDevice;\ntypedef Eigen::ThreadPoolDevice CPUDevice;\ntypedef FunctionLibraryRuntime::Handle FHandle;\ntypedef std::vector<Tensor> TensorVec;\n\nnamespace {\n\n// Helper to instantiate function \"func\" in the library \"lib\".\nStatus Instantiate(FunctionLibraryRuntime* lib, const NameAttrList& func,\n                   FunctionLibraryRuntime::Handle* handle) {\n  return lib->Instantiate(func.name(), AttrSlice(&func.attr()), handle);\n}\n\nStatus Instantiate(OpKernelContext* ctx, const NameAttrList& func,\n                   FunctionLibraryRuntime::Handle* handle) {\n  FunctionLibraryRuntime::InstantiateOptions opts;\n  opts.executor_type = ctx->executor_type();\n  return ctx->function_library()->Instantiate(\n      func.name(), AttrSlice(&func.attr()), opts, handle);\n}\n\n// If \"t\" is a scalar of a supported type, returns t != 0 in \"*v\".\nStatus ToBool(gtl::ArraySlice<Tensor> t, bool* v) {\n  if (t.size() != 1) {\n    return errors::InvalidArgument(\n        \"Expected a single scalar which can be converted to a boolean, got \",\n        t.size(), \" tensors.\");\n  }\n  if (TensorShapeUtils::IsScalar(t[0].shape())) {\n    switch (t[0].dtype()) {\n#define CASE(T)                   \\\n  case DataTypeToEnum<T>::value:  \\\n    *v = t[0].scalar<T>()() != 0; \\\n    break;\n\n      CASE(float);\n      CASE(double);\n      CASE(int32);\n      CASE(uint8);\n      CASE(int16);\n      CASE(int8);\n      CASE(int64_t);\n#undef CASE\n      case DT_BOOL:\n        *v = t[0].scalar<bool>()();\n        break;\n      case DT_STRING:\n        *v = !t[0].scalar<tstring>()().empty();\n        break;\n      default:\n        return errors::InvalidArgument(DataTypeString(t[0].dtype()),\n                                       \" cannot be converted to a boolean\");\n    }\n  } else {\n    *v = t[0].NumElements() > 0;\n  }\n  return OkStatus();\n}\n\n// Sets \"rets\" to be the output of \"ctx\". Validates rets' types based\n// on \"kernel\".\nStatus SetOutputs(const OpKernel* kernel, OpKernelContext* ctx,\n                  gtl::ArraySlice<Tensor> rets) {\n  if (rets.size() != ctx->num_outputs()) {\n    return errors::Internal(\"Expect to produce \", ctx->num_outputs(),\n                            \" tensors, but only get \", rets.size());\n  }\n  for (int i = 0; i < rets.size(); ++i) {\n    if (rets[i].dtype() != kernel->output_type(i)) {\n      return errors::Internal(\"Expect \", i, \"-th output is of type \",\n                              DataTypeString(kernel->output_type(i)),\n                              \" but get \", DataTypeString(rets[i].dtype()));\n    }\n    ctx->set_output(i, rets[i]);\n  }\n  return OkStatus();\n}\n\nvoid SetRunOptions(OpKernelContext* ctx, FunctionLibraryRuntime::Options* opts,\n                   bool always_collect_stats) {\n  opts->rendezvous = ctx->rendezvous();\n  opts->cancellation_manager = ctx->cancellation_manager();\n  opts->collective_executor = ctx->collective_executor();\n  if (always_collect_stats) {\n    opts->stats_collector = ctx->stats_collector();\n  }\n  opts->runner = ctx->runner();\n  opts->run_all_kernels_inline = ctx->run_all_kernels_inline();\n  opts->step_container = ctx->step_container();\n}\n\nclass IfOp : public AsyncOpKernel {\n public:\n  explicit IfOp(OpKernelConstruction* ctx) : AsyncOpKernel(ctx) {\n    auto lib = ctx->function_library();\n    OP_REQUIRES(ctx, lib != nullptr, errors::Internal(\"No function library\"));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"then_branch\", &then_func_));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"else_branch\", &else_func_));\n  }\n\n  ~IfOp() override {}\n\n  void ComputeAsync(OpKernelContext* ctx, DoneCallback done) override {\n    FHandle then_handle;\n    FHandle else_handle;\n    OP_REQUIRES_OK_ASYNC(ctx, GetHandles(ctx, &then_handle, &else_handle),\n                         done);\n    bool cond;\n    OP_REQUIRES_OK(ctx, ToBool({ctx->input(0)}, &cond));\n    (new State(this, ctx, cond, then_handle, else_handle, done))->Start();\n  }\n\n private:\n  NameAttrList then_func_;\n  NameAttrList else_func_;\n\n  mutex mu_;\n  std::unordered_map<FunctionLibraryRuntime*, std::pair<FHandle, FHandle>>\n      handles_ ABSL_GUARDED_BY(mu_);\n\n  class State {\n   public:\n    State(IfOp* kernel, OpKernelContext* ctx, bool cond, FHandle then_handle,\n          FHandle else_handle, DoneCallback done)\n        : kernel_(kernel),\n          ctx_(ctx),\n          cond_(cond),\n          then_handle_(then_handle),\n          else_handle_(else_handle),\n          done_(std::move(done)),\n          lib_(CHECK_NOTNULL(ctx_->function_library())),\n          opts_(ctx->step_id()) {\n      SetRunOptions(ctx_, &opts_, true /* always_collect_stats */);\n      for (int i = 1; i < ctx_->num_inputs(); ++i) {\n        args_.push_back(ctx_->input(i));\n      }\n    }\n\n    ~State() {}\n\n    void Start() {\n      FHandle handle = cond_ ? then_handle_ : else_handle_;\n      rets_.clear();\n      profiler::TraceMe trace_me(\"IfOp\");\n      lib_->Run(\n          // Evaluate one of the branch.\n          opts_, handle, args_, &rets_,\n          // Done callback\n          [this](Status s) {\n            if (s.ok()) {\n              s = SetOutputs(kernel_, ctx_, rets_);\n            }\n            ctx_->SetStatus(s);\n            DoneCallback captured_done(std::move(done_));\n            delete this;\n            captured_done();\n          });\n    }\n\n   private:\n    IfOp* const kernel_;\n    OpKernelContext* const ctx_;\n    const bool cond_;\n    FHandle then_handle_;\n    FHandle else_handle_;\n    DoneCallback done_;\n    FunctionLibraryRuntime* const lib_;\n    FunctionLibraryRuntime::Options opts_;\n    TensorVec args_;\n    TensorVec rets_;\n  };\n\n  Status GetHandles(OpKernelContext* ctx, FHandle* then_handle,\n                    FHandle* else_handle) {\n    // TODO(b/37549631): Because this op has `SetIsStateful()` in its\n    // op registration, this kernel may be shared by multiple\n    // subgraphs, which have different associated\n    // `FunctionLibraryRuntime` objects and hence different `FHandle`\n    // namespaces. We currently work around this by caching the map\n    // from `FunctionLibraryRuntime*` to `FHandle` pairs for the two\n    // functions this op uses.\n    auto lib = ctx->function_library();\n    if (lib == nullptr) return errors::Internal(\"No function library\");\n    *then_handle = kInvalidHandle;\n    *else_handle = kInvalidHandle;\n    {\n      tf_shared_lock l(mu_);\n      const auto iter = handles_.find(lib);\n      if (TF_PREDICT_TRUE(iter != handles_.end())) {\n        *then_handle = iter->second.first;\n        *else_handle = iter->second.second;\n      }\n    }\n    if (TF_PREDICT_FALSE(*then_handle == kInvalidHandle)) {\n      mutex_lock l(mu_);\n      const auto iter = handles_.find(lib);\n      if (TF_PREDICT_TRUE(iter != handles_.end())) {\n        *then_handle = iter->second.first;\n        *else_handle = iter->second.second;\n      } else {\n        TF_RETURN_IF_ERROR(Instantiate(ctx, then_func_, then_handle));\n        TF_RETURN_IF_ERROR(Instantiate(ctx, else_func_, else_handle));\n        handles_[lib] = {*then_handle, *else_handle};\n      }\n    }\n    return OkStatus();\n  }\n};\n\nclass CaseOp : public AsyncOpKernel {\n public:\n  explicit CaseOp(OpKernelConstruction* ctx) : AsyncOpKernel(ctx) {\n    auto lib = ctx->function_library();\n    OP_REQUIRES(ctx, lib != nullptr, errors::Internal(\"No function library\"));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"branches\", &branch_funcs_));\n  }\n\n  ~CaseOp() override {}\n\n  void ComputeAsync(OpKernelContext* ctx, DoneCallback done) override {\n    auto lib = ctx->function_library();\n    OP_REQUIRES_ASYNC(ctx, lib != nullptr,\n                      errors::Internal(\"No function library\"), done);\n\n    // TODO(b/37549631): Because this op has `SetIsStateful()` in its op\n    // registration, this kernel may be shared by multiple subgraphs, which have\n    // different associated `FunctionLibraryRuntime` objects and hence different\n    // `FHandle` namespaces. So we must call Instantiate() to make sure we get\n    // the correct function handles with respect to `lib`. Note the underlying\n    // `lib->Instantiate()` caches the created function handles, so calling\n    // `Instantiate()` repeatedly on the same `lib` and function is cheap.\n    std::vector<FHandle> branch_handles(branch_funcs_.size());\n    for (int i = 0; i < branch_funcs_.size(); i++) {\n      OP_REQUIRES_OK_ASYNC(\n          ctx, Instantiate(lib, branch_funcs_[i], &branch_handles[i]), done);\n    }\n\n    const Tensor& branch_index = ctx->input(0);\n    OP_REQUIRES_ASYNC(ctx, TensorShapeUtils::IsScalar(branch_index.shape()),\n                      errors::InvalidArgument(\"branch_index must be scalar\"),\n                      done);\n    int32_t branch = branch_index.scalar<int32>()();\n    (new State(this, ctx, branch, branch_handles, done))->Start();\n  }\n\n private:\n  std::vector<NameAttrList> branch_funcs_;\n\n  class State {\n   public:\n    State(CaseOp* kernel, OpKernelContext* ctx, int branch,\n          std::vector<FHandle> branch_handles, DoneCallback done)\n        : kernel_(kernel),\n          ctx_(ctx),\n          branch_(branch),\n          branch_handles_(branch_handles),\n          done_(std::move(done)),\n          lib_(CHECK_NOTNULL(ctx_->function_library())),\n          opts_(ctx->step_id()) {\n      SetRunOptions(ctx_, &opts_, true /* always_collect_stats */);\n      for (int i = 1; i < ctx_->num_inputs(); ++i) {\n        args_.push_back(ctx_->input(i));\n      }\n    }\n\n    ~State() {}\n\n    void Start() {\n      int branch = branch_;\n      // The last branch is the default branch.\n      if (branch < 0 || branch >= branch_handles_.size()) {\n        branch = branch_handles_.size() - 1;\n      }\n      rets_.clear();\n      profiler::TraceMe trace_me(\"CaseOp\");\n      lib_->Run(\n          // Evaluate one of the branch.\n          opts_, branch_handles_[branch], args_, &rets_,\n          // Done callback\n          [this](Status s) {\n            if (s.ok()) {\n              s = SetOutputs(kernel_, ctx_, rets_);\n            }\n            ctx_->SetStatus(s);\n            DoneCallback captured_done(std::move(done_));\n            delete this;\n            captured_done();\n          });\n    }\n\n   private:\n    CaseOp* const kernel_;\n    OpKernelContext* const ctx_;\n    const int branch_;\n    std::vector<FHandle> branch_handles_;\n    DoneCallback done_;\n    FunctionLibraryRuntime* const lib_;\n    FunctionLibraryRuntime::Options opts_;\n    TensorVec args_;\n    TensorVec rets_;\n  };\n};\n\n// TODO(drpng): remove this.\nREGISTER_KERNEL_BUILDER(Name(\"_If\").Device(DEVICE_CPU), IfOp);\nREGISTER_KERNEL_BUILDER(Name(\"_If\").Device(DEVICE_DEFAULT).HostMemory(\"cond\"),\n                        IfOp);\n\nREGISTER_KERNEL_BUILDER(Name(\"If\").Device(DEVICE_CPU), IfOp);\nREGISTER_KERNEL_BUILDER(Name(\"If\").Device(DEVICE_DEFAULT).HostMemory(\"cond\"),\n                        IfOp);\n\nREGISTER_KERNEL_BUILDER(Name(\"Case\").Device(DEVICE_CPU), CaseOp);\nREGISTER_KERNEL_BUILDER(\n    Name(\"Case\").Device(DEVICE_DEFAULT).HostMemory(\"branch_index\"), CaseOp);\nREGISTER_KERNEL_BUILDER(Name(\"StatelessCase\").Device(DEVICE_CPU), CaseOp);\nREGISTER_KERNEL_BUILDER(\n    Name(\"StatelessCase\").Device(DEVICE_DEFAULT).HostMemory(\"branch_index\"),\n    CaseOp);\n\nREGISTER_KERNEL_BUILDER(Name(\"StatelessIf\").Device(DEVICE_CPU), IfOp);\nREGISTER_KERNEL_BUILDER(\n    Name(\"StatelessIf\").Device(DEVICE_DEFAULT).HostMemory(\"cond\"), IfOp);\n\nclass WhileOp : public AsyncOpKernel {\n public:\n  explicit WhileOp(OpKernelConstruction* ctx) : AsyncOpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"cond\", &cond_func_));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"body\", &body_func_));\n  }\n\n  ~WhileOp() override {}\n\n  void ComputeAsync(OpKernelContext* ctx, DoneCallback done) override {\n    if (ctx->run_all_kernels_inline()) {\n      // Use the non-callback-based implementation when kernels (and function\n      // callbacks) execute inline to avoid stack overflow.\n      OP_REQUIRES_OK_ASYNC(ctx, DoComputeSync(ctx), done);\n      done();\n    } else {\n      FHandle cond_handle;\n      FHandle body_handle;\n      OP_REQUIRES_OK_ASYNC(ctx, GetHandles(ctx, &cond_handle, &body_handle),\n                           done);\n      (new State(this, ctx, cond_handle, body_handle, done))->Start();\n    }\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    // Use the non-callback-based implementation when the synchronous Compute()\n    // method is invoked, because the caller is explicitly donating a thread.\n    Status s = DoComputeSync(ctx);\n    // NOTE: Unfortunately, we cannot use OP_REQUIRES_OK here, because this is\n    // still an AsyncOpKernel, and there is a run-time check to avoid calling\n    // OP_REQUIRES_OK in AsyncOpKernel::ComputeAsync() (which would deadlock in\n    // the event of an error).\n    if (TF_PREDICT_FALSE(!s.ok())) {\n      ctx->SetStatus(s);\n    }\n  }\n\n private:\n  NameAttrList cond_func_;\n  NameAttrList body_func_;\n\n  mutex mu_;\n  std::unordered_map<FunctionLibraryRuntime*, std::pair<FHandle, FHandle>>\n      handles_ ABSL_GUARDED_BY(mu_);\n\n  static Status CondResultToBool(OpKernelContext* ctx,\n                                 const FunctionLibraryRuntime::Options& opts,\n                                 const Tensor& cond_t, bool* out_result) {\n    bool is_pluggable = ctx->op_device_context() &&\n                        ctx->op_device_context()->IsPluggableDevice();\n    const DeviceBase::AcceleratorDeviceInfo* accelerator_device_info =\n        ctx->device()->tensorflow_accelerator_device_info();\n    const bool is_hostmem_dtype =\n        cond_t.dtype() == DT_INT32 || cond_t.dtype() == DT_INT64;\n    if (!is_hostmem_dtype && (is_pluggable || accelerator_device_info) &&\n        (opts.rets_alloc_attrs.empty() ||\n         !opts.rets_alloc_attrs[0].on_host())) {\n      // Copy the ret value to host if it's allocated on device.\n      Device* device = down_cast<Device*>(ctx->device());\n      DeviceContext* device_ctx = ctx->op_device_context();\n      Tensor host_cond_t = Tensor(cond_t.dtype(), cond_t.shape());\n      TF_RETURN_IF_ERROR(device_ctx->CopyDeviceTensorToCPUSync(\n          &cond_t, /*tensor_name=*/\"\", device, &host_cond_t));\n      return ToBool({host_cond_t}, out_result);\n    }\n    return ToBool({cond_t}, out_result);\n  }\n\n  // The initial loop variable args are the inputs to the kernel.\n  //\n  // We attempt to forward the input so that it can be consumed inside the\n  // body function (and participate in buffer forwarding, etc.).\n  static void GetArgsFromContext(OpKernelContext* ctx,\n                                 std::vector<Tensor>* out_args,\n                                 DataTypeVector* out_var_types) {\n    const int num_loop_vars = ctx->num_inputs();\n    out_args->reserve(num_loop_vars);\n    out_var_types->resize(num_loop_vars);\n    for (int i = 0; i < num_loop_vars; ++i) {\n      const Tensor& input = ctx->input(i);\n      (*out_var_types)[i] = input.dtype();\n      std::unique_ptr<Tensor> maybe_forwarded_input = ctx->forward_input(\n          i, /* output_index= */ OpKernelContext::Params::kNoReservation,\n          input.dtype(), input.shape(), ctx->input_memory_type(i),\n          ctx->input_alloc_attr(i));\n      if (maybe_forwarded_input) {\n        out_args->push_back(std::move(*maybe_forwarded_input));\n      } else {\n        out_args->push_back(input);\n      }\n    }\n  }\n\n  class BodyFuncCallFrame : public CallFrameInterface {\n   public:\n    BodyFuncCallFrame(std::vector<Tensor>* args, std::vector<Tensor>* retvals,\n                      DataTypeSlice ret_types)\n        : args_(args), retvals_(retvals), ret_types_(ret_types) {}\n\n    size_t num_args() const override { return args_->size(); }\n    size_t num_retvals() const override { return retvals_->size(); }\n\n    Status GetArg(int index, const Tensor** val) override {\n      if (index < args_->size()) {\n        *val = &(*args_)[index];\n        return OkStatus();\n      } else {\n        return errors::InvalidArgument(\"Argument \", index, \" is out of range.\");\n      }\n    }\n\n    void ConsumeArg(int index, Tensor* val) override {\n      DCHECK_GE(index, 0);\n      DCHECK_LT(index, args_->size());\n      *val = std::move((*args_)[index]);\n    }\n    bool CanConsumeArg(int index) const override {\n      return index >= 0 && index < args_->size();\n    }\n\n    Status SetRetval(int index, const Tensor& val) override {\n      if (TF_PREDICT_FALSE(index < 0)) {\n        return errors::InvalidArgument(\n            \"Expected non-negative return value index, but got: \", index, \".\");\n      } else if (TF_PREDICT_FALSE(index >= retvals_->size())) {\n        return errors::InvalidArgument(\"While loop body returned \", index + 1,\n                                       \" arguments. Expected: \", num_retvals(),\n                                       \".\");\n      } else if (TF_PREDICT_FALSE(val.dtype() != ret_types_[index])) {\n        return errors::InvalidArgument(\"Expected type \",\n                                       DataTypeString(ret_types_[index]),\n                                       \" for return value \", index, \" but got \",\n                                       DataTypeString(val.dtype()), \".\");\n      }\n      (*retvals_)[index] = val;\n      return OkStatus();\n    }\n\n   private:\n    std::vector<Tensor>* const args_;     // Not owned.\n    std::vector<Tensor>* const retvals_;  // Not owned.\n    DataTypeSlice ret_types_;\n\n    TF_DISALLOW_COPY_AND_ASSIGN(BodyFuncCallFrame);\n  };\n\n  class State {\n   public:\n    State(WhileOp* kernel, OpKernelContext* ctx, FHandle cond_handle,\n          FHandle body_handle, DoneCallback done)\n        : kernel_(kernel),\n          ctx_(ctx),\n          cond_handle_(cond_handle),\n          body_handle_(body_handle),\n          done_(std::move(done)),\n          lib_(CHECK_NOTNULL(ctx_->function_library())),\n          opts_(ctx->step_id()) {\n      SetRunOptions(ctx_, &opts_, false /* always_collect_stats */);\n      GetArgsFromContext(ctx, &args_, &loop_var_types_);\n      body_frame_ =\n          std::make_unique<BodyFuncCallFrame>(&args_, &rets_, loop_var_types_);\n    }\n\n    ~State() {}\n\n    void Start() { EvalCond(); }\n\n   private:\n    WhileOp* const kernel_;\n    OpKernelContext* const ctx_;\n    const FHandle cond_handle_;\n    const FHandle body_handle_;\n    const DoneCallback done_;\n    FunctionLibraryRuntime* const lib_;\n    FunctionLibraryRuntime::Options opts_;\n    TensorVec args_;\n    TensorVec rets_;\n    DataTypeVector loop_var_types_;\n    std::unique_ptr<BodyFuncCallFrame> body_frame_;\n\n    void EvalCond() {\n      profiler::TraceMe trace_me(\"WhileOp-EvalCond\");\n      lib_->Run(\n          // Evaluate the condition.\n          opts_, cond_handle_, args_, &rets_,\n          // Done cb.\n          [this](const Status& s) {\n            if (!s.ok()) {\n              return Finish(s);\n            }\n            StartBody();\n          });\n    }\n\n    void StartBody() {\n      Status s;\n      if (rets_.size() != 1) {\n        s = errors::InvalidArgument(\n            \"Expected a single scalar return value from WhileOp cond, got \",\n            rets_.size(), \" tensors.\");\n        return Finish(s);\n      }\n\n      if (!s.ok()) {\n        return Finish(s);\n      }\n      bool cond;\n      s = CondResultToBool(ctx_, opts_, rets_[0], &cond);\n      if (!s.ok()) {\n        return Finish(s);\n      }\n\n      if (!cond) {\n        return Finish(OkStatus());\n      }\n      rets_.clear();\n      rets_.resize(args_.size());\n      profiler::TraceMe trace_me(\"WhileOp-StartBody\");\n      lib_->Run(\n          // Evaluate the body.\n          opts_, body_handle_, body_frame_.get(),\n          // Done callback\n          [this](const Status& s) {\n            if (!s.ok()) {\n              return Finish(s);\n            }\n            if (args_.size() != rets_.size()) {\n              return Finish(errors::InvalidArgument(\n                  \"While loop body returned \", rets_.size(),\n                  \" arguments. Expected: \", args_.size()));\n            }\n            args_.clear();\n            using std::swap;\n            swap(args_, rets_);\n            EvalCond();\n          });\n    }\n\n    void Finish(Status s) {\n      if (s.ok()) {\n        s = SetOutputs(kernel_, ctx_, args_);\n      }\n      ctx_->SetStatus(s);\n      done_();\n      delete this;\n    }\n  };\n\n  Status DoComputeSync(OpKernelContext* ctx) {\n    FHandle cond_handle;\n    FHandle body_handle;\n    TF_RETURN_IF_ERROR(GetHandles(ctx, &cond_handle, &body_handle));\n    auto lib = ctx->function_library();\n    FunctionLibraryRuntime::Options opts;\n    SetRunOptions(ctx, &opts, false /* always_collect_stats */);\n\n    // Pre-allocate argument and return value vectors for the cond and body\n    // functions.\n    std::vector<Tensor> args;\n    const int num_loop_vars = ctx->num_inputs();\n    DataTypeVector loop_var_types(num_loop_vars);\n    GetArgsFromContext(ctx, &args, &loop_var_types);\n    std::vector<Tensor> cond_rets;\n    cond_rets.reserve(1);\n    std::vector<Tensor> body_rets;\n    body_rets.reserve(num_loop_vars);\n\n    // Implement the logic of the while loop as a single C++ do-while loop that\n    // executes the cond and body functions synchronously.\n    do {\n      // Evaluate the cond function on the current loop variables.\n      {\n        profiler::TraceMe trace_me(\"WhileOp-EvalCond\");\n        TF_RETURN_IF_ERROR(lib->RunSync(opts, cond_handle, args, &cond_rets));\n      }\n      if (cond_rets.size() != 1) {\n        return errors::InvalidArgument(\n            \"Expected a single scalar return value from WhileOp cond, got \",\n            cond_rets.size(), \" tensors.\");\n      }\n\n      // If the cond function evaluates to false, we are done: output the\n      // current loop variables.\n      bool cond_result;\n      TF_RETURN_IF_ERROR(\n          CondResultToBool(ctx, opts, cond_rets[0], &cond_result));\n      if (!cond_result) {\n        return SetOutputs(this, ctx, args);\n      }\n\n      // Evaluate the body function on the current loop variables, to get an\n      // updated vector of loop variables.\n      {\n        profiler::TraceMe trace_me(\"WhileOp-StartBody\");\n        body_rets.resize(num_loop_vars);\n        BodyFuncCallFrame call_frame(&args, &body_rets, loop_var_types);\n        TF_RETURN_IF_ERROR(lib->RunSync(opts, body_handle, &call_frame));\n      }\n      std::swap(body_rets, args);\n      body_rets.clear();\n    } while (true);\n  }\n\n  Status GetHandles(OpKernelContext* ctx, FHandle* cond_handle,\n                    FHandle* body_handle) {\n    // TODO(b/37549631): Because this op has `SetIsStateful()` in its\n    // op registration, this kernel may be shared by multiple\n    // subgraphs, which have different associated\n    // `FunctionLibraryRuntime` objects and hence different `FHandle`\n    // namespaces. We currently work around this by caching the map\n    // from `FunctionLibraryRuntime*` to `FHandle` pairs for the two\n    // functions this op uses.\n    auto lib = ctx->function_library();\n    if (lib == nullptr) return errors::Internal(\"No function library\");\n    *cond_handle = kInvalidHandle;\n    *body_handle = kInvalidHandle;\n    {\n      tf_shared_lock l(mu_);\n      const auto iter = handles_.find(lib);\n      if (TF_PREDICT_TRUE(iter != handles_.end())) {\n        *cond_handle = iter->second.first;\n        *body_handle = iter->second.second;\n      }\n    }\n    if (TF_PREDICT_FALSE(*cond_handle == kInvalidHandle)) {\n      mutex_lock l(mu_);\n      const auto iter = handles_.find(lib);\n      if (TF_PREDICT_TRUE(iter != handles_.end())) {\n        *cond_handle = iter->second.first;\n        *body_handle = iter->second.second;\n      } else {\n        TF_RETURN_IF_ERROR(Instantiate(ctx, cond_func_, cond_handle));\n        TF_RETURN_IF_ERROR(Instantiate(ctx, body_func_, body_handle));\n        handles_[lib] = {*cond_handle, *body_handle};\n      }\n    }\n    return OkStatus();\n  }\n};\n// TODO(drpng): remove these.\nREGISTER_KERNEL_BUILDER(Name(\"_While\").Device(DEVICE_CPU), WhileOp);\nREGISTER_KERNEL_BUILDER(Name(\"_While\").Device(DEVICE_DEFAULT), WhileOp);\n\nREGISTER_KERNEL_BUILDER(Name(\"While\").Device(DEVICE_CPU), WhileOp);\nREGISTER_KERNEL_BUILDER(Name(\"While\").Device(DEVICE_DEFAULT), WhileOp);\n\nREGISTER_KERNEL_BUILDER(Name(\"StatelessWhile\").Device(DEVICE_CPU), WhileOp);\nREGISTER_KERNEL_BUILDER(Name(\"StatelessWhile\").Device(DEVICE_DEFAULT), WhileOp);\n\nclass ToBoolOp : public OpKernel {\n public:\n  explicit ToBoolOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n  void Compute(OpKernelContext* ctx) override {\n    bool b;\n    OP_REQUIRES_OK(ctx, ToBool({ctx->input(0)}, &b));\n    Tensor* out;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({}), &out));\n    out->scalar<bool>()() = b;\n  }\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"ToBool\").Device(DEVICE_CPU), ToBoolOp);\n\nStatus GetScalar(OpKernelContext* ctx, int index, int32* value,\n                 const char* label) {\n  Tensor t = ctx->input(index);\n  if (!TensorShapeUtils::IsScalar(t.shape())) {\n    return errors::InvalidArgument(label, \" must be a scalar, but \",\n                                   t.shape().DebugString());\n  }\n  *value = t.scalar<int32>()();\n  return OkStatus();\n}\n\nclass ForOp : public AsyncOpKernel {\n public:\n  explicit ForOp(OpKernelConstruction* ctx) : AsyncOpKernel(ctx) {\n    auto lib = ctx->function_library();\n    OP_REQUIRES(ctx, lib != nullptr, errors::Internal(\"No function library\"));\n    const NameAttrList* func;\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"body\", &func));\n    OP_REQUIRES_OK(ctx, Instantiate(lib, *func, &body_handle_));\n  }\n\n  ~ForOp() override {}\n\n  void ComputeAsync(OpKernelContext* ctx, DoneCallback done) override {\n    (new State(this, ctx, done))->Start();\n  }\n\n private:\n  FHandle body_handle_;\n\n  class State {\n   public:\n    State(ForOp* kernel, OpKernelContext* ctx, DoneCallback done)\n        : kernel_(kernel),\n          ctx_(ctx),\n          done_(std::move(done)),\n          lib_(CHECK_NOTNULL(ctx_->function_library())),\n          opts_(ctx->step_id()),\n          args_(1 + ctx_->num_inputs() - 3) {\n      args_[0] = Tensor(DT_INT32, {});\n      iter_ = &args_[0].scalar<int32>()();\n\n      const int32_t num_loop_inputs = ctx_->num_inputs() - 3;\n      rets_.reserve(num_loop_inputs);\n      for (int i = 0; i < num_loop_inputs; ++i) {\n        rets_.push_back(ctx_->input(3 + i));\n      }\n    }\n\n    ~State() {}\n\n    void Start() {\n      Status s = StartLoop();\n      if (!s.ok()) Finish(s);\n    }\n\n   private:\n    ForOp* const kernel_;\n    OpKernelContext* const ctx_;\n    const DoneCallback done_;\n    FunctionLibraryRuntime* const lib_;\n    FunctionLibraryRuntime::Options opts_;\n    TensorVec args_;\n    TensorVec rets_;\n\n    int32* iter_;  // points to args_[0].\n    int32 limit_;\n    int32 delta_;\n\n    // If an error e is returned, caller must call Finish(e).\n    // If OK is returned, the async loop execution has been started.\n    Status StartLoop() {\n      SetRunOptions(ctx_, &opts_, false /* always_collect_stats */);\n\n      TF_RETURN_IF_ERROR(GetScalar(ctx_, 0, iter_, \"start\"));\n      TF_RETURN_IF_ERROR(GetScalar(ctx_, 1, &limit_, \"limit\"));\n      TF_RETURN_IF_ERROR(GetScalar(ctx_, 2, &delta_, \"delta\"));\n\n      if ((delta_ > 0 && *iter_ <= limit_) ||\n          (delta_ < 0 && *iter_ >= limit_) ||\n          (delta_ == 0 && *iter_ == limit_)) {\n        RunNext();\n        return OkStatus();\n      } else {\n        return errors::InvalidArgument(\"Invalid start/limit/delta: \", *iter_,\n                                       \" \", limit_, \" \", delta_);\n      }\n    }\n\n    void RunNext() {\n      bool done_loop;\n      if (delta_ > 0) {\n        done_loop = *iter_ >= limit_;\n      } else {\n        done_loop = *iter_ <= limit_;\n      }\n      if (done_loop) {\n        Finish(OkStatus());\n        return;\n      }\n\n      if (rets_.size() >= args_.size()) {\n        Finish(errors::InvalidArgument(\n            \"For loop body returned \", rets_.size(),\n            \" arguments. Expected: \", args_.size() - 1));\n        return;\n      }\n      for (int i = 0; i < rets_.size(); ++i) {\n        args_[1 + i] = std::move(rets_[i]);\n      }\n      rets_.clear();\n      profiler::TraceMe trace_me(\"ForOp\");\n      lib_->Run(opts_, kernel_->body_handle_, args_, &rets_,\n                [this](const Status& s) {\n                  if (s.ok()) {\n                    *iter_ += delta_;\n                    RunNext();\n                  } else {\n                    Finish(s);\n                  }\n                });\n    }\n\n    void Finish(Status s) {\n      if (s.ok()) {\n        s = SetOutputs(kernel_, ctx_, rets_);\n      }\n      ctx_->SetStatus(s);\n      done_();\n      delete this;\n    }\n  };\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"For\").Device(DEVICE_CPU), ForOp);\nREGISTER_KERNEL_BUILDER(Name(\"For\")\n                            .Device(DEVICE_DEFAULT)\n                            .HostMemory(\"start\")\n                            .HostMemory(\"limit\")\n                            .HostMemory(\"delta\"),\n                        ForOp);\n\n// FakeParamOp allocates a tensor with a shape conforming to the expected\n// output. This is necessary if the value will be stored in a while_loop's\n// TensorList. The output is otherwise not expected to be consumed by anything\n// else.\nclass FakeParamOp : public OpKernel {\n public:\n  explicit FakeParamOp(OpKernelConstruction* context) : OpKernel(context) {\n    DataType dtype;\n    OP_REQUIRES_OK(context, context->GetAttr(\"dtype\", &dtype));\n\n    // Set shape to the specified shape, setting unknown dimensions to empty.\n    // If the specified shape is unknown, leave as an empty shape.\n    TensorShape shape;\n    PartialTensorShape partial_shape;\n    OP_REQUIRES_OK(context, context->GetAttr(\"shape\", &partial_shape));\n    if (!partial_shape.unknown_rank()) {\n      for (int64_t d : partial_shape.dim_sizes()) {\n        shape.AddDim(d == -1 ? 0 : d);\n      }\n    }\n\n    // Create a tensor that we can repeatedly return to save memory.\n    // TODO(b/119612758): add optimization to prevent sending this across\n    // devices on each Compute() call.\n    OP_REQUIRES_OK(context, context->allocate_temp(dtype, shape, &value_));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    context->set_output(0, value_);\n  }\n\n private:\n  Tensor value_;\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"FakeParam\").Device(DEVICE_CPU), FakeParamOp);\nREGISTER_KERNEL_BUILDER(Name(\"FakeParam\").Device(DEVICE_DEFAULT), FakeParamOp);\n\n// DeviceIndexOP returns the current device index.\nclass DeviceIndexOp : public OpKernel {\n public:\n  explicit DeviceIndexOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"device_names\", &device_names_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    Tensor* device_name_t;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(0, TensorShape({}), &device_name_t));\n    DeviceNameUtils::ParsedName parsed_name;\n    int index = device_names_.size();\n    if (DeviceNameUtils::ParseFullName(ctx->device()->name(), &parsed_name) &&\n        parsed_name.has_type) {\n      auto it = absl::c_find(device_names_, parsed_name.type);\n      if (it != device_names_.end()) {\n        index = it - device_names_.begin();\n      }\n    }\n    device_name_t->scalar<int32>()() = index;\n  }\n\n private:\n  std::vector<string> device_names_;\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"DeviceIndex\").Device(DEVICE_CPU), DeviceIndexOp);\nREGISTER_KERNEL_BUILDER(\n    Name(\"DeviceIndex\").Device(DEVICE_DEFAULT).HostMemory(\"index\"),\n    DeviceIndexOp);\n\n}  // namespace\n}  // namespace tensorflow"