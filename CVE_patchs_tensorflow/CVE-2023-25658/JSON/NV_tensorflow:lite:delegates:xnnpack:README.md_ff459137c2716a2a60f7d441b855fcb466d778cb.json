"# XNNPACK backend for TensorFlow Lite\n\nXNNPACK is a highly optimized library of neural network inference operators for\nARM, x86, and WebAssembly architectures in Android, iOS, Windows, Linux, macOS,\nand Emscripten environments. This document describes how to use the XNNPACK\nlibrary as an inference engine for TensorFlow Lite.\n\n## Using XNNPACK engine with TensorFlow Lite interpreter\n\nXNNPACK integrates with TensorFlow Lite interpreter through the delegation\nmechanism. TensorFlow Lite supports several methods to enable XNNPACK\nfor floating-point inference.\n\n### Enable XNNPACK via Java API on Android (recommended on Android)\n\nPre-built [nightly TensorFlow Lite binaries for Android](https://www.tensorflow.org/lite/guide/android#use_the_tensorflow_lite_aar_from_mavencentral)\ninclude XNNPACK, albeit it is disabled by default. Use the `setUseXNNPACK`\nmethod in `Interpreter.Options` class to enable it:\n\n```java\nInterpreter.Options interpreterOptions = new Interpreter.Options();\ninterpreterOptions.setUseXNNPACK(true);\nInterpreter interpreter = new Interpreter(model, interpreterOptions);\n```\n\n### Enable XNNPACK via Swift/Objective-C API on iOS (recommended on iOS)\n\nPre-built [nightly TensorFlow Lite CocoaPods](https://www.tensorflow.org/lite/guide/ios#specifying_versions)\ninclude XNNPACK, but do not enable it by default. Swift developers can use\n`InterpreterOptions` object to enable XNNPACK:\n\n```swift\nvar options = InterpreterOptions()\noptions.isXNNPackEnabled = true\nvar interpreter = try Interpreter(modelPath: \"model/path\", options: options)\n```\n\nObjective-C developers can enable XNNPACK via a new property in the\n`TFLInterpreterOptions` class:\n\n```objc\nTFLInterpreterOptions *options = [[TFLInterpreterOptions alloc] init];\noptions.useXNNPACK = YES;\nNSError *error;\nTFLInterpreter *interpreter =\n    [[TFLInterpreter alloc] initWithModelPath:@\"model/path\"\n                                      options:options\n                                        error:&error];\n```\n\n### Enable XNNPACK via Bazel build flags (recommended on desktop)\n\nWhen building TensorFlow Lite with Bazel, add\n`--define tflite_with_xnnpack=true`, and the TensorFlow Lite interpreter will\nuse XNNPACK engine by default.\n\nThe exact command depends on the target platform, e.g. for Android AAR you'd use\n\n```\nbazel build -c opt --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a \\\n  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\n  --define tflite_with_xnnpack=true \\\n  //tensorflow/lite/java:tensorflow-lite\n```\n\nNote that in this case `Interpreter::SetNumThreads` invocation does not take\neffect on number of threads used by XNNPACK engine. In order to specify number\nof threads available for XNNPACK engine you should manually pass the value when\nconstructing the interpreter. The snippet below illustrates this assuming you\nare using `InterpreterBuilder` to construct the interpreter:\n\n```c++\n// Load model\ntflite::Model* model;\n...\n\n// Construct the interprepter\ntflite::ops::builtin::BuiltinOpResolver resolver;\nstd::unique_ptr<tflite::Interpreter> interpreter;\n\nTfLiteStatus res = tflite::InterpreterBuilder(model, resolver, num_threads);\n```\n\n**XNNPACK engine used by TensorFlow Lite interpreter uses a single thread for\ninference by default.**\n\n### Enable XNNPACK via additional dependency\n\nAnother way to enable XNNPACK is to build and link the\n`//tensorflow/lite:tflite_with_xnnpack` target into your application alongside\nthe TensorFlow Lite framework.\n\nThis method works on platforms which support POSIX-style weak symbols (Android,\niOS, Linux, Mac, but **NOT** Windows).\n\n### Enable XNNPACK via low-level delegate API (not recommended)\n\nWhile it is possible to use low-level delegate API to enable XNNPACK, this\nmethod is **NOT RECOMMENDED** unless you need to use TensorFlow Lite both with\nand without XNNPACK (e.g. for benchmarking).\n\nWith low-level delegate API users create an XNNPACK delegate with the\n`TfLiteXNNPackDelegateCreate` function, and then call\n`Interpreter::ModifyGraphWithDelegate` to delegate supported parts of\nthe model to the XNNPACK delegate. The users must destroy the delegate with\n`TfLiteXNNPackDelegateDelete` **after** releasing the TensorFlow Lite\ninterpreter. The snippet below illustrates the typical usage:\n\n```c++\n// Build the interpreter\nstd::unique_ptr<tflite::Interpreter> interpreter;\n...\n\n// IMPORTANT: initialize options with TfLiteXNNPackDelegateOptionsDefault() for\n// API-compatibility with future extensions of the TfLiteXNNPackDelegateOptions\n// structure.\nTfLiteXNNPackDelegateOptions xnnpack_options =\n    TfLiteXNNPackDelegateOptionsDefault();\nxnnpack_options.num_threads = num_threads;\n\nTfLiteDelegate* xnnpack_delegate =\n    TfLiteXNNPackDelegateCreate(&xnnpack_options);\nif (interpreter->ModifyGraphWithDelegate(xnnpack_delegate) != kTfLiteOk) {\n  // Report error and fall back to another delegate, or the default backend\n}\n\n...\n\n// Run inference using XNNPACK\ninterpreter->Invoke()\n\n...\n\n// IMPORTANT: release the interpreter before destroying the delegate\ninterpreter.reset();\nTfLiteXNNPackDelegateDelete(xnnpack_delegate);\n```\n\n### Using the XNNPACK weights cache\n\nXNNPACK internally packs static weights for operations (like convolutions) in\norder to make accessing weights more memory friendly. XNNPACK needs to allocate\nmemory internally to hold these packed weights. If you are starting multiple\nTFLite interpreter instances based on the same model, there can be multiple\ncopies of the same packed weights in each instance. This can cause high memory\nusage. The weights cache can be used to share packed weights between multiple\nTFLite instances.\n\n```c++\n// Create 2 interpreters which share the same model.\nstd::unique_ptr<tflite::Interpreter> interpreter1;\nstd::unique_ptr<tflite::Interpreter> interpreter2;\n\n// Create a weights cache that you can pass to XNNPACK delegate.\nTfLiteXNNPackDelegateWeightsCache* weights_cache =\n    TfLiteXNNPackDelegateWeightsCacheCreate();\n\n// Like using the low-level API above, initialize options, and pass this cache\n// to XNNPACK delegate via the options.\nTfLiteXNNPackDelegateOptions xnnpack_options =\n    TfLiteXNNPackDelegateOptionsDefault();\nxnnpack_options.weights_cache = weights_cache;\n\n// Modify graph with delegate, as above...\nTfLiteDelegate* delegate1 = TfLiteXNNPackDelegateCreate(&xnnpack_options);\nif (interpreter1->ModifyGraphWithDelegate(delegate1) != kTfLiteOk) {\n    // Static weights will be packed and written into weights_cache.\n}\nTfLiteDelegate* delegate2 = TfLiteXNNPackDelegateCreate(&xnnpack_options);\nif (interpreter1->ModifyGraphWithDelegate(delegate2) != kTfLiteOk) {\n    // XNNPACK will reuse packed weights if they can be found in the weights\n    // cache.\n}\n\n// Finalize the weights cache.\n// Hard finalization has the lowest memory overhead, but requires that all\n// TFLite interpreter instances must be created up front before any finalization\n// and inference.\nTfLiteXNNPackDelegateWeightsCacheFinalizeHard(weights_cache);\n\n// Alternatively, soft-finalizate the weights cache. This is useful if more\n// delegates using the same model will to be created after finalization.\n// TfLiteXNNPackDelegateWeightsCacheFinalizeSoft(weights_cache);\n\n// Later, after all the interpreters and XNNPACK delegates using the cache are\n// destroyed, release the weights cache.\nTfLiteXNNPackDelegateWeightsCacheDelete(weights_cache);\n```\n\nThe weights cache is a contents-based cache. Every time XNNPACK has to pack\nweights, it first packs into a temporary buffer, then tries to look up if the\npacked weights can be found in the weights cache, based on the contents of the\npacked weights. If it can be found, we access the packed weights in the\ncache for subsequent operations, and the temporary buffer is freed. Otherwise,\nthe packed weights is added to the cache.\n\nThe weights cache has to be finalized before any inference, it will be an error\notherwise. Hard finalization and soft finalization depends on whether new\nXNNPACK delegate instances will be created after finalization. Hard finalization\ndoes not allow new instances to be created, and has lower memory overhead. Soft\nfinalization allows new instances to be created, and has higher memory overhead\n(up to the size of the largest packed weights, rounded up to page alignment).\n\n### Using XNNPACK for variable operations\n\nXNNPACK can handle resource variables and associated operations: `VAR_HANDLE`,\n`READ_VARIABLE`, and `ASSIGN_VARIABLE`, but needs to be opted in by the user\nusing delegate options:\n\n```c++\nTfLiteXNNPackDelegateOptions xnnpack_options =\n    TfLiteXNNPackDelegateOptionsDefault();\nxnnpack_options.handle_variable_ops = true;\n```\n\nWhen XNNPACK handles resource variables,\n[tflite::Subgraph::resources](https://github.com/tensorflow/tensorflow/blob/5b4239ba9cf127fd26cd9f03c04dfc4c94c078d4/tensorflow/lite/core/subgraph.h#L197)\ncannot be used to access resources, because the resources are now internal to\nXNNPACK, and the changes are not reflected in tflite::Subgraph::resources. There\nis currently no way to access resources if XNNPACK handles resource variables.\n\n## Profiling\nWhen TfLite profiling is enabled, XNNPACK will time each operator and report the\nresults to TfLite which will print them as part of the overall execution profile.\n\n## Limitations and supported operators\n\nXNNPACK delegate is a work-in-progress, and currently supports a limited set of\noperators. Unsupported operators will fall back to the default implementations,\nso models using a combination of supported and unsupported operators can still\nbenefit from XNNPACK delegate.\n\n### Floating-Point (IEEE FP32) Operators\n\nBelow is the list of currently supported floating-point operators:\n\n#### `ABS`\n\n* Inputs and outputs must be in 32-bit floating-point format.\n\n#### `ADD`\n\n* Inputs and outputs must be in 32-bit floating-point format.\n* Only addition with two inputs is supported.\n* Fused `NONE`, `RELU`, `RELU_N1_TO_1`, and `RELU6` activations are supported,\n  but fused `TANH` and `SIGN_BIT` activations are not.\n\n#### `AVERAGE_POOL_2D`\n\n* Inputs and outputs must be in 32-bit floating-point format.\n* 1x1 pooling with non-unit stride is not supported.\n* Fused `NONE`, `RELU`, `RELU_N1_TO_1`, and `RELU6` activations are supported,\n  but fused `TANH` and `SIGN_BIT` activations are not.\n\n#### `CEIL`\n\n* Inputs and outputs must be in 32-bit floating-point format.\n\n#### `CONCATENATION`\n\n* Inputs and outputs must be in 32-bit floating-point format.\n* Only concatenation with two, three, or four inputs is supported.\n\n#### `CONV_2D`\n\n* Inputs and outputs must be in 32-bit floating-point format.\n* Bias is mandatory.\n* Both filter and bias must be static (use `kTfLiteMmapRo` allocation type).\n* Fused `NONE`, `RELU`, `RELU_N1_TO_1`, and `RELU6` activations are supported,\n  but fused `TANH` and `SIGN_BIT` activations are not.\n\n#### `DEPTH_TO_SPACE`\n\n* Inputs and outputs must be in 32-bit floating-point format.\n* Block size must be greater than 1.\n\n#### `DEPTHWISE_CONV_2D`\n\n* Inputs and outputs must be in 32-bit floating-point format.\n* Bias is mandatory.\n* Both filter and bias must be static (use `kTfLiteMmapRo` allocation type).\n* Fused `NONE`, `RELU`, `RELU_N1_TO_1`, and `RELU6` activations are supported,\n  but fused `TANH` and `SIGN_BIT` activations are not.\n\n#### `DIV`\n\n* Inputs and outputs must be in 32-bit floating-point format.\n* Fused `NONE`, `RELU`, `RELU_N1_TO_1`, and `RELU6` activations are supported,\n  but fused `TANH` and `SIGN_BIT` activations are not.\n\n#### `ELU`\n\n* Inputs and outputs must be in 32-bit floating-point format.\n\n#### `FULLY_CONNECTED`\n\n* Inputs and outputs must be in 32-bit floating-point format.\n* Both filter and bias must be static (use `kTfLiteMmapRo` allocation type).\n* Fused `NONE`, `RELU`, `RELU_N1_TO_1`, and `RELU6` activations are supported,\n  but fused `TANH` and `SIGN_BIT` activations are not.\n\n#### `FLOOR`\n\n* Inputs and outputs must be in 32-bit floating-point format.\n\n#### `HARD_SWISH`\n\n* Inputs and outputs must be in 32-bit floating-point format.\n\n#### `LEAKY_RELU`\n\n* Inputs and outputs must be in 32-bit floating-point format.\n\n#### `LOGISTIC`\n\n* Inputs and outputs must be in 32-bit floating-point format.\n\n#### `MAX_POOL_2D`\n\n* Inputs and outputs must be in 32-bit floating-point format.\n* 1x1 pooling with non-unit stride is not supported.\n* Fused `NONE`, `RELU`, `RELU_N1_TO_1`, and `RELU6` activations are supported,\n  but fused `TANH` and `SIGN_BIT` activations are not.\n\n#### `MAXIMUM`\n\n* Inputs and outputs must be in 32-bit floating-point format.\n\n#### `MEAN`\n\n* The first input and the output must be 4D tensors in 32-bit\n  floating-point format.\n* The second input (the input with the axes specification) must be static\n  (use `kTfLiteMmapRo` allocation type).\n* Only [1, 2], [2, 1], and [2] axes specification (i.e. reduction across either\n  both spatial dimensions or across the width dimension) is supported.\n\n#### `MINIMUM`\n\n* Inputs and outputs must be in 32-bit floating-point format.\n\n#### `MUL`\n\n* Inputs and outputs must be in 32-bit floating-point format.\n* Fused `NONE`, `RELU`, `RELU_N1_TO_1`, and `RELU6` activations are supported,\n  but fused `TANH` and `SIGN_BIT` activations are not.\n\n#### `NEG`\n\n* Inputs and outputs must be in 32-bit floating-point format.\n\n#### `PAD`\n\n* The first input and the output must be in 32-bit floating-point format.\n* The second input (the input with the padding specification) must be static\n  (use `kTfLiteMmapRo` allocation type).\n* The numbers of padding elements must be non-negative.\n\n#### `PRELU`\n\n* Inputs and outputs must be in 32-bit floating-point format.\n* Slope must be static (use `kTfLiteMmapRo` allocation type).\n* Slope must be either a 1D tensor, or have all its non-channel dimensions equal\n  1.\n\n#### `RELU`\n\n* Inputs and outputs must be in 32-bit floating-point format.\n\n#### `RELU6`\n\n* Inputs and outputs must be in 32-bit floating-point format.\n\n#### `RELU_N1_TO_1`\n\n* Inputs and outputs must be in 32-bit floating-point format.\n\n#### `RESHAPE`\n\n* The first input and the output must be in 32-bit floating-point format.\n* The second input (the input with the new shape specification) must be either\n  static (use `kTfLiteMmapRo` allocation type), or absent (with the new shape\n  specified via `ReshapeOptions` table).\n\n#### `RESIZE_BILINEAR`\n\n* The first input and the output must be 4D tensors in 32-bit floating-point\n  format.\n* The second input (the input with the new shape specification) must be\n  static (use `kTfLiteMmapRo` allocation type).\n\n#### `ROUND`\n\n* Inputs and outputs must be in 32-bit floating-point format.\n\n#### `SLICE`\n\n* The first input and the output must be in 32-bit floating-point format.\n* The second and third inputs (the inputs with the slices' begin and size\n  specification) must be static (use `kTfLiteMmapRo` allocation type).\n\n#### `SOFTMAX`\n\n* Inputs and outputs must be in 32-bit floating-point format.\n* Only `beta = 1.0` is supported.\n\n#### `SPACE_TO_DEPTH`\n\n* Inputs and outputs must be in 32-bit floating-point format.\n* Block size must be greater than 1.\n\n#### `SPLIT`\n\n* Inputs and outputs must be in 32-bit floating-point format.\n* Only split into two, three, or four outputs is supported.\n\n#### `SQRT`\n\n* Inputs and outputs must be in 32-bit floating-point format.\n\n#### `SQUARE`\n\n* Inputs and outputs must be in 32-bit floating-point format.\n\n#### `SQUARED_DIFFERENCE`\n\n* Inputs and outputs must be in 32-bit floating-point format.\n\n#### `STRIDED_SLICE`\n\n* The first input and the output must be in 32-bit floating-point format.\n* The second, third, and fourth inputs (the inputs with the slices' begin, end,\n  and stride specification) must be static (use `kTfLiteMmapRo` allocation\n  type).\n* The fourth input (strides) must be all ones.\n* The ellipsis mask, new axis mask, and shrink axis mask must be 0.\n\n#### `SUB`\n\n* Inputs and outputs must be in 32-bit floating-point format.\n* Fused `NONE`, `RELU`, `RELU_N1_TO_1`, and `RELU6` activations are supported,\n  but fused `TANH` and `SIGN_BIT` activations are not.\n\n#### `TRANSPOSE`\n\n* The first input and the output must be in 32-bit floating-point format.\n* The second input (the input with the permutation specification) must be\n  static (use `kTfLiteMmapRo` allocation type).\n\n#### `TRANSPOSE_CONV`\n\n* Input, filter, bias (if present) and output tensors must be in 32-bit\n  floating-point format.\n* Output size, filter and bias (if present) must be static (use\n  `kTfLiteMmapRo` allocation type).\n\n### Floating-Point (IEEE FP16) Operators\n\nXNNPACK supports half-precision (using IEEE FP16 format) inference for a subset\nof floating-point operators. XNNPACK automatically enables half-precision\ninference when the following conditions are met:\n\n* XNNPACK runs on hardware that natively supports computations in IEEE FP16\nformat. Currently, this hardware is limited to ARM & ARM64 devices with\nARMv8.2 FP16 arithmetics extension, and includes Android phones starting with\nPixel 3, Galaxy S9 (Snapdragon SoC), Galaxy S10 (Exynos SoC), iOS devices with\nA11 or newer SoCs, all Apple Silicon Macs, and Windows ARM64 laptops based with\nSnapdragon 850 SoC or newer.\n\n* IEEE FP16 inference is supported for every floating-point operator in the\nmodel.\n\n* The model's \"reduced_precision_support\" metadata indicates that the model\nis compatible with FP16 inference. The metadata can be added during model\nconversion using the `_experimental_supported_accumulation_type` attribute\nof the [tf.lite.TargetSpec](https://www.tensorflow.org/api_docs/python/tf/lite/TargetSpec)\nobject:\n\n```python\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n...\nconverter.target_spec.supported_types = [tf.float16]\nconverter.target_spec._experimental_supported_accumulation_type = tf.dtypes.float16\n```\n\nWhen the above conditions are met, XNNPACK replace FP32 operators with their\nFP16 equivalents, and insert additional operators to convert model inputs\nfrom FP32 to FP16 and convert model outputs back from FP16 to FP32. If the\nabove conditions are not met, XNNPACK will perform model inference with FP32\ncalculations.\n\nAdditionally, XNNPACK delegate provides an option to force FP16 inference\nregardless of model metadata. This option is intended for development workflows,\nand in particular for testing end-to-end accuracy of model when FP16 inference\nis used. Forcing FP16 inference has several effects:\n\n* Besides ARM64 devices with ARMv8.2 FP16 arithmetics extension, forced FP16\ninference is supported on x86/x86-64 devices with AVX2 extension in emulation\nmode: all elementary floating-point operations are computed in FP32, then\nconverted to FP16 and back to FP32. Note that such simulation is not bit-exact\nequivalent to native FP16 inference, but simulates the effects of restricted\nmantissa precision and exponent range in the native FP16 arithmetics.\n\n* On devices that support neither the native FP16 arithmetics (ARM64 devices\nwith ARMv8.2 FP16 arithmetics extension), nor emulation (x86/x86-64 devices with\nAVX2 extension), inference will fail rather than fall back to FP32.\n\n* If any floating-point operator offloaded to XNNPACK is not supported for FP16\ninference, inference will fail rather than fall back to FP32.\n\nTo force FP16 inference, either build the delegate with\n`--define xnnpack_force_float_precision=fp16` option, or add\n`TFLITE_XNNPACK_DELEGATE_FLAG_FORCE_FP16` flag to the\n`TfLiteXNNPackDelegateOptions.flags` bitmask passed into\nthe `TfLiteXNNPackDelegateCreate` call:\n\n```c\nTfLiteXNNPackDelegateOptions xnnpack_options =\n    TfLiteXNNPackDelegateOptionsDefault();\n...\nxnnpack_options.flags |= TFLITE_XNNPACK_DELEGATE_FLAG_FORCE_FP16;\nTfLiteDelegate* xnnpack_delegate =\n    TfLiteXNNPackDelegateCreate(&xnnpack_options);\n```\n\nXNNPACK has full feature parity between FP32 and FP16 operators: all operators\nthat are supported for FP32 inference are also supported for FP16 inference,\nand vice versa. In particular, sparse inference operators are supported for FP16\ninference on ARM processors.\n\n### Quantized Operators\n\nBy default, quantized inference in XNNPACK delegate is disabled, and XNNPACK is\nused only for floating-point models. Support for quantized inference in XNNPACK\nmust be enabled by adding extra Bazel flags when building TensorFlow Lite.\n\n* `--define tflite_with_xnnpack_qs8=true` flag enables XNNPACK inference for\n  quantized operators using signed quantization schema. This schema is used by\n  models produced by [Model Optimization\n  Toolkit](https://www.tensorflow.org/model_optimization) through either\n  post-training integer quantization or quantization-aware training.\n  Post-training dynamic range quantization is not supported in XNNPACK.\n\n* `--define tflite_with_xnnpack_qu8=true` flag enables XNNPACK inference for\n  quantized operators using unsigned quantization schema, produced via the\n  legacy TensorFlow 1.X quantization tooling. This option is experimental and\n  may perform suboptimally on mobile processors with NEON DOT product\n  instructions.\n\nBelow is the list of currently supported quantized operators:\n\n#### `ADD`\n\n* Inputs and outputs must be in 8-bit quantized format.\n* Only addition with two inputs is supported.\n* Fused `NONE`, `RELU`, `RELU_N1_TO_1`, and `RELU6` activations are supported,\n  but fused `TANH` and `SIGN_BIT` activations are not.\n\n#### `CONCATENATION`\n\n* Inputs and outputs must be in 8-bit quantized format.\n* Only concatenation with two, three, or four inputs is supported.\n\n#### `CONV_2D`\n\n* Inputs and outputs must be in 8-bit quantized format (bias must be in 32-bit\n  quantized format).\n* Bias is mandatory.\n* Both filter and bias must be static (use `kTfLiteMmapRo` allocation type),\n  and can use either per-tensor or per-channel quantization parameters.\n* Fused `NONE`, `RELU`, `RELU_N1_TO_1`, and `RELU6` activations are supported,\n  but fused `TANH` and `SIGN_BIT` activations are not.\n\n#### `DEPTH_TO_SPACE`\n\n* Inputs and outputs must be in 8-bit quantized format.\n* Block size must be greater than 1.\n\n#### `DEPTHWISE_CONV_2D`\n\n* Inputs and outputs must be in 8-bit quantized format (bias must be in\n  32-bit quantized format).\n* Bias is mandatory.\n* Both filter and bias must be static (use `kTfLiteMmapRo` allocation type),\n  and can use either per-tensor or per-channel quantization parameters.\n* Fused `NONE`, `RELU`, `RELU_N1_TO_1`, and `RELU6` activations are supported,\n  but fused `TANH` and `SIGN_BIT` activations are not.\n\n#### `DEQUANTIZE`\n\n* Input tensor must be in 8-bit quantized format without per-channel\n  quantization.\n* Output tensor must be in 32-bit floating-point format.\n\n#### `ELU`\n\n* Inputs and outputs must be in 8-bit signed quantized format.\n\n#### `FULLY_CONNECTED`\n\n* Inputs and outputs must be in 8-bit quantized format (bias, if present, must\n  be in 32-bit quantized format).\n* Both filter and bias must be static (use `kTfLiteMmapRo` allocation type).\n* Fused `NONE`, `RELU`, `RELU_N1_TO_1`, and `RELU6` activations are supported,\n  but fused `TANH` and `SIGN_BIT` activations are not.\n\n#### `LEAKY_RELU`\n\n* Inputs and outputs must be in 8-bit quantized format.\n* The ratio of input scale to output scale must be within [1/256, 128].\n* The product of negative slope by the ratio of input scale to output scale\n  must be within either [-127.99609375, -1/256] range or [1/256, 128] range.\n\n#### `LOGISTIC`\n\n* Inputs and outputs must be in 8-bit quantized format.\n\n#### `MAX_POOL_2D`\n\n* Inputs and outputs must be in 8-bit quantized format.\n* 1x1 pooling with non-unit stride is not supported.\n* Fused `NONE`, `RELU`, `RELU_N1_TO_1`, and `RELU6` activations are supported,\n  but fused `TANH` and `SIGN_BIT` activations are not.\n\n#### `MEAN`\n\n* The first input and the output must be 4D tensors in 8-bit quantized format.\n* The second input (the input with the axes specification) must be static\n  (use `kTfLiteMmapRo` allocation type).\n* Only [1, 2], [2, 1], and [2] axes specification (i.e. reduction across either\n  both spatial dimensions or across the width dimension) is supported.\n\n#### `MUL`\n\n* Inputs and outputs must be in 8-bit quantized format.\n* Fused `NONE`, `RELU`, `RELU_N1_TO_1`, and `RELU6` activations are supported,\n  but fused `TANH` and `SIGN_BIT` activations are not.\n\n#### `PAD`\n\n* The first input and the output must be in 8-bit quantized format.\n* The second input (the input with the padding specification) must be static\n  (use `kTfLiteMmapRo` allocation type).\n* The numbers of padding elements must be non-negative.\n\n#### `QUANTIZE`\n\n* Input tensor must be in 32-bit floating-point format or in 8-bit quantized\n  format.\n* Output tensor must be in 8-bit quantized format without per-channel\n  quantization.\n* If inputs are in 8-bit quantized format, they must have the same signedness\n  as the outputs, and the ratio of input scale to output scale must be in the\n  [2**-8, 2**7] range.\n\n#### `RESHAPE`\n\n*   The first input and the output must be in 8-bit quantized format.\n*   The second input (the input with the new shape specification) must be either\n    static (use `kTfLiteMmapRo` allocation type), or absent (with the new shape\n    specified via `ReshapeOptions` table).\n\n#### `RESIZE_BILINEAR`\n\n* The first input and the output must be 4D tensors in 8-bit quantized format.\n* The second input (the input with the new shape specification) must be\n  static (use `kTfLiteMmapRo` allocation type).\n\n#### `SLICE`\n\n* The first input and the output must be in 8-bit quantized format.\n* The second and third inputs (the inputs with the slices' begin and size\n  specification) must be static (use `kTfLiteMmapRo` allocation type).\n\n#### `SPACE_TO_DEPTH`\n\n* Inputs and outputs must be in 8-bit quantized format.\n* Block size must be greater than 1.\n\n#### `SPLIT`\n\n* Inputs and outputs must be in 8-bit quantized format.\n* Only split into two, three, or four outputs is supported.\n\n#### `SUB`\n\n* Inputs and outputs must be in 8-bit quantized format.\n* Fused `NONE`, `RELU`, `RELU_N1_TO_1`, and `RELU6` activations are supported,\n  but fused `TANH` and `SIGN_BIT` activations are not.\n\n#### `TRANSPOSE`\n\n* The first input and the output must be in 8-bit quantized format.\n* The second input (the input with the permutation specification) must be\n  static (use `kTfLiteMmapRo` allocation type).\n\n#### `TRANSPOSE_CONV`\n\n* Input, filter, and output tensors must be in 8-bit quantized format (bias, if\n  present, must be in 32-bit quantized format).\n* Output size, filter and bias (if present) must be static (use\n  `kTfLiteMmapRo` allocation type).\n\n### Sparse Inference\n\nXNNPACK backend supports sparse inference for CNN models described in the\n[Fast Sparse ConvNets](https://arxiv.org/abs/1911.09723) paper. Sparse\ninference is restricted to subgraphs with the following floating-point\noperators:\n\n* Sparse subgraph must store its weights in sparse representation (using\n  `DENSIFY` operators in the TensorFlow Lite schema).\n* Sparse subgraph must start with a 3x3 stride-2 `CONV_2D` operator with\n  padding 1 on each side, no dilation, and 3 input channels.\n* Sparse subgraph must end with either a `MEAN` operator with reduction across\n  spatial axes, or a `DEPTH_TO_SPACE` operator.\n* Sparse subgraph may contain the following operators:\n  * `CONV_2D` with 1x1 kernel and no padding. At least 2/3rd of filter weights\n    in the 1x1 `CONV_2D` operators across the sparse subgraph must be zeroes\n    to enable sparse inference.\n  * `DEPTHWISE_CONV_2D` with 3x3 kernel, stride 1, no dilation, and padding 1\n    on each side.\n  * `DEPTHWISE_CONV_2D` with 3x3 kernel, stride 2, no dilation, and padding 1\n    on each side.\n  * `DEPTHWISE_CONV_2D` with 5x5 kernel, stride 1, no dilation, and padding 2\n    on each side.\n  * `DEPTHWISE_CONV_2D` with 5x5 kernel, stride 2, no dilation, and padding 2\n    on each side.\n  * `RESIZE_BILINEAR` operator with output dimensions greater than 1.\n  * `MEAN` operator with reduction across spatial axes.\n  * `ADD` and `MUL` operators where both inputs are 4D tensors. If one of the\n    inputs to `ADD` or `MUL` is a constant tensor, it must be representable as\n    either a scalar, or a 1D vector.\n  * Unary elementwise operators `ABS`, `CEIL`, `ELU`, `FLOOR`, `HARD_SWISH`,\n    `LEAKY_RELU`, `LOGISTIC`, `NEG`, `RELU`, `RELU6`, `RELU_N1_TO_1`, `ROUND`,\n    `SIGMOID`, and `SQUARE`.\n\nPre-trained [Fast Sparse ConvNets models](https://github.com/google-research/google-research/tree/master/fastconvnets)\nprovide examples that satisfy these constrains.\n\n### Other limitations\n\n* Dynamically allocated (with `kTfLiteDynamic` allocation type) inputs and\n  outputs are not supported.\n* Resizing model inputs (via `Interpreter::ResizeInputTensor`) is supported, but\n  cause a complete reinitialization of the delegate instance, which has\n  considerable overhead."