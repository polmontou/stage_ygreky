"/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"third_party/gpus/cuda/include/cublas_v2.h\"\n#include \"third_party/gpus/cuda/include/cuda.h\"\n\n#define SE_CUDA_DATA_HALF CUDA_R_16F\n\n#include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.h\"\n\n// Both Eigen Half.h and CUDA cuda_fp16.h provide similar typedef for __half. As\n// such, there are two ways to get the typedef for __half:\n//\n// (1) Includes cuda_fp16.h and defines EIGEN_HAS_CUDA_FP16.\n// (2) Neither includes cuda_fp16.h nor defines EIGEN_HAS_CUDA_FP16.\n//\n// Due to issue b/73793421, when the first approach is used and NVCC is used to\n// compile this file, NVCC will complain duplicated definition for\n// EIGEN_HAS_CUDA_FP16. On the other hand, when the second approach is used and\n// clang is used to compile this file, clang will not understand __half\n// due to missing the definition and macro EIGEN_HAS_CUDA_FP16.\n//\n// Because this file may be compiled with CLANG but will never be compiled with\n// NVCC, we choose the first approach for CUDA < 9.0. For CUDA >= 9.0, we have\n// to use the second approach because the data member in the __half defined\n// by CUDA > 9.0 is `__x` while Eigen expects it to be `x`.\n//\n// TODO(b/73793421): Remove the following code block to switch to the second\n// approach when the issue is fixed.\n#if CUDA_VERSION < 9000\n#include \"third_party/gpus/cuda/include/cuda_fp16.h\"\n#define EIGEN_HAS_CUDA_FP16\n#endif\n\n#include <complex>\n#include <cstdint>\n\n#include \"absl/strings/str_cat.h\"\n#include \"absl/strings/str_format.h\"\n#include \"third_party/eigen3/Eigen/Core\"\n#include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_activation.h\"\n#include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_blas_utils.h\"\n#include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.h\"\n#include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_helpers.h\"\n#include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_platform_id.h\"\n#include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_stream.h\"\n#include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_timer.h\"\n#include \"tensorflow/compiler/xla/stream_executor/device_memory.h\"\n#include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_executor.h\"\n#include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_helpers.h\"\n#include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_stream.h\"\n#include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_timer.h\"\n#include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_types.h\"\n#include \"tensorflow/compiler/xla/stream_executor/lib/initialize.h\"\n#include \"tensorflow/compiler/xla/stream_executor/platform/logging.h\"\n#include \"tensorflow/compiler/xla/stream_executor/platform/port.h\"\n#include \"tensorflow/compiler/xla/stream_executor/plugin_registry.h\"\n#include \"tensorflow/compiler/xla/stream_executor/scratch_allocator.h\"\n#include \"tensorflow/compiler/xla/stream_executor/stream_executor.h\"\n#include \"tensorflow/tsl/platform/status.h\"\n#include \"tensorflow/tsl/platform/tensor_float_32_utils.h\"\n\nnamespace stream_executor {\nnamespace cuda {\n\nusing gpu::AsGpuStream;\nusing gpu::AsGpuStreamValue;\nusing gpu::GpuComplex;\nusing gpu::GpuComplexT;\nusing gpu::GpuComplexType;\nusing gpu::GpuComplexValue;\nusing gpu::GpuDoubleComplexType;\nusing gpu::GpuExecutor;\nusing gpu::GpuMemory;\nusing gpu::GpuMemoryMutable;\nusing gpu::GpuTimer;\nusing gpu::GpuTimerDeleter;\n\nPLUGIN_REGISTRY_DEFINE_PLUGIN_ID(kCuBlasPlugin);\n\n// cuBLAS has interfaces that permit pointers to be passed from either the host\n// memory space or the device memory space; however, you must instruct it as to\n// which address space those pointers are in with cublasSetPointerMode.\n//\n// This helper sets the cuBLAS pointer mode to a desired value for a cuBLAS call\n// you are about to perform in a given scope.\n//\n// The prior cuBLAS pointer mode is retained and restored when this object goes\n// out of scope.\nclass ScopedCublasPointerMode {\n public:\n  // Note that, because the setting of the cublas pointer mode is fallible,\n  // construction of this scoped datatype must be paired with a call to\n  // Init().\n  //\n  // Parameters:\n  //  handle: The cublas library handle to act upon in setting the pointer mode.\n  explicit ScopedCublasPointerMode(cublasHandle_t handle)\n      : handle_(handle), ok_(false) {}\n\n  // Attempts the switch to the requested scoped pointer mode, new_mode.\n  //\n  // Note that when false is returned, an appropriate error has already been\n  // logged.\n  bool Init(cublasPointerMode_t new_mode) {\n    cublasStatus_t ret = cublasGetPointerMode(handle_, &old_mode_);\n    if (ret != CUBLAS_STATUS_SUCCESS) {\n      LOG(ERROR) << \"failed to get old cublas pointer mode: \" << ToString(ret);\n      return ok_ = false;\n    }\n\n    ret = cublasSetPointerMode(handle_, new_mode);\n    if (ret != CUBLAS_STATUS_SUCCESS) {\n      LOG(ERROR) << \"failed to set new cublas pointer mode: \" << ToString(ret);\n      return ok_ = false;\n    }\n\n    return ok_ = true;\n  }\n\n  // Switches back to the prior pointer mode, if the switch operation was\n  // successful in the first place.\n  ~ScopedCublasPointerMode() {\n    if (ok_) {\n      cublasStatus_t ret = cublasSetPointerMode(handle_, old_mode_);\n      if (ret != CUBLAS_STATUS_SUCCESS) {\n        LOG(ERROR) << \"failed to set former cublas pointer mode: \"\n                   << ToString(ret);\n      }\n    }\n  }\n\n private:\n  cublasHandle_t handle_;         // Handle to the cuBLAS instance of interest.\n  cublasPointerMode_t old_mode_;  // Prior cuBLAS pointer mode, to be restored.\n  bool ok_;                       // Whether the change was successful.\n};\n\n#if CUDA_VERSION >= 9000\n// cuBLAS has interfaces that permit computations to use the Volta hardware.\n// This must be enabled via the cublasGet/SetMathMode APIs.\n//\n// This helper sets the cuBLAS math mode to a desired value for a cuBLAS call\n// you are about to perform in a given scope.\n//\n// The prior cuBLAS math mode is retained and restored when this object goes\n// out of scope.\nclass ScopedCublasMathMode {\n public:\n  // Note that, because the setting of the cublas math mode is fallible,\n  // construction of this scoped datatype must be paired with a call to\n  // Init().\n  //\n  // Parameters:\n  //  handle: The cublas library handle to act upon in setting the math mode.\n  explicit ScopedCublasMathMode(cublasHandle_t handle)\n      : handle_(handle), ok_(false) {}\n\n  // Attempts the switch to the requested scoped math mode, new_mode.\n  //\n  // Note that when false is returned, an appropriate error has already been\n  // logged.\n  bool Init(cublasMath_t new_mode) {\n    cublasStatus_t ret = cublasGetMathMode(handle_, &old_mode_);\n    if (ret != CUBLAS_STATUS_SUCCESS) {\n      LOG(ERROR) << \"failed to get old cublas math mode: \" << ToString(ret);\n      return ok_ = false;\n    }\n\n    ret = cublasSetMathMode(handle_, new_mode);\n    if (ret != CUBLAS_STATUS_SUCCESS) {\n      LOG(ERROR) << \"failed to set new cublas math mode: \" << ToString(ret);\n      return ok_ = false;\n    }\n    return ok_ = true;\n  }\n\n  // Switches back to the prior math mode, if the switch operation was\n  // successful in the first place.\n  ~ScopedCublasMathMode() {\n    if (ok_) {\n      cublasStatus_t ret = cublasSetMathMode(handle_, old_mode_);\n      if (ret != CUBLAS_STATUS_SUCCESS) {\n        LOG(ERROR) << \"failed to set former cublas math mode: \"\n                   << ToString(ret);\n      }\n    }\n  }\n\n private:\n  cublasHandle_t handle_;  // Handle to the cuBLAS instance of interest.\n  cublasMath_t old_mode_;  // Prior cuBLAS math mode, to be restored.\n  bool ok_;                // Whether the change was successful.\n};\n#endif  // CUDA_VERSION >= 9000\n\nstatic const char *const kCublasNotInitializedExplanation =\n    \"Failure to initialize cublas may be due to OOM (cublas needs some free \"\n    \"memory when you initialize it, and your deep-learning framework may have \"\n    \"preallocated more than its fair share), or may be because this binary was \"\n    \"not built with support for the GPU in your machine.\";\n\nbool CUDABlas::Init() {\n  gpu::ScopedActivateExecutorContext sac{parent_};\n  cublasStatus_t ret = cublasCreate(&blas_);\n  if (ret != CUBLAS_STATUS_SUCCESS) {\n    LOG(ERROR) << \"failed to create cublas handle: \" << ToString(ret);\n    if (ret == CUBLAS_STATUS_NOT_INITIALIZED ||\n        ret == CUBLAS_STATUS_ALLOC_FAILED) {\n      LOG(ERROR) << kCublasNotInitializedExplanation;\n    }\n    return false;\n  }\n\n#if CUDA_VERSION >= 11000\n  if (!blas_lt_.Init().ok()) {\n    LOG(ERROR) << kCublasNotInitializedExplanation;\n    return false;\n  }\n#endif  // CUDA_VERSION >= 11000\n\n  return true;\n}\n\nCUDABlas::CUDABlas(gpu::GpuExecutor *parent)\n    : parent_(CHECK_NOTNULL(parent)),\n      blas_(nullptr)\n#if CUDA_VERSION >= 11000\n      ,\n      blas_lt_(parent)\n#endif\n{\n}\n\nCUDABlas::~CUDABlas() {\n  if (blas_ != nullptr) {\n    gpu::ScopedActivateExecutorContext sac{parent_};\n    cublasDestroy(blas_);\n  }\n}\n\nbool CUDABlas::SetStream(Stream *stream) {\n  CHECK(stream != nullptr);\n  CHECK(AsGpuStreamValue(stream) != nullptr);\n  CHECK(blas_ != nullptr);\n  gpu::ScopedActivateExecutorContext sac{parent_};\n  cublasStatus_t ret = cublasSetStream(blas_, AsGpuStreamValue(stream));\n  if (ret != CUBLAS_STATUS_SUCCESS) {\n    LOG(ERROR) << \"failed to set stream for cuBLAS calls: \" << ToString(ret);\n    return false;\n  }\n\n  return true;\n}\n\ncudaStream_t CUDABlas::CUDAStream(Stream *stream) {\n  CHECK(stream != nullptr);\n  CHECK(AsGpuStreamValue(stream) != nullptr);\n  gpu::ScopedActivateExecutorContext sac{parent_};\n  return AsGpuStreamValue(stream);\n}\n\nnamespace {\n\n// Helper functions transforming blas arguments into cuBLAS arguments.\n\ncublasFillMode_t CUDABlasUpperLower(blas::UpperLower uplo) {\n  switch (uplo) {\n    case blas::UpperLower::kUpper:\n      return CUBLAS_FILL_MODE_UPPER;\n    case blas::UpperLower::kLower:\n      return CUBLAS_FILL_MODE_LOWER;\n    default:\n      LOG(FATAL) << \"Invalid value of blas::UpperLower.\";\n  }\n}\n\ncublasDiagType_t CUDABlasDiagonal(blas::Diagonal diag) {\n  switch (diag) {\n    case blas::Diagonal::kUnit:\n      return CUBLAS_DIAG_UNIT;\n    case blas::Diagonal::kNonUnit:\n      return CUBLAS_DIAG_NON_UNIT;\n    default:\n      LOG(FATAL) << \"Invalid value of blas::Diagonal.\";\n  }\n}\n\ncublasSideMode_t CUDABlasSide(blas::Side side) {\n  switch (side) {\n    case blas::Side::kLeft:\n      return CUBLAS_SIDE_LEFT;\n    case blas::Side::kRight:\n      return CUBLAS_SIDE_RIGHT;\n    default:\n      LOG(FATAL) << \"Invalid value of blas::Side.\";\n  }\n}\n\n// CUDADataType<T>::type translates from a C++ type (e.g. float) to a\n// cudaDataType_t (e.g. CUDA_R_32F).\n//\n// These are used to build the argument type and computation type args to\n// cublasGemmEx.\ntemplate <typename T>\nstruct CUDADataType;\n\ntemplate <>\nstruct CUDADataType<Eigen::half> {\n  static constexpr cudaDataType_t type = SE_CUDA_DATA_HALF;\n};\n\n#if CUDA_VERSION >= 11000\ntemplate <>\nstruct CUDADataType<Eigen::bfloat16> {\n  static constexpr cudaDataType_t type = CUDA_R_16BF;  // NOLINT\n};\n#endif  // CUDA_VERSION >= 11000\n\ntemplate <>\nstruct CUDADataType<std::complex<Eigen::half>> {\n  static constexpr cudaDataType_t type = CUDA_C_16F;\n};\n\ntemplate <>\nstruct CUDADataType<float> {\n  static constexpr cudaDataType_t type = CUDA_R_32F;\n};\n\ntemplate <>\nstruct CUDADataType<std::complex<float>> {\n  static constexpr cudaDataType_t type = CUDA_C_32F;\n};\n\ntemplate <>\nstruct CUDADataType<double> {\n  static constexpr cudaDataType_t type = CUDA_R_64F;\n};\n\ntemplate <>\nstruct CUDADataType<std::complex<double>> {\n  static constexpr cudaDataType_t type = CUDA_C_64F;\n};\n\ntemplate <>\nstruct CUDADataType<int> {\n  static constexpr cudaDataType_t type = CUDA_R_32I;\n};\n\ntemplate <>\nstruct CUDADataType<int8_t> {\n  static constexpr cudaDataType_t type = CUDA_R_8I;\n};\n\ntemplate <>\nstruct CUDADataType<std::complex<int8_t>> {\n  static constexpr cudaDataType_t type = CUDA_C_8I;\n};\n\ntemplate <>\nstruct CUDADataType<uint8_t> {\n  static constexpr cudaDataType_t type = CUDA_R_8U;\n};\n\ntemplate <>\nstruct CUDADataType<std::complex<uint8_t>> {\n  static constexpr cudaDataType_t type = CUDA_C_8U;\n};\n\n}  // namespace\n\ntemplate <typename FuncT, typename... Args>\ntsl::Status CUDABlas::DoBlasInternalImpl(FuncT cublas_func, Stream *stream,\n                                         bool pointer_mode_host,\n                                         cublasMath_t math_type, Args... args) {\n  absl::MutexLock lock(&mu_);\n\n  CHECK(blas_ != nullptr);\n  if (!SetStream(stream)) {\n    return tsl::errors::Internal(\"Failed setting stream\");\n  }\n\n#if CUDA_VERSION >= 9000\n  ScopedCublasMathMode math_mode{blas_};\n#if CUBLAS_VER_MAJOR >= 11\n  if (math_type == CUBLAS_TF32_TENSOR_OP_MATH &&\n      tsl::tensor_float_32_execution_enabled()) {\n#else\n  if (math_type == CUBLAS_TENSOR_OP_MATH) {\n#endif\n    if (!math_mode.Init(math_type)) {\n      return tsl::errors::Internal(\"Failed initializing math mode\");\n    }\n  }\n#endif\n\n  gpu::ScopedActivateExecutorContext sac{parent_};\n  ScopedCublasPointerMode pointer_mode{blas_};\n  if (!pointer_mode.Init(pointer_mode_host ? CUBLAS_POINTER_MODE_HOST\n                                           : CUBLAS_POINTER_MODE_DEVICE)) {\n    return tsl::errors::Internal(\"Failed setting error mode\");\n  }\n  cublasStatus_t ret = cublas_func(blas_, args...);\n  if (ret == CUBLAS_STATUS_SUCCESS) {\n    return ::tsl::OkStatus();\n  }\n  return tsl::errors::Internal(ToString(ret));\n}\n\n// cublas_func may be overloaded, so we need to figure out which one we really\n// need to call based on the args. One way to do it is to wrap it in lambda.\n#define AS_LAMBDA(func)                                            \\\n  [](auto &&...args) -> decltype(func(                             \\\n                         std::forward<decltype(args)>(args)...)) { \\\n    return func(std::forward<decltype(args)>(args)...);            \\\n  }\n\nbool CUDABlas::DoBlasAxpy(Stream *stream, uint64_t elem_count, float alpha,\n                          const DeviceMemory<float> &x, int incx,\n                          DeviceMemory<float> *y, int incy) {\n  return DoBlasInternal(cublasSaxpy, stream, true /* = pointer_mode_host */,\n                        elem_count, &alpha, GpuMemory(x), incx,\n                        GpuMemoryMutable(y), incy);\n}\n\nbool CUDABlas::DoBlasAxpy(Stream *stream, uint64_t elem_count, double alpha,\n                          const DeviceMemory<double> &x, int incx,\n                          DeviceMemory<double> *y, int incy) {\n  return DoBlasInternal(cublasDaxpy, stream, true /* = pointer_mode_host */,\n                        elem_count, &alpha, GpuMemory(x), incx,\n                        GpuMemoryMutable(y), incy);\n}\n\nbool CUDABlas::DoBlasAxpy(Stream *stream, uint64_t elem_count,\n                          std::complex<float> alpha,\n                          const DeviceMemory<std::complex<float>> &x, int incx,\n                          DeviceMemory<std::complex<float>> *y, int incy) {\n  auto cb_alpha = GpuComplexValue(alpha);\n  return DoBlasInternal(cublasCaxpy, stream, true /* = pointer_mode_host */,\n                        elem_count, GpuComplex(&cb_alpha),\n                        GpuComplex(GpuMemory(x)), incx,\n                        GpuComplex(GpuMemoryMutable(y)), incy);\n}\n\nbool CUDABlas::DoBlasAxpy(Stream *stream, uint64_t elem_count,\n                          std::complex<double> alpha,\n                          const DeviceMemory<std::complex<double>> &x, int incx,\n                          DeviceMemory<std::complex<double>> *y, int incy) {\n  auto cb_alpha = GpuComplexValue(alpha);\n  return DoBlasInternal(cublasZaxpy, stream, true /* = pointer_mode_host */,\n                        elem_count, GpuComplex(&cb_alpha),\n                        GpuComplex(GpuMemory(x)), incx,\n                        GpuComplex(GpuMemoryMutable(y)), incy);\n}\n\nbool CUDABlas::DoBlasCopy(Stream *stream, uint64_t elem_count,\n                          const DeviceMemory<float> &x, int incx,\n                          DeviceMemory<float> *y, int incy) {\n  return DoBlasInternal(cublasScopy, stream, true /* = pointer_mode_host */,\n                        elem_count, GpuMemory(x), incx, GpuMemoryMutable(y),\n                        incy);\n}\n\nbool CUDABlas::DoBlasCopy(Stream *stream, uint64_t elem_count,\n                          const DeviceMemory<double> &x, int incx,\n                          DeviceMemory<double> *y, int incy) {\n  return DoBlasInternal(cublasDcopy, stream, true /* = pointer_mode_host */,\n                        elem_count, GpuMemory(x), incx, GpuMemoryMutable(y),\n                        incy);\n}\n\nbool CUDABlas::DoBlasCopy(Stream *stream, uint64_t elem_count,\n                          const DeviceMemory<std::complex<float>> &x, int incx,\n                          DeviceMemory<std::complex<float>> *y, int incy) {\n  return DoBlasInternal(cublasCcopy, stream, true /* = pointer_mode_host */,\n                        elem_count, GpuComplex(GpuMemory(x)), incx,\n                        GpuComplex(GpuMemoryMutable(y)), incy);\n}\n\nbool CUDABlas::DoBlasCopy(Stream *stream, uint64_t elem_count,\n                          const DeviceMemory<std::complex<double>> &x, int incx,\n                          DeviceMemory<std::complex<double>> *y, int incy) {\n  return DoBlasInternal(cublasZcopy, stream, true /* = pointer_mode_host */,\n                        elem_count, GpuComplex(GpuMemory(x)), incx,\n                        GpuComplex(GpuMemoryMutable(y)), incy);\n}\n\nbool CUDABlas::DoBlasScal(Stream *stream, uint64_t elem_count, float alpha,\n                          DeviceMemory<float> *x, int incx) {\n  return DoBlasInternal(cublasSscal, stream, true /* = pointer_mode_host */,\n                        elem_count, &alpha, GpuMemoryMutable(x), incx);\n}\n\nbool CUDABlas::DoBlasScal(Stream *stream, uint64_t elem_count, double alpha,\n                          DeviceMemory<double> *x, int incx) {\n  return DoBlasInternal(cublasDscal, stream, true /* = pointer_mode_host */,\n                        elem_count, &alpha, GpuMemoryMutable(x), incx);\n}\n\nbool CUDABlas::DoBlasScal(Stream *stream, uint64_t elem_count, float alpha,\n                          DeviceMemory<std::complex<float>> *x, int incx) {\n  return DoBlasInternal(cublasCsscal, stream, true /* = pointer_mode_host */,\n                        elem_count, &alpha, GpuComplex(GpuMemoryMutable(x)),\n                        incx);\n}\n\nbool CUDABlas::DoBlasScal(Stream *stream, uint64_t elem_count, double alpha,\n                          DeviceMemory<std::complex<double>> *x, int incx) {\n  return DoBlasInternal(cublasZdscal, stream, true /* = pointer_mode_host */,\n                        elem_count, &alpha, GpuComplex(GpuMemoryMutable(x)),\n                        incx);\n}\n\nbool CUDABlas::DoBlasScal(Stream *stream, uint64_t elem_count,\n                          std::complex<float> alpha,\n                          DeviceMemory<std::complex<float>> *x, int incx) {\n  auto cb_alpha = GpuComplexValue(alpha);\n  return DoBlasInternal(cublasCscal, stream, true /* = pointer_mode_host */,\n                        elem_count, GpuComplex(&cb_alpha),\n                        GpuComplex(GpuMemoryMutable(x)), incx);\n}\n\nbool CUDABlas::DoBlasScal(Stream *stream, uint64_t elem_count,\n                          std::complex<double> alpha,\n                          DeviceMemory<std::complex<double>> *x, int incx) {\n  auto cb_alpha = GpuComplexValue(alpha);\n  return DoBlasInternal(cublasZscal, stream, true /* = pointer_mode_host */,\n                        elem_count, GpuComplex(&cb_alpha),\n                        GpuComplex(GpuMemoryMutable(x)), incx);\n}\n\nbool CUDABlas::DoBlasGemv(Stream *stream, blas::Transpose trans, uint64_t m,\n                          uint64_t n, float alpha, const DeviceMemory<float> &a,\n                          int lda, const DeviceMemory<float> &x, int incx,\n                          float beta, DeviceMemory<float> *y, int incy) {\n  return DoBlasInternal(cublasSgemv, stream, true /* = pointer_mode_host */,\n                        AsCublasOperation(trans), m, n, &alpha, GpuMemory(a),\n                        lda, GpuMemory(x), incx, &beta, GpuMemoryMutable(y),\n                        incy);\n}\n\nbool CUDABlas::DoBlasGemv(Stream *stream, blas::Transpose trans, uint64_t m,\n                          uint64_t n, double alpha,\n                          const DeviceMemory<double> &a, int lda,\n                          const DeviceMemory<double> &x, int incx, double beta,\n                          DeviceMemory<double> *y, int incy) {\n  return DoBlasInternal(cublasDgemv, stream, true /* = pointer_mode_host */,\n                        AsCublasOperation(trans), m, n, &alpha, GpuMemory(a),\n                        lda, GpuMemory(x), incx, &beta, GpuMemoryMutable(y),\n                        incy);\n}\n\nbool CUDABlas::DoBlasGemv(Stream *stream, blas::Transpose trans, uint64_t m,\n                          uint64_t n, std::complex<float> alpha,\n                          const DeviceMemory<std::complex<float>> &a, int lda,\n                          const DeviceMemory<std::complex<float>> &x, int incx,\n                          std::complex<float> beta,\n                          DeviceMemory<std::complex<float>> *y, int incy) {\n  auto cb_alpha = GpuComplexValue(alpha);\n  auto cb_beta = GpuComplexValue(beta);\n  return DoBlasInternal(cublasCgemv, stream, true /* = pointer_mode_host */,\n                        AsCublasOperation(trans), m, n, GpuComplex(&cb_alpha),\n                        GpuComplex(GpuMemory(a)), lda, GpuComplex(GpuMemory(x)),\n                        incx, GpuComplex(&cb_beta),\n                        GpuComplex(GpuMemoryMutable(y)), incy);\n}\n\nbool CUDABlas::DoBlasGemv(Stream *stream, blas::Transpose trans, uint64_t m,\n                          uint64_t n, std::complex<double> alpha,\n                          const DeviceMemory<std::complex<double>> &a, int lda,\n                          const DeviceMemory<std::complex<double>> &x, int incx,\n                          std::complex<double> beta,\n                          DeviceMemory<std::complex<double>> *y, int incy) {\n  auto cb_alpha = GpuComplexValue(alpha);\n  auto cb_beta = GpuComplexValue(beta);\n  return DoBlasInternal(cublasZgemv, stream, true /* = pointer_mode_host */,\n                        AsCublasOperation(trans), m, n, GpuComplex(&cb_alpha),\n                        GpuComplex(GpuMemory(a)), lda, GpuComplex(GpuMemory(x)),\n                        incx, GpuComplex(&cb_beta),\n                        GpuComplex(GpuMemoryMutable(y)), incy);\n}\n\nbool CUDABlas::DoBlasSbmv(Stream *stream, blas::UpperLower uplo, uint64_t n,\n                          uint64_t k, float alpha, const DeviceMemory<float> &a,\n                          int lda, const DeviceMemory<float> &x, int incx,\n                          float beta, DeviceMemory<float> *y, int incy) {\n  return DoBlasInternal(cublasSsbmv, stream, true /* = pointer_mode_host */,\n                        CUDABlasUpperLower(uplo), n, k, &alpha, GpuMemory(a),\n                        lda, GpuMemory(x), incx, &beta, GpuMemoryMutable(y),\n                        incy);\n}\n\nbool CUDABlas::DoBlasSbmv(Stream *stream, blas::UpperLower uplo, uint64_t n,\n                          uint64_t k, double alpha,\n                          const DeviceMemory<double> &a, int lda,\n                          const DeviceMemory<double> &x, int incx, double beta,\n                          DeviceMemory<double> *y, int incy) {\n  return DoBlasInternal(cublasDsbmv, stream, true /* = pointer_mode_host */,\n                        CUDABlasUpperLower(uplo), n, k, &alpha, GpuMemory(a),\n                        lda, GpuMemory(x), incx, &beta, GpuMemoryMutable(y),\n                        incy);\n}\n\ntsl::Status CUDABlas::DoBlasGemm(Stream *stream, blas::Transpose transa,\n                                 blas::Transpose transb, uint64_t m, uint64 n,\n                                 uint64_t k, blas::DataType dtype,\n                                 const void *alpha, const DeviceMemoryBase &a,\n                                 int lda, const DeviceMemoryBase &b, int ldb,\n                                 const void *beta, DeviceMemoryBase *c, int ldc,\n                                 blas::ComputePrecision precision) {\n  cublasMath_t math_type = CUBLAS_DEFAULT_MATH;\n\n#if CUDA_VERSION < 11000\n  if (dtype == blas::DataType::kHalf) {\n    math_type = CUBLAS_TENSOR_OP_MATH;\n  }\n#else\n  if (dtype == blas::DataType::kFloat) {\n    math_type = CUBLAS_TF32_TENSOR_OP_MATH;\n    if (stream->GetCudaComputeCapability().IsAtLeast(\n            CudaComputeCapability::AMPERE)) {\n      // TODO(reedwm): Remove or make this VLOG(1) once TensorFloat-32 is more\n      // well tested.\n      if (tsl::tensor_float_32_execution_enabled()) {\n        LOG_FIRST_N(INFO, 1) << \"TensorFloat-32 will be used for the matrix \"\n                                \"multiplication. This will only be logged \"\n                                \"once.\";\n      }\n    }\n    if (precision > blas::kDefaultComputePrecision) {\n      math_type = CUBLAS_DEFAULT_MATH;\n    }\n  }\n#endif\n\n  // TODO(cheshire): Return an error instead.\n  // TODO(cheshire): Why are these checked only for `half` and `float`?\n  if (dtype == blas::DataType::kHalf || dtype == blas::DataType::kFloat) {\n    if (transa == blas::Transpose::kNoTranspose) {\n      if (lda < static_cast<int64_t>(m)) {\n        LOG(WARNING) << \"GEMM lda was smaller than m (no transpose case); \"\n                        \"precondition violation\";\n      }\n    } else {\n      if (lda < static_cast<int64_t>(k)) {\n        LOG(WARNING) << \"GEMM lda (\" << lda << \") was smaller than k (\" << k\n                     << \") (transpose case); precondition violation\";\n      }\n    }\n    if (transb == blas::Transpose::kNoTranspose) {\n      if (ldb < static_cast<int64_t>(k)) {\n        LOG(WARNING) << \"GEMM ldb (\" << ldb << \") was smaller than k (\" << k\n                     << \") (no transpose case); precondition violation\";\n      }\n    } else {\n      if (ldb < static_cast<int64_t>(n)) {\n        LOG(WARNING) << \"GEMM ldb was smaller than n (transpose case); \"\n                        \"precondition violation\";\n      }\n    }\n  }\n\n  VLOG(1) << absl::StrFormat(\n      \"doing cuBLAS SGEMM: at=%d bt=%d m=%u n=%u \"\n      \"k=%u alpha=%p a=%p lda=%d b=%p ldb=%d beta=%p \"\n      \"c=%p ldc=%d\",\n      static_cast<int>(transa), static_cast<int>(transb), m, n, k, alpha,\n      a.opaque(), lda, b.opaque(), ldb, beta, c->opaque(), ldc);\n\n  switch (dtype) {\n    case blas::DataType::kHalf: {\n#if CUDA_VERSION < 7050\n      return tsl::errors::Internal(\n          \"fp16 sgemm is not implemented in this cuBLAS version \"\n          \"(need at least CUDA 7.5)\");\n#endif\n\n      return DoBlasInternalImpl(\n          cublasSgemmEx, stream, true /* = pointer_mode_host */, math_type,\n          AsCublasOperation(transa), AsCublasOperation(transb), m, n, k,\n          static_cast<const float *>(alpha), a.opaque(), SE_CUDA_DATA_HALF, lda,\n          b.opaque(), SE_CUDA_DATA_HALF, ldb, static_cast<const float *>(beta),\n          c->opaque(), SE_CUDA_DATA_HALF, ldc);\n    }\n#if CUDA_VERSION > 11000\n    case blas::DataType::kBF16: {\n      return DoBlasInternalImpl(\n          cublasSgemmEx, stream, true /* = pointer_mode_host */, math_type,\n          AsCublasOperation(transa), AsCublasOperation(transb), m, n, k,\n          static_cast<const float *>(alpha), a.opaque(), CUDA_R_16BF, lda,\n          b.opaque(), CUDA_R_16BF, ldb, static_cast<const float *>(beta),\n          c->opaque(), CUDA_R_16BF, ldc);\n    }\n#endif\n    case dnn::kFloat:\n      return DoBlasInternalImpl(\n          cublasSgemm, stream, true /* = pointer_mode_host */, math_type,\n          AsCublasOperation(transa), AsCublasOperation(transb), m, n, k,\n          static_cast<const float *>(alpha),\n          static_cast<const float *>(a.opaque()), lda,\n          static_cast<const float *>(b.opaque()), ldb,\n          static_cast<const float *>(beta), static_cast<float *>(c->opaque()),\n          ldc);\n    case dnn::kDouble:\n      return DoBlasInternalImpl(\n          cublasDgemm, stream, true /* = pointer_mode_host */, math_type,\n          AsCublasOperation(transa), AsCublasOperation(transb), m, n, k,\n          static_cast<const double *>(alpha),\n          static_cast<const double *>(a.opaque()), lda,\n          static_cast<const double *>(b.opaque()), ldb,\n          static_cast<const double *>(beta), static_cast<double *>(c->opaque()),\n          ldc);\n    case dnn::kComplexFloat: {\n      GpuComplexType cb_alpha =\n          GpuComplexValue(*static_cast<const std::complex<float> *>(alpha));\n      GpuComplexType cb_beta =\n          GpuComplexValue(*static_cast<const std::complex<float> *>(beta));\n      return DoBlasInternalImpl(\n          cublasCgemm, stream, true /* = pointer_mode_host */, math_type,\n          AsCublasOperation(transa), AsCublasOperation(transb), m, n, k,\n          &cb_alpha, static_cast<const GpuComplexType *>(a.opaque()), lda,\n          static_cast<const GpuComplexType *>(b.opaque()), ldb, &cb_beta,\n          static_cast<GpuComplexType *>(c->opaque()), ldc);\n    }\n    case dnn::kComplexDouble: {\n      GpuDoubleComplexType cb_alpha =\n          GpuComplexValue(*static_cast<const std::complex<double> *>(alpha));\n      GpuDoubleComplexType cb_beta =\n          GpuComplexValue(*static_cast<const std::complex<double> *>(beta));\n      return DoBlasInternalImpl(\n          cublasZgemm, stream, true /* = pointer_mode_host */, math_type,\n          AsCublasOperation(transa), AsCublasOperation(transb), m, n, k,\n          &cb_alpha, static_cast<const GpuDoubleComplexType *>(a.opaque()), lda,\n          static_cast<const GpuDoubleComplexType *>(b.opaque()), ldb, &cb_beta,\n          static_cast<GpuDoubleComplexType *>(c->opaque()), ldc);\n    }\n    default:\n      return tsl::errors::Internal(\"Unsupported datatype for GEMM: \",\n                                   blas::DataTypeString(dtype));\n  }\n}\n\nbool CUDABlas::DoBlasGemvWithProfiling(\n    Stream *stream, blas::Transpose trans, uint64_t m, uint64 n, float alpha,\n    const DeviceMemory<float> &a, int lda, const DeviceMemory<float> &x,\n    int incx, float beta, DeviceMemory<float> *y, int incy,\n    blas::ProfileResult *output_profile_result) {\n  return DoBlasGemvWithProfilingImpl(stream, trans, m, n, alpha, a, lda, x,\n                                     incx, beta, y, incy,\n                                     output_profile_result);\n}\n\nbool CUDABlas::DoBlasGemvWithProfiling(\n    Stream *stream, blas::Transpose trans, uint64_t m, uint64 n, double alpha,\n    const DeviceMemory<double> &a, int lda, const DeviceMemory<double> &x,\n    int incx, double beta, DeviceMemory<double> *y, int incy,\n    blas::ProfileResult *output_profile_result) {\n  return DoBlasGemvWithProfilingImpl(stream, trans, m, n, alpha, a, lda, x,\n                                     incx, beta, y, incy,\n                                     output_profile_result);\n}\n\nbool CUDABlas::DoBlasGemvWithProfiling(\n    Stream *stream, blas::Transpose trans, uint64_t m, uint64 n,\n    std::complex<float> alpha, const DeviceMemory<std::complex<float>> &a,\n    int lda, const DeviceMemory<std::complex<float>> &x, int incx,\n    std::complex<float> beta, DeviceMemory<std::complex<float>> *y, int incy,\n    blas::ProfileResult *output_profile_result) {\n  return DoBlasGemvWithProfilingImpl(stream, trans, m, n, alpha, a, lda, x,\n                                     incx, beta, y, incy,\n                                     output_profile_result);\n}\n\nbool CUDABlas::DoBlasGemvWithProfiling(\n    Stream *stream, blas::Transpose trans, uint64_t m, uint64 n,\n    std::complex<double> alpha, const DeviceMemory<std::complex<double>> &a,\n    int lda, const DeviceMemory<std::complex<double>> &x, int incx,\n    std::complex<double> beta, DeviceMemory<std::complex<double>> *y, int incy,\n    blas::ProfileResult *output_profile_result) {\n  return DoBlasGemvWithProfilingImpl(stream, trans, m, n, alpha, a, lda, x,\n                                     incx, beta, y, incy,\n                                     output_profile_result);\n}\n\nbool CUDABlas::DoBlasGemmWithProfiling(\n    Stream *stream, blas::Transpose transa, blas::Transpose transb, uint64_t m,\n    uint64_t n, uint64 k, float alpha, const DeviceMemory<Eigen::half> &a,\n    int lda, const DeviceMemory<Eigen::half> &b, int ldb, float beta,\n    DeviceMemory<Eigen::half> *c, int ldc,\n    blas::ProfileResult *output_profile_result) {\n  return DoBlasGemmWithProfilingImpl(stream, transa, transb, m, n, k, alpha, a,\n                                     lda, b, ldb, beta, c, ldc,\n                                     output_profile_result);\n}\n\nbool CUDABlas::DoBlasGemmWithProfiling(\n    Stream *stream, blas::Transpose transa, blas::Transpose transb, uint64_t m,\n    uint64_t n, uint64 k, float alpha, const DeviceMemory<float> &a, int lda,\n    const DeviceMemory<float> &b, int ldb, float beta, DeviceMemory<float> *c,\n    int ldc, blas::ProfileResult *output_profile_result) {\n  return DoBlasGemmWithProfilingImpl(stream, transa, transb, m, n, k, alpha, a,\n                                     lda, b, ldb, beta, c, ldc,\n                                     output_profile_result);\n}\n\nbool CUDABlas::DoBlasGemmWithProfiling(\n    Stream *stream, blas::Transpose transa, blas::Transpose transb, uint64_t m,\n    uint64_t n, uint64 k, double alpha, const DeviceMemory<double> &a, int lda,\n    const DeviceMemory<double> &b, int ldb, double beta,\n    DeviceMemory<double> *c, int ldc,\n    blas::ProfileResult *output_profile_result) {\n  return DoBlasGemmWithProfilingImpl(stream, transa, transb, m, n, k, alpha, a,\n                                     lda, b, ldb, beta, c, ldc,\n                                     output_profile_result);\n}\n\nbool CUDABlas::DoBlasGemmWithProfiling(\n    Stream *stream, blas::Transpose transa, blas::Transpose transb, uint64_t m,\n    uint64_t n, uint64 k, std::complex<float> alpha,\n    const DeviceMemory<std::complex<float>> &a, int lda,\n    const DeviceMemory<std::complex<float>> &b, int ldb,\n    std::complex<float> beta, DeviceMemory<std::complex<float>> *c, int ldc,\n    blas::ProfileResult *output_profile_result) {\n  return DoBlasGemmWithProfilingImpl(stream, transa, transb, m, n, k, alpha, a,\n                                     lda, b, ldb, beta, c, ldc,\n                                     output_profile_result);\n}\n\nbool CUDABlas::DoBlasGemmWithProfiling(\n    Stream *stream, blas::Transpose transa, blas::Transpose transb, uint64_t m,\n    uint64_t n, uint64 k, std::complex<double> alpha,\n    const DeviceMemory<std::complex<double>> &a, int lda,\n    const DeviceMemory<std::complex<double>> &b, int ldb,\n    std::complex<double> beta, DeviceMemory<std::complex<double>> *c, int ldc,\n    blas::ProfileResult *output_profile_result) {\n  return DoBlasGemmWithProfilingImpl(stream, transa, transb, m, n, k, alpha, a,\n                                     lda, b, ldb, beta, c, ldc,\n                                     output_profile_result);\n}\n\ntemplate <typename T>\nbool CUDABlas::DoBlasGemvWithProfilingImpl(\n    Stream *stream, blas::Transpose trans, uint64_t m, uint64 n, const T &alpha,\n    const DeviceMemory<T> &a, int lda, const DeviceMemory<T> &x, int incx,\n    const T &beta, DeviceMemory<T> *y, int incy,\n    blas::ProfileResult *output_profile_result) {\n  std::unique_ptr<GpuTimer, GpuTimerDeleter> timer;\n  if (output_profile_result != nullptr) {\n    timer.reset(new GpuTimer(parent_));\n    if (!timer->Init() || !timer->Start(AsGpuStream(stream))) {\n      return false;\n    }\n  }\n\n  // Call blasGemm\n  bool result =\n      DoBlasGemv(stream, trans, m, n, alpha, a, lda, x, incx, beta, y, incy);\n\n  if (timer != nullptr && result) {\n    // GpuTimer will CHECK-fail if we Stop() it while the stream is in an error\n    // state.\n    if (!timer->Stop(AsGpuStream(stream))) {\n      return false;\n    }\n    output_profile_result->set_is_valid(true);\n    output_profile_result->set_algorithm(blas::kDefaultBlasGemv);\n    output_profile_result->set_elapsed_time_in_ms(\n        timer->GetElapsedMilliseconds());\n  }\n  return result;\n}\n\ntemplate <typename T, typename ParamType>\nbool CUDABlas::DoBlasGemmWithProfilingImpl(\n    Stream *stream, blas::Transpose transa, blas::Transpose transb, uint64_t m,\n    uint64_t n, uint64 k, const ParamType &alpha, const DeviceMemory<T> &a,\n    int lda, const DeviceMemory<T> &b, int ldb, const ParamType &beta,\n    DeviceMemory<T> *c, int ldc, blas::ProfileResult *output_profile_result) {\n  std::unique_ptr<GpuTimer, GpuTimerDeleter> timer;\n  if (output_profile_result != nullptr) {\n    timer.reset(new GpuTimer(parent_));\n    if (!timer->Init() || !timer->Start(AsGpuStream(stream))) {\n      return false;\n    }\n  }\n\n  // Call blasGemm\n  bool result = DoBlasGemm(stream, transa, transb, m, n, k,\n                           blas::ToDataType<T>::value, &alpha, a, lda, b, ldb,\n                           &beta, c, ldc, blas::kDefaultComputePrecision)\n                    .ok();\n\n  if (timer != nullptr && result) {\n    // GpuTimer will CHECK-fail if we Stop() it while the stream is in an error\n    // state.\n    if (!timer->Stop(AsGpuStream(stream))) {\n      return false;\n    }\n    output_profile_result->set_is_valid(true);\n    output_profile_result->set_algorithm(blas::kDefaultBlasGemm);\n    output_profile_result->set_elapsed_time_in_ms(\n        timer->GetElapsedMilliseconds());\n  }\n  return result;\n}\n\nstatic bool UsesTensorOps(blas::AlgorithmType algo) {\n#if CUDA_VERSION >= 9000\n  cublasGemmAlgo_t cublas_algo = static_cast<cublasGemmAlgo_t>(algo);\n  return cublas_algo >= CUBLAS_GEMM_DEFAULT_TENSOR_OP;\n#else\n  return false;\n#endif\n}\n\nstatic tsl::StatusOr<cublasMath_t> GetMathTypeForGemmEx(\n    Stream *stream, blas::AlgorithmType algorithm, blas::DataType type_a,\n    blas::DataType type_b, blas::ComputePrecision precision) {\n  if (type_a != type_b) {\n    return tsl::errors::Internal(\"Types of inputs mismatch\");\n  }\n\n  // GPUs < sm_50 don't support cublasGemmEx.\n  CudaComputeCapability cc = stream->GetCudaComputeCapability();\n  if (cc.major < 5) {\n    return tsl::errors::Internal(\"sm_\", cc.major,\n                                 \" does not support explicit gemm algorithms.\");\n  }\n\n  bool algo_uses_tensor_ops = UsesTensorOps(algorithm);\n  cublasMath_t math_type = CUBLAS_DEFAULT_MATH;\n  if (algo_uses_tensor_ops) {\n    if (cc.major < 7) {\n      return tsl::errors::Internal(\n          \"Algorithm \", algorithm,\n          \" uses tensor ops, but tensor ops are not available in sm\", cc.major,\n          \"X devices.\");\n    } else if (type_a == blas::DataType::kFloat) {\n#if CUDA_VERSION < 11000\n      return tsl::errors::Internal(\n          \"Algorithm \", algorithm,\n          \" uses tensor ops, but tensor ops are not available for fp32\");\n#else\n      if (cc.major < 8) {\n        return tsl::errors::Internal(\n            \"Algorithm \", algorithm,\n            \" uses tensor ops, but tensor ops are not available in sm\",\n            cc.major, \"X devices for float input types.\");\n      } else if (!tsl::tensor_float_32_execution_enabled()) {\n        return tsl::errors::Internal(\n            \"Algorithm \", algorithm,\n            \" uses tensor ops, but tensor ops are disabled for fp32 inputs\");\n      }\n      math_type = CUBLAS_TF32_TENSOR_OP_MATH;\n#endif\n    } else if (type_a == blas::DataType::kHalf) {\n#if CUDA_VERSION < 11000\n      math_type = CUBLAS_TENSOR_OP_MATH;\n#endif\n    } else {\n      return tsl::errors::Internal(\n          \"Algorithm \", algorithm,\n          \" uses tensor ops which are not supported for input\");\n    }\n  }\n  if (precision > blas::kDefaultComputePrecision) {\n    math_type = CUBLAS_DEFAULT_MATH;\n  }\n\n  // Return false if we might be hitting a cuBLAS bug that produces the wrong\n  // result. See nvbugs/2156201, b/79126339.\n#if CUDA_VERSION >= 9000 && CUDA_VERSION < 9020\n  if ((algorithm == CUBLAS_GEMM_DEFAULT || algorithm >= CUBLAS_GEMM_ALGO13) &&\n      std::max({m, n, k}) >= 2097153 && cc_major < 7) {\n    return tsl::errors::Internal(\n        \"DoBlasGemmWithAlgorithm returning false to work around cudnn \"\n        \"<9.2 bug with m, n, or k >= 2097153.  See b/79126339.\");\n  }\n#endif\n  return math_type;\n}\n\nstatic tsl::StatusOr<std::unique_ptr<GpuTimer, GpuTimerDeleter>>\nStartGpuTimerForProfile(Stream *stream, GpuExecutor *executor,\n                        blas::ProfileResult *output_profile_result) {\n  std::unique_ptr<GpuTimer, GpuTimerDeleter> timer;\n  if (output_profile_result) {\n    timer.reset(new GpuTimer(executor));\n    if (!timer->Init() || !timer->Start(AsGpuStream(stream))) {\n      return tsl::errors::Internal(\n          \"output_profile_result given, but unable to create a GpuTimer\");\n    }\n  }\n  return timer;\n}\n\nstatic tsl::Status PopulateProfileFromTimer(\n    GpuTimer *timer, blas::AlgorithmType algorithm,\n    blas::ProfileResult *output_profile_result, Stream *stream) {\n  if (timer) {\n    // GpuTimer will CHECK-fail if we Stop() it while the stream is in an error\n    // state.\n    if (!timer->Stop(AsGpuStream(stream))) {\n      return tsl::errors::Internal(\"unable to stop GpuTimer.\");\n    }\n    output_profile_result->set_is_valid(true);\n    output_profile_result->set_algorithm(algorithm);\n    output_profile_result->set_elapsed_time_in_ms(\n        timer->GetElapsedMilliseconds());\n  }\n  return ::tsl::OkStatus();\n}\n\ntsl::Status CUDABlas::DoBlasGemmWithAlgorithm(\n    Stream *stream, blas::Transpose transa, blas::Transpose transb, uint64_t m,\n    uint64_t n, uint64 k, const void *alpha, const DeviceMemoryBase &a,\n    blas::DataType type_a, int lda, const DeviceMemoryBase &b,\n    blas::DataType type_b, int ldb, const void *beta, DeviceMemoryBase *c,\n    blas::DataType type_c, int ldc, blas::ComputationType computation_type,\n    blas::AlgorithmType algorithm, blas::ComputePrecision precision,\n    blas::ProfileResult *output_profile_result) {\n  TF_ASSIGN_OR_RETURN(\n      cublasMath_t math_type,\n      GetMathTypeForGemmEx(stream, algorithm, type_a, type_b, precision));\n\n  TF_ASSIGN_OR_RETURN(auto timer, StartGpuTimerForProfile(\n                                      stream, parent_, output_profile_result));\n\n  // Since we are converting 'algorithm' to cublasGemmAlgo_t by static_cast,\n  // we do the following compile-time check on the default value:\n  static_assert(blas::kDefaultGemmAlgo == CUBLAS_GEMM_DFALT, \"\");\n\n  TF_RETURN_IF_ERROR(DoBlasInternalImpl(\n      AS_LAMBDA(cublasGemmEx), stream, /*pointer_mode_host=*/true, math_type,\n      AsCublasOperation(transa), AsCublasOperation(transb), m, n, k, alpha,\n      a.opaque(), AsCudaDataType(type_a), lda, b.opaque(),\n      AsCudaDataType(type_b), ldb, beta, c->opaque(), AsCudaDataType(type_c),\n      ldc, AsCublasComputeType(computation_type),\n      static_cast<cublasGemmAlgo_t>(algorithm)));\n  TF_RETURN_IF_ERROR(PopulateProfileFromTimer(timer.get(), algorithm,\n                                              output_profile_result, stream));\n  return ::tsl::OkStatus();\n}\n\ntsl::Status CUDABlas::DoBlasGemmStridedBatchedWithAlgorithm(\n    Stream *stream, blas::Transpose transa, blas::Transpose transb, uint64_t m,\n    uint64_t n, uint64 k, const void *alpha, const DeviceMemoryBase &a,\n    blas::DataType type_a, int lda, int64_t stride_a, const DeviceMemoryBase &b,\n    blas::DataType type_b, int ldb, int64_t stride_b, const void *beta,\n    DeviceMemoryBase *c, blas::DataType type_c, int ldc, int64_t stride_c,\n    int batch_count, blas::ComputationType computation_type,\n    blas::AlgorithmType algorithm, blas::ComputePrecision precision,\n    blas::ProfileResult *output_profile_result) {\n  TF_ASSIGN_OR_RETURN(\n      cublasMath_t math_type,\n      GetMathTypeForGemmEx(stream, algorithm, type_a, type_b, precision));\n  TF_ASSIGN_OR_RETURN(auto timer, StartGpuTimerForProfile(\n                                      stream, parent_, output_profile_result));\n\n  cudaDataType_t cuda_in_type = AsCudaDataType(type_a);\n\n#if CUDA_VERSION >= 11000\n  // Workaround CUDA bug where batched GEMM is erroneously marked as\n  // unsupported by manually unbatching it on Pascal.\n  if (cuda_in_type == CUDA_R_16BF &&\n      !stream->GetCudaComputeCapability().IsAtLeast(7)) {\n    for (int batch = 0; batch < batch_count; ++batch) {\n      const auto *a_matrix = reinterpret_cast<const __nv_bfloat16 *>(\n          static_cast<const Eigen::bfloat16 *>(a.opaque()) + batch * stride_a);\n      const auto *b_matrix = reinterpret_cast<const __nv_bfloat16 *>(\n          static_cast<const Eigen::bfloat16 *>(b.opaque()) + batch * stride_b);\n      auto *c_matrix = reinterpret_cast<__nv_bfloat16 *>(\n          static_cast<Eigen::bfloat16 *>(c->opaque()) + batch * stride_c);\n      TF_RETURN_IF_ERROR(DoBlasInternalImpl(\n          AS_LAMBDA(cublasGemmEx), stream, /*pointer_mode_host=*/true,\n          math_type, AsCublasOperation(transa), AsCublasOperation(transb), m, n,\n          k, static_cast<const float *>(alpha), a_matrix, CUDA_R_16BF, lda,\n          b_matrix, CUDA_R_16BF, ldb, static_cast<const float *>(beta),\n          c_matrix, CUDA_R_16BF, ldc, AsCublasComputeType(computation_type),\n          static_cast<cublasGemmAlgo_t>(algorithm)));\n    }\n    TF_RETURN_IF_ERROR(PopulateProfileFromTimer(timer.get(), algorithm,\n                                                output_profile_result, stream));\n    return tsl::OkStatus();\n  }\n#endif\n\n  TF_RETURN_IF_ERROR(DoBlasInternalImpl(\n      AS_LAMBDA(cublasGemmStridedBatchedEx), stream, /*pointer_mode_host=*/true,\n      math_type, AsCublasOperation(transa), AsCublasOperation(transb), m, n, k,\n      alpha, a.opaque(), cuda_in_type, lda, stride_a, b.opaque(), cuda_in_type,\n      ldb, stride_b, beta, c->opaque(), AsCudaDataType(type_c), ldc, stride_c,\n      batch_count, AsCublasComputeType(computation_type),\n      static_cast<cublasGemmAlgo_t>(algorithm)));\n  TF_RETURN_IF_ERROR(PopulateProfileFromTimer(timer.get(), algorithm,\n                                              output_profile_result, stream));\n  return ::tsl::OkStatus();\n}\n\nbool CUDABlas::GetBlasGemmAlgorithms(\n    Stream *stream, std::vector<blas::AlgorithmType> *out_algorithms) {\n  // cublasGemmAlgo_t (and the function that accepts this type, cublasGemmEx)\n  // were first introduced in CUDA 8.\n  //\n  // Note that when CUDA version and compute capability is not sufficient, we\n  // still return the out_algorithms. Caller needs to make sure that in this\n  // case, the returned vector is empty.\n  if (stream->GetCudaComputeCapability().IsAtLeast(\n          CudaComputeCapability::AMPERE)) {\n    // Note: for NVIDIA Ampere Architecture GPUs and beyond, i.e. SM version >=\n    // 80, the numbered algorithm options are equivalent to CUBLAS_GEMM_DEFAULT\n    // or CUBLAS_GEMM_DEFAULT_TENSOR_OP respectively.\n    *out_algorithms = {\n        CUBLAS_GEMM_DFALT,\n        CUBLAS_GEMM_DFALT_TENSOR_OP,\n    };\n  } else {\n    *out_algorithms = {\n      CUBLAS_GEMM_DFALT,\n      CUBLAS_GEMM_ALGO0,\n      CUBLAS_GEMM_ALGO1,\n      CUBLAS_GEMM_ALGO2,\n      CUBLAS_GEMM_ALGO3,\n      CUBLAS_GEMM_ALGO4,\n      CUBLAS_GEMM_ALGO5,\n      CUBLAS_GEMM_ALGO6,\n      CUBLAS_GEMM_ALGO7,\n#if CUDA_VERSION >= 9000\n      CUBLAS_GEMM_ALGO8,\n      CUBLAS_GEMM_ALGO9,\n      CUBLAS_GEMM_ALGO10,\n      CUBLAS_GEMM_ALGO11,\n      CUBLAS_GEMM_ALGO12,\n      CUBLAS_GEMM_ALGO13,\n      CUBLAS_GEMM_ALGO14,\n      CUBLAS_GEMM_ALGO15,\n      CUBLAS_GEMM_ALGO16,\n      CUBLAS_GEMM_ALGO17,\n      CUBLAS_GEMM_DFALT_TENSOR_OP,\n      CUBLAS_GEMM_ALGO0_TENSOR_OP,\n      CUBLAS_GEMM_ALGO1_TENSOR_OP,\n      CUBLAS_GEMM_ALGO2_TENSOR_OP,\n      CUBLAS_GEMM_ALGO3_TENSOR_OP,\n      CUBLAS_GEMM_ALGO4_TENSOR_OP,\n#endif\n#if CUDA_VERSION >= 9020\n      CUBLAS_GEMM_ALGO18,\n      CUBLAS_GEMM_ALGO19,\n      CUBLAS_GEMM_ALGO20,\n      CUBLAS_GEMM_ALGO21,\n      CUBLAS_GEMM_ALGO22,\n      CUBLAS_GEMM_ALGO23,\n      CUBLAS_GEMM_ALGO5_TENSOR_OP,\n      CUBLAS_GEMM_ALGO6_TENSOR_OP,\n      CUBLAS_GEMM_ALGO7_TENSOR_OP,\n      CUBLAS_GEMM_ALGO8_TENSOR_OP,\n      CUBLAS_GEMM_ALGO9_TENSOR_OP,\n      CUBLAS_GEMM_ALGO10_TENSOR_OP,\n      CUBLAS_GEMM_ALGO11_TENSOR_OP,\n      CUBLAS_GEMM_ALGO12_TENSOR_OP,\n      CUBLAS_GEMM_ALGO13_TENSOR_OP,\n      CUBLAS_GEMM_ALGO14_TENSOR_OP,\n      CUBLAS_GEMM_ALGO15_TENSOR_OP,\n#endif\n    };\n  }\n  return true;\n}\n\ntemplate <typename T>\nstruct HalfAsFloat {\n  typedef T type;\n};\n\ntemplate <>\nstruct HalfAsFloat<Eigen::half> {\n  typedef float type;\n};\n\ntemplate <>\nstruct HalfAsFloat<Eigen::bfloat16> {\n  typedef float type;\n};\n\nnamespace {\n// pass-through for non-complex types that don't need conversion to\n// cublas-specific type.\ntemplate <typename T>\nT inline GpuComplexValue(T v) {\n  return v;\n}\n}  // namespace\n\ntemplate <typename T, typename Scalar, typename FuncT>\ntsl::Status CUDABlas::DoBlasGemmBatchedInternal(\n    FuncT cublas_func, Stream *stream, blas::Transpose transa,\n    blas::Transpose transb, uint64_t m, uint64 n, uint64 k, Scalar alpha,\n    const DeviceMemorySlice<T> &a_ptrs_to_wrappers, int lda,\n    const DeviceMemorySlice<T> &b_ptrs_to_wrappers, int ldb, Scalar beta,\n    const DeviceMemorySlice<T> &c_ptrs_to_wrappers, int ldc, int batch_count,\n    ScratchAllocator *scratch_allocator) {\n  std::vector<T *> a_raw_ptrs, b_raw_ptrs, c_raw_ptrs;\n  for (int i = 0; i < batch_count; ++i) {\n    a_raw_ptrs.push_back(static_cast<T *>(a_ptrs_to_wrappers[i]->opaque()));\n    b_raw_ptrs.push_back(static_cast<T *>(b_ptrs_to_wrappers[i]->opaque()));\n    c_raw_ptrs.push_back(static_cast<T *>(c_ptrs_to_wrappers[i]->opaque()));\n  }\n\n  typedef typename HalfAsFloat<typename GpuComplexT<T>::type>::type CUDA_T;\n\n  const size_t size = batch_count * sizeof(CUDA_T *);\n\n  // Device-side copy of pointers to matrices.\n  DeviceMemory<CUDA_T *> a;\n  DeviceMemory<CUDA_T *> b;\n  DeviceMemory<CUDA_T *> c;\n\n  // If temporary space is allocated for device-side copies of pointers to\n  // matrices, that temporary space should not be freed until this function\n  // returns. Although the values for these unique_ptrs are not set here, they\n  // are declared at this scope so they will be destroyed when the function\n  // returns.\n  //\n  // If a scratch allocator is provided, these pointers will not be used at all.\n  std::unique_ptr<TemporaryDeviceMemory<CUDA_T *>> a_temporary;\n  std::unique_ptr<TemporaryDeviceMemory<CUDA_T *>> b_temporary;\n  std::unique_ptr<TemporaryDeviceMemory<CUDA_T *>> c_temporary;\n\n  // Decide how to allocate device-side copy of pointers to matrices based on\n  // whether a scratch allocator was passed.\n  if (scratch_allocator != nullptr) {\n    TF_ASSIGN_OR_RETURN(DeviceMemory<uint8_t> a_bytes,\n                        scratch_allocator->AllocateBytes(size));\n    TF_ASSIGN_OR_RETURN(DeviceMemory<uint8_t> b_bytes,\n                        scratch_allocator->AllocateBytes(size));\n    TF_ASSIGN_OR_RETURN(DeviceMemory<uint8_t> c_bytes,\n                        scratch_allocator->AllocateBytes(size));\n    a = DeviceMemory<CUDA_T *>(a_bytes);\n    b = DeviceMemory<CUDA_T *>(b_bytes);\n    c = DeviceMemory<CUDA_T *>(c_bytes);\n  } else {\n    TF_ASSIGN_OR_RETURN(a_temporary,\n                        stream->AllocateTemporaryArray<CUDA_T *>(batch_count));\n    TF_ASSIGN_OR_RETURN(b_temporary,\n                        stream->AllocateTemporaryArray<CUDA_T *>(batch_count));\n    TF_ASSIGN_OR_RETURN(c_temporary,\n                        stream->AllocateTemporaryArray<CUDA_T *>(batch_count));\n    a = DeviceMemory<CUDA_T *>(*a_temporary->mutable_device_memory());\n    b = DeviceMemory<CUDA_T *>(*b_temporary->mutable_device_memory());\n    c = DeviceMemory<CUDA_T *>(*c_temporary->mutable_device_memory());\n  }\n\n  if (!stream->ThenMemcpy(&a, a_raw_ptrs.data(), size).ok() ||\n      !stream->ThenMemcpy(&b, b_raw_ptrs.data(), size).ok() ||\n      !stream->ThenMemcpy(&c, c_raw_ptrs.data(), size).ok()) {\n    return tsl::Status(tsl::error::INTERNAL,\n                       \"failed to copy memory from host to device in \"\n                       \"CUDABlas::DoBlasGemmBatched\");\n  }\n\n  cudaDataType_t data_type = CUDADataType<T>::type;\n\n#if CUDA_VERSION >= 9010\n  if (stream->GetCudaComputeCapability().IsAtLeast(5)) {\n    cublasMath_t math_type;\n    cublasGemmAlgo_t algo;\n\n#if CUDA_VERSION >= 11000\n    bool is_16bit = data_type == CUDA_R_16F || data_type == CUDA_R_16BF;\n#else\n    bool is_16bit = data_type == CUDA_R_16F;\n#endif  // CUDA_VERSION >= 11000\n\n    if (is_16bit) {\n#if CUDA_VERSION < 11000\n      math_type = CUBLAS_TENSOR_OP_MATH;\n#else\n      math_type = CUBLAS_DEFAULT_MATH;\n#endif\n      algo = CUBLAS_GEMM_DFALT_TENSOR_OP;\n#if CUBLAS_VER_MAJOR >= 11\n    } else if (data_type == CUDA_R_32F) {\n      // DoBlassInternalImpl will switch math_type back to CUBLAS_DEFAULT_MATH\n      // if TensorFloat-32 is disabled.\n      math_type = CUBLAS_TF32_TENSOR_OP_MATH;\n      algo = tsl::tensor_float_32_execution_enabled()\n                 ? CUBLAS_GEMM_DFALT_TENSOR_OP\n                 : CUBLAS_GEMM_DFALT;\n#endif\n    } else {\n      math_type = CUBLAS_DEFAULT_MATH;\n      algo = CUBLAS_GEMM_DFALT;\n    }\n    cudaDataType_t compute_type = is_16bit ? CUDA_R_32F : data_type;\n    const void **a_void_ptrs = reinterpret_cast<const void **>(\n        const_cast<const CUDA_T **>(GpuMemory(a)));\n    const void **b_void_ptrs = reinterpret_cast<const void **>(\n        const_cast<const CUDA_T **>(GpuMemory(b)));\n    void **c_void_ptrs =\n        reinterpret_cast<void **>(const_cast<CUDA_T **>(GpuMemory(c)));\n    return DoBlasInternalImpl(\n        AS_LAMBDA(cublasGemmBatchedEx), stream, true /* = pointer_mode_host */,\n        math_type, AsCublasOperation(transa), AsCublasOperation(transb), m, n,\n        k, &alpha, a_void_ptrs, data_type, lda, b_void_ptrs, data_type, ldb,\n        &beta, c_void_ptrs, data_type, ldc, batch_count, compute_type, algo);\n  }\n#endif\n  // either CUDA_VERSION < 9.1 or SM < 5.0\n  if (data_type != CUDA_R_16F) {\n    auto cb_alpha = GpuComplexValue(alpha);\n    auto cb_beta = GpuComplexValue(beta);\n    bool ok = DoBlasInternal(\n        cublas_func, stream, true /* = pointer_mode_host */,\n        AsCublasOperation(transa), AsCublasOperation(transb), m, n, k,\n        GpuComplex(&cb_alpha), const_cast<const CUDA_T **>(GpuMemory(a)), lda,\n        const_cast<const CUDA_T **>(GpuMemory(b)), ldb, GpuComplex(&cb_beta),\n        const_cast<CUDA_T **>(GpuMemory(c)), ldc, batch_count);\n    if (ok) {\n      return ::tsl::OkStatus();\n    }\n    return tsl::Status(tsl::error::INTERNAL,\n                       \"failed BLAS call, see log for details\");\n  } else {\n    // Fall back to a loop for fp16\n    for (int b = 0; b < batch_count; ++b) {\n      const DeviceMemory<T> &a_matrix = *a_ptrs_to_wrappers[b];\n      const DeviceMemory<T> &b_matrix = *b_ptrs_to_wrappers[b];\n      DeviceMemory<T> *c_matrix = c_ptrs_to_wrappers[b];\n      TF_RETURN_IF_ERROR(DoBlasGemm(\n          stream, transa, transb, m, n, k, blas::ToDataType<T>::value, &alpha,\n          a_matrix, lda, b_matrix, ldb, &beta, c_matrix, ldc,\n          blas::kDefaultComputePrecision));\n    }\n    return ::tsl::OkStatus();\n  }\n}\n\nbool CUDABlas::DoBlasGemmBatched(\n    Stream *stream, blas::Transpose transa, blas::Transpose transb, uint64_t m,\n    uint64_t n, uint64 k, float alpha, DeviceMemorySlice<Eigen::half> a_array,\n    int lda, DeviceMemorySlice<Eigen::half> b_array, int ldb, float beta,\n    DeviceMemorySlice<Eigen::half> c_array, int ldc, int batch_count,\n    ScratchAllocator *scratch_allocator) {\n  // Note: The func passed here (cublasSgemmBatched) is not actually called,\n  // due to special handling of fp16 inside DoBlasGemmBatchedInternal.\n  tsl::Status status = DoBlasGemmBatchedInternal(\n      cublasSgemmBatched, stream, transa, transb, m, n, k, alpha, a_array, lda,\n      b_array, ldb, beta, c_array, ldc, batch_count, scratch_allocator);\n  if (!status.ok()) {\n    LOG(ERROR) << status;\n  }\n  return status.ok();\n}\n\nbool CUDABlas::DoBlasGemmBatched(\n    Stream *stream, blas::Transpose transa, blas::Transpose transb, uint64_t m,\n    uint64_t n, uint64 k, float alpha,\n    DeviceMemorySlice<Eigen::bfloat16> a_array, int lda,\n    DeviceMemorySlice<Eigen::bfloat16> b_array, int ldb, float beta,\n    DeviceMemorySlice<Eigen::bfloat16> c_array, int ldc, int batch_count,\n    ScratchAllocator *scratch_allocator) {\n  // Note: The func passed here (cublasSgemmBatched) is not actually called,\n  // due to special handling of bf16 inside DoBlasGemmBatchedInternal.\n  tsl::Status status = DoBlasGemmBatchedInternal(\n      cublasSgemmBatched, stream, transa, transb, m, n, k, alpha, a_array, lda,\n      b_array, ldb, beta, c_array, ldc, batch_count, scratch_allocator);\n  if (!status.ok()) {\n    LOG(ERROR) << status;\n  }\n  return status.ok();\n}\n\nbool CUDABlas::DoBlasGemmBatched(Stream *stream, blas::Transpose transa,\n                                 blas::Transpose transb, uint64_t m, uint64_t n,\n                                 uint64 k, float alpha,\n                                 DeviceMemorySlice<float> a_array, int lda,\n                                 DeviceMemorySlice<float> b_array, int ldb,\n                                 float beta, DeviceMemorySlice<float> c_array,\n                                 int ldc, int batch_count,\n                                 ScratchAllocator *scratch_allocator) {\n  tsl::Status status = DoBlasGemmBatchedInternal(\n      cublasSgemmBatched, stream, transa, transb, m, n, k, alpha, a_array, lda,\n      b_array, ldb, beta, c_array, ldc, batch_count, scratch_allocator);\n  if (!status.ok()) {\n    LOG(ERROR) << status;\n  }\n  return status.ok();\n}\n\nbool CUDABlas::DoBlasGemmBatched(Stream *stream, blas::Transpose transa,\n                                 blas::Transpose transb, uint64_t m, uint64_t n,\n                                 uint64 k, double alpha,\n                                 DeviceMemorySlice<double> a_array, int lda,\n                                 DeviceMemorySlice<double> b_array, int ldb,\n                                 double beta, DeviceMemorySlice<double> c_array,\n                                 int ldc, int batch_count,\n                                 ScratchAllocator *scratch_allocator) {\n  tsl::Status status = DoBlasGemmBatchedInternal(\n      cublasDgemmBatched, stream, transa, transb, m, n, k, alpha, a_array, lda,\n      b_array, ldb, beta, c_array, ldc, batch_count, scratch_allocator);\n  if (!status.ok()) {\n    LOG(ERROR) << status;\n  }\n  return status.ok();\n}\n\nbool CUDABlas::DoBlasGemmBatched(\n    Stream *stream, blas::Transpose transa, blas::Transpose transb, uint64_t m,\n    uint64_t n, uint64 k, std::complex<float> alpha,\n    DeviceMemorySlice<std::complex<float>> a_array, int lda,\n    DeviceMemorySlice<std::complex<float>> b_array, int ldb,\n    std::complex<float> beta, DeviceMemorySlice<std::complex<float>> c_array,\n    int ldc, int batch_count, ScratchAllocator *scratch_allocator) {\n  tsl::Status status = DoBlasGemmBatchedInternal(\n      cublasCgemmBatched, stream, transa, transb, m, n, k, alpha, a_array, lda,\n      b_array, ldb, beta, c_array, ldc, batch_count, scratch_allocator);\n  if (!status.ok()) {\n    LOG(ERROR) << status;\n  }\n  return status.ok();\n}\n\nbool CUDABlas::DoBlasGemmBatched(\n    Stream *stream, blas::Transpose transa, blas::Transpose transb, uint64_t m,\n    uint64_t n, uint64 k, std::complex<double> alpha,\n    DeviceMemorySlice<std::complex<double>> a_array, int lda,\n    DeviceMemorySlice<std::complex<double>> b_array, int ldb,\n    std::complex<double> beta, DeviceMemorySlice<std::complex<double>> c_array,\n    int ldc, int batch_count, ScratchAllocator *scratch_allocator) {\n  tsl::Status status = DoBlasGemmBatchedInternal(\n      cublasZgemmBatched, stream, transa, transb, m, n, k, alpha, a_array, lda,\n      b_array, ldb, beta, c_array, ldc, batch_count, scratch_allocator);\n  if (!status.ok()) {\n    LOG(ERROR) << status;\n  }\n  return status.ok();\n}\n\ntsl::Status CUDABlas::DoBlasGemmStridedBatched(\n    Stream *stream, blas::Transpose transa, blas::Transpose transb, uint64_t m,\n    uint64_t n, uint64 k, blas::DataType dtype, const void *alpha,\n    const DeviceMemoryBase &a, int lda, int64_t stride_a,\n    const DeviceMemoryBase &b, int ldb, int64_t stride_b, const void *beta,\n    DeviceMemoryBase *c, int ldc, int64_t stride_c, int batch_count,\n    blas::ComputePrecision precision) {\n  cublasMath_t math_type = CUBLAS_DEFAULT_MATH;\n#if CUDA_VERSION < 11000\n  if (dtype == dnn::kHalf) {\n    math_type = CUBLAS_TENSOR_OP_MATH;\n  }\n#else\n  if (dtype == dnn::kFloat) {\n    math_type = CUBLAS_TF32_TENSOR_OP_MATH;\n  }\n  if (precision > blas::kDefaultComputePrecision) {\n    math_type = CUBLAS_DEFAULT_MATH;\n  }\n#endif\n\n  switch (dtype) {\n#if CUDA_VERSION >= 11000\n    case dnn::kBF16: {\n      CudaComputeCapability cc = stream->GetCudaComputeCapability();\n      if (cc.IsAtLeast(7)) {\n        cublasGemmAlgo_t algo =\n            (cc.major >= 7 ? CUBLAS_GEMM_DFALT_TENSOR_OP : CUBLAS_GEMM_DFALT);\n        return DoBlasInternalImpl(\n            AS_LAMBDA(cublasGemmStridedBatchedEx), stream,\n            true /* = pointer_mode_host */, math_type,\n            AsCublasOperation(transa), AsCublasOperation(transb), m, n, k,\n            alpha, a.opaque(), CUDA_R_16BF, lda, stride_a, b.opaque(),\n            CUDA_R_16BF, ldb, stride_b, beta, c->opaque(), CUDA_R_16BF, ldc,\n            stride_c, batch_count,\n            /*compute_type=*/CUDA_R_32F, algo);\n      }\n      // Fall back to a loop.\n      for (int batch = 0; batch < batch_count; ++batch) {\n        const auto *a_matrix = reinterpret_cast<const __nv_bfloat16 *>(\n            static_cast<const Eigen::bfloat16 *>(a.opaque()) +\n            batch * stride_a);\n        const auto *b_matrix = reinterpret_cast<const __nv_bfloat16 *>(\n            static_cast<const Eigen::bfloat16 *>(b.opaque()) +\n            batch * stride_b);\n        auto *c_matrix = reinterpret_cast<__nv_bfloat16 *>(\n            static_cast<Eigen::bfloat16 *>(c->opaque()) + batch * stride_c);\n        TF_RETURN_IF_ERROR(DoBlasInternalImpl(\n            cublasSgemmEx, stream, true /* = pointer_mode_host */,\n            CUBLAS_DEFAULT_MATH, AsCublasOperation(transa),\n            AsCublasOperation(transb), m, n, k,\n            static_cast<const float *>(alpha), a_matrix, CUDA_R_16BF, lda,\n            b_matrix, CUDA_R_16BF, ldb, static_cast<const float *>(beta),\n            c_matrix, CUDA_R_16BF, ldc));\n      }\n      return tsl::OkStatus();\n    }\n#endif\n    case dnn::kHalf: {\n#if CUDA_VERSION >= 9010\n      CudaComputeCapability cc = stream->GetCudaComputeCapability();\n      if (cc.major >= 5) {\n        cublasGemmAlgo_t algo =\n            (cc.major >= 7 ? CUBLAS_GEMM_DFALT_TENSOR_OP : CUBLAS_GEMM_DFALT);\n        return DoBlasInternalImpl(\n            AS_LAMBDA(cublasGemmStridedBatchedEx), stream,\n            true /* = pointer_mode_host */, math_type,\n            AsCublasOperation(transa), AsCublasOperation(transb), m, n, k,\n            alpha, a.opaque(), CUDA_R_16F, lda, stride_a, b.opaque(),\n            CUDA_R_16F, ldb, stride_b, beta, c->opaque(), CUDA_R_16F, ldc,\n            stride_c, batch_count, CUDA_R_32F, algo);\n      }\n#endif\n      // Either CUDA_VERSION < 9.1 or SM < 5.0. Fall back to a loop.\n      for (int batch = 0; batch < batch_count; ++batch) {\n        const auto *a_matrix = reinterpret_cast<const __half *>(\n            static_cast<const Eigen::half *>(a.opaque()) + batch * stride_a);\n        const auto *b_matrix = reinterpret_cast<const __half *>(\n            static_cast<const Eigen::half *>(b.opaque()) + batch * stride_b);\n        auto *c_matrix = reinterpret_cast<__half *>(\n            static_cast<Eigen::half *>(c->opaque()) + batch * stride_c);\n        TF_RETURN_IF_ERROR(DoBlasInternalImpl(\n            cublasSgemmEx, stream, true /* = pointer_mode_host */,\n            CUBLAS_DEFAULT_MATH, AsCublasOperation(transa),\n            AsCublasOperation(transb), m, n, k,\n            static_cast<const float *>(alpha), a_matrix, SE_CUDA_DATA_HALF, lda,\n            b_matrix, SE_CUDA_DATA_HALF, ldb, static_cast<const float *>(beta),\n            c_matrix, SE_CUDA_DATA_HALF, ldc));\n      }\n      return ::tsl::OkStatus();\n    }\n    case dnn::kFloat: {\n      return DoBlasInternalImpl(\n          cublasSgemmStridedBatched, stream, true /* = pointer_mode_host */,\n          math_type, AsCublasOperation(transa), AsCublasOperation(transb), m, n,\n          k, static_cast<const float *>(alpha),\n          static_cast<const float *>(a.opaque()), lda, stride_a,\n          static_cast<const float *>(b.opaque()), ldb, stride_b,\n          static_cast<const float *>(beta), static_cast<float *>(c->opaque()),\n          ldc, stride_c, batch_count);\n    }\n    case dnn::kDouble:\n      return DoBlasInternalImpl(\n          cublasDgemmStridedBatched, stream, true /* = pointer_mode_host */,\n          math_type, AsCublasOperation(transa), AsCublasOperation(transb), m, n,\n          k, static_cast<const double *>(alpha),\n          static_cast<const double *>(a.opaque()), lda, stride_a,\n          static_cast<const double *>(b.opaque()), ldb, stride_b,\n          static_cast<const double *>(beta), static_cast<double *>(c->opaque()),\n          ldc, stride_c, batch_count);\n    case dnn::kComplexFloat: {\n      GpuComplexType cb_alpha =\n          GpuComplexValue(*static_cast<const std::complex<float> *>(alpha));\n      GpuComplexType cb_beta =\n          GpuComplexValue(*static_cast<const std::complex<float> *>(beta));\n      return DoBlasInternalImpl(\n          cublasCgemmStridedBatched, stream, true /* = pointer_mode_host */,\n          math_type, AsCublasOperation(transa), AsCublasOperation(transb), m, n,\n          k, GpuComplex(&cb_alpha),\n          static_cast<const GpuComplexType *>(a.opaque()), lda, stride_a,\n          static_cast<const GpuComplexType *>(b.opaque()), ldb, stride_b,\n          GpuComplex(&cb_beta), static_cast<GpuComplexType *>(c->opaque()), ldc,\n          stride_c, batch_count);\n    }\n    case dnn::kComplexDouble: {\n      GpuDoubleComplexType cb_alpha =\n          GpuComplexValue(*static_cast<const std::complex<double> *>(alpha));\n      GpuDoubleComplexType cb_beta =\n          GpuComplexValue(*static_cast<const std::complex<double> *>(beta));\n      return DoBlasInternalImpl(\n          cublasZgemmStridedBatched, stream, true /* = pointer_mode_host */,\n          math_type, AsCublasOperation(transa), AsCublasOperation(transb), m, n,\n          k, GpuComplex(&cb_alpha),\n          static_cast<const GpuDoubleComplexType *>(a.opaque()), lda, stride_a,\n          static_cast<const GpuDoubleComplexType *>(b.opaque()), ldb, stride_b,\n          GpuComplex(&cb_beta),\n          static_cast<GpuDoubleComplexType *>(c->opaque()), ldc, stride_c,\n          batch_count);\n    }\n    default:\n      return tsl::errors::Internal(\"Unsupported datatype for GEMM: \",\n                                   blas::DataTypeString(dtype));\n  }\n}\n\nbool CUDABlas::DoBlasTrsm(Stream *stream, blas::Side side,\n                          blas::UpperLower uplo, blas::Transpose transa,\n                          blas::Diagonal diag, uint64_t m, uint64 n,\n                          float alpha, const DeviceMemory<float> &a, int lda,\n                          DeviceMemory<float> *b, int ldb) {\n  return DoBlasInternal(cublasStrsm, stream, true /* = pointer_mode_host */,\n                        CUDABlasSide(side), CUDABlasUpperLower(uplo),\n                        AsCublasOperation(transa), CUDABlasDiagonal(diag), m, n,\n                        &alpha, GpuMemory(a), lda, GpuMemoryMutable(b), ldb);\n}\n\nbool CUDABlas::DoBlasTrsm(Stream *stream, blas::Side side,\n                          blas::UpperLower uplo, blas::Transpose transa,\n                          blas::Diagonal diag, uint64_t m, uint64 n,\n                          double alpha, const DeviceMemory<double> &a, int lda,\n                          DeviceMemory<double> *b, int ldb) {\n  return DoBlasInternal(cublasDtrsm, stream, true /* = pointer_mode_host */,\n                        CUDABlasSide(side), CUDABlasUpperLower(uplo),\n                        AsCublasOperation(transa), CUDABlasDiagonal(diag), m, n,\n                        &alpha, GpuMemory(a), lda, GpuMemoryMutable(b), ldb);\n}\n\nbool CUDABlas::DoBlasTrsm(Stream *stream, blas::Side side,\n                          blas::UpperLower uplo, blas::Transpose transa,\n                          blas::Diagonal diag, uint64_t m, uint64 n,\n                          std::complex<float> alpha,\n                          const DeviceMemory<std::complex<float>> &a, int lda,\n                          DeviceMemory<std::complex<float>> *b, int ldb) {\n  auto cb_alpha = GpuComplexValue(alpha);\n  return DoBlasInternal(cublasCtrsm, stream, true /* = pointer_mode_host */,\n                        CUDABlasSide(side), CUDABlasUpperLower(uplo),\n                        AsCublasOperation(transa), CUDABlasDiagonal(diag), m, n,\n                        GpuComplex(&cb_alpha), GpuComplex(GpuMemory(a)), lda,\n                        GpuComplex(GpuMemoryMutable(b)), ldb);\n}\n\nbool CUDABlas::DoBlasTrsm(Stream *stream, blas::Side side,\n                          blas::UpperLower uplo, blas::Transpose transa,\n                          blas::Diagonal diag, uint64_t m, uint64 n,\n                          std::complex<double> alpha,\n                          const DeviceMemory<std::complex<double>> &a, int lda,\n                          DeviceMemory<std::complex<double>> *b, int ldb) {\n  auto cb_alpha = GpuComplexValue(alpha);\n  return DoBlasInternal(cublasZtrsm, stream, true /* = pointer_mode_host */,\n                        CUDABlasSide(side), CUDABlasUpperLower(uplo),\n                        AsCublasOperation(transa), CUDABlasDiagonal(diag), m, n,\n                        GpuComplex(&cb_alpha), GpuComplex(GpuMemory(a)), lda,\n                        GpuComplex(GpuMemoryMutable(b)), ldb);\n}\n\nbool CUDABlas::DoBlasTrsmBatched(Stream *stream, blas::Side side,\n                                 blas::UpperLower uplo, blas::Transpose transa,\n                                 blas::Diagonal diag, uint64_t m, uint64 n,\n                                 float alpha, const DeviceMemory<float *> &as,\n                                 int lda, DeviceMemory<float *> *bs, int ldb,\n                                 int batch_count) {\n  return DoBlasInternal(cublasStrsmBatched, stream,\n                        true /* = pointer_mode_host */, CUDABlasSide(side),\n                        CUDABlasUpperLower(uplo), AsCublasOperation(transa),\n                        CUDABlasDiagonal(diag), m, n, &alpha, GpuMemory(as),\n                        lda, GpuMemoryMutable(bs), ldb, batch_count);\n}\n\nbool CUDABlas::DoBlasTrsmBatched(Stream *stream, blas::Side side,\n                                 blas::UpperLower uplo, blas::Transpose transa,\n                                 blas::Diagonal diag, uint64_t m, uint64 n,\n                                 double alpha, const DeviceMemory<double *> &as,\n                                 int lda, DeviceMemory<double *> *bs, int ldb,\n                                 int batch_count) {\n  return DoBlasInternal(cublasDtrsmBatched, stream,\n                        true /* = pointer_mode_host */, CUDABlasSide(side),\n                        CUDABlasUpperLower(uplo), AsCublasOperation(transa),\n                        CUDABlasDiagonal(diag), m, n, &alpha, GpuMemory(as),\n                        lda, GpuMemoryMutable(bs), ldb, batch_count);\n}\n\nbool CUDABlas::DoBlasTrsmBatched(Stream *stream, blas::Side side,\n                                 blas::UpperLower uplo, blas::Transpose transa,\n                                 blas::Diagonal diag, uint64_t m, uint64 n,\n                                 std::complex<float> alpha,\n                                 const DeviceMemory<std::complex<float> *> &as,\n                                 int lda,\n                                 DeviceMemory<std::complex<float> *> *bs,\n                                 int ldb, int batch_count) {\n  auto cb_alpha = GpuComplexValue(alpha);\n  return DoBlasInternal(\n      cublasCtrsmBatched, stream, true /* = pointer_mode_host */,\n      CUDABlasSide(side), CUDABlasUpperLower(uplo), AsCublasOperation(transa),\n      CUDABlasDiagonal(diag), m, n, &cb_alpha,\n      reinterpret_cast<float2 *const *>(GpuMemory(as)), lda,\n      reinterpret_cast<float2 **>(GpuMemoryMutable(bs)), ldb, batch_count);\n}\n\nbool CUDABlas::DoBlasTrsmBatched(Stream *stream, blas::Side side,\n                                 blas::UpperLower uplo, blas::Transpose transa,\n                                 blas::Diagonal diag, uint64_t m, uint64 n,\n                                 std::complex<double> alpha,\n                                 const DeviceMemory<std::complex<double> *> &as,\n                                 int lda,\n                                 DeviceMemory<std::complex<double> *> *bs,\n                                 int ldb, int batch_count) {\n  auto cb_alpha = GpuComplexValue(alpha);\n  return DoBlasInternal(\n      cublasZtrsmBatched, stream, true /* = pointer_mode_host */,\n      CUDABlasSide(side), CUDABlasUpperLower(uplo), AsCublasOperation(transa),\n      CUDABlasDiagonal(diag), m, n, &cb_alpha,\n      reinterpret_cast<double2 *const *>(GpuMemory(as)), lda,\n      reinterpret_cast<double2 **>(GpuMemoryMutable(bs)), ldb, batch_count);\n}\n\ntsl::Status CUDABlas::GetVersion(std::string *version) {\n  absl::MutexLock lock(&mu_);\n\n  int v;\n  auto status = cublasGetVersion(blas_, &v);\n  if (status != CUBLAS_STATUS_SUCCESS) {\n    return tsl::errors::Internal(ToString(status));\n  }\n  *version = std::to_string(v);\n  return ::tsl::OkStatus();\n}\n\nvoid initialize_cublas() {\n  tsl::Status status =\n      PluginRegistry::Instance()->RegisterFactory<PluginRegistry::BlasFactory>(\n          kCudaPlatformId, kCuBlasPlugin, \"cuBLAS\",\n          [](::stream_executor::internal::StreamExecutorInterface *parent)\n              -> blas::BlasSupport * {\n            gpu::GpuExecutor *cuda_executor =\n                dynamic_cast<gpu::GpuExecutor *>(parent);\n            if (cuda_executor == nullptr) {\n              LOG(ERROR)\n                  << \"Attempting to initialize an instance of the cuBLAS \"\n                  << \"support library with a non-CUDA StreamExecutor\";\n              return nullptr;\n            }\n\n            CUDABlas *blas = new CUDABlas(cuda_executor);\n            if (!blas->Init()) {\n              // Note: Init() will log a more specific error.\n              delete blas;\n              return nullptr;\n            }\n            return blas;\n          });\n\n  if (!status.ok()) {\n    LOG(ERROR) << \"Unable to register cuBLAS factory: \"\n               << status.error_message();\n  }\n\n  PluginRegistry::Instance()->SetDefaultFactory(\n      cuda::kCudaPlatformId, PluginKind::kBlas, kCuBlasPlugin);\n}\n\n}  // namespace cuda\n}  // namespace stream_executor\n\nREGISTER_MODULE_INITIALIZER(register_cublas,\n                            { stream_executor::cuda::initialize_cublas(); });"