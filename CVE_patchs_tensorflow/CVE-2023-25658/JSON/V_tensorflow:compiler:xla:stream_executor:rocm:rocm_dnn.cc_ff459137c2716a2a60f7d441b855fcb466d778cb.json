"/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/compiler/xla/stream_executor/rocm/rocm_dnn.h\"\n\n#include <functional>\n#include <memory>\n\n#include \"absl/algorithm/container.h\"\n#include \"absl/base/thread_annotations.h\"\n#include \"absl/strings/str_cat.h\"\n#include \"absl/strings/str_format.h\"\n#include \"absl/types/span.h\"\n#include \"third_party/eigen3/Eigen/Core\"\n#include \"rocm/include/miopen/miopen.h\"\n#include \"tensorflow/compiler/xla/stream_executor/dnn.h\"\n#include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_activation.h\"\n#include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_driver.h\"\n#include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_executor.h\"\n#include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_stream.h\"\n#include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_timer.h\"\n#include \"tensorflow/compiler/xla/stream_executor/lib/error.h\"\n#include \"tensorflow/compiler/xla/stream_executor/lib/initialize.h\"\n#include \"tensorflow/compiler/xla/stream_executor/platform/dso_loader.h\"\n#include \"tensorflow/compiler/xla/stream_executor/platform/logging.h\"\n#include \"tensorflow/compiler/xla/stream_executor/plugin_registry.h\"\n#include \"tensorflow/compiler/xla/stream_executor/rocm/rocm_diagnostics.h\"\n#include \"tensorflow/compiler/xla/stream_executor/rocm/rocm_platform_id.h\"\n#include \"tensorflow/compiler/xla/stream_executor/scratch_allocator.h\"\n#include \"tensorflow/compiler/xla/stream_executor/stream.h\"\n#include \"tensorflow/compiler/xla/stream_executor/stream_executor_pimpl.h\"\n#include \"tensorflow/tsl/platform/env.h\"\n#include \"tensorflow/tsl/platform/hash.h\"\n#include \"tensorflow/tsl/util/determinism.h\"\n#include \"tensorflow/tsl/util/env_var.h\"\n\nnamespace {\n\n// Converts (via narrowing) a type T value to a type U, and checks that the\n// value has no value change due to the conversion.\ntemplate <typename WideT, typename NarrowT>\nNarrowT CheckedNarrowing(const WideT& wide) {\n  NarrowT narrow = wide;\n  CHECK_EQ(narrow, wide)\n      << \"checked narrowing failed; values not equal post-conversion\";\n  return narrow;\n}\n\nconst int kConvDebugVlogLevel = 3;\n\n}  // namespace\n\nnamespace stream_executor {\n\nusing dnn::AlgorithmDesc;\nusing dnn::BatchDescriptor;\nusing dnn::ConvolutionDescriptor;\nusing dnn::FilterDescriptor;\nusing dnn::NormalizeDescriptor;\nusing dnn::PoolingDescriptor;\n\nnamespace gpu {\n\nPLUGIN_REGISTRY_DEFINE_PLUGIN_ID(kMIOpenPlugin);\n\nstring ToString(miopenStatus_t status) {\n  switch (status) {\n    case miopenStatusSuccess:\n      return \"miopenStatusSuccess\";\n    case miopenStatusNotInitialized:\n      return \"miopenStatusNotInitialized\";\n    case miopenStatusAllocFailed:\n      return \"miopenStatusAllocFailed\";\n    case miopenStatusBadParm:\n      return \"miopenStatusBadParm\";\n    case miopenStatusInternalError:\n      return \"miopenStatusInternalError\";\n    case miopenStatusInvalidValue:\n      return \"miopenStatusInvalidValue\";\n    case miopenStatusNotImplemented:\n      return \"miopenStatusNotImplemented\";\n    case miopenStatusUnknownError:\n      return \"miopenStatusUnknownError\";\n    default:\n      return absl::StrCat(\"<unknown miopen status: \", static_cast<int>(status),\n                          \">\");\n  }\n}\n\nstring ToString(miopenConvFwdAlgorithm_t algorithm) {\n  string s;\n  switch (algorithm) {\n    case miopenConvolutionFwdAlgoGEMM:\n      s = \"GEMM\";\n      break;\n    case miopenConvolutionFwdAlgoDirect:\n      s = \"Direct\";\n      break;\n    case miopenConvolutionFwdAlgoFFT:\n      s = \"FFT\";\n      break;\n    case miopenConvolutionFwdAlgoWinograd:\n      s = \"Winograd\";\n      break;\n    case miopenConvolutionFwdAlgoImplicitGEMM:\n      s = \"Implicit GEMM\";\n      break;\n  }\n  return s;\n}\n\nstring ToString(miopenConvBwdWeightsAlgorithm_t algorithm) {\n  string s;\n  switch (algorithm) {\n    case miopenConvolutionBwdWeightsAlgoGEMM:\n      s = \"GEMM\";\n      break;\n    case miopenConvolutionBwdWeightsAlgoDirect:\n      s = \"Direct\";\n      break;\n    case miopenConvolutionBwdWeightsAlgoWinograd:\n      s = \"Winograd\";\n      break;\n    case miopenConvolutionBwdWeightsAlgoImplicitGEMM:\n      s = \"Implicit GEMM\";\n      break;\n  }\n  return s;\n}\n\nstring ToString(miopenConvBwdDataAlgorithm_t algorithm) {\n  string s;\n  switch (algorithm) {\n    case miopenConvolutionBwdDataAlgoGEMM:\n      s = \"GEMM\";\n      break;\n    case miopenConvolutionBwdDataAlgoDirect:\n      s = \"Direct\";\n      break;\n    case miopenConvolutionBwdDataAlgoFFT:\n      s = \"FFT\";\n      break;\n    case miopenConvolutionBwdDataAlgoWinograd:\n      s = \"Winograd\";\n      break;\n    case miopenTransposeBwdDataAlgoGEMM:\n      s = \"Transpose GEMM\";\n      break;\n    case miopenConvolutionBwdDataAlgoImplicitGEMM:\n      s = \"Implicit GEMM\";\n      break;\n  }\n  return s;\n}\n\nstring ToString(miopenConvAlgorithm_t algorithm) {\n  string s;\n  switch (algorithm) {\n    case miopenConvolutionAlgoGEMM:\n      s = \"GEMM\";\n      break;\n    case miopenConvolutionAlgoDirect:\n      s = \"Direct\";\n      break;\n    case miopenConvolutionAlgoFFT:\n      s = \"FFT\";\n      break;\n    case miopenConvolutionAlgoWinograd:\n      s = \"Winograd\";\n      break;\n    case miopenConvolutionAlgoImplicitGEMM:\n      s = \"Implicit GEMM\";\n      break;\n  }\n  return s;\n}\n\n// RAII wrapper for all calls to MIOpen with a MIOpen handle argument.\n//\n// See MIOpenAccess::GetHandle() for details.\nclass MIOpenHandle {\n public:\n  // Takes ownership of the executor context and the lock to access MIOpen\n  // using handle.\n  MIOpenHandle(gpu::ScopedActivateExecutorContext context,\n               std::unique_ptr<absl::MutexLock> lock, miopenHandle_t handle)\n      : context_(std::move(context)), lock_(std::move(lock)), handle_(handle) {}\n\n  // Returns MIOpen handle. To be passed directly to MIOpen APIs, don't keep\n  // a copy.\n  miopenHandle_t handle() const { return handle_; }\n\n private:\n  gpu::ScopedActivateExecutorContext context_;\n  std::unique_ptr<absl::MutexLock> lock_;\n  miopenHandle_t handle_;  // Not owned.\n};\n\nnamespace wrap {\n\n#ifdef PLATFORM_GOOGLE\n#define STREAM_EXECUTOR_MIOPEN_WRAP(__name)      \\\n  struct WrapperShim__##__name {                 \\\n    template <typename... Args>                  \\\n    miopenStatus_t operator()(Args... args) {    \\\n      miopenStatus_t retval = ::__name(args...); \\\n      return retval;                             \\\n    }                                            \\\n  } __name;\n\n#else\n\n#define STREAM_EXECUTOR_MIOPEN_WRAP(__name)                              \\\n  struct DynLoadShim__##__name {                                         \\\n    static const char* kName;                                            \\\n    using FuncPtrT = std::add_pointer<decltype(::__name)>::type;         \\\n    static void* GetDsoHandle() {                                        \\\n      auto s = internal::CachedDsoLoader::GetMiopenDsoHandle();          \\\n      return s.value();                                                  \\\n    }                                                                    \\\n    static FuncPtrT LoadOrDie() {                                        \\\n      void* f;                                                           \\\n      auto s = tsl::Env::Default()->GetSymbolFromLibrary(GetDsoHandle(), \\\n                                                         kName, &f);     \\\n      CHECK(s.ok()) << \"could not find \" << kName                        \\\n                    << \" in miopen DSO; dlerror: \" << s.error_message(); \\\n      return reinterpret_cast<FuncPtrT>(f);                              \\\n    }                                                                    \\\n    static FuncPtrT DynLoad() {                                          \\\n      static FuncPtrT f = LoadOrDie();                                   \\\n      return f;                                                          \\\n    }                                                                    \\\n    template <typename... Args>                                          \\\n    miopenStatus_t operator()(Args... args) {                            \\\n      return DynLoad()(args...);                                         \\\n    }                                                                    \\\n  } __name;                                                              \\\n  const char* DynLoadShim__##__name::kName = #__name;\n\n#endif\n\n#if (TF_ROCM_VERSION >= 50300)\n// clang-format off\n#define MIOPEN_DNN_ROUTINE_EACH(__macro)                             \\\n  __macro(miopenBatchNormalizationBackward)                          \\\n  __macro(miopenBatchNormalizationForwardInference)                  \\\n  __macro(miopenBatchNormalizationForwardTraining)                   \\\n  __macro(miopenGetConvolutionForwardOutputDim)                      \\\n  __macro(miopenGetConvolutionNdForwardOutputDim)                    \\\n  __macro(miopenFindConvolutionForwardAlgorithm)                     \\\n  __macro(miopenCreateTensorDescriptor)                              \\\n  __macro(miopenDestroyTensorDescriptor)                             \\\n  __macro(miopenSetNdPoolingDescriptor)                              \\\n  __macro(miopenSetPoolingIndexType)                                 \\\n  __macro(miopenSetLRNDescriptor)                                    \\\n  __macro(miopenLRNGetWorkSpaceSize)                                 \\\n  __macro(miopenCreateConvolutionDescriptor)                         \\\n  __macro(miopenCreatePoolingDescriptor)                             \\\n  __macro(miopenDestroyPoolingDescriptor)                            \\\n  __macro(miopenCreateLRNDescriptor)                                 \\\n  __macro(miopenDestroyLRNDescriptor)                                \\\n  __macro(miopenDestroyConvolutionDescriptor)                        \\\n  __macro(miopenCreateWithStream)                                    \\\n  __macro(miopenDestroy)                                             \\\n  __macro(miopenSetStream)                                           \\\n  __macro(miopenSetAllocator)                                        \\\n  __macro(miopenActivationForward)                                   \\\n  __macro(miopenConvolutionForward)                                  \\\n  __macro(miopenConvolutionBackwardBias)                             \\\n  __macro(miopenConvolutionForwardGetWorkSpaceSize)                  \\\n  __macro(miopenInitConvolutionDescriptor)                           \\\n  __macro(miopenInitConvolutionNdDescriptor)                         \\\n  __macro(miopenGetConvolutionDescriptor)                            \\\n  __macro(miopenGetConvolutionNdDescriptor)                          \\\n  __macro(miopenSetConvolutionGroupCount)                            \\\n  __macro(miopenSet4dTensorDescriptor)                               \\\n  __macro(miopenGetTensorDescriptor)                                 \\\n  __macro(miopenSetTensorDescriptor)                                 \\\n  __macro(miopenGetTensorDescriptorSize)                             \\\n  __macro(miopenPoolingForward)                                      \\\n  __macro(miopenPoolingGetWorkSpaceSizeV2)                           \\\n  __macro(miopenPoolingBackward)                                     \\\n  __macro(miopenLRNForward)                                          \\\n  __macro(miopenLRNBackward)                                         \\\n  __macro(miopenOpTensor)                                            \\\n  __macro(miopenConvolutionBackwardData)                             \\\n  __macro(miopenConvolutionBackwardWeights)                          \\\n  __macro(miopenConvolutionBackwardWeightsGetWorkSpaceSize)          \\\n  __macro(miopenFindConvolutionBackwardDataAlgorithm)                \\\n  __macro(miopenFindConvolutionBackwardWeightsAlgorithm)             \\\n  __macro(miopenConvolutionBackwardDataGetWorkSpaceSize)             \\\n  __macro(miopenCreateRNNDescriptor)                                 \\\n  __macro(miopenSetRNNDescriptor)                                    \\\n  __macro(miopenDestroyRNNDescriptor)                                \\\n  __macro(miopenGetRNNParamsSize)                                    \\\n  __macro(miopenGetRNNLayerParam)                                    \\\n  __macro(miopenGetRNNLayerBias)                                     \\\n  __macro(miopenGetRNNWorkspaceSize)                                 \\\n  __macro(miopenGetRNNTrainingReserveSize)                           \\\n  __macro(miopenRNNForwardInference)                                 \\\n  __macro(miopenRNNForwardTraining)                                  \\\n  __macro(miopenRNNBackwardData)                                     \\\n  __macro(miopenRNNBackwardWeights)                                  \\\n  __macro(miopenGetRNNLayerParamOffset)                              \\\n  __macro(miopenGetRNNLayerParamSize)                                \\\n  __macro(miopenGetRNNLayerBiasOffset)                               \\\n  __macro(miopenGetRNNLayerBiasSize)                                 \\\n  __macro(miopenGetRNNParamsDescriptor)                              \\\n  __macro(miopenCreateActivationDescriptor)                          \\\n  __macro(miopenSetActivationDescriptor)                             \\\n  __macro(miopenGetActivationDescriptor)                             \\\n  __macro(miopenDestroyActivationDescriptor)                         \\\n  __macro(miopenCreateFusionPlan)                                    \\\n  __macro(miopenCreateOpConvForward)                                 \\\n  __macro(miopenCreateOpBiasForward)                                 \\\n  __macro(miopenCreateOpActivationForward)                           \\\n  __macro(miopenCreateOpActivationBackward)                          \\\n  __macro(miopenCreateOpBatchNormInference)                          \\\n  __macro(miopenCreateOpBatchNormForward)                            \\\n  __macro(miopenCreateOpBatchNormBackward)                           \\\n  __macro(miopenCompileFusionPlan)                                   \\\n  __macro(miopenFusionPlanGetOp)                                     \\\n  __macro(miopenCreateOperatorArgs)                                  \\\n  __macro(miopenSetOpArgsConvForward)                                \\\n  __macro(miopenSetOpArgsBiasForward)                                \\\n  __macro(miopenSetOpArgsActivForward)                               \\\n  __macro(miopenSetOpArgsActivBackward)                              \\\n  __macro(miopenSetOpArgsBatchNormInference)                         \\\n  __macro(miopenSetOpArgsBatchNormForward)                           \\\n  __macro(miopenSetOpArgsBatchNormBackward)                          \\\n  __macro(miopenExecuteFusionPlan)                                   \\\n  __macro(miopenDestroyOperatorArgs)                                 \\\n  __macro(miopenDestroyFusionPlan)                                   \\\n  __macro(miopenConvolutionForwardGetSolutionCount)                  \\\n  __macro(miopenConvolutionForwardGetSolution)                       \\\n  __macro(miopenConvolutionForwardGetSolutionWorkspaceSize)          \\\n  __macro(miopenConvolutionForwardCompileSolution)                   \\\n  __macro(miopenConvolutionForwardImmediate)                         \\\n  __macro(miopenConvolutionBackwardDataGetSolutionCount)             \\\n  __macro(miopenConvolutionBackwardDataGetSolution)                  \\\n  __macro(miopenConvolutionBackwardDataGetSolutionWorkspaceSize)     \\\n  __macro(miopenConvolutionBackwardDataCompileSolution)              \\\n  __macro(miopenConvolutionBackwardDataImmediate)                    \\\n  __macro(miopenConvolutionBackwardWeightsGetSolutionCount)          \\\n  __macro(miopenConvolutionBackwardWeightsGetSolution)               \\\n  __macro(miopenConvolutionBackwardWeightsGetSolutionWorkspaceSize)  \\\n  __macro(miopenConvolutionBackwardWeightsCompileSolution)           \\\n  __macro(miopenConvolutionBackwardWeightsImmediate)                 \\\n  __macro(miopenCreateCTCLossDescriptor)                             \\\n  __macro(miopenSetCTCLossDescriptor)                                \\\n  __macro(miopenGetCTCLossWorkspaceSize)                             \\\n  __macro(miopenCTCLoss)                                             \\\n  __macro(miopenDestroyCTCLossDescriptor)                            \\\n  __macro(miopenSetConvolutionAttribute)  // clang-format on\n#else\n// clang-format off\n#define MIOPEN_DNN_ROUTINE_EACH(__macro)                             \\\n  __macro(miopenBatchNormalizationBackward)                          \\\n  __macro(miopenBatchNormalizationForwardInference)                  \\\n  __macro(miopenBatchNormalizationForwardTraining)                   \\\n  __macro(miopenGetConvolutionForwardOutputDim)                      \\\n  __macro(miopenGetConvolutionNdForwardOutputDim)                    \\\n  __macro(miopenFindConvolutionForwardAlgorithm)                     \\\n  __macro(miopenCreateTensorDescriptor)                              \\\n  __macro(miopenDestroyTensorDescriptor)                             \\\n  __macro(miopenSetNdPoolingDescriptor)                              \\\n  __macro(miopenSetPoolingIndexType)                                 \\\n  __macro(miopenSetLRNDescriptor)                                    \\\n  __macro(miopenLRNGetWorkSpaceSize)                                 \\\n  __macro(miopenCreateConvolutionDescriptor)                         \\\n  __macro(miopenCreatePoolingDescriptor)                             \\\n  __macro(miopenDestroyPoolingDescriptor)                            \\\n  __macro(miopenCreateLRNDescriptor)                                 \\\n  __macro(miopenDestroyLRNDescriptor)                                \\\n  __macro(miopenDestroyConvolutionDescriptor)                        \\\n  __macro(miopenCreateWithStream)                                    \\\n  __macro(miopenDestroy)                                             \\\n  __macro(miopenSetStream)                                           \\\n  __macro(miopenSetAllocator)                                        \\\n  __macro(miopenActivationForward)                                   \\\n  __macro(miopenConvolutionForward)                                  \\\n  __macro(miopenConvolutionBackwardBias)                             \\\n  __macro(miopenConvolutionForwardGetWorkSpaceSize)                  \\\n  __macro(miopenInitConvolutionDescriptor)                           \\\n  __macro(miopenInitConvolutionNdDescriptor)                         \\\n  __macro(miopenGetConvolutionDescriptor)                            \\\n  __macro(miopenGetConvolutionNdDescriptor)                          \\\n  __macro(miopenSetConvolutionGroupCount)                            \\\n  __macro(miopenSet4dTensorDescriptor)                               \\\n  __macro(miopenGetTensorDescriptor)                                 \\\n  __macro(miopenSetTensorDescriptor)                                 \\\n  __macro(miopenGetTensorDescriptorSize)                             \\\n  __macro(miopenPoolingForward)                                      \\\n  __macro(miopenPoolingGetWorkSpaceSizeV2)                           \\\n  __macro(miopenPoolingBackward)                                     \\\n  __macro(miopenLRNForward)                                          \\\n  __macro(miopenLRNBackward)                                         \\\n  __macro(miopenOpTensor)                                            \\\n  __macro(miopenConvolutionBackwardData)                             \\\n  __macro(miopenConvolutionBackwardWeights)                          \\\n  __macro(miopenConvolutionBackwardWeightsGetWorkSpaceSize)          \\\n  __macro(miopenFindConvolutionBackwardDataAlgorithm)                \\\n  __macro(miopenFindConvolutionBackwardWeightsAlgorithm)             \\\n  __macro(miopenConvolutionBackwardDataGetWorkSpaceSize)             \\\n  __macro(miopenCreateRNNDescriptor)                                 \\\n  __macro(miopenSetRNNDescriptor)                                    \\\n  __macro(miopenDestroyRNNDescriptor)                                \\\n  __macro(miopenGetRNNParamsSize)                                    \\\n  __macro(miopenGetRNNLayerParam)                                    \\\n  __macro(miopenGetRNNLayerBias)                                     \\\n  __macro(miopenGetRNNWorkspaceSize)                                 \\\n  __macro(miopenGetRNNTrainingReserveSize)                           \\\n  __macro(miopenRNNForwardInference)                                 \\\n  __macro(miopenRNNForwardTraining)                                  \\\n  __macro(miopenRNNBackwardData)                                     \\\n  __macro(miopenRNNBackwardWeights)                                  \\\n  __macro(miopenGetRNNLayerParamOffset)                              \\\n  __macro(miopenGetRNNLayerParamSize)                                \\\n  __macro(miopenGetRNNLayerBiasOffset)                               \\\n  __macro(miopenGetRNNLayerBiasSize)                                 \\\n  __macro(miopenGetRNNParamsDescriptor)                              \\\n  __macro(miopenCreateActivationDescriptor)                          \\\n  __macro(miopenSetActivationDescriptor)                             \\\n  __macro(miopenGetActivationDescriptor)                             \\\n  __macro(miopenDestroyActivationDescriptor)                         \\\n  __macro(miopenCreateFusionPlan)                                    \\\n  __macro(miopenCreateOpConvForward)                                 \\\n  __macro(miopenCreateOpBiasForward)                                 \\\n  __macro(miopenCreateOpActivationForward)                           \\\n  __macro(miopenCreateOpActivationBackward)                          \\\n  __macro(miopenCreateOpBatchNormInference)                          \\\n  __macro(miopenCreateOpBatchNormForward)                            \\\n  __macro(miopenCreateOpBatchNormBackward)                           \\\n  __macro(miopenCompileFusionPlan)                                   \\\n  __macro(miopenFusionPlanGetOp)                                     \\\n  __macro(miopenCreateOperatorArgs)                                  \\\n  __macro(miopenSetOpArgsConvForward)                                \\\n  __macro(miopenSetOpArgsBiasForward)                                \\\n  __macro(miopenSetOpArgsActivForward)                               \\\n  __macro(miopenSetOpArgsActivBackward)                              \\\n  __macro(miopenSetOpArgsBatchNormInference)                         \\\n  __macro(miopenSetOpArgsBatchNormForward)                           \\\n  __macro(miopenSetOpArgsBatchNormBackward)                          \\\n  __macro(miopenExecuteFusionPlan)                                   \\\n  __macro(miopenDestroyOperatorArgs)                                 \\\n  __macro(miopenDestroyFusionPlan)                                   \\\n  __macro(miopenConvolutionForwardGetSolutionCount)                  \\\n  __macro(miopenConvolutionForwardGetSolution)                       \\\n  __macro(miopenConvolutionForwardGetSolutionWorkspaceSize)          \\\n  __macro(miopenConvolutionForwardCompileSolution)                   \\\n  __macro(miopenConvolutionForwardImmediate)                         \\\n  __macro(miopenConvolutionBackwardDataGetSolutionCount)             \\\n  __macro(miopenConvolutionBackwardDataGetSolution)                  \\\n  __macro(miopenConvolutionBackwardDataGetSolutionWorkspaceSize)     \\\n  __macro(miopenConvolutionBackwardDataCompileSolution)              \\\n  __macro(miopenConvolutionBackwardDataImmediate)                    \\\n  __macro(miopenConvolutionBackwardWeightsGetSolutionCount)          \\\n  __macro(miopenConvolutionBackwardWeightsGetSolution)               \\\n  __macro(miopenConvolutionBackwardWeightsGetSolutionWorkspaceSize)  \\\n  __macro(miopenConvolutionBackwardWeightsCompileSolution)           \\\n  __macro(miopenConvolutionBackwardWeightsImmediate)                 \\\n  __macro(miopenCreateCTCLossDescriptor)                             \\\n  __macro(miopenSetCTCLossDescriptor)                                \\\n  __macro(miopenGetCTCLossWorkspaceSize)                             \\\n  __macro(miopenCTCLoss)                                             \\\n  __macro(miopenDestroyCTCLossDescriptor)\n// clang-format on\n#endif\n\nMIOPEN_DNN_ROUTINE_EACH(STREAM_EXECUTOR_MIOPEN_WRAP)\n\n#undef MIOPEN_DNN_ROUTINE_EACH\n\n}  // namespace wrap\n\nnamespace {\n\n// These routines should ideally be provided as an MIOpen API.\n// They are called for *every* _ROCMmFusedOp*::Compute call, and they need to be\n// efficient! Instead of calculating the hash value by quering the MIOpen Get*\n// APIs for the descriptor components, it would be a lot more efficient if,\n// MIOpen calculated the hash value when creating the descriptor, stored it on\n// the descriptor datastructure, and provided an API routine to query it.\n\nconst int kMaxMIOpenTensorSize = 5;\n\nuint64_t GetHashValue(miopenTensorDescriptor_t tensor_desc) {\n  miopenDataType_t datatype = miopenFloat;\n  int dims[kMaxMIOpenTensorSize] = {0};\n  int strides[kMaxMIOpenTensorSize] = {0};\n  wrap::miopenGetTensorDescriptor(tensor_desc, &datatype, dims, strides);\n\n  uint64_t hash_value = tsl::hash<int>()(datatype);\n  for (int dim : dims)\n    hash_value = tsl::Hash64Combine(hash_value, tsl::hash<int>()(dim));\n  for (int stride : strides)\n    hash_value = tsl::Hash64Combine(hash_value, tsl::hash<int>()(stride));\n\n  return hash_value;\n}\n\nuint64_t GetHashValue(miopenConvolutionDescriptor_t conv_desc) {\n  miopenConvolutionMode_t c_mode = miopenConvolution;\n  int nd = 0;\n  wrap::miopenGetConvolutionNdDescriptor(conv_desc, 0, &nd, nullptr, nullptr,\n                                         nullptr, &c_mode);\n\n  std::vector<int> stride(nd);\n  std::vector<int> pad(nd);\n  std::vector<int> dilation(nd);\n\n  wrap::miopenGetConvolutionNdDescriptor(\n      conv_desc, nd, &nd, pad.data(), stride.data(), dilation.data(), &c_mode);\n\n  uint64_t hash_value = tsl::hash<int>()(c_mode);\n  auto hash64Combine = [&hash_value](int element) {\n    tsl::Hash64Combine(hash_value, tsl::hash<int>()(element));\n  };\n  std::for_each(pad.begin(), pad.end(), hash64Combine);\n  std::for_each(stride.begin(), stride.end(), hash64Combine);\n  std::for_each(dilation.begin(), dilation.end(), hash64Combine);\n\n  return hash_value;\n}\n\nbool RequireMIOpenDeterminism() { return tsl::OpDeterminismRequired(); }\n\n// Class to implement a cache of compiled fusion plans\nclass CachedFusionPlans {\n public:\n  // Check if we already have a fusion_plan corresponding to the given hash\n  // value.\n  // If we do, then\n  //   return true (+ the cached fusion plan via given pointer)\n  // Else\n  //   create a new fusion plan descriptor,\n  //   associate it with the given hash value in the cache\n  //   return false (+ newly created fusion plan via given pointer)\n  static bool FindOrCreate(uint64_t hash,\n                           miopenFusionPlanDescriptor_t* fusion_plan,\n                           miopenFusionDirection_t fusion_direction,\n                           miopenTensorDescriptor_t input_descriptor) {\n    absl::MutexLock lock{&cached_plans_mutex};\n\n    bool found_cached_plan = false;\n\n    auto it = cached_plans.find(hash);\n    if (it != cached_plans.end()) {\n      *fusion_plan = it->second;\n      found_cached_plan = true;\n    } else {\n      auto status = wrap::miopenCreateFusionPlan(fusion_plan, fusion_direction,\n                                                 input_descriptor);\n      if (status != miopenStatusSuccess) {\n        LOG(FATAL) << \"call to miopenCreateFusionPlan failed: \"\n                   << ToString(status);\n      } else {\n        cached_plans[hash] = *fusion_plan;\n      }\n    }\n\n    return found_cached_plan;\n  }\n\n  // Need to figure out the right place to call this routine\n  static void Clear() {\n    absl::MutexLock lock{&cached_plans_mutex};\n\n    for (auto it : cached_plans) {\n      auto status = wrap::miopenDestroyFusionPlan(it.second);\n      if (status != miopenStatusSuccess) {\n        LOG(FATAL) << \"call to miopenDestroyFusionPlan failed: \"\n                   << ToString(status);\n      }\n    }\n\n    cached_plans.clear();\n\n    unsupported_plans.clear();\n  }\n\n  // Is the Fusion plan corresponding to this hash unsupported\n  static bool IsUnsupportedFusionPlan(uint64_t hash) {\n    absl::MutexLock lock{&cached_plans_mutex};\n    return unsupported_plans.count(hash) > 0;\n  }\n\n  // Mark the given hash value as corresponding to an unsupported fusion plan\n  static void MarkFusionPlanUnsupported(uint64_t hash) {\n    absl::MutexLock lock{&cached_plans_mutex};\n    unsupported_plans.insert(hash);\n  }\n\n private:\n  // Mutex to guard access to all data within this class\n  static absl::Mutex cached_plans_mutex;\n\n  // Map of hash-value to MIOpen Fusion plan descriptors\n  // Need to be able share this across more than one stream and hence static\n  static std::map<uint64_t, miopenFusionPlanDescriptor_t> cached_plans;\n\n  // Set of hash-values that correspond to MIOpen Fusion plans that will fail\n  // compile and hence are not supported.\n  static std::set<uint64_t> unsupported_plans;\n};\n\nabsl::Mutex CachedFusionPlans::cached_plans_mutex;\nstd::map<uint64_t, miopenFusionPlanDescriptor_t>\n    CachedFusionPlans::cached_plans;\nstd::set<uint64_t> CachedFusionPlans::unsupported_plans;\n\ndnn::ProfileResult GetProfileResultFromConvSolution(\n    miopenConvSolution_t solution) {\n  dnn::ProfileResult profile_result;\n  profile_result.set_algorithm(\n      {solution.solution_id, false, solution.workspace_size});\n  profile_result.set_elapsed_time_in_ms(solution.time);\n  profile_result.set_scratch_size(solution.workspace_size);\n  return profile_result;\n}\n\ndnn::ProfileResult GetProfileResultFromConvAlgoPerf(\n    dnn::ConvolutionKind kind, miopenConvAlgoPerf_t algorithm) {\n  int64_t algo_id;\n  switch (kind) {\n    case dnn::ConvolutionKind::FORWARD:\n      algo_id = algorithm.fwd_algo;\n      break;\n    case dnn::ConvolutionKind::BACKWARD_DATA:\n      algo_id = algorithm.bwd_data_algo;\n      break;\n    case dnn::ConvolutionKind::BACKWARD_FILTER:\n      algo_id = algorithm.bwd_weights_algo;\n      break;\n    default:\n      LOG(FATAL) << \"Unexpected convolution kind \" << static_cast<int>(kind);\n      break;\n  }\n\n  dnn::ProfileResult profile_result;\n  profile_result.set_algorithm({algo_id, false, algorithm.memory});\n  profile_result.set_elapsed_time_in_ms(algorithm.time);\n  profile_result.set_scratch_size(algorithm.memory);\n  return profile_result;\n}\n}  // namespace\n\n// Wraps a MIOpen handle and provides access to it through miopenHandle_t\n// instances, which also locks a mutex, acquires the ROCm context, and sets\n// the stream that MIOpen should use to enqueue any work.\n//\n// Note: MIOpenSupport::miopen_ should be the only instantiation of this class.\nclass MIOpenAccess {\n public:\n  // Takes ownership of the handle.\n  explicit MIOpenAccess(miopenHandle_t handle) : handle_(handle) {}\n\n  ~MIOpenAccess() {\n    absl::MutexLock lock(&mutex_);\n    wrap::miopenDestroy(handle_);\n  }\n\n  // Creates a MIOpenHandle instance for stream.\n  //\n  // MIOpen API calls using the same handle instance need to be serialized\n  // across threads. This is guaranteed by MIOpenHandle instances locking the\n  // mutex owned by this class.\n  //\n  // Most MIOpen APIs taking a handle perform work on a HIP stream. The\n  // MIOpenHandle instance acquires the executor's ROCm context and sets MIOpen\n  // to use the provided stream.\n  //\n  // The stream argument may be null, which translates to the null stream.\n  // The null stream synchronizes with all other streams and it is\n  // therefore a bad idea (performance wise) to call any MIOpen APIs that\n  // enqueue work in the stream.\n  MIOpenHandle GetHandle(GpuExecutor* executor, Stream* stream) {\n    auto lock = std::make_unique<absl::MutexLock>(&mutex_);\n    mutex_.AssertHeld();\n    gpu::ScopedActivateExecutorContext context(executor);\n    hipStream_t hip_stream = stream ? AsGpuStreamValue(stream) : nullptr;\n    auto status = wrap::miopenSetStream(handle_, hip_stream);\n    CHECK_EQ(status, miopenStatusSuccess) << \"Failed to set MIOpen stream.\";\n    return MIOpenHandle(std::move(context), std::move(lock), handle_);\n  }\n\n private:\n  // Guards the enqueueing of MIOpen operations via the handle_ below.\n  absl::Mutex mutex_;\n\n  // MIOpen library handle.\n  miopenHandle_t handle_ ABSL_GUARDED_BY(mutex_);  // Owned.\n};\n\nMIOpenSupport::MIOpenSupport(GpuExecutor* parent) : parent_(parent) {\n  // by default, the Get*Algorithm API will return the list of all applicable\n  // algorithms\n  return_best_algo_only_ = false;\n  // but if the env var TF_ROCM_RETURN_BEST_ALGO_ONLY is set, only the best\n  // (i.e. most efficient) algorithm will be returned\n  TF_CHECK_OK(tsl::ReadBoolFromEnvVar(\"TF_ROCM_RETURN_BEST_ALGO_ONLY\", false,\n                                      &return_best_algo_only_));\n\n  // by default, use Find Mode APIs for convolution\n  use_immediate_mode_ = false;\n  // swich to Find Mode if env var TF_ROCM_USE_IMMEDIATE_MODE is set\n\n  TF_CHECK_OK(tsl::ReadBoolFromEnvVar(\"TF_ROCM_USE_IMMEDIATE_MODE\", false,\n                                      &use_immediate_mode_));\n\n  bool enable_pooling_cache = false;\n  TF_CHECK_OK(tsl::ReadBoolFromEnvVar(\"TF_ROCM_BW_POOL_CACHE\", false,\n                                      &enable_pooling_cache));\n  if (enable_pooling_cache) m_pooling_cache_allowed = true;\n}\n\ntsl::Status MIOpenSupport::Init() {\n  ScopedActivateExecutorContext context(parent_);\n  miopenHandle_t miopen_handle = nullptr;\n  auto status = wrap::miopenCreateWithStream(\n      reinterpret_cast<miopenHandle_t*>(&miopen_handle), (hipStream_t)(0));\n  if (status == miopenStatusSuccess) {\n    miopen_.reset(new MIOpenAccess(miopen_handle));\n    return tsl::OkStatus();\n  }\n\n  CHECK_EQ(miopen_handle, nullptr);\n  LOG(ERROR) << \"could not create miopen handle: \" << ToString(status);\n  if (status == miopenStatusNotInitialized) {\n    auto result = rocm::Diagnostician::FindKernelDriverVersion();\n    if (!result.ok()) {\n      LOG(ERROR) << \"error retrieving driver version: \"\n                 << rocm::DriverVersionStatusToString(result);\n    } else {\n      const auto& version = result.value();\n      LOG(INFO) << \"possibly insufficient driver version: \"\n                << rocm::DriverVersionToString(version);\n    }\n  }\n\n  return tsl::Status{tsl::error::INTERNAL,\n                     absl::StrCat(\"miopen library could not create a handle: \",\n                                  ToString(status))};\n}\n\ntsl::StatusOr<perftools::gputools::dnn::VersionInfo>\nMIOpenSupport::GetVersion() {\n  // ROCM TODO: retrieve MIOpen version with its API\n  return perftools::gputools::dnn::VersionInfo(1, 3, 0);\n}\n\n// Turns a BatchDescriptor structure into a miopen tensor handle within a scope.\nclass ScopedTensorDescriptor {\n public:\n  ScopedTensorDescriptor(const BatchDescriptor& batch_descriptor,\n                         miopenDataType_t elem_type)\n      : handle_(nullptr) {\n    auto status = wrap::miopenCreateTensorDescriptor(&handle_);\n    if (status != miopenStatusSuccess) {\n      LOG(FATAL) << \"could not create miopen tensor descriptor: \"\n                 << ToString(status);\n    }\n\n    switch (batch_descriptor.layout()) {\n      case dnn::DataLayout::kBatchYXDepth:\n      case dnn::DataLayout::kBatchDepthYX: {\n        const int nd = batch_descriptor.ndims() + 2;\n\n        // MIOpen requires the strides and dims to be ordered as BDYX.\n        std::vector<int64_t> strides64 =\n            batch_descriptor.full_strides(dnn::DataLayout::kBatchDepthYX);\n        std::vector<int64_t> dims64 =\n            batch_descriptor.full_dims(dnn::DataLayout::kBatchDepthYX);\n\n        // MIOpen requires arrays of ints.\n        std::vector<int> strides(nd);\n        std::vector<int> dims(nd);\n        std::transform(strides64.cbegin(), strides64.cend(), strides.begin(),\n                       &CheckedNarrowing<int64_t, int>);\n        std::transform(dims64.cbegin(), dims64.cend(), dims.begin(),\n                       &CheckedNarrowing<int64_t, int>);\n        status = wrap::miopenSetTensorDescriptor(handle_, elem_type, nd,\n                                                 dims.data(), strides.data());\n\n        if (status != miopenStatusSuccess) {\n          LOG(FATAL) << \"could not convert BatchDescriptor \"\n                     << batch_descriptor.ToString()\n                     << \" to miopen tensor descriptor: \" << ToString(status);\n        }\n      } break;\n      default:\n        LOG(FATAL) << \"Unsupported tensor format \"\n                   << DataLayoutString(batch_descriptor.layout());\n        break;\n    }\n  }\n\n  ~ScopedTensorDescriptor() {\n    auto status = wrap::miopenDestroyTensorDescriptor(handle_);\n    if (status != miopenStatusSuccess) {\n      LOG(ERROR) << \"could not destroy miopen tensor descriptor: \"\n                 << ToString(status);\n    }\n  }\n\n  miopenTensorDescriptor_t handle() const { return handle_; }\n\n private:\n  miopenTensorDescriptor_t handle_;  // Owned.\n\n  SE_DISALLOW_COPY_AND_ASSIGN(ScopedTensorDescriptor);\n};\n\n// Turns a FilterDescriptor structure into a miopen filter handle within a\n// scope.\nclass ScopedFilterDescriptor {\n public:\n  ScopedFilterDescriptor(const FilterDescriptor& filter_descriptor,\n                         miopenDataType_t elem_type)\n      : handle_(nullptr) {\n    auto status = wrap::miopenCreateTensorDescriptor(&handle_);\n    if (status != miopenStatusSuccess) {\n      LOG(FATAL) << \"could not create miopen filter descriptor: \"\n                 << ToString(status);\n    }\n\n    // We need to pass two vectors to the miopenSetTensorDescriptor routine\n    // \"dims\" (length == number of dims, elem value == dimension size)\n    // \"strides\" (length == number of dims, elem value == stride size)\n    //\n    // Irrespective of the actual filter layout, the indexing of both those\n    // vectors must be the following (coz that is what MIOpen expects)\n    // dims[0] = strides[0] = N or output\n    // dims[1] = strides[1] = C or input\n    // dims[2] = strides[2] = H or spatial dim 0\n    // dims[3] = strides[3] = W or spatial dim 1\n    //\n    // assume you have a tensor with dimensions\n    // batch descriptor name    filter descriptor name    value\n    //   N (batch size)            O (output features)    256\n    //   C (channels)              I (input features)       3\n    //   H (height)                H (height)               7\n    //   W (width)                 W (width)                5\n    //\n    // The content of \"dims\" will be the same irrespective of layout\n    // layout (NCHW or NHWC), and MIOpen expects it should be\n    //                           NCHW layout   NHWC layout\n    // dims[0] = size of N dim =    256           256\n    // dims[1] = size of C dim =      3             3\n    // dims[2] = size of H dim =      7             7\n    // dims[3] = size of W dim =      5             5\n    //\n    // The content of \"strides\" will be different based on layout\n    //                                  NCHW layout   NHWC layout\n    //  strides[0] = stride of N dim =     7x5x3       7x5x3\n    //  strides[1] = stride of C dim =     7x5         1\n    //  strides[2] = stride of H dim =     5           5x3\n    //  strides[3] = stride of W dim =     1           3\n\n    switch (filter_descriptor.layout()) {\n      case dnn::FilterLayout::kOutputYXInput:\n      case dnn::FilterLayout::kOutputInputYX: {\n        const int nd = filter_descriptor.ndims() + 2;\n\n        // MIOpen requires the strides and dims to be ordered as BDYX.\n        std::vector<int64_t> strides64 =\n            filter_descriptor.full_strides(dnn::FilterLayout::kOutputInputYX);\n        std::vector<int64_t> dims64 =\n            filter_descriptor.full_dims(dnn::FilterLayout::kOutputInputYX);\n\n        // MIOpen requires arrays of ints.\n        std::vector<int> strides;\n        std::vector<int> dims;\n        absl::c_transform(strides64, std::back_inserter(strides),\n                          &CheckedNarrowing<int64_t, int>);\n        absl::c_transform(dims64, std::back_inserter(dims),\n                          &CheckedNarrowing<int64_t, int>);\n        status = wrap::miopenSetTensorDescriptor(handle_, elem_type, nd,\n                                                 dims.data(), strides.data());\n\n        if (status != miopenStatusSuccess) {\n          LOG(FATAL) << \"could not convert FilterDescriptor \"\n                     << filter_descriptor.ToString()\n                     << \" to miopen tensor descriptor: \" << ToString(status);\n        }\n      } break;\n      default:\n        LOG(FATAL) << \"Unsupported tensor format \"\n                   << FilterLayoutString(filter_descriptor.layout());\n        break;\n    }\n  }\n\n  ~ScopedFilterDescriptor() {\n    auto status = wrap::miopenDestroyTensorDescriptor(handle_);\n    if (status != miopenStatusSuccess) {\n      LOG(ERROR) << \"could not destroy miopen filter descriptor: \"\n                 << ToString(status);\n    }\n  }\n\n  miopenTensorDescriptor_t handle() const { return handle_; }\n\n private:\n  // miopen filter descriptor this object creates. Owned.\n  miopenTensorDescriptor_t handle_;\n\n  SE_DISALLOW_COPY_AND_ASSIGN(ScopedFilterDescriptor);\n};\n\n// Turns a ConvolutionDescriptor structure into a miopen convolution handle\n// within a scope.\nclass ScopedConvolutionDescriptor {\n public:\n  ScopedConvolutionDescriptor(\n      const ConvolutionDescriptor& convolution_descriptor,\n      miopenDataType_t data_type)\n      : handle_(nullptr) {\n    auto status = wrap::miopenCreateConvolutionDescriptor(&handle_);\n    if (status != miopenStatusSuccess) {\n      LOG(FATAL) << \"could not create miopen convolution descriptor: \"\n                 << ToString(status);\n    }\n    const auto& strides64 = convolution_descriptor.strides();\n    const auto& padding64 = convolution_descriptor.padding();\n    if (convolution_descriptor.pad_alignment() ==\n        dnn::PadAlignment::kTensorFlowPadding) {\n      LOG(ERROR) << \"TensorFlow padding alignment is not supported.\";\n    }\n\n    // MIOpen requires arrays of ints.\n    std::vector<int> strides(convolution_descriptor.ndims());\n    std::vector<int> padding(convolution_descriptor.ndims());\n    std::transform(strides64.cbegin(), strides64.cend(), strides.begin(),\n                   &CheckedNarrowing<int64_t, int>);\n    std::transform(padding64.cbegin(), padding64.cend(), padding.begin(),\n                   &CheckedNarrowing<int64_t, int>);\n\n    std::vector<int> upscale(convolution_descriptor.ndims());\n    const auto& dilations64 = convolution_descriptor.dilations();\n    std::transform(dilations64.cbegin(), dilations64.cend(), upscale.begin(),\n                   &CheckedNarrowing<int64_t, int>);\n\n    status = wrap::miopenInitConvolutionNdDescriptor(\n        handle_, convolution_descriptor.ndims(), padding.data(), strides.data(),\n        upscale.data(), miopenConvolution);\n    if (status != miopenStatusSuccess) {\n      LOG(FATAL) << \"could not set miopen convolution descriptor: \"\n                 << ToString(status);\n    }\n\n    VLOG(2) << \"Requesting grouped convolution: \"\n            << convolution_descriptor.group_count();\n    status = wrap::miopenSetConvolutionGroupCount(\n        handle_, convolution_descriptor.group_count());\n    if (status != miopenStatusSuccess) {\n      LOG(FATAL) << \"could not set miopen convolution group count: \"\n                 << ToString(status);\n    }\n\n#if (TF_ROCM_VERSION >= 50300)\n    if (RequireMIOpenDeterminism()) {\n      status = wrap::miopenSetConvolutionAttribute(\n          handle_, MIOPEN_CONVOLUTION_ATTRIB_DETERMINISTIC, 1);\n      if (status != miopenStatusSuccess) {\n        LOG(FATAL) << \"could not set miopen convolution attribute: \"\n                   << ToString(status);\n      }\n    }\n#endif\n  }\n  ~ScopedConvolutionDescriptor() {\n    auto status = wrap::miopenDestroyConvolutionDescriptor(handle_);\n    if (status != miopenStatusSuccess) {\n      LOG(ERROR) << \"could not destroy miopen convolution descriptor: \"\n                 << ToString(status);\n    }\n  }\n\n  miopenConvolutionDescriptor_t handle() const { return handle_; }\n\n private:\n  miopenConvolutionDescriptor_t handle_;  // Owned.\n\n  SE_DISALLOW_COPY_AND_ASSIGN(ScopedConvolutionDescriptor);\n};\n\n// Turns a PoolingDescriptor structure into a miopen pooling descriptor handle\n// within a scope.\nclass ScopedPoolingDescriptor {\n public:\n  ScopedPoolingDescriptor(const PoolingDescriptor& pooling_descriptor)\n      : handle_(nullptr) {\n    auto status = wrap::miopenCreatePoolingDescriptor(&handle_);\n    if (status != miopenStatusSuccess) {\n      LOG(FATAL) << \"could not create miopen pooling descriptor: \"\n                 << ToString(status);\n    }\n\n    absl::Span<const int64_t> strides64 = pooling_descriptor.strides();\n    absl::Span<const int64_t> padding64 = pooling_descriptor.padding();\n    absl::Span<const int64_t> shape64 = pooling_descriptor.window();\n\n    const int nd = pooling_descriptor.ndims();\n    std::vector<int> shape(nd);\n    std::vector<int> padding(nd);\n    std::vector<int> strides(nd);\n    std::transform(strides64.cbegin(), strides64.cend(), strides.begin(),\n                   &CheckedNarrowing<int64_t, int>);\n    std::transform(padding64.cbegin(), padding64.cend(), padding.begin(),\n                   &CheckedNarrowing<int64_t, int>);\n    std::transform(shape64.cbegin(), shape64.cend(), shape.begin(),\n                   &CheckedNarrowing<int64_t, int>);\n\n    status = wrap::miopenSetNdPoolingDescriptor(\n        handle_,\n        (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum\n             ? miopenPoolingMax\n             : miopenPoolingAverage),\n        nd, shape.data(), padding.data(), strides.data());\n\n    // Note: The index type has to be uint32 type for now because MIOpen\n    // API assumes all input indexes to be the same type. Since a tensor\n    // descriptor can only use int32 type, the index type here need to be\n    // aligned with the tensor index type of the (input) tensor descritptor\n    status = wrap::miopenSetPoolingIndexType(handle_, miopenIndexUint32);\n\n    if (status != miopenStatusSuccess) {\n      LOG(FATAL) << \"could not set miopen pooling descriptor: \"\n                 << ToString(status);\n    }\n  }\n  ~ScopedPoolingDescriptor() {\n    auto status = wrap::miopenDestroyPoolingDescriptor(handle_);\n    if (status != miopenStatusSuccess) {\n      LOG(ERROR) << \"could not destroy miopen pooling descriptor: \"\n                 << ToString(status);\n    }\n  }\n\n  miopenPoolingDescriptor_t handle() const { return handle_; }\n\n private:\n  miopenPoolingDescriptor_t handle_;  // Owned.\n\n  SE_DISALLOW_COPY_AND_ASSIGN(ScopedPoolingDescriptor);\n};\n\n// Turns a NormalizeDescriptor structure into a miopen LRN descriptor handle.\nclass ScopedNormalizeDescriptor {\n public:\n  ScopedNormalizeDescriptor(const NormalizeDescriptor& normalize_descriptor)\n      : handle_(nullptr) {\n    auto status = wrap::miopenCreateLRNDescriptor(&handle_);\n    if (status != miopenStatusSuccess) {\n      LOG(FATAL) << \"could not create miopen LRN descriptor: \"\n                 << ToString(status);\n    }\n\n    // The range specifies that the indices in the closed range\n    // [i - range, i + range] should be included in the normalization for index\n    // i. The lrnN value is the total number of elements in the range, so\n    // lrnN = 2*range + 1.\n    unsigned lrn_N = 2 * normalize_descriptor.range() + 1;\n\n    // Note that SE defines the normalization operation as\n    //\n    //  U_i = V_i / ((bias +  alpha      * (sum_j V_j^2)) ^ beta)\n    //\n    // but MIOpen defines it as\n    //\n    //  U_i = V_i / ((bias + (alpha / n) * (sum_j V_j^2)) ^ beta)\n    //\n    // i.e. there is a factor of n difference between the meaning of the alphas\n    // in the two contexts. The MIOpen alpha is n times the SE alpha.\n    double lrn_alpha = lrn_N * normalize_descriptor.alpha();\n\n    double lrn_beta = normalize_descriptor.beta();\n    double lrn_k = normalize_descriptor.bias();\n    status = wrap::miopenSetLRNDescriptor(handle_, miopenLRNCrossChannel, lrn_N,\n                                          lrn_alpha, lrn_beta, lrn_k);\n    if (status != miopenStatusSuccess) {\n      LOG(FATAL) << \"could not set miopen LRN descriptor: \" << ToString(status);\n    }\n  }\n\n  ~ScopedNormalizeDescriptor() {\n    auto status = wrap::miopenDestroyLRNDescriptor(handle_);\n    if (status != miopenStatusSuccess) {\n      LOG(ERROR) << \"could not destroy miopen LRN descriptor: \"\n                 << ToString(status);\n    }\n  }\n\n  miopenLRNDescriptor_t handle() const { return handle_; }\n\n private:\n  miopenLRNDescriptor_t handle_;  // Owned.\n\n  SE_DISALLOW_COPY_AND_ASSIGN(ScopedNormalizeDescriptor);\n};\n\n// Turns a activation mode into a miopen activation mode descriptor with a scope\n// around it\nclass ScopedActivationDescriptor {\n public:\n  ScopedActivationDescriptor(dnn::ActivationMode activation_mode)\n      : handle_(nullptr),\n        miopen_activation_mode_(miopenActivationPASTHRU),\n        alpha_(0.0),\n        beta_(0.0),\n        gamma_(0.0) {\n    auto status = wrap::miopenCreateActivationDescriptor(&handle_);\n    if (status != miopenStatusSuccess) {\n      LOG(FATAL) << \"call to miopenCreateActivationDescriptor failed: \"\n                 << ToString(status);\n    } else {\n      switch (activation_mode) {\n        case dnn::ActivationMode::kNone:\n          miopen_activation_mode_ = miopenActivationPASTHRU;\n          break;\n\n        case dnn::ActivationMode::kSigmoid:\n          miopen_activation_mode_ = miopenActivationLOGISTIC;\n          break;\n\n        case dnn::ActivationMode::kRelu:\n          miopen_activation_mode_ = miopenActivationRELU;\n          break;\n\n        case dnn::ActivationMode::kRelu6:\n          miopen_activation_mode_ = miopenActivationRELU;\n          alpha_ = 6.0;\n          break;\n\n        case dnn::ActivationMode::kTanh:\n          miopen_activation_mode_ = miopenActivationTANH;\n          break;\n\n        default:\n          LOG(FATAL) << \"Activation mode (\"\n                     << dnn::ActivationModeString(activation_mode)\n                     << \") not yet implemented\";\n          break;\n      }\n\n      status = wrap::miopenSetActivationDescriptor(\n          handle_, miopen_activation_mode_, alpha_, beta_, gamma_);\n      if (status != miopenStatusSuccess) {\n        LOG(FATAL) << \"call to miopenSetActivationDescriptor failed: \"\n                   << ToString(status);\n      }\n    }\n  }\n\n  ~ScopedActivationDescriptor() {\n    auto status = wrap::miopenDestroyActivationDescriptor(handle_);\n    if (status != miopenStatusSuccess) {\n      LOG(FATAL) << \"call to miopenDestroyActivationDescriptor failed: \"\n                 << ToString(status);\n    }\n  }\n\n  miopenActivationDescriptor_t handle() const { return handle_; }\n\n  uint64_t GetHashValue() {\n    uint64_t hash_value = tsl::hash<int>()(miopen_activation_mode_);\n    hash_value = tsl::Hash64Combine(hash_value, tsl::hash<double>()(alpha_));\n    hash_value = tsl::Hash64Combine(hash_value, tsl::hash<double>()(beta_));\n    hash_value = tsl::Hash64Combine(hash_value, tsl::hash<double>()(gamma_));\n\n    return hash_value;\n  }\n\n private:\n  miopenActivationDescriptor_t handle_;  // Owned.\n\n  SE_DISALLOW_COPY_AND_ASSIGN(ScopedActivationDescriptor);\n\n public:\n  // caching these values here to avoid calling miopenGetActivationDescriptor\n  // to do the same. miopenGetActivationDescriptor gets called twice during each\n  // call to execute a fusion plan (that involves the activation op)...once call\n  // during calculating hashvalue for the fusion op, and another before calling\n  // SetOpArgs for the activation op\n  miopenActivationMode_t miopen_activation_mode_;\n  double alpha_;\n  double beta_;\n  double gamma_;\n};\n\n// base class for all fusion plan implementations to derive from\nclass ScopedFusionPlanBase {\n public:\n  ScopedFusionPlanBase(miopenHandle_t miopen_handle,\n                       const miopenFusionDirection_t fuse_direction,\n                       const miopenTensorDescriptor_t input_descriptor)\n      : miopen_handle_(miopen_handle),\n        fusion_plan_(nullptr),\n        fusion_args_(nullptr),\n        fusion_plan_compiled_(false) {\n    auto status = wrap::miopenCreateOperatorArgs(&fusion_args_);\n    if (status != miopenStatusSuccess) {\n      LOG(FATAL) << \"call to miopenCreateOperatorArgs failed: \"\n                 << ToString(status);\n    }\n  }\n\n  virtual ~ScopedFusionPlanBase() {\n    auto status = wrap::miopenDestroyOperatorArgs(fusion_args_);\n    if (status != miopenStatusSuccess) {\n      LOG(FATAL) << \"call to miopenDestroyoperatorArgs failed: \"\n                 << ToString(status);\n    }\n  }\n\n  miopenStatus_t Execute(miopenTensorDescriptor_t input_descriptor,\n                         const void* input_data,\n                         miopenTensorDescriptor_t output_descriptor,\n                         void* output_data) {\n    auto status = wrap::miopenExecuteFusionPlan(\n        miopen_handle_, fusion_plan_, input_descriptor, input_data,\n        output_descriptor, output_data, fusion_args_);\n    if (status != miopenStatusSuccess) {\n      LOG(FATAL) << \"call to miopenExecuteFusionPlan failed: \"\n                 << ToString(status);\n    }\n\n    return status;\n  }\n\n  bool CompilationSucceeded() { return fusion_plan_compiled_; }\n\n protected:\n  miopenStatus_t SetConvolutionArgs(const int op_idx, const float* alpha,\n                                    const float* beta, const void* data) {\n    miopenFusionOpDescriptor_t conv_op;\n    auto status = wrap::miopenFusionPlanGetOp(fusion_plan_, op_idx, &conv_op);\n    if (status != miopenStatusSuccess) {\n      LOG(FATAL) << \"call to miopenFusionPlanGetOp failed: \"\n                 << ToString(status);\n    }\n\n    status = wrap::miopenSetOpArgsConvForward(fusion_args_, conv_op, alpha,\n                                              beta, data);\n    if (status != miopenStatusSuccess) {\n      LOG(FATAL) << \"call to miopenSetOpArgsConvForward failed: \"\n                 << ToString(status);\n    }\n    return status;\n  }\n\n  miopenStatus_t SetBiasArgs(const int op_idx, const float* alpha,\n                             const float* beta, const void* data) {\n    miopenFusionOpDescriptor_t bias_op;\n    auto status = wrap::miopenFusionPlanGetOp(fusion_plan_, op_idx, &bias_op);\n    if (status != miopenStatusSuccess) {\n      LOG(FATAL) << \"call to miopenFusionPlanGetOp failed: \"\n                 << ToString(status);\n    }\n\n    status = wrap::miopenSetOpArgsBiasForward(fusion_args_, bias_op, alpha,\n                                              beta, data);\n    if (status != miopenStatusSuccess) {\n      LOG(FATAL) << \"call to miopenSetOpArgsBiasForward failed: \"\n                 << ToString(status);\n    }\n    return status;\n  }\n\n  miopenStatus_t SetBatchNormInferenceArgs(const int op_idx, const float* alpha,\n                                           const float* beta, const void* scale,\n                                           const void* offset, const void* mean,\n                                           const void* variance,\n                                           double epsilon) {\n    miopenFusionOpDescriptor_t batchnorm_op;\n    auto status =\n        wrap::miopenFusionPlanGetOp(fusion_plan_, op_idx, &batchnorm_op);\n    if (status != miopenStatusSuccess) {\n      LOG(FATAL) << \"call to miopenFusionPlanGetOp failed: \"\n                 << ToString(status);\n    }\n\n    status = wrap::miopenSetOpArgsBatchNormInference(fusion_args_, batchnorm_op,\n                                                     alpha, beta, scale, offset,\n                                                     mean, variance, epsilon);\n    if (status != miopenStatusSuccess) {\n      LOG(FATAL) << \"call to miopenSetOpArgsBatchNormInference failed: \"\n                 << ToString(status);\n    }\n    return status;\n  }\n\n  miopenStatus_t SetBatchNormForwardArgs(\n      const int op_idx, const float* alpha, const float* beta,\n      const void* scale, const void* offset, void* running_mean,\n      void* running_variance, void* saved_mean, void* saved_inv_variance,\n      double epsilon, double exponential_average_factor) {\n    miopenFusionOpDescriptor_t batchnorm_op;\n    auto status =\n        wrap::miopenFusionPlanGetOp(fusion_plan_, op_idx, &batchnorm_op);\n    if (status != miopenStatusSuccess) {\n      LOG(FATAL) << \"call to miopenFusionPlanGetOp failed: \"\n                 << ToString(status);\n    }\n\n    status = wrap::miopenSetOpArgsBatchNormForward(\n        fusion_args_, batchnorm_op, alpha, beta, scale, offset, saved_mean,\n        saved_inv_variance, running_mean, running_variance, epsilon,\n        exponential_average_factor);\n    if (status != miopenStatusSuccess) {\n      LOG(FATAL) << \"call to miopenSetOpArgsBatchNormForward failed: \"\n                 << ToString(status);\n    }\n    return status;\n  }\n\n  miopenStatus_t SetBatchNormBackwardArgs(const int op_idx, const float* alpha,\n                                          const float* beta, const void* x,\n                                          const void* scale, const void* offset,\n                                          void* scale_grad, void* offset_grad,\n                                          const void* saved_mean,\n                                          const void* saved_inv_variance) {\n    miopenFusionOpDescriptor_t batchnorm_op;\n    auto status =\n        wrap::miopenFusionPlanGetOp(fusion_plan_, op_idx, &batchnorm_op);\n    if (status != miopenStatusSuccess) {\n      LOG(FATAL) << \"call to miopenFusionPlanGetOp failed: \"\n                 << ToString(status);\n    }\n\n    status = wrap::miopenSetOpArgsBatchNormBackward(\n        fusion_args_, batchnorm_op, alpha, beta, x, scale, offset, scale_grad,\n        offset_grad, saved_mean, saved_inv_variance);\n    if (status != miopenStatusSuccess) {\n      LOG(FATAL) << \"call to miopenSetOpArgsBatchNormBackward failed: \"\n                 << ToString(status);\n    }\n    return status;\n  }\n\n  miopenStatus_t SetActivationForwardArgs(const int op_idx, const float* alpha,\n                                          const float* beta, double activ_alpha,\n                                          double activ_beta,\n                                          double activ_gamma) {\n    miopenFusionOpDescriptor_t actv_op;\n    auto status = wrap::miopenFusionPlanGetOp(fusion_plan_, op_idx, &actv_op);\n    if (status != miopenStatusSuccess) {\n      LOG(FATAL) << \"call to miopenFusionPlanGetOp failed: \"\n                 << ToString(status);\n    }\n\n    status =\n        wrap::miopenSetOpArgsActivForward(fusion_args_, actv_op, alpha, beta,\n                                          activ_alpha, activ_beta, activ_gamma);\n    if (status != miopenStatusSuccess) {\n      LOG(FATAL) << \"call to miopenSetOpArgsActivForward failed: \"\n                 << ToString(status);\n    }\n    return status;\n  }\n\n  miopenStatus_t SetActivationBackwardArgs(const int op_idx, const float* alpha,\n                                           const float* beta, const void* y,\n                                           double activ_alpha,\n                                           double activ_beta,\n                                           double activ_gamma) {\n    miopenFusionOpDescriptor_t actv_op;\n    auto status = wrap::miopenFusionPlanGetOp(fusion_plan_, op_idx, &actv_op);\n    if (status != miopenStatusSuccess) {\n      LOG(FATAL) << \"call to miopenFusionPlanGetOp failed: \"\n                 << ToString(status);\n    }\n\n    status = wrap::miopenSetOpArgsActivBackward(fusion_args_, actv_op, alpha,\n                                                beta, y, nullptr, activ_alpha,\n                                                activ_beta, activ_gamma);\n    if (status != miopenStatusSuccess) {\n      LOG(FATAL) << \"call to miopenSetOpArgsActivBackward failed: \"\n                 << ToString(status);\n    }\n    return status;\n  }\n\n  miopenHandle_t miopen_handle_;\n  miopenFusionPlanDescriptor_t fusion_plan_;\n  miopenOperatorArgs_t fusion_args_;  // Owned.\n  bool fusion_plan_compiled_;\n\n  SE_DISALLOW_COPY_AND_ASSIGN(ScopedFusionPlanBase);\n};\n\n// class to represent the Convolution+Bias+Activation fusion plan\nclass ScopedFusionPlanConvolutionBiasActivation : public ScopedFusionPlanBase {\n public:\n  ScopedFusionPlanConvolutionBiasActivation(\n      miopenHandle_t miopen_handle, miopenTensorDescriptor_t input_descriptor,\n      miopenTensorDescriptor_t filter_descriptor,\n      miopenConvolutionDescriptor_t conv_descriptor,\n      miopenTensorDescriptor_t bias_descriptor,\n      ScopedActivationDescriptor& activation_descriptor)\n      : ScopedFusionPlanBase(miopen_handle, miopenVerticalFusion,\n                             input_descriptor) {\n    uint64_t hash = GetFusionOpHashValue(\n        miopen_handle, input_descriptor, filter_descriptor, conv_descriptor,\n        bias_descriptor, activation_descriptor);\n\n    bool is_compiled = CachedFusionPlans::FindOrCreate(\n        hash, &fusion_plan_, miopenVerticalFusion, input_descriptor);\n    if (!is_compiled) {\n      miopenFusionOpDescriptor_t conv_op;\n      auto status = wrap::miopenCreateOpConvForward(\n          fusion_plan_, &conv_op, conv_descriptor, filter_descriptor);\n      if (status != miopenStatusSuccess) {\n        LOG(FATAL) << \"call to miopenCreateOpConvForward failed: \"\n                   << ToString(status);\n      }\n\n      miopenFusionOpDescriptor_t bias_op;\n      status = wrap::miopenCreateOpBiasForward(fusion_plan_, &bias_op,\n                                               bias_descriptor);\n      if (status != miopenStatusSuccess) {\n        LOG(FATAL) << \"call to miopenCreateOpBiasForward failed: \"\n                   << ToString(status);\n      }\n\n      miopenFusionOpDescriptor_t actv_op;\n      status = wrap::miopenCreateOpActivationForward(\n          fusion_plan_, &actv_op,\n          activation_descriptor.miopen_activation_mode_);\n      if (status != miopenStatusSuccess) {\n        LOG(FATAL) << \"call to miopenCreateOpActivationForward failed: \"\n                   << ToString(status);\n      }\n\n      status = wrap::miopenCompileFusionPlan(miopen_handle_, fusion_plan_);\n      if (status != miopenStatusSuccess) {\n        VLOG(2) << \"call to miopenCompileFusionPlan (CBA) failed: \"\n                << ToString(status);\n\n        CachedFusionPlans::MarkFusionPlanUnsupported(hash);\n      } else {\n        VLOG(2) << \"Fusion Plan compile succedded (CBA) \";\n        fusion_plan_compiled_ = true;\n      }\n    } else {\n      // fusion plan was already compiled...check whether it failed to compile\n      fusion_plan_compiled_ = !CachedFusionPlans::IsUnsupportedFusionPlan(hash);\n    }\n  }\n\n  miopenStatus_t SetConvolutionArgs(const void* filter_data) {\n    float alpha = 1.0;\n    float beta = 0.0;\n    return ScopedFusionPlanBase::SetConvolutionArgs(k_conv_op_idx, &alpha,\n                                                    &beta, filter_data);\n  }\n\n  miopenStatus_t SetBiasArgs(const void* bias_data) {\n    float alpha = 1.0;\n    float beta = 0.0;\n    return ScopedFusionPlanBase::SetBiasArgs(k_bias_op_idx, &alpha, &beta,\n                                             bias_data);\n  }\n\n  miopenStatus_t SetActivationForwardArgs(\n      ScopedActivationDescriptor& activation_descriptor) {\n    float alpha = 1.0;\n    float beta = 0.0;\n\n    return ScopedFusionPlanBase::SetActivationForwardArgs(\n        k_actv_op_idx, &alpha, &beta, activation_descriptor.alpha_,\n        activation_descriptor.beta_, activation_descriptor.gamma_);\n  }\n\n  uint64_t GetFusionOpHashValue(\n      miopenHandle_t miopen_handle, miopenTensorDescriptor_t input_descriptor,\n      miopenTensorDescriptor_t filter_descriptor,\n      miopenConvolutionDescriptor_t conv_descriptor,\n      miopenTensorDescriptor_t bias_descriptor,\n      ScopedActivationDescriptor& activation_descriptor) {\n    uint64_t hash_value = tsl::Hash64(\"ConvolutionBiasActivation\");\n\n    hash_value = tsl::Hash64Combine(hash_value,\n                                    tsl::hash<miopenHandle_t>()(miopen_handle));\n\n    hash_value = tsl::Hash64Combine(hash_value, GetHashValue(input_descriptor));\n    hash_value =\n        tsl::Hash64Combine(hash_value, GetHashValue(filter_descriptor));\n    hash_value = tsl::Hash64Combine(hash_value, GetHashValue(conv_descriptor));\n    hash_value = tsl::Hash64Combine(hash_value, GetHashValue(bias_descriptor));\n    hash_value =\n        tsl::Hash64Combine(hash_value, activation_descriptor.GetHashValue());\n    return hash_value;\n  }\n\n private:\n  const int k_conv_op_idx = 0;\n  const int k_bias_op_idx = 1;\n  const int k_actv_op_idx = 2;\n\n  SE_DISALLOW_COPY_AND_ASSIGN(ScopedFusionPlanConvolutionBiasActivation);\n};\n\n// class to represent the BatchNorm+Activation (inference) fusion plan\nclass ScopedFusionPlanBatchNormActivationInference\n    : public ScopedFusionPlanBase {\n public:\n  ScopedFusionPlanBatchNormActivationInference(\n      miopenHandle_t miopen_handle, miopenTensorDescriptor_t input_descriptor,\n      miopenTensorDescriptor_t scale_offset_mean_variance_descriptor,\n      ScopedActivationDescriptor& activation_descriptor)\n      : ScopedFusionPlanBase(miopen_handle, miopenVerticalFusion,\n                             input_descriptor) {\n    uint64_t hash = GetFusionOpHashValue(miopen_handle, input_descriptor,\n                                         scale_offset_mean_variance_descriptor,\n                                         activation_descriptor);\n\n    bool is_compiled = CachedFusionPlans::FindOrCreate(\n        hash, &fusion_plan_, miopenVerticalFusion, input_descriptor);\n\n    if (!is_compiled) {\n      miopenFusionOpDescriptor_t batchnorm_op;\n      auto status = wrap::miopenCreateOpBatchNormInference(\n          fusion_plan_, &batchnorm_op, miopenBNSpatial,\n          scale_offset_mean_variance_descriptor);\n\n      if (status != miopenStatusSuccess) {\n        LOG(FATAL) << \"call to miopenCreateOpBatchNormInference failed: \"\n                   << ToString(status);\n      }\n\n      miopenFusionOpDescriptor_t actv_op;\n      status = wrap::miopenCreateOpActivationForward(\n          fusion_plan_, &actv_op,\n          activation_descriptor.miopen_activation_mode_);\n      if (status != miopenStatusSuccess) {\n        LOG(FATAL) << \"call to miopenCreateOpActivationForward failed: \"\n                   << ToString(status);\n      }\n\n      status = wrap::miopenCompileFusionPlan(miopen_handle_, fusion_plan_);\n      if (status != miopenStatusSuccess) {\n        VLOG(2) << \"call to miopenCompileFusionPlan (BnA inference) failed: \"\n                << ToString(status);\n\n        CachedFusionPlans::MarkFusionPlanUnsupported(hash);\n      } else {\n        VLOG(2) << \"Fusion Plan compile succedded (BnA inference) \";\n        fusion_plan_compiled_ = true;\n      }\n    } else {\n      // fusion plan was already compiled...check whether it failed to compile\n      fusion_plan_compiled_ = !CachedFusionPlans::IsUnsupportedFusionPlan(hash);\n    }\n  }\n\n  miopenStatus_t SetBatchNormInferenceArgs(const void* scale,\n                                           const void* offset, const void* mean,\n                                           const void* variance,\n                                           double epsilon) {\n    float alpha = 1.0;\n    float beta = 0.0;\n    return ScopedFusionPlanBase::SetBatchNormInferenceArgs(\n        k_batchnorm_op_idx, &alpha, &beta, scale, offset, mean, variance,\n        epsilon);\n  }\n\n  miopenStatus_t SetActivationForwardArgs(\n      ScopedActivationDescriptor& activation_descriptor) {\n    float alpha = 1.0;\n    float beta = 0.0;\n\n    return ScopedFusionPlanBase::SetActivationForwardArgs(\n        k_actv_op_idx, &alpha, &beta, activation_descriptor.alpha_,\n        activation_descriptor.beta_, activation_descriptor.gamma_);\n  }\n\n  uint64_t GetFusionOpHashValue(\n      miopenHandle_t miopen_handle, miopenTensorDescriptor_t input_descriptor,\n      miopenTensorDescriptor_t scale_offset_mean_variance_descriptor,\n      ScopedActivationDescriptor& activation_descriptor) {\n    uint64_t hash_value = tsl::Hash64(\"BatchNormActivationInference\");\n\n    hash_value = tsl::Hash64Combine(hash_value,\n                                    tsl::hash<miopenHandle_t>()(miopen_handle));\n\n    hash_value = tsl::Hash64Combine(hash_value, GetHashValue(input_descriptor));\n\n    hash_value = tsl::Hash64Combine(\n        hash_value, GetHashValue(scale_offset_mean_variance_descriptor));\n\n    hash_value =\n        tsl::Hash64Combine(hash_value, activation_descriptor.GetHashValue());\n    return hash_value;\n  }\n\n private:\n  const int k_batchnorm_op_idx = 0;\n  const int k_actv_op_idx = 1;\n\n  SE_DISALLOW_COPY_AND_ASSIGN(ScopedFusionPlanBatchNormActivationInference);\n};\n\n// class to represent the BatchNorm+Activation (training-forward) fusion plan\nclass ScopedFusionPlanBatchNormActivationForward : public ScopedFusionPlanBase {\n public:\n  ScopedFusionPlanBatchNormActivationForward(\n      miopenHandle_t miopen_handle, miopenTensorDescriptor_t input_descriptor,\n      miopenTensorDescriptor_t scale_offset_mean_variance_descriptor,\n      ScopedActivationDescriptor& activation_descriptor)\n      : ScopedFusionPlanBase(miopen_handle, miopenVerticalFusion,\n                             input_descriptor) {\n    uint64_t hash = GetFusionOpHashValue(miopen_handle, input_descriptor,\n                                         scale_offset_mean_variance_descriptor,\n                                         activation_descriptor);\n\n    bool is_compiled = CachedFusionPlans::FindOrCreate(\n        hash, &fusion_plan_, miopenVerticalFusion, input_descriptor);\n\n    if (!is_compiled) {\n      miopenFusionOpDescriptor_t batchnorm_op;\n      auto status = wrap::miopenCreateOpBatchNormForward(\n          fusion_plan_, &batchnorm_op, miopenBNSpatial,\n          true /* runningMeanVariance */);\n\n      if (status != miopenStatusSuccess) {\n        LOG(FATAL) << \"call to miopenCreateOpBatchNormForward failed: \"\n                   << ToString(status);\n      }\n\n      miopenFusionOpDescriptor_t actv_op;\n      status = wrap::miopenCreateOpActivationForward(\n          fusion_plan_, &actv_op,\n          activation_descriptor.miopen_activation_mode_);\n      if (status != miopenStatusSuccess) {\n        LOG(FATAL) << \"call to miopenCreateOpActivationForward failed: \"\n                   << ToString(status);\n      }\n\n      status = wrap::miopenCompileFusionPlan(miopen_handle_, fusion_plan_);\n      if (status != miopenStatusSuccess) {\n        VLOG(2) << \"call to miopenCompileFusionPlan (BnA forward) failed: \"\n                << ToString(status);\n\n        CachedFusionPlans::MarkFusionPlanUnsupported(hash);\n      } else {\n        VLOG(2) << \"Fusion Plan compile succedded (BnA forward) \";\n        fusion_plan_compiled_ = true;\n      }\n    } else {\n      // fusion plan was already compiled...check whether it failed to compile\n      fusion_plan_compiled_ = !CachedFusionPlans::IsUnsupportedFusionPlan(hash);\n    }\n  }\n\n  miopenStatus_t SetBatchNormForwardArgs(const void* scale, const void* offset,\n                                         void* batch_mean, void* batch_var,\n                                         void* saved_mean, void* saved_var,\n                                         double epsilon) {\n    float alpha = 1.0;\n    float beta = 0.0;\n    return ScopedFusionPlanBase::SetBatchNormForwardArgs(\n        k_batchnorm_op_idx, &alpha, &beta, scale, offset, batch_mean, batch_var,\n        saved_mean, saved_var, epsilon, /*exponential_average_factor=*/1.0);\n  }\n\n  miopenStatus_t SetActivationForwardArgs(\n      ScopedActivationDescriptor& activation_descriptor) {\n    float alpha = 1.0;\n    float beta = 0.0;\n\n    return ScopedFusionPlanBase::SetActivationForwardArgs(\n        k_actv_op_idx, &alpha, &beta, activation_descriptor.alpha_,\n        activation_descriptor.beta_, activation_descriptor.gamma_);\n  }\n\n  uint64_t GetFusionOpHashValue(\n      miopenHandle_t miopen_handle, miopenTensorDescriptor_t input_descriptor,\n      miopenTensorDescriptor_t scale_offset_mean_variance_descriptor,\n      ScopedActivationDescriptor& activation_descriptor) {\n    uint64_t hash_value = tsl::Hash64(\"BatchNormActivationForward\");\n\n    hash_value = tsl::Hash64Combine(hash_value,\n                                    tsl::hash<miopenHandle_t>()(miopen_handle));\n\n    hash_value = tsl::Hash64Combine(hash_value, GetHashValue(input_descriptor));\n\n    hash_value = tsl::Hash64Combine(\n        hash_value, GetHashValue(scale_offset_mean_variance_descriptor));\n\n    hash_value =\n        tsl::Hash64Combine(hash_value, activation_descriptor.GetHashValue());\n    return hash_value;\n  }\n\n private:\n  const int k_batchnorm_op_idx = 0;\n  const int k_actv_op_idx = 1;\n\n  SE_DISALLOW_COPY_AND_ASSIGN(ScopedFusionPlanBatchNormActivationForward);\n};\n\n// class to represent the BatchNorm+Activation (training-backward) fusion plan\nclass ScopedFusionPlanBatchNormActivationBackward\n    : public ScopedFusionPlanBase {\n public:\n  ScopedFusionPlanBatchNormActivationBackward(\n      miopenHandle_t miopen_handle, miopenTensorDescriptor_t input_descriptor,\n      miopenTensorDescriptor_t scale_offset_mean_variance_descriptor,\n      ScopedActivationDescriptor& activation_descriptor)\n      : ScopedFusionPlanBase(miopen_handle, miopenVerticalFusion,\n                             input_descriptor) {\n    uint64_t hash = GetFusionOpHashValue(miopen_handle, input_descriptor,\n                                         scale_offset_mean_variance_descriptor,\n                                         activation_descriptor);\n\n    bool is_compiled = CachedFusionPlans::FindOrCreate(\n        hash, &fusion_plan_, miopenVerticalFusion, input_descriptor);\n\n    if (!is_compiled) {\n      miopenFusionOpDescriptor_t batchnorm_op;\n      auto status = wrap::miopenCreateOpBatchNormBackward(\n          fusion_plan_, &batchnorm_op, miopenBNSpatial);\n\n      if (status != miopenStatusSuccess) {\n        LOG(FATAL) << \"call to miopenCreateOpBatchNormBackward failed: \"\n                   << ToString(status);\n      }\n\n      miopenFusionOpDescriptor_t actv_op;\n      status = wrap::miopenCreateOpActivationBackward(\n          fusion_plan_, &actv_op,\n          activation_descriptor.miopen_activation_mode_);\n      if (status != miopenStatusSuccess) {\n        LOG(FATAL) << \"call to miopenCreateOpActivationBackward failed: \"\n                   << ToString(status);\n      }\n\n      status = wrap::miopenCompileFusionPlan(miopen_handle_, fusion_plan_);\n      if (status != miopenStatusSuccess) {\n        VLOG(2) << \"call to miopenCompileFusionPlan (BnA backward) failed: \"\n                << ToString(status);\n\n        CachedFusionPlans::MarkFusionPlanUnsupported(hash);\n      } else {\n        VLOG(2) << \"Fusion Plan compile succedded (BnA backward) \";\n        fusion_plan_compiled_ = true;\n      }\n    } else {\n      // fusion plan was already compiled...check whether it failed to compile\n      fusion_plan_compiled_ = !CachedFusionPlans::IsUnsupportedFusionPlan(hash);\n    }\n  }\n\n  miopenStatus_t SetBatchNormBackwardArgs(const void* x, const void* scale,\n                                          const void* offset,\n                                          const void* saved_mean,\n                                          const void* saved_var,\n                                          void* scale_grad, void* offset_grad) {\n    float alpha = 1.0;\n    float beta = 0.0;\n\n    return ScopedFusionPlanBase::SetBatchNormBackwardArgs(\n        k_batchnorm_op_idx, &alpha, &beta, x, scale, offset, scale_grad,\n        offset_grad, saved_mean, saved_var);\n  }\n\n  miopenStatus_t SetActivationBackwardArgs(\n      ScopedActivationDescriptor& activation_descriptor, const void* y) {\n    float alpha = 1.0;\n    float beta = 0.0;\n\n    return ScopedFusionPlanBase::SetActivationBackwardArgs(\n        k_actv_op_idx, &alpha, &beta, y, activation_descriptor.alpha_,\n        activation_descriptor.beta_, activation_descriptor.gamma_);\n  }\n\n  uint64_t GetFusionOpHashValue(\n      miopenHandle_t miopen_handle, miopenTensorDescriptor_t input_descriptor,\n      miopenTensorDescriptor_t scale_offset_mean_variance_descriptor,\n      ScopedActivationDescriptor& activation_descriptor) {\n    uint64_t hash_value = tsl::Hash64(\"BatchNormActivationBackward\");\n\n    hash_value = tsl::Hash64Combine(hash_value,\n                                    tsl::hash<miopenHandle_t>()(miopen_handle));\n\n    hash_value = tsl::Hash64Combine(hash_value, GetHashValue(input_descriptor));\n\n    hash_value = tsl::Hash64Combine(\n        hash_value, GetHashValue(scale_offset_mean_variance_descriptor));\n\n    hash_value =\n        tsl::Hash64Combine(hash_value, activation_descriptor.GetHashValue());\n    return hash_value;\n  }\n\n private:\n  const int k_batchnorm_op_idx = 0;\n  const int k_actv_op_idx = 1;\n\n  SE_DISALLOW_COPY_AND_ASSIGN(ScopedFusionPlanBatchNormActivationBackward);\n};\n\nnamespace {\nmiopenDataType_t ToMIOpenDataType(\n    dnn::DataType data_type,\n    dnn::DataLayout data_layout = dnn::DataLayout::kBatchDepthYX) {\n  switch (data_type) {\n    case dnn::DataType::kFloat:\n      return miopenFloat;\n    case dnn::DataType::kHalf:\n      return miopenHalf;\n    case dnn::DataType::kDouble:\n    default:\n      LOG(FATAL) << \"Invalid DNN data type: \" << static_cast<int>(data_type);\n  }\n}\n\nmiopenRNNInputMode_t ToMIOpenRnnInputMode(dnn::RnnInputMode input_mode) {\n  switch (input_mode) {\n    case dnn::RnnInputMode::kRnnLinearSkip:\n      return miopenRNNlinear;\n    case dnn::RnnInputMode::kRnnSkipInput:\n      return miopenRNNskip;\n    default:\n      LOG(FATAL) << \"Invalid RNN input mode: \" << static_cast<int>(input_mode);\n  }\n}\n\nmiopenRNNDirectionMode_t ToMIOpenRnnDirectionMode(\n    dnn::RnnDirectionMode direction_mode) {\n  switch (direction_mode) {\n    case dnn::RnnDirectionMode::kRnnUnidirectional:\n      return miopenRNNunidirection;\n    case dnn::RnnDirectionMode::kRnnBidirectional:\n      return miopenRNNbidirection;\n    default:\n      LOG(FATAL) << \"Invalid RNN direction mode: \"\n                 << static_cast<int>(direction_mode);\n  }\n}\n\nmiopenRNNMode_t ToMIOpenRnnMode(dnn::RnnMode rnn_mode) {\n  switch (rnn_mode) {\n    case dnn::RnnMode::kRnnRelu:\n      return miopenRNNRELU;\n    case dnn::RnnMode::kRnnTanh:\n      return miopenRNNTANH;\n    case dnn::RnnMode::kRnnLstm:\n      return miopenLSTM;\n    case dnn::RnnMode::kRnnGru:\n      return miopenGRU;\n    default:\n      LOG(FATAL) << \"Invalid RNN Mode: \" << static_cast<int>(rnn_mode);\n  }\n}\n\ntemplate <typename Base>\nclass MixinBase : public Base {};\ntemplate <>\nclass MixinBase<void> {};\n\n}  // namespace\n\n#define RETURN_IF_MIOPEN_ERROR(STATUS, ...)                              \\\n  if (!SE_PREDICT_TRUE((STATUS) == miopenStatusSuccess)) {               \\\n    string error_msg = absl::StrCat(ToString(STATUS), \" \", __VA_ARGS__); \\\n    SetFailure(::tsl::Status(tsl::error::UNKNOWN, error_msg));           \\\n    LOG(ERROR) << error_msg;                                             \\\n    return;                                                              \\\n  }\n\ntemplate <typename Base>\nclass MIOpenDescriptorCommon : public MixinBase<Base> {\n public:\n  bool ok() const { return status_.ok(); }\n  tsl::Status Status() const { return status_; }\n\n protected:\n  void SetFailure(const tsl::Status& status) { status_.Update(status); }\n  tsl::Status status_;\n};\n\nclass MIOpenRnnParamsDescriptor : public MIOpenDescriptorCommon<void> {\n public:\n  typedef dnn::RnnDescriptor::ParamsRegion ParamsRegion;\n  typedef dnn::RnnDescriptor::ParamsRegions ParamsRegions;\n  MIOpenRnnParamsDescriptor(miopenHandle_t miopen_handle,\n                            const MIOpenRnnDescriptor& rnn_desc);\n  ~MIOpenRnnParamsDescriptor() {\n    auto status = wrap::miopenDestroyTensorDescriptor(handle_);\n    RETURN_IF_MIOPEN_ERROR(status, \"Failed to destroy RNN tensor descriptor\");\n  }\n  miopenTensorDescriptor_t handle() const {\n    if (!ok()) return nullptr;\n    return handle_;\n  }\n  int64_t params_size_in_bytes() const { return params_size_in_bytes_; }\n  ParamsRegions params_weights() const {\n    if (!ok()) return ParamsRegions();\n    return weights_;\n  }\n  ParamsRegions params_biases() const {\n    if (!ok()) return ParamsRegions();\n    return biases_;\n  }\n\n private:\n  int GetRegionCountPerLayer() const;\n  miopenTensorDescriptor_t handle_;\n  const MIOpenRnnDescriptor* rnn_desc_;\n  int64_t params_size_in_bytes_;\n  ParamsRegions weights_;\n  ParamsRegions biases_;\n  tsl::Status status_;\n  SE_DISALLOW_COPY_AND_ASSIGN(MIOpenRnnParamsDescriptor);\n};\n\nclass MIOpenRnnDescriptor : public MIOpenDescriptorCommon<dnn::RnnDescriptor> {\n public:\n  MIOpenRnnDescriptor(miopenHandle_t miopen_handle, int num_layers,\n                      int hidden_size, int input_size,\n                      miopenRNNInputMode_t input_mode,\n                      miopenRNNDirectionMode_t direction_mode,\n                      miopenRNNMode_t rnn_mode, miopenDataType_t data_type,\n                      float dropout, uint64_t seed,\n                      ScratchAllocator* state_allocator)\n      : rnn_desc_(nullptr),\n        num_layers_(num_layers),\n        hidden_size_(hidden_size),\n        input_size_(input_size),\n        input_mode_(input_mode),\n        direction_mode_(direction_mode),\n        rnn_mode_(rnn_mode),\n        data_type_(data_type) {\n    // Create the RNN handle\n    auto status = wrap::miopenCreateRNNDescriptor(&rnn_desc_);\n    RETURN_IF_MIOPEN_ERROR(status, \"Unable to create RNN descriptor\");\n    status = wrap::miopenSetRNNDescriptor(\n        rnn_desc_ /*rnnDesc*/, hidden_size /*hiddenSize*/,\n        num_layers /*numLayers*/, input_mode /*inputMode*/,\n        direction_mode /*direction*/, rnn_mode /*mode*/,\n        miopenRNNwithBias /*biasMode*/, miopenRNNdefault /*algo*/,\n        data_type /*dataType*/);\n    RETURN_IF_MIOPEN_ERROR(status, \"Unable to update RNN descriptor\");\n    // Create the params handle.\n    miopen_params_desc_.reset(\n        new MIOpenRnnParamsDescriptor(miopen_handle, *this));\n    if (!miopen_params_desc_->ok()) {\n      SetFailure(miopen_params_desc_->Status());\n      return;\n    }\n  }\n  ~MIOpenRnnDescriptor() override {\n    if (rnn_desc_) {\n      auto status = wrap::miopenDestroyRNNDescriptor(rnn_desc_);\n      RETURN_IF_MIOPEN_ERROR(status, \"Unable to destroy RNN descriptor\");\n    }\n  }\n  miopenRNNDescriptor_t handle() const {\n    if (!ok()) return nullptr;\n    return rnn_desc_;\n  }\n  int num_layers() const { return num_layers_; }\n  int hidden_size() const { return hidden_size_; }\n  int input_size() const { return input_size_; }\n  miopenRNNInputMode_t input_mode() const { return input_mode_; }\n  miopenRNNDirectionMode_t direction_mode() const { return direction_mode_; }\n  miopenRNNMode_t rnn_mode() const { return rnn_mode_; }\n  miopenDataType_t data_type() const { return data_type_; }\n  int64_t ParamsSizeInBytes() const override {\n    return miopen_params_desc_->params_size_in_bytes();\n  }\n  miopenTensorDescriptor_t params_handle() const {\n    if (!miopen_params_desc_) return nullptr;\n    return miopen_params_desc_->handle();\n  }\n  ParamsRegions ParamsWeightRegions() const override {\n    if (!ok()) return ParamsRegions();\n    return miopen_params_desc_->params_weights();\n  }\n  ParamsRegions ParamsBiasRegions() const override {\n    if (!ok()) return ParamsRegions();\n    return miopen_params_desc_->params_biases();\n  }\n\n private:\n  miopenRNNDescriptor_t rnn_desc_;\n  int num_layers_;\n  int hidden_size_;\n  int input_size_;\n  miopenRNNInputMode_t input_mode_;\n  miopenRNNDirectionMode_t direction_mode_;\n  miopenRNNMode_t rnn_mode_;\n  miopenDataType_t data_type_;\n  tsl::Status status_;\n  // no dropout in MIOpen.\n  // std::unique_ptr<miopenDropoutDescriptor> miopen_dropout_desc_;\n  std::unique_ptr<MIOpenRnnParamsDescriptor> miopen_params_desc_;\n  SE_DISALLOW_COPY_AND_ASSIGN(MIOpenRnnDescriptor);\n};\n\n// Get ID of the internal parameter tensor.\n//\nint MIOpenRnnParamsDescriptor::GetRegionCountPerLayer() const {\n  auto rnn_mode = rnn_desc_->rnn_mode();\n  switch (rnn_mode) {\n    case miopenRNNRELU:\n    case miopenRNNTANH:\n      return 2;\n    case miopenLSTM:\n      return 8;\n    case miopenGRU:\n      return 6;\n    default:\n      LOG(FATAL) << \"Invalid RNN Mode: \" << static_cast<int>(rnn_mode);\n  }\n}\n\nclass MIOpenRnnSequenceTensorDescriptor\n    : public MIOpenDescriptorCommon<dnn::RnnSequenceTensorDescriptor> {\n public:\n  MIOpenRnnSequenceTensorDescriptor(int seq_length, int batch_size,\n                                    int data_size, miopenDataType_t data_type)\n      : seq_length_(seq_length),\n        batch_size_(batch_size),\n        data_size_(data_size),\n        data_type_(data_type) {\n    miopenTensorDescriptor_t handle = nullptr;\n    if (seq_length <= 0) {\n      string error_msg =\n          absl::StrCat(\"sequence length must be positive: \", seq_length);\n      LOG(ERROR) << error_msg;\n      SetFailure(tsl::Status(tsl::error::UNKNOWN, error_msg));\n      return;\n    }\n    auto status = wrap::miopenCreateTensorDescriptor(&handle);\n    RETURN_IF_MIOPEN_ERROR(status, \"Failed to create tensor descriptor\");\n    std::array<int, 2> dims = {{batch_size, data_size}};\n    status = wrap::miopenSetTensorDescriptor(\n        handle /*tensorDesc*/, data_type /*dataType*/, 2 /*nbDims*/,\n        dims.data() /*dimA*/, nullptr /*strideA*/);\n    RETURN_IF_MIOPEN_ERROR(status, \"Failed to update tensor descriptor\");\n    // Replicate handle across the number of steps.\n    handles_.assign(seq_length, handle);\n  }\n\n  ~MIOpenRnnSequenceTensorDescriptor() override {\n    // Only the first one needs to be destroyed. All others are the same.\n    auto status = wrap::miopenDestroyTensorDescriptor(handles_[0]);\n    RETURN_IF_MIOPEN_ERROR(status,\n                           \"Failed to destroy sequence tensor descriptor\");\n  }\n\n  const miopenTensorDescriptor_t* handles() const {\n    if (!ok()) return nullptr;\n    CHECK(!handles_.empty()) << \"handles cannot be empty\";\n    return handles_.data();\n  }\n\n  int seq_length() const { return seq_length_; }\n  int batch_size() const { return batch_size_; }\n  int data_size() const { return data_size_; }\n\n private:\n  int seq_length_;\n  int batch_size_;\n  int data_size_;\n  miopenDataType_t data_type_;\n  std::vector<miopenTensorDescriptor_t> handles_;\n  tsl::Status status_;\n  SE_DISALLOW_COPY_AND_ASSIGN(MIOpenRnnSequenceTensorDescriptor);\n};\n\nclass MIOpenRnnStateTensorDescriptor\n    : public MIOpenDescriptorCommon<dnn::RnnStateTensorDescriptor> {\n public:\n  MIOpenRnnStateTensorDescriptor(int num_layers, int batch_size, int data_size,\n                                 miopenDataType_t data_type)\n      : handle_(nullptr),\n        num_layers_(num_layers),\n        batch_size_(batch_size),\n        data_size_(data_size),\n        data_type_(data_type) {\n    auto status = wrap::miopenCreateTensorDescriptor(&handle_);\n    RETURN_IF_MIOPEN_ERROR(status, \"Failed to create tensor descriptor\");\n    std::array<int, 3> dims = {{num_layers, batch_size, data_size}};\n    status = wrap::miopenSetTensorDescriptor(\n        handle_ /*tensorDesc*/, data_type /*dataType*/, 3 /*nbDims*/,\n        dims.data() /*dimA*/, nullptr /*strideA*/);\n    RETURN_IF_MIOPEN_ERROR(status, \"Failed to update tensor descriptor\");\n  }\n\n  ~MIOpenRnnStateTensorDescriptor() override {\n    if (!handle_) {\n      auto status = wrap::miopenDestroyTensorDescriptor(handle_);\n      RETURN_IF_MIOPEN_ERROR(status, \"Unable to destroy RNN state tensor\");\n    }\n  }\n\n  miopenTensorDescriptor_t handle() const {\n    if (!ok()) return nullptr;\n    return handle_;\n  }\n  int num_layers() const { return num_layers_; }\n  int batch_size() const { return batch_size_; }\n  int data_size() const { return data_size_; }\n\n private:\n  miopenTensorDescriptor_t handle_;\n  int num_layers_;\n  int batch_size_;\n  int data_size_;\n  tsl::Status status_;\n  miopenDataType_t data_type_;\n  SE_DISALLOW_COPY_AND_ASSIGN(MIOpenRnnStateTensorDescriptor);\n};\n\nnamespace {\n\nstruct RnnModelDims {\n  int num_layers = 0;\n  int batch_size = 0;\n  int seq_length = 0;\n  int hidden_size = 0;\n  int input_size = 0;\n  int dir_count = 0;\n};\n\ntemplate <class T>\nbool ExtractAndCheckRnnForward(\n    const MIOpenRnnDescriptor& rnn_desc,\n    const MIOpenRnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<T>& input_data,\n    const MIOpenRnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<T>& input_h_data,\n    const MIOpenRnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<T>& input_c_data, const DeviceMemory<T>& params,\n    const MIOpenRnnSequenceTensorDescriptor& output_desc,\n    const DeviceMemory<T>& output_data,\n    const MIOpenRnnStateTensorDescriptor& output_h_desc,\n    const DeviceMemory<T>& output_h_data,\n    const MIOpenRnnStateTensorDescriptor& output_c_desc,\n    const DeviceMemory<T>& output_c_data, RnnModelDims* model_dims) {\n  // extract model parameters\n  model_dims->num_layers = rnn_desc.num_layers();\n  model_dims->batch_size = input_desc.batch_size();\n  model_dims->seq_length = input_desc.seq_length();\n  model_dims->hidden_size = rnn_desc.hidden_size();\n  model_dims->input_size = input_desc.data_size();\n  model_dims->dir_count =\n      (rnn_desc.direction_mode() == miopenRNNbidirection) ? 2 : 1;\n\n  // check parameters\n  if (!(input_h_desc.num_layers() ==\n            model_dims->num_layers * model_dims->dir_count &&\n        input_h_desc.batch_size() == model_dims->batch_size &&\n        input_h_desc.data_size() == model_dims->hidden_size)) {\n    LOG(ERROR) << \"Invalid input_h shape\";\n    return false;\n  }\n  if (!(input_h_desc.num_layers() == input_c_desc.num_layers() &&\n        input_h_desc.batch_size() == input_c_desc.batch_size() &&\n        input_h_desc.data_size() == input_c_desc.data_size())) {\n    LOG(ERROR) << \"Invalid input_c shape\";\n    return false;\n  }\n  if (!(output_desc.seq_length() == model_dims->seq_length &&\n        output_desc.batch_size() == model_dims->batch_size &&\n        output_desc.data_size() ==\n            model_dims->hidden_size * model_dims->dir_count)) {\n    LOG(ERROR) << \"Invalid output shape\";\n    return false;\n  }\n  if (!(input_h_desc.num_layers() == output_h_desc.num_layers() &&\n        input_h_desc.batch_size() == output_h_desc.batch_size() &&\n        input_h_desc.data_size() == output_h_desc.data_size())) {\n    LOG(ERROR) << \"Invalid output_h shape\";\n    return false;\n  }\n  if (!(input_h_desc.num_layers() == output_c_desc.num_layers() &&\n        input_h_desc.batch_size() == output_c_desc.batch_size() &&\n        input_h_desc.data_size() == output_c_desc.data_size())) {\n    LOG(ERROR) << \"Invalid output_h shape\";\n    return false;\n  }\n\n  return true;\n}\n\nbool CheckRNNParameterSize(\n    miopenHandle_t miopen_handle, const MIOpenRnnDescriptor& rnn_desc,\n    const MIOpenRnnSequenceTensorDescriptor& input_desc) {\n  size_t params_size_in_bytes = 0;\n  auto status = wrap::miopenGetRNNParamsSize(\n      miopen_handle /*handle*/, rnn_desc.handle() /*rnnDesc*/,\n      input_desc.handles()[0] /*xDesc*/, &params_size_in_bytes /*sizeInBytes*/,\n      rnn_desc.data_type() /*dataType*/);\n  if (status != miopenStatusSuccess) {\n    LOG(ERROR) << \"Unable to check RNN param size: \" << ToString(status);\n    return false;\n  }\n  return static_cast<int64_t>(params_size_in_bytes) ==\n         rnn_desc.ParamsSizeInBytes();\n}\n\nbool CreateRnnWorkspace(Stream* stream, miopenHandle_t miopen_handle,\n                        const MIOpenRnnDescriptor& rnn_desc,\n                        const MIOpenRnnSequenceTensorDescriptor& input_desc,\n                        ScratchAllocator* workspace_allocator,\n                        DeviceMemory<uint8>* workspace) {\n  // Query the workspace size.\n  size_t workspace_size_in_bytes = 0;\n  auto status = wrap::miopenGetRNNWorkspaceSize(\n      miopen_handle /*handle*/, rnn_desc.handle() /*rnnDesc*/,\n      input_desc.seq_length() /*seqLength*/, input_desc.handles() /*xDesc*/,\n      &workspace_size_in_bytes /*sizeInBytes*/);\n  if (status != miopenStatusSuccess) {\n    LOG(ERROR) << \"Unable to query workspace size: \" << ToString(status);\n    return false;\n  }\n  // Allocate the workspace.\n  if (workspace_size_in_bytes > 0) {\n    auto allocated =\n        workspace_allocator->AllocateBytes(workspace_size_in_bytes);\n    if (!allocated.ok() || (*workspace = allocated.value()) == nullptr) {\n      LOG(ERROR) << \"Failed to allocate RNN workspace\";\n\n      return false;\n    }\n    stream->ThenMemZero(workspace, workspace_size_in_bytes);\n  } else {\n    *workspace = DeviceMemory<uint8>();\n  }\n  return true;\n}\n\n}  // namespace\n\ntemplate <class T>\nbool MIOpenSupport::DoRnnForwardImpl(\n    Stream* stream, const MIOpenRnnDescriptor& rnn_desc,\n    const MIOpenRnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<T>& input_data,\n    const MIOpenRnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<T>& input_h_data,\n    const MIOpenRnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<T>& input_c_data, const DeviceMemory<T>& params,\n    const MIOpenRnnSequenceTensorDescriptor& output_desc,\n    DeviceMemory<T>* output_data,\n    const MIOpenRnnStateTensorDescriptor& output_h_desc,\n    DeviceMemory<T>* output_h_data,\n    const MIOpenRnnStateTensorDescriptor& output_c_desc,\n    DeviceMemory<T>* output_c_data, bool is_training,\n    ScratchAllocator* reserve_space_allocator,\n    ScratchAllocator* workspace_allocator) {\n  // extract model parameters\n  RnnModelDims model_dims;\n  bool res = ExtractAndCheckRnnForward(\n      rnn_desc, input_desc, input_data, input_h_desc, input_h_data,\n      input_c_desc, input_c_data, params, output_desc, *output_data,\n      output_h_desc, *output_h_data, output_c_desc, *output_c_data,\n      &model_dims);\n  if (!res) {\n    LOG(ERROR) << \"Invalid parameters for RNN Model\";\n    return false;\n  }\n\n  auto miopen = miopen_->GetHandle(parent_, stream);\n\n  // check params size\n\n  if (!CheckRNNParameterSize(miopen.handle(), rnn_desc, input_desc)) {\n    LOG(ERROR) << \"Invalid parameters\";\n    return false;\n  }\n\n  // create the workspace\n  DeviceMemory<uint8> workspace;\n  if (!CreateRnnWorkspace(stream, miopen.handle(), rnn_desc, input_desc,\n                          workspace_allocator, &workspace)) {\n    LOG(ERROR) << \"Unable to create rnn workspace\";\n\n    return false;\n  }\n\n  // query the reserve space size\n  // allocate the reserve space\n  DeviceMemory<uint8> reserve_space;\n  if (is_training) {\n    size_t reserve_space_size_in_bytes = 0;\n    auto status = wrap::miopenGetRNNTrainingReserveSize(\n        miopen.handle() /*handle*/, rnn_desc.handle() /*rnnDesc*/,\n        model_dims.seq_length /*seqLength*/, input_desc.handles() /*xDesc*/,\n        &reserve_space_size_in_bytes /*sizeInBytes*/);\n    if (status != miopenStatusSuccess) {\n      LOG(ERROR) << \"Unable to query reserve space size: \" << ToString(status);\n      return false;\n    }\n\n    if (reserve_space_size_in_bytes > 0) {\n      auto allocated =\n          reserve_space_allocator->AllocateBytes(reserve_space_size_in_bytes);\n      if (!allocated.ok() ||\n          (reserve_space = allocated.value()) == nullptr) {\n        LOG(ERROR) << \"Fail to allocate RNN reserve space\";\n        return false;\n      }\n      stream->ThenMemZero(&reserve_space, reserve_space_size_in_bytes);\n    }\n  }\n\n  // make the forward call\n  if (!is_training) {\n    auto status = wrap::miopenRNNForwardInference(\n        miopen.handle() /*handle*/, rnn_desc.handle() /*rnnDesc*/,\n        model_dims.seq_length /*seqLength*/, input_desc.handles() /*xDesc*/,\n        input_data.opaque() /*x*/, input_h_desc.handle() /*hxDesc*/,\n        input_h_data.opaque() /*hx*/, input_c_desc.handle() /*cxDesc*/,\n        input_c_data.opaque() /*cx*/, rnn_desc.params_handle() /*wDesc*/,\n        params.opaque() /*w*/, output_desc.handles() /*yDesc*/,\n        output_data->opaque() /*y*/, output_h_desc.handle() /*hyDesc*/,\n        output_h_data->opaque() /*hy*/, output_c_desc.handle() /*cyDesc*/,\n        output_c_data->opaque() /*cy*/, workspace.opaque() /*workspace*/,\n        workspace.size() /*workSpaceSizeInBytes*/);\n\n    if (status != miopenStatusSuccess) {\n      LOG(ERROR) << \"Failed to call miopenRNNForwardInference: \"\n                 << ToString(status);\n      return false;\n    }\n  } else {\n    auto status = wrap::miopenRNNForwardTraining(\n        miopen.handle() /*handle*/, rnn_desc.handle() /*rnnDesc*/,\n        model_dims.seq_length /*seqLength*/, input_desc.handles() /*xDesc*/,\n        input_data.opaque() /*x*/, input_h_desc.handle() /*hxDesc*/,\n        input_h_data.opaque() /*hx*/, input_c_desc.handle() /*cxDesc*/,\n        input_c_data.opaque() /*cx*/, rnn_desc.params_handle() /*wDesc*/,\n        params.opaque() /*w*/, output_desc.handles() /*yDesc*/,\n        output_data->opaque() /*y*/, output_h_desc.handle() /*hyDesc*/,\n        output_h_data->opaque() /*hy*/, output_c_desc.handle() /*cyDesc*/,\n        output_c_data->opaque() /*cy*/, workspace.opaque() /*workspace*/,\n        workspace.size() /*workSpaceSizeInBytes*/,\n        reserve_space.opaque() /*reserveSpace*/,\n        reserve_space.size() /*reserveSpaceSizeInBytes*/);\n    if (status != miopenStatusSuccess) {\n      LOG(ERROR) << \"Failed to call miopenRNNForwardTraining\"\n                 << ToString(status);\n      return false;\n    }\n  }\n  return true;\n}\n\ntemplate <class T>\nbool MIOpenSupport::DoRnnBackwardImpl(\n    Stream* stream, const MIOpenRnnDescriptor& rnn_desc,\n    const MIOpenRnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<T>& input_data,\n    const MIOpenRnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<T>& input_h_data,\n    const MIOpenRnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<T>& input_c_data, const DeviceMemory<T>& params,\n    const MIOpenRnnSequenceTensorDescriptor& output_desc,\n    const DeviceMemory<T>& output_data,\n    const MIOpenRnnStateTensorDescriptor& output_h_desc,\n    const DeviceMemory<T>& output_h_data,\n    const MIOpenRnnStateTensorDescriptor& output_c_desc,\n    const DeviceMemory<T>& output_c_data,\n    const DeviceMemory<T>& output_backprop_data,\n    const DeviceMemory<T>& output_h_backprop_data,\n    const DeviceMemory<T>& output_c_backprop_data,\n    DeviceMemory<T>* input_backprop_data,\n    DeviceMemory<T>* input_h_backprop_data,\n    DeviceMemory<T>* input_c_backprop_data,\n    DeviceMemory<T>* params_backprop_data,\n    DeviceMemory<uint8>* reserve_space_data,\n    ScratchAllocator* workspace_allocator) {\n  // extract model parameters\n  RnnModelDims model_dims;\n  bool res = ExtractAndCheckRnnForward(\n      rnn_desc, input_desc, input_data, input_h_desc, input_h_data,\n      input_c_desc, input_c_data, params, output_desc, output_data,\n      output_h_desc, output_h_data, output_c_desc, output_c_data, &model_dims);\n  if (!res) {\n    LOG(ERROR) << \"Invalid parameters for RNN Model\";\n    return false;\n  }\n\n  auto miopen = miopen_->GetHandle(parent_, stream);\n\n  // check params size\n\n  if (!CheckRNNParameterSize(miopen.handle(), rnn_desc, input_desc)) {\n    LOG(ERROR) << \"Invalid parameters\";\n    return false;\n  }\n\n  // create the workspace\n  DeviceMemory<uint8> workspace;\n  if (!CreateRnnWorkspace(stream, miopen.handle(), rnn_desc, input_desc,\n                          workspace_allocator, &workspace)) {\n    LOG(ERROR) << \"Unable to create rnn workspace\";\n    return false;\n  }\n\n  // workaround for missing initialization support in MIOpen.\n  // TODO: remove this when MIOpen is ready.\n  auto type_size = std::is_same<T, Eigen::half>::value ? 2 : sizeof(T);\n  auto size_data = input_desc.seq_length() * input_desc.batch_size() *\n                   input_desc.data_size();\n  if ((size_data > 0) && (input_backprop_data->opaque() != nullptr))\n    stream->ThenMemZero(input_backprop_data, size_data * type_size);\n\n  size_data = input_h_desc.num_layers() * input_h_desc.batch_size() *\n              input_h_desc.data_size();\n  if ((size_data > 0) && (input_h_backprop_data->opaque() != nullptr))\n    stream->ThenMemZero(input_h_backprop_data, size_data * type_size);\n\n  size_data = input_c_desc.num_layers() * input_c_desc.batch_size() *\n              input_c_desc.data_size();\n  if ((size_data > 0) && (input_c_backprop_data->opaque() != nullptr))\n    stream->ThenMemZero(input_c_backprop_data, size_data * type_size);\n\n  // make the backward data call\n  auto status = wrap::miopenRNNBackwardData(\n      miopen.handle() /*handle*/, rnn_desc.handle() /*rnnDesc*/,\n      model_dims.seq_length /*seqLength*/, output_desc.handles() /*yDesc*/,\n      output_data.opaque() /*y*/, output_desc.handles() /*dyDesc*/,\n      output_backprop_data.opaque() /*dy*/, output_h_desc.handle() /*dhyDesc*/,\n      output_h_backprop_data.opaque() /*dhy*/,\n      output_c_desc.handle() /*dcyDesc*/,\n      output_c_backprop_data.opaque() /*dcy*/,\n      rnn_desc.params_handle() /*wDesc*/, params.opaque() /*w*/,\n      input_h_desc.handle() /*hxDesc*/, input_h_data.opaque() /*hx*/,\n      input_c_desc.handle() /*cxDesc*/, input_c_data.opaque() /*cx*/,\n      input_desc.handles() /*dxDesc*/, input_backprop_data->opaque() /*dx*/,\n      input_h_desc.handle() /*dhxDesc*/,\n      input_h_backprop_data->opaque() /*dhx*/,\n      input_c_desc.handle() /*dcxDesc*/,\n      input_c_backprop_data->opaque() /*dcx*/, workspace.opaque() /*workspace*/,\n      workspace.size() /*workSpaceSizeInBytes*/,\n      reserve_space_data->opaque() /*reserveSpace*/,\n      reserve_space_data->size() /*reserveSpaceSizeInBytes*/);\n  if (status != miopenStatusSuccess) {\n    LOG(ERROR) << \"Failed to call miopenRNNBackwardData: \" << ToString(status);\n    return false;\n  }\n\n  if (params_backprop_data != nullptr) {\n    // Clear the dw to zeros.\n    stream->ThenMemZero(params_backprop_data, params_backprop_data->size());\n    // make the backward weight call\n    status = wrap::miopenRNNBackwardWeights(\n        miopen.handle() /*handle*/, rnn_desc.handle() /*rnnDesc*/,\n        model_dims.seq_length /*seqLength*/, input_desc.handles() /*xDesc*/,\n        input_data.opaque() /*x*/, input_h_desc.handle() /*hxDesc*/,\n        input_h_data.opaque() /*hx*/, output_desc.handles() /*yDesc*/,\n        output_data.opaque() /*y*/, rnn_desc.params_handle() /*dwDesc*/,\n        params_backprop_data->opaque() /*dw*/, workspace.opaque() /*workspace*/,\n        workspace.size() /*workSpaceSizeInBytes*/,\n        reserve_space_data->opaque() /*reserveSpace*/,\n        reserve_space_data->size() /*reserveSpaceSizeInBytes*/);\n    if (status != miopenStatusSuccess) {\n      LOG(ERROR) << \"Failed to call miopenRNNBackwardWeights: \"\n                 << ToString(status);\n      return false;\n    }\n  }\n\n  return true;\n}\n\nMIOpenRnnParamsDescriptor::MIOpenRnnParamsDescriptor(\n    miopenHandle_t miopen_handle, const MIOpenRnnDescriptor& rnn_desc)\n    : handle_(nullptr), rnn_desc_(&rnn_desc), params_size_in_bytes_(0) {\n  miopenTensorDescriptor_t input_desc = nullptr;\n  {\n    // Query the params size.\n    auto status = wrap::miopenCreateTensorDescriptor(&input_desc);\n    RETURN_IF_MIOPEN_ERROR(status, \"MIOpen fails to create tensor descriptor\");\n    std::array<int, 2> dims = {{1, rnn_desc.input_size()}};\n    status = wrap::miopenSetTensorDescriptor(\n        input_desc /*tensorDesc*/, rnn_desc.data_type() /*dataType*/,\n        2 /*nbDims*/, dims.data() /*dimA*/, nullptr /*strideA*/);\n    RETURN_IF_MIOPEN_ERROR(status, \"MIOpen fails to set tensor descriptor\");\n\n    size_t params_size = 0;\n    status = wrap::miopenGetRNNParamsSize(\n        miopen_handle /*handle*/, rnn_desc.handle() /*rnnDesc*/,\n        input_desc /*xDesc*/, &params_size /*sizeInBytes*/,\n        rnn_desc.data_type() /*dataType*/);\n    RETURN_IF_MIOPEN_ERROR(status, \"MIOpen fails to get RNN parameter size\");\n    params_size_in_bytes_ = static_cast<int64_t>(params_size);\n  }\n\n  {\n    // Create the params descriptor.\n    auto status = wrap::miopenCreateTensorDescriptor(&handle_);\n    RETURN_IF_MIOPEN_ERROR(status,\n                           \"MIOpen fails to create RNN params descriptor\");\n    status = wrap::miopenGetRNNParamsDescriptor(miopen_handle,\n                                                rnn_desc.handle(), input_desc,\n                                                handle_, rnn_desc.data_type());\n    RETURN_IF_MIOPEN_ERROR(status,\n                           \"MIOpen fails to update RNN filter descriptor\");\n  }\n  {\n    // Release the dummy input tensor descriptor.\n    auto status = wrap::miopenDestroyTensorDescriptor(input_desc);\n    RETURN_IF_MIOPEN_ERROR(status, \"MIOpen fails to destroy tensor descriptor\");\n  }\n}\n\nclass MIOpenCTCLossDescriptor {\n public:\n  explicit MIOpenCTCLossDescriptor(miopenDataType_t data_type) {\n    auto status = wrap::miopenCreateCTCLossDescriptor(&handle_);\n    if (status != miopenStatusSuccess) {\n      LOG(FATAL) << \"call to miopenCreateCTCLossDescriptor failed: \"\n                 << ToString(status);\n    }\n\n    bool apply_softmax_layer = true;\n    status = wrap::miopenSetCTCLossDescriptor(handle_, data_type, 0,\n                                              apply_softmax_layer);\n    if (status != miopenStatusSuccess) {\n      LOG(FATAL) << \"call to miopenSetCTCLossDescriptor failed: \"\n                 << ToString(status);\n    }\n  }\n\n  ~MIOpenCTCLossDescriptor() {\n    auto status = wrap::miopenDestroyCTCLossDescriptor(handle_);\n    if (status != miopenStatusSuccess) {\n      LOG(FATAL) << \"call to miopenDestroyCTCLossDescriptor failed: \"\n                 << ToString(status);\n    }\n  }\n\n  miopenCTCLossDescriptor_t handle() const { return handle_; }\n\n private:\n  miopenCTCLossDescriptor_t handle_;  // Owned\n\n  SE_DISALLOW_COPY_AND_ASSIGN(MIOpenCTCLossDescriptor);\n};\n\ntsl::Status MIOpenSupport::DoPrepareForCtcLoss(\n    Stream* stream, dnn::DataType element_type,\n    const dnn::RnnStateTensorDescriptor& probs_desc,\n    const dnn::RnnStateTensorDescriptor& grads_desc,\n    absl::Span<const int> labels_data,\n    absl::Span<const int> labels_lengths_data,\n    absl::Span<const int> input_lengths_data,\n    ScratchAllocator* scratch_allocator, DeviceMemory<uint8>* scratch_memory,\n    int* ctc_loss_algo_id) {\n  auto miopen = miopen_->GetHandle(parent_, stream);\n\n  MIOpenCTCLossDescriptor miopen_ctc_loss_desc(ToMIOpenDataType(element_type));\n\n  // Query the workspace size.\n  size_t workspace_size_in_bytes = 0;\n\n  const MIOpenRnnStateTensorDescriptor& miopen_probs_desc =\n      static_cast<const MIOpenRnnStateTensorDescriptor&>(probs_desc);\n\n  const MIOpenRnnStateTensorDescriptor& miopen_grads_desc =\n      static_cast<const MIOpenRnnStateTensorDescriptor&>(grads_desc);\n\n  auto status = wrap::miopenGetCTCLossWorkspaceSize(\n      miopen.handle(), miopen_probs_desc.handle(), miopen_grads_desc.handle(),\n      labels_data.data(), labels_lengths_data.data(), input_lengths_data.data(),\n      MIOPEN_CTC_LOSS_ALGO_DETERMINISTIC, miopen_ctc_loss_desc.handle(),\n      &workspace_size_in_bytes);\n\n  if (status != miopenStatusSuccess) {\n    LOG(FATAL) << \"call to miopenDestroyCTCLossDescriptor failed: \"\n               << ToString(status);\n    return tsl::errors::Internal(\n        \"Failed to determine scratch memory size for MIOpen CTC Loss\");\n  }\n\n  *scratch_memory = DeviceMemory<uint8>();\n\n  // Allocate the workspace.\n  if (workspace_size_in_bytes != 0) {\n    if (scratch_allocator == nullptr) {\n      return tsl::errors::Internal(\n          \"An allocator must be specified when scratch memory is needed\");\n    }\n    auto scratch_or = scratch_allocator->AllocateBytes(workspace_size_in_bytes);\n    if (scratch_or.ok()) {\n      *scratch_memory = scratch_or.value();\n    } else {\n      LOG(ERROR)\n          << \"Failed to allocate scratch memory - \"\n          << scratch_or.status().error_message() << \"\\n\"\n          << \"\\tYou can set the env var TF_CUDNN_WORKSPACE_LIMIT_IN_MB to a \"\n             \"larger number (e.g. 8192) to increase the max memory limit.\\n\"\n          << \"\\tIncreasing the max memory limit might help resolve this \"\n             \"error\";\n      return tsl::errors::Internal(\n          \"Failed to allocate scratch memory for MIOpen CTC Loss, of size: \",\n          workspace_size_in_bytes);\n    }\n  }\n\n  return tsl::OkStatus();\n}\n\ntsl::Status MIOpenSupport::DoCtcLossImpl(\n    Stream* stream, const MIOpenRnnStateTensorDescriptor& probs_desc,\n    const DeviceMemoryBase probs_data, absl::Span<const int> labels_data,\n    absl::Span<const int> labels_lengths_data,\n    absl::Span<const int> input_lengths_data, DeviceMemoryBase costs_data,\n    const MIOpenRnnStateTensorDescriptor& grads_desc,\n    DeviceMemoryBase grads_data, const MIOpenCTCLossDescriptor& ctc_loss_desc,\n    DeviceMemory<uint8> scratch_memory, int ctc_loss_algo_id) {\n  auto miopen = miopen_->GetHandle(parent_, stream);\n\n  int kNumTimestamps = probs_desc.num_layers();\n  int kBatchSize = probs_desc.batch_size();\n  int kNumLabels = probs_desc.data_size();\n  int total_size = kNumLabels * kNumTimestamps * kBatchSize;\n  (void)total_size;\n\n  auto status = wrap::miopenCTCLoss(\n      miopen.handle(), probs_desc.handle(), probs_data.opaque(),\n      labels_data.data(), labels_lengths_data.data(), input_lengths_data.data(),\n      costs_data.opaque(), grads_desc.handle(), grads_data.opaque(),\n      MIOPEN_CTC_LOSS_ALGO_DETERMINISTIC, ctc_loss_desc.handle(),\n      scratch_memory.opaque(), scratch_memory.size());\n  if (status != miopenStatusSuccess) {\n    LOG(FATAL) << \"call to miopenCTCLoss failed: \" << ToString(status);\n    return tsl::errors::Internal(\"Failure during MIOpen CTC Loss\");\n  }\n\n  return tsl::OkStatus();\n}\n\ntsl::Status MIOpenSupport::DoCtcLoss(\n    Stream* stream, dnn::DataType element_type,\n    const dnn::RnnStateTensorDescriptor& probs_desc,\n    const DeviceMemoryBase probs_data, absl::Span<const int> labels_data,\n    absl::Span<const int> labels_lengths_data,\n    absl::Span<const int> input_lengths_data, DeviceMemoryBase costs_data,\n    const dnn::RnnStateTensorDescriptor& grads_desc,\n    DeviceMemoryBase grads_data, DeviceMemory<uint8> scratch_memory,\n    int ctc_loss_algo_id) {\n  // Current MIOPen CTC Loss only supports the float datatype\n  if (element_type != dnn::DataType::kFloat) {\n    return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                       \"MIOpenCTCLossDescriptor is supported only when the \"\n                       \"DataType is float\");\n  }\n\n  MIOpenCTCLossDescriptor miopen_ctc_loss_desc(ToMIOpenDataType(element_type));\n\n  const MIOpenRnnStateTensorDescriptor& miopen_probs_desc =\n      static_cast<const MIOpenRnnStateTensorDescriptor&>(probs_desc);\n\n  const MIOpenRnnStateTensorDescriptor& miopen_grads_desc =\n      static_cast<const MIOpenRnnStateTensorDescriptor&>(grads_desc);\n\n  return DoCtcLossImpl(stream, miopen_probs_desc, probs_data, labels_data,\n                       labels_lengths_data, input_lengths_data, costs_data,\n                       miopen_grads_desc, grads_data, miopen_ctc_loss_desc,\n                       scratch_memory, ctc_loss_algo_id);\n}\n\ntsl::StatusOr<std::unique_ptr<dnn::RnnDescriptor>>\nMIOpenSupport::createRnnDescriptor(\n    int num_layers, int hidden_size, int input_size, int cell_size,\n    int batch_size, dnn::RnnInputMode input_mode,\n    dnn::RnnDirectionMode direction_mode, dnn::RnnMode rnn_mode,\n    dnn::DataType data_type, const dnn::AlgorithmConfig& algorithm_config,\n    float dropout, uint64_t seed, ScratchAllocator* state_allocator,\n    bool use_padded_io) {\n  // ROCM TODO: batch_size is used in dynamic persistent RNN algorithm and is\n  // not supported by MIOpen now.\n  if (use_padded_io) {\n    return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                       \"ROCm MIOpen only supports packed input output.\");\n  }\n\n  bool use_projection = cell_size != 0 && hidden_size < cell_size;\n  if (use_projection) {\n    return tsl::Status(\n        tsl::error::INVALID_ARGUMENT,\n        \"ROCm MIOpen does not support RNN ProjectionLayers yet.\");\n  }\n\n  auto miopen = miopen_->GetHandle(parent_, nullptr);\n  std::unique_ptr<MIOpenRnnDescriptor> rnn_desc(new MIOpenRnnDescriptor(\n      miopen.handle(), num_layers, hidden_size, input_size,\n      ToMIOpenRnnInputMode(input_mode),\n      ToMIOpenRnnDirectionMode(direction_mode), ToMIOpenRnnMode(rnn_mode),\n      ToMIOpenDataType(data_type), dropout, seed, state_allocator));\n  if (!rnn_desc->ok()) {\n    return rnn_desc->Status();\n  }\n  return tsl::StatusOr<std::unique_ptr<dnn::RnnDescriptor>>(\n      std::move(rnn_desc));\n}\n\ntsl::StatusOr<std::unique_ptr<dnn::RnnSequenceTensorDescriptor>>\nMIOpenSupport::createRnnSequenceTensorDescriptor(int seq_length, int batch_size,\n                                                 int data_size,\n                                                 dnn::DataType data_type) {\n  std::unique_ptr<MIOpenRnnSequenceTensorDescriptor> seq_desc(\n      new MIOpenRnnSequenceTensorDescriptor(seq_length, batch_size, data_size,\n                                            ToMIOpenDataType(data_type)));\n  if (!seq_desc->ok()) {\n    return seq_desc->Status();\n  }\n  return tsl::StatusOr<std::unique_ptr<dnn::RnnSequenceTensorDescriptor>>(\n      std::move(seq_desc));\n}\n\ntsl::StatusOr<std::unique_ptr<dnn::RnnStateTensorDescriptor>>\nMIOpenSupport::createRnnStateTensorDescriptor(int num_layer, int batch_size,\n                                              int data_size,\n                                              dnn::DataType data_type) {\n  std::unique_ptr<MIOpenRnnStateTensorDescriptor> state_desc(\n      new MIOpenRnnStateTensorDescriptor(num_layer, batch_size, data_size,\n                                         ToMIOpenDataType(data_type)));\n  if (!state_desc->ok()) {\n    return state_desc->Status();\n  }\n  return tsl::StatusOr<std::unique_ptr<dnn::RnnStateTensorDescriptor>>(\n      std::move(state_desc));\n}\n\nbool MIOpenSupport::DoRnnForward(\n    Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n    const dnn::RnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<Eigen::half>& input_data,\n    const DeviceMemory<int>& seq_lengths_data,\n    const dnn::RnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<Eigen::half>& input_h_data,\n    const dnn::RnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<Eigen::half>& input_c_data,\n    const DeviceMemory<Eigen::half>& params,\n    const dnn::RnnSequenceTensorDescriptor& output_desc,\n    DeviceMemory<Eigen::half>* output_data,\n    const dnn::RnnStateTensorDescriptor& output_h_desc,\n    DeviceMemory<Eigen::half>* output_h_data,\n    const dnn::RnnStateTensorDescriptor& output_c_desc,\n    DeviceMemory<Eigen::half>* output_c_data, bool is_training,\n    ScratchAllocator* reserve_space_allocator,\n    ScratchAllocator* workspace_allocator,\n    dnn::ProfileResult* output_profile_result) {\n  // ROCM TODO: output_profile_result is ignore for now\n\n  const MIOpenRnnDescriptor& miopen_rnn_desc =\n      static_cast<const MIOpenRnnDescriptor&>(rnn_desc);\n  const MIOpenRnnSequenceTensorDescriptor& miopen_input_desc =\n      static_cast<const MIOpenRnnSequenceTensorDescriptor&>(input_desc);\n  const MIOpenRnnStateTensorDescriptor& miopen_input_h_desc =\n      static_cast<const MIOpenRnnStateTensorDescriptor&>(input_h_desc);\n  const MIOpenRnnStateTensorDescriptor& miopen_input_c_desc =\n      static_cast<const MIOpenRnnStateTensorDescriptor&>(input_c_desc);\n  const MIOpenRnnSequenceTensorDescriptor& miopen_output_desc =\n      static_cast<const MIOpenRnnSequenceTensorDescriptor&>(output_desc);\n  const MIOpenRnnStateTensorDescriptor& miopen_output_h_desc =\n      static_cast<const MIOpenRnnStateTensorDescriptor&>(output_h_desc);\n  const MIOpenRnnStateTensorDescriptor& miopen_output_c_desc =\n      static_cast<const MIOpenRnnStateTensorDescriptor&>(output_c_desc);\n\n  return DoRnnForwardImpl<Eigen::half>(\n      stream, miopen_rnn_desc, miopen_input_desc, input_data,\n      miopen_input_h_desc, input_h_data, miopen_input_c_desc, input_c_data,\n      params, miopen_output_desc, output_data, miopen_output_h_desc,\n      output_h_data, miopen_output_c_desc, output_c_data, is_training,\n      reserve_space_allocator, workspace_allocator);\n}\n\nbool MIOpenSupport::DoRnnForward(\n    Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n    const dnn::RnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<float>& input_data,\n    const DeviceMemory<int>& seq_lengths_data,\n    const dnn::RnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<float>& input_h_data,\n    const dnn::RnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<float>& input_c_data, const DeviceMemory<float>& params,\n    const dnn::RnnSequenceTensorDescriptor& output_desc,\n    DeviceMemory<float>* output_data,\n    const dnn::RnnStateTensorDescriptor& output_h_desc,\n    DeviceMemory<float>* output_h_data,\n    const dnn::RnnStateTensorDescriptor& output_c_desc,\n    DeviceMemory<float>* output_c_data, bool is_training,\n    ScratchAllocator* reserve_space_allocator,\n    ScratchAllocator* workspace_allocator,\n    dnn::ProfileResult* output_profile_result) {\n  // ROCM TODO: output_profile_result is ignore for now\n\n  const MIOpenRnnDescriptor& miopen_rnn_desc =\n      static_cast<const MIOpenRnnDescriptor&>(rnn_desc);\n  const MIOpenRnnSequenceTensorDescriptor& miopen_input_desc =\n      static_cast<const MIOpenRnnSequenceTensorDescriptor&>(input_desc);\n  const MIOpenRnnStateTensorDescriptor& miopen_input_h_desc =\n      static_cast<const MIOpenRnnStateTensorDescriptor&>(input_h_desc);\n  const MIOpenRnnStateTensorDescriptor& miopen_input_c_desc =\n      static_cast<const MIOpenRnnStateTensorDescriptor&>(input_c_desc);\n  const MIOpenRnnSequenceTensorDescriptor& miopen_output_desc =\n      static_cast<const MIOpenRnnSequenceTensorDescriptor&>(output_desc);\n  const MIOpenRnnStateTensorDescriptor& miopen_output_h_desc =\n      static_cast<const MIOpenRnnStateTensorDescriptor&>(output_h_desc);\n  const MIOpenRnnStateTensorDescriptor& miopen_output_c_desc =\n      static_cast<const MIOpenRnnStateTensorDescriptor&>(output_c_desc);\n\n  return DoRnnForwardImpl<float>(\n      stream, miopen_rnn_desc, miopen_input_desc, input_data,\n      miopen_input_h_desc, input_h_data, miopen_input_c_desc, input_c_data,\n      params, miopen_output_desc, output_data, miopen_output_h_desc,\n      output_h_data, miopen_output_c_desc, output_c_data, is_training,\n      reserve_space_allocator, workspace_allocator);\n}\n\nbool MIOpenSupport::DoRnnForward(\n    Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n    const dnn::RnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<double>& input_data,\n    const DeviceMemory<int>& seq_lengths_data,\n    const dnn::RnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<double>& input_h_data,\n    const dnn::RnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<double>& input_c_data,\n    const DeviceMemory<double>& params,\n    const dnn::RnnSequenceTensorDescriptor& output_desc,\n    DeviceMemory<double>* output_data,\n    const dnn::RnnStateTensorDescriptor& output_h_desc,\n    DeviceMemory<double>* output_h_data,\n    const dnn::RnnStateTensorDescriptor& output_c_desc,\n    DeviceMemory<double>* output_c_data, bool is_training,\n    ScratchAllocator* reserve_space_allocator,\n    ScratchAllocator* workspace_allocator,\n    dnn::ProfileResult* output_profile_result) {\n  LOG(ERROR) << \"miopen does not support double type RNN fwd yet\";\n  return false;\n}\n\nbool MIOpenSupport::DoRnnBackward(\n    Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n    const dnn::RnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<Eigen::half>& input_data,\n    const DeviceMemory<int>& seq_lengths_data,\n    const dnn::RnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<Eigen::half>& input_h_data,\n    const dnn::RnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<Eigen::half>& input_c_data,\n    const DeviceMemory<Eigen::half>& params,\n    const dnn::RnnSequenceTensorDescriptor& output_desc,\n    const DeviceMemory<Eigen::half>& output_data,\n    const dnn::RnnStateTensorDescriptor& output_h_desc,\n    const DeviceMemory<Eigen::half>& output_h_data,\n    const dnn::RnnStateTensorDescriptor& output_c_desc,\n    const DeviceMemory<Eigen::half>& output_c_data,\n    const DeviceMemory<Eigen::half>& output_backprop_data,\n    const DeviceMemory<Eigen::half>& output_h_backprop_data,\n    const DeviceMemory<Eigen::half>& output_c_backprop_data,\n    DeviceMemory<Eigen::half>* input_backprop_data,\n    DeviceMemory<Eigen::half>* input_h_backprop_data,\n    DeviceMemory<Eigen::half>* input_c_backprop_data,\n    DeviceMemory<Eigen::half>* params_backprop_data,\n    DeviceMemory<uint8>* reserve_space_data,\n    ScratchAllocator* workspace_allocator,\n    dnn::ProfileResult* output_profile_result) {\n  // ROCM TODO: output_profile_result is ignore for now\n\n  const MIOpenRnnDescriptor& miopen_rnn_desc =\n      static_cast<const MIOpenRnnDescriptor&>(rnn_desc);\n  const MIOpenRnnSequenceTensorDescriptor& miopen_input_desc =\n      static_cast<const MIOpenRnnSequenceTensorDescriptor&>(input_desc);\n  const MIOpenRnnStateTensorDescriptor& miopen_input_h_desc =\n      static_cast<const MIOpenRnnStateTensorDescriptor&>(input_h_desc);\n  const MIOpenRnnStateTensorDescriptor& miopen_input_c_desc =\n      static_cast<const MIOpenRnnStateTensorDescriptor&>(input_c_desc);\n  const MIOpenRnnSequenceTensorDescriptor& miopen_output_desc =\n      static_cast<const MIOpenRnnSequenceTensorDescriptor&>(output_desc);\n  const MIOpenRnnStateTensorDescriptor& miopen_output_h_desc =\n      static_cast<const MIOpenRnnStateTensorDescriptor&>(output_h_desc);\n  const MIOpenRnnStateTensorDescriptor& miopen_output_c_desc =\n      static_cast<const MIOpenRnnStateTensorDescriptor&>(output_c_desc);\n\n  return DoRnnBackwardImpl<Eigen::half>(\n      stream, miopen_rnn_desc, miopen_input_desc, input_data,\n      miopen_input_h_desc, input_h_data, miopen_input_c_desc, input_c_data,\n      params, miopen_output_desc, output_data, miopen_output_h_desc,\n      output_h_data, miopen_output_c_desc, output_c_data, output_backprop_data,\n      output_h_backprop_data, output_c_backprop_data, input_backprop_data,\n      input_h_backprop_data, input_c_backprop_data, params_backprop_data,\n      reserve_space_data, workspace_allocator);\n}\n\nbool MIOpenSupport::DoRnnBackward(\n    Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n    const dnn::RnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<float>& input_data,\n    const DeviceMemory<int>& seq_lengths_data,\n    const dnn::RnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<float>& input_h_data,\n    const dnn::RnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<float>& input_c_data, const DeviceMemory<float>& params,\n    const dnn::RnnSequenceTensorDescriptor& output_desc,\n    const DeviceMemory<float>& output_data,\n    const dnn::RnnStateTensorDescriptor& output_h_desc,\n    const DeviceMemory<float>& output_h_data,\n    const dnn::RnnStateTensorDescriptor& output_c_desc,\n    const DeviceMemory<float>& output_c_data,\n    const DeviceMemory<float>& output_backprop_data,\n    const DeviceMemory<float>& output_h_backprop_data,\n    const DeviceMemory<float>& output_c_backprop_data,\n    DeviceMemory<float>* input_backprop_data,\n    DeviceMemory<float>* input_h_backprop_data,\n    DeviceMemory<float>* input_c_backprop_data,\n    DeviceMemory<float>* params_backprop_data,\n    DeviceMemory<uint8>* reserve_space_data,\n    ScratchAllocator* workspace_allocator,\n    dnn::ProfileResult* output_profile_result) {\n  // ROCM TODO: output_profile_result is ignore for now\n\n  const MIOpenRnnDescriptor& miopen_rnn_desc =\n      static_cast<const MIOpenRnnDescriptor&>(rnn_desc);\n  const MIOpenRnnSequenceTensorDescriptor& miopen_input_desc =\n      static_cast<const MIOpenRnnSequenceTensorDescriptor&>(input_desc);\n  const MIOpenRnnStateTensorDescriptor& miopen_input_h_desc =\n      static_cast<const MIOpenRnnStateTensorDescriptor&>(input_h_desc);\n  const MIOpenRnnStateTensorDescriptor& miopen_input_c_desc =\n      static_cast<const MIOpenRnnStateTensorDescriptor&>(input_c_desc);\n  const MIOpenRnnSequenceTensorDescriptor& miopen_output_desc =\n      static_cast<const MIOpenRnnSequenceTensorDescriptor&>(output_desc);\n  const MIOpenRnnStateTensorDescriptor& miopen_output_h_desc =\n      static_cast<const MIOpenRnnStateTensorDescriptor&>(output_h_desc);\n  const MIOpenRnnStateTensorDescriptor& miopen_output_c_desc =\n      static_cast<const MIOpenRnnStateTensorDescriptor&>(output_c_desc);\n\n  return DoRnnBackwardImpl<float>(\n      stream, miopen_rnn_desc, miopen_input_desc, input_data,\n      miopen_input_h_desc, input_h_data, miopen_input_c_desc, input_c_data,\n      params, miopen_output_desc, output_data, miopen_output_h_desc,\n      output_h_data, miopen_output_c_desc, output_c_data, output_backprop_data,\n      output_h_backprop_data, output_c_backprop_data, input_backprop_data,\n      input_h_backprop_data, input_c_backprop_data, params_backprop_data,\n      reserve_space_data, workspace_allocator);\n}\n\nbool MIOpenSupport::DoRnnBackward(\n    Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n    const dnn::RnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<double>& input_data,\n    const DeviceMemory<int>& seq_lengths_data,\n    const dnn::RnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<double>& input_h_data,\n    const dnn::RnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<double>& input_c_data,\n    const DeviceMemory<double>& params,\n    const dnn::RnnSequenceTensorDescriptor& output_desc,\n    const DeviceMemory<double>& output_data,\n    const dnn::RnnStateTensorDescriptor& output_h_desc,\n    const DeviceMemory<double>& output_h_data,\n    const dnn::RnnStateTensorDescriptor& output_c_desc,\n    const DeviceMemory<double>& output_c_data,\n    const DeviceMemory<double>& output_backprop_data,\n    const DeviceMemory<double>& output_h_backprop_data,\n    const DeviceMemory<double>& output_c_backprop_data,\n    DeviceMemory<double>* input_backprop_data,\n    DeviceMemory<double>* input_h_backprop_data,\n    DeviceMemory<double>* input_c_backprop_data,\n    DeviceMemory<double>* params_backprop_data,\n    DeviceMemory<uint8>* reserve_space_data,\n    ScratchAllocator* workspace_allocator,\n    dnn::ProfileResult* output_profile_result) {\n  LOG(ERROR) << \"miopen does not support half type RNN bwd yet\";\n  return false;\n}\n\n// This is the context required to use the TF scratch allocator:\nstruct MIOpenAllocatorContext {\n  MIOpenAllocatorContext(ScratchAllocator* scratch_allocator, Stream* stream)\n      : scratch_allocator_(scratch_allocator), stream_(stream) {}\n\n  ScratchAllocator* scratch_allocator_;\n  Stream* stream_;\n};\n\nvoid* MIOpenAllocatorCallback(void* ctx, size_t size_in_bytes) {\n  auto* mac = static_cast<MIOpenAllocatorContext*>(ctx);\n  auto allocated = mac->scratch_allocator_->AllocateBytes(size_in_bytes);\n\n  DeviceMemory<uint8> scratch;\n  if (allocated.ok()) {\n    scratch = allocated.value();\n    return scratch.opaque();\n  } else {\n    return nullptr;\n  }\n}\n\nvoid MIOpenDeallocatorCallback(void* ctx, void* mem) {\n  // Don't need deallocator since the TensorFlow heap will automatically\n  // reclaim the memory\n}\n\ntsl::Status MIOpenSupport::DoPrepareForConvolution(\n    dnn::ConvolutionKind kind, dnn::DataType element_type, Stream* stream,\n    const dnn::BatchDescriptor& input_descriptor, DeviceMemoryBase input_data,\n    const dnn::FilterDescriptor& filter_descriptor,\n    DeviceMemoryBase filter_data, const dnn::BatchDescriptor& output_descriptor,\n    DeviceMemoryBase output_data,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    const dnn::AlgorithmConfig& algorithm_config,\n    ScratchAllocator* scratch_allocator, dnn::AlgorithmDesc* algorithm_desc,\n    DeviceMemory<uint8>* scratch_memory) {\n  std::optional<dnn::AlgorithmDesc> input_algo_desc =\n      algorithm_config.algorithm();\n\n  assert(input_algo_desc.has_value());\n\n  // An algorithm has been specified.\n  *algorithm_desc = *input_algo_desc;\n\n  assert(algorithm_config.scratch_size().has_value());\n\n  size_t scratch_memory_size = *(algorithm_config.scratch_size());\n\n  // allocate scratch memory\n  if (scratch_memory_size != 0) {\n    if (scratch_allocator == nullptr) {\n      return tsl::errors::Internal(\n          \"An allocator must be specified when scratch memory is needed\");\n    }\n    auto allocated = scratch_allocator->AllocateBytes(scratch_memory_size);\n    if (allocated.ok()) {\n      *scratch_memory = allocated.value();\n    } else {\n      LOG(ERROR)\n          << \"Failed to allocate scratch memory - \"\n          << allocated.status().error_message() << \"\\n\"\n          << \"\\tYou can set the env var TF_CUDNN_WORKSPACE_LIMIT_IN_MB to a \"\n             \"larger number (e.g. 8192) to increase the max memory limit.\\n\"\n          << \"\\tIncreasing the max memory limit might help resolve this \"\n             \"error\";\n      return tsl::errors::Internal(\n          \"Failed to allocate scratch memory of size: \", scratch_memory_size);\n    }\n  }\n\n  return tsl::OkStatus();\n}\n\nclass RocmConvRunner : public dnn::ConvRunner {\n public:\n  RocmConvRunner(GpuExecutor* parent, MIOpenAccess* miopen, int64_t algo_id,\n                 size_t workspace_size, dnn::ConvolutionKind kind,\n                 dnn::DataType input_type, bool use_immediate_mode,\n                 BatchDescriptor input_descriptor,\n                 BatchDescriptor output_descriptor,\n                 FilterDescriptor filter_descriptor,\n                 ConvolutionDescriptor conv_descriptor)\n      : parent_(parent),\n        miopen_(miopen),\n        algo_id_(algo_id),\n        workspace_size_(workspace_size),\n        kind_(kind),\n        use_immediate_mode_(use_immediate_mode),\n        input_desc_{input_descriptor, ToMIOpenDataType(input_type)},\n        output_desc_{output_descriptor, ToMIOpenDataType(input_type)},\n        filter_desc_{filter_descriptor, ToMIOpenDataType(input_type)},\n        conv_desc_{conv_descriptor, ToMIOpenDataType(input_type)} {}\n\n  std::string ToString() const override {\n    return dnn::AlgorithmDesc{algo_id_, false, workspace_size_}.ToString();\n  }\n\n  size_t GetWorkspaceSize() const override { return workspace_size_; }\n\n  tsl::StatusOr<AlgorithmDesc> ToAlgorithmDesc() const override {\n    return {{algo_id_, false, workspace_size_}};\n  }\n\n  tsl::Status operator()(Stream* stream, dnn::ProfileResult* profile_result,\n                         DeviceMemoryBase scratch_memory,\n                         DeviceMemoryBase input_data,\n                         DeviceMemoryBase filter_data,\n                         DeviceMemoryBase output_data) const override {\n    auto miopen = miopen_->GetHandle(parent_, stream);\n    // Alpha is the scaling factor for input.\n    float alpha = 1.0;\n    // Beta is the scaling factor for output.\n    float beta = 0.0;\n\n    const bool is_profiling = profile_result != nullptr;\n\n    std::unique_ptr<GpuTimer> timer;\n    if (is_profiling) {\n      timer.reset(new GpuTimer(parent_));\n      if (!timer->Init()) {\n        return tsl::Status(tsl::error::INTERNAL, \"Failed to init timer\");\n      }\n      // The start and stop of the timer should be as close to the MIOpen call\n      // as possible. It is still possible for other threads to issue workload\n      // on to this stream. So it could take multiple profiling measurements.\n      if (!timer->Start(AsGpuStream(stream))) {\n        timer->Destroy();\n        return tsl::Status(tsl::error::INTERNAL, \"Failed to start timer\");\n      }\n    }\n\n    miopenStatus_t status = miopenStatusSuccess;\n    switch (kind_) {\n      case dnn::ConvolutionKind::FORWARD: {\n        if (use_immediate_mode_) {\n          status = wrap::miopenConvolutionForwardImmediate(\n              miopen.handle(), filter_desc_.handle(), filter_data.opaque(),\n              input_desc_.handle(), input_data.opaque(), conv_desc_.handle(),\n              output_desc_.handle(), output_data.opaque(),\n              scratch_memory.opaque(), scratch_memory.size(),\n              static_cast<uint64_t>(algo_id_));\n        } else {\n          status = wrap::miopenConvolutionForward(\n              miopen.handle(), &alpha, input_desc_.handle(),\n              input_data.opaque(), filter_desc_.handle(), filter_data.opaque(),\n              conv_desc_.handle(),\n              static_cast<miopenConvFwdAlgorithm_t>(algo_id_), &beta,\n              output_desc_.handle(), output_data.opaque(),\n              scratch_memory.opaque(), scratch_memory.size());\n        }\n\n        break;\n      }\n      case dnn::ConvolutionKind::BACKWARD_DATA: {\n        if (use_immediate_mode_) {\n          status = wrap::miopenConvolutionBackwardDataImmediate(\n              miopen.handle(), output_desc_.handle(), output_data.opaque(),\n              filter_desc_.handle(), filter_data.opaque(), conv_desc_.handle(),\n              input_desc_.handle(), input_data.opaque(),\n              scratch_memory.opaque(), scratch_memory.size(),\n              static_cast<uint64_t>(algo_id_));\n        } else {\n          status = wrap::miopenConvolutionBackwardData(\n              miopen.handle(), &alpha, output_desc_.handle(),\n              output_data.opaque(), filter_desc_.handle(), filter_data.opaque(),\n              conv_desc_.handle(),\n              static_cast<miopenConvBwdDataAlgorithm_t>(algo_id_), &beta,\n              input_desc_.handle(), input_data.opaque(),\n              scratch_memory.opaque(), scratch_memory.size());\n        }\n        break;\n      }\n      case dnn::ConvolutionKind::BACKWARD_FILTER: {\n        if (use_immediate_mode_) {\n          status = wrap::miopenConvolutionBackwardWeightsImmediate(\n              miopen.handle(), output_desc_.handle(), output_data.opaque(),\n              input_desc_.handle(), input_data.opaque(), conv_desc_.handle(),\n              filter_desc_.handle(), filter_data.opaque(),\n              scratch_memory.opaque(), scratch_memory.size(),\n              static_cast<uint64_t>(algo_id_));\n        } else {\n          status = wrap::miopenConvolutionBackwardWeights(\n              miopen.handle(), &alpha, output_desc_.handle(),\n              output_data.opaque(), input_desc_.handle(), input_data.opaque(),\n              conv_desc_.handle(),\n              static_cast<miopenConvBwdWeightsAlgorithm_t>(algo_id_), &beta,\n              filter_desc_.handle(), filter_data.opaque(),\n              scratch_memory.opaque(), scratch_memory.size());\n        }\n        break;\n      }\n      default:\n        return tsl::errors::Internal(\"Unexpected convolution kind \",\n                                     static_cast<int>(kind_));\n    }\n\n    if (is_profiling) {\n      if (!timer->Stop(AsGpuStream(stream))) {\n        timer->Destroy();\n        return tsl::Status(tsl::error::INTERNAL, \"Failed to stop timer\");\n      }\n      if (status == miopenStatusSuccess) {\n        dnn::AlgorithmDesc algotype(algo_id_, false);\n        profile_result->set_algorithm(algotype);\n        profile_result->set_elapsed_time_in_ms(timer->GetElapsedMilliseconds());\n        profile_result->set_scratch_size(scratch_memory.size());\n      }\n      timer->Destroy();\n    }\n\n    if (status != miopenStatusSuccess) {\n      return tsl::errors::Internal(\"Failed to enqueue convolution on stream: \",\n                                   ::stream_executor::gpu::ToString(status));\n    }\n\n    return tsl::OkStatus();\n  }\n\n private:\n  GpuExecutor* parent_;\n  MIOpenAccess* miopen_;\n  int64_t algo_id_;\n  size_t workspace_size_;\n  dnn::ConvolutionKind kind_;\n  bool use_immediate_mode_;\n\n  ScopedTensorDescriptor input_desc_;\n  ScopedTensorDescriptor output_desc_;\n  ScopedFilterDescriptor filter_desc_;\n  ScopedConvolutionDescriptor conv_desc_;\n};\n\ntsl::Status MIOpenSupport::DoConvolve(\n    dnn::ConvolutionKind kind, dnn::DataType element_type,\n    dnn::DataType output_type, Stream* stream,\n    const dnn::BatchDescriptor& input_descriptor, DeviceMemoryBase input_data,\n    const dnn::FilterDescriptor& filter_descriptor,\n    DeviceMemoryBase filter_data, const dnn::BatchDescriptor& output_descriptor,\n    DeviceMemoryBase output_data,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    dnn::AlgorithmDesc algorithm_desc, DeviceMemory<uint8> scratch_memory,\n    dnn::ProfileResult* output_profile_result) {\n  TF_ASSIGN_OR_RETURN(\n      auto runner,\n      ConvolveRunnerFromDesc(stream, algorithm_desc, kind, element_type,\n                             output_type, input_descriptor, filter_descriptor,\n                             output_descriptor, convolution_descriptor));\n\n  return (*runner)(stream, output_profile_result, scratch_memory, input_data,\n                   filter_data, output_data);\n}\n\nbool MIOpenSupport::GetConvolveAlgorithms(\n    // ROCM TODO: refactor cc_major / cc_minor\n    CudaComputeCapability cuda_compute_capability, dnn::DataType input_type,\n    std::vector<dnn::AlgorithmDesc>* out_algorithms) {\n  out_algorithms->assign({\n      // clang-format off\n      dnn::AlgorithmDesc(miopenConvolutionFwdAlgoGEMM, false),\n      dnn::AlgorithmDesc(miopenConvolutionFwdAlgoDirect, false),\n      dnn::AlgorithmDesc(miopenConvolutionFwdAlgoFFT, false),\n      dnn::AlgorithmDesc(miopenConvolutionFwdAlgoWinograd, false),\n      // clang-format on\n  });\n  return true;\n}\n\ntsl::Status MIOpenSupport::GetConvolveRunners(\n    bool use_cudnn_frontend, dnn::ConvolutionKind kind,\n    dnn::DataType input_type, dnn::DataType output_type, Stream* stream,\n    const dnn::BatchDescriptor& input_descriptor, DeviceMemoryBase input_data,\n    const dnn::FilterDescriptor& filter_descriptor,\n    DeviceMemoryBase filter_data, const dnn::BatchDescriptor& output_descriptor,\n    DeviceMemoryBase output_data,\n    const dnn::ConvolutionDescriptor& convolution_descriptor, bool use_fallback,\n    ScratchAllocator* scratch_allocator,\n    std::vector<std::unique_ptr<const dnn::ConvRunner>>* out_runners) {\n  if (input_type != output_type) {\n    return tsl::errors::Unimplemented(\n        absl::StrFormat(\"MIOpen backend does not support different input and \"\n                        \"output types: %d != %d\",\n                        input_type, output_type));\n  }\n\n  std::vector<dnn::ProfileResult> profile_results;\n  if (!GetMIOpenConvolveAlgorithms(\n          kind, input_type, stream, input_descriptor, input_data,\n          filter_descriptor, filter_data, output_descriptor, output_data,\n          convolution_descriptor, scratch_allocator, &profile_results)) {\n    return tsl::Status(\n        tsl::error::UNKNOWN,\n        \"GetConvolveRunners: GetMIOpenConvolveAlgorithms failed\");\n  }\n\n  for (const auto& profile_result : profile_results) {\n    TF_ASSIGN_OR_RETURN(\n        auto runner, ConvolveRunnerFromDesc(\n                         stream, profile_result.algorithm(), kind, input_type,\n                         output_type, input_descriptor, filter_descriptor,\n                         output_descriptor, convolution_descriptor));\n    out_runners->push_back(std::move(runner));\n  }\n\n  return tsl::OkStatus();\n}\n\ntsl::StatusOr<std::unique_ptr<const dnn::ConvRunner>>\nMIOpenSupport::ConvolveRunnerFromDesc(\n    Stream* stream, const dnn::AlgorithmDesc& algorithm_desc,\n    dnn::ConvolutionKind kind, dnn::DataType input_type,\n    dnn::DataType output_type, const dnn::BatchDescriptor& input_descriptor,\n    const dnn::FilterDescriptor& filter_descriptor,\n    const dnn::BatchDescriptor& output_descriptor,\n    const dnn::ConvolutionDescriptor& convolution_descriptor) {\n  if (input_type != output_type) {\n    return tsl::errors::Unimplemented(\n        absl::StrFormat(\"MIOpen backend does not support different input and \"\n                        \"output types: %d != %d\",\n                        input_type, output_type));\n  }\n\n  auto workspace_size = algorithm_desc.workspace_size();\n  if (!workspace_size) {\n    return tsl::errors::InvalidArgument(\n        \"MIOpenSupport::ConvolveRunnerFromDesc requires \"\n        \"AlgorithmProto.workspace_size, but it was missing.\");\n  }\n  return {std::make_unique<RocmConvRunner>(\n      parent_, miopen_.get(), algorithm_desc.algo_id(), *workspace_size, kind,\n      input_type, use_immediate_mode_, input_descriptor, output_descriptor,\n      filter_descriptor, convolution_descriptor)};\n}\n\nbool MIOpenSupport::GetMIOpenConvolveAlgorithms(\n    dnn::ConvolutionKind kind, dnn::DataType element_type, Stream* stream,\n    const dnn::BatchDescriptor& input_descriptor, DeviceMemoryBase input_data,\n    const dnn::FilterDescriptor& filter_descriptor,\n    DeviceMemoryBase filter_data, const dnn::BatchDescriptor& output_descriptor,\n    DeviceMemoryBase output_data,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    ScratchAllocator* scratch_allocator,\n    std::vector<dnn::ProfileResult>* out_algorithms) {\n  return use_immediate_mode_\n             ? GetMIOpenConvolveAlgorithmsImmediateMode(\n                   kind, element_type, stream, input_descriptor, input_data,\n                   filter_descriptor, filter_data, output_descriptor,\n                   output_data, convolution_descriptor, scratch_allocator,\n                   out_algorithms)\n             : GetMIOpenConvolveAlgorithmsFindMode(\n                   kind, element_type, stream, input_descriptor, input_data,\n                   filter_descriptor, filter_data, output_descriptor,\n                   output_data, convolution_descriptor, scratch_allocator,\n                   out_algorithms);\n}\n\nbool MIOpenSupport::GetMIOpenConvolveAlgorithmsImmediateMode(\n    dnn::ConvolutionKind kind, dnn::DataType element_type, Stream* stream,\n    const dnn::BatchDescriptor& input_descriptor, DeviceMemoryBase input_data,\n    const dnn::FilterDescriptor& filter_descriptor,\n    DeviceMemoryBase filter_data, const dnn::BatchDescriptor& output_descriptor,\n    DeviceMemoryBase output_data,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    ScratchAllocator* scratch_allocator,\n    std::vector<dnn::ProfileResult>* out_algorithms) {\n  auto miopen = miopen_->GetHandle(parent_, stream);\n\n  ScopedTensorDescriptor input_nd{input_descriptor,\n                                  ToMIOpenDataType(element_type)};\n  ScopedTensorDescriptor output_nd{output_descriptor,\n                                   ToMIOpenDataType(element_type)};\n  ScopedFilterDescriptor filter{filter_descriptor,\n                                ToMIOpenDataType(element_type)};\n  ScopedConvolutionDescriptor conv{convolution_descriptor,\n                                   ToMIOpenDataType(element_type)};\n\n  // First determine the number of algorityhms available\n  size_t maxSolutionCount = 0;\n\n  switch (kind) {\n    case dnn::ConvolutionKind::FORWARD: {\n      auto status = wrap::miopenConvolutionForwardGetSolutionCount(\n          miopen.handle(), filter.handle(), input_nd.handle(), conv.handle(),\n          output_nd.handle(), &maxSolutionCount);\n      if (status != miopenStatusSuccess) {\n        LOG(FATAL)\n            << \"call to miopenConvolutionForwardGetSolutionCount failed: \"\n            << ToString(status);\n        return false;\n      }\n      break;\n    }\n    case dnn::ConvolutionKind::BACKWARD_DATA: {\n      auto status = wrap::miopenConvolutionBackwardDataGetSolutionCount(\n          miopen.handle(), output_nd.handle(), filter.handle(), conv.handle(),\n          input_nd.handle(), &maxSolutionCount);\n      if (status != miopenStatusSuccess) {\n        LOG(FATAL) << \"call to miopenConvolutionBackwardDataGetSolutionCount \"\n                      \"failed: \"\n                   << ToString(status);\n        return false;\n      }\n      break;\n    }\n    case dnn::ConvolutionKind::BACKWARD_FILTER: {\n      auto status = wrap::miopenConvolutionBackwardWeightsGetSolutionCount(\n          miopen.handle(), output_nd.handle(), input_nd.handle(), conv.handle(),\n          filter.handle(), &maxSolutionCount);\n      if (status != miopenStatusSuccess) {\n        LOG(FATAL)\n            << \"call to miopenConvolutionBackwardWeightsGetSolutionCount \"\n               \"failed: \"\n            << ToString(status);\n        return false;\n      }\n      break;\n    }\n    default: {\n      LOG(FATAL) << \"Unexpected convolution kind \" << static_cast<int>(kind);\n      return false;\n      break;\n    }\n  }\n\n  VLOG(kConvDebugVlogLevel)\n      << \"Number of conv solutions max: \" << maxSolutionCount;\n\n  if (return_best_algo_only_) {\n    VLOG(kConvDebugVlogLevel) << \"TF_ROCM_RETURN_BEST_ALGO_ONLY is set, \"\n                              << \"setting maxSolutionCount to 1\";\n    maxSolutionCount = 1;\n  }\n\n  size_t solutionCount = 0;\n  std::unique_ptr<miopenConvSolution_t[]> solutions(\n      new miopenConvSolution_t[maxSolutionCount]);\n\n  switch (kind) {\n    case dnn::ConvolutionKind::FORWARD: {\n      auto status = wrap::miopenConvolutionForwardGetSolution(\n          miopen.handle(), filter.handle(), input_nd.handle(), conv.handle(),\n          output_nd.handle(), maxSolutionCount, &solutionCount,\n          solutions.get());\n\n      if (status != miopenStatusSuccess) {\n        LOG(FATAL) << \"call to miopenConvolutionForwardGetSolution failed: \"\n                   << ToString(status);\n        return false;\n      }\n\n      VLOG(kConvDebugVlogLevel)\n          << \"Number of conv solutions actual: \" << solutionCount;\n\n      for (size_t i = 0; i < solutionCount; i++) {\n        miopenConvSolution_t solution = solutions[i];\n\n        VLOG(kConvDebugVlogLevel)\n            << \"solution \" << i << \" (time, mem, id, algo) =  \" << solution.time\n            << \", \" << solution.workspace_size << \", \" << solution.solution_id\n            << \", \" << ToString(solution.algorithm);\n\n        status = wrap::miopenConvolutionForwardCompileSolution(\n            miopen.handle(), filter.handle(), input_nd.handle(), conv.handle(),\n            output_nd.handle(), solution.solution_id);\n\n        if (status != miopenStatusSuccess) {\n          LOG(FATAL)\n              << \"call to miopenConvolutionForwardCompileSolution failed: \"\n              << ToString(status);\n          return false;\n        }\n\n        out_algorithms->emplace_back(\n            GetProfileResultFromConvSolution(solution));\n      }\n      break;\n    }\n\n    case dnn::ConvolutionKind::BACKWARD_DATA: {\n      auto status = wrap::miopenConvolutionBackwardDataGetSolution(\n          miopen.handle(), output_nd.handle(), filter.handle(), conv.handle(),\n          input_nd.handle(), maxSolutionCount, &solutionCount, solutions.get());\n      if (status != miopenStatusSuccess) {\n        LOG(FATAL)\n            << \"call to miopenConvolutionBackwardDataGetSolution failed: \"\n            << ToString(status);\n        return false;\n      }\n\n      VLOG(kConvDebugVlogLevel)\n          << \"Number of conv solutions actual: \" << solutionCount;\n\n      for (size_t i = 0; i < solutionCount; i++) {\n        miopenConvSolution_t solution = solutions[i];\n\n        VLOG(kConvDebugVlogLevel)\n            << \"solution \" << i << \" (time, mem, id, algo) =  \" << solution.time\n            << \", \" << solution.workspace_size << \", \" << solution.solution_id\n            << \", \" << ToString(solution.algorithm);\n\n        status = wrap::miopenConvolutionBackwardDataCompileSolution(\n            miopen.handle(), output_nd.handle(), filter.handle(), conv.handle(),\n            input_nd.handle(), solution.solution_id);\n\n        if (status != miopenStatusSuccess) {\n          LOG(FATAL) << \" call to miopenConvolutionBackwardDataCompileSolution \"\n                        \"failed: \"\n                     << ToString(status);\n          return false;\n        }\n\n        out_algorithms->emplace_back(\n            GetProfileResultFromConvSolution(solution));\n      }\n      break;\n    }\n    case dnn::ConvolutionKind::BACKWARD_FILTER: {\n      auto status = wrap::miopenConvolutionBackwardWeightsGetSolution(\n          miopen.handle(), output_nd.handle(), input_nd.handle(), conv.handle(),\n          filter.handle(), maxSolutionCount, &solutionCount, solutions.get());\n      if (status != miopenStatusSuccess) {\n        LOG(FATAL)\n            << \"call to miopenConvolutionBackwardWeightsGetSolution failed: \"\n            << ToString(status);\n        return false;\n      }\n\n      VLOG(kConvDebugVlogLevel)\n          << \"Number of conv solutions actual: \" << solutionCount;\n\n      for (size_t i = 0; i < solutionCount; i++) {\n        miopenConvSolution_t solution = solutions[i];\n\n        VLOG(kConvDebugVlogLevel)\n            << \"solution \" << i << \" (time, mem, id, algo) =  \" << solution.time\n            << \", \" << solution.workspace_size << \", \" << solution.solution_id\n            << \", \" << ToString(solution.algorithm);\n\n        status = wrap::miopenConvolutionBackwardWeightsCompileSolution(\n            miopen.handle(), output_nd.handle(), input_nd.handle(),\n            conv.handle(), filter.handle(), solution.solution_id);\n\n        if (status != miopenStatusSuccess) {\n          LOG(FATAL)\n              << \"call to miopenConvolutionBackwardWeightsCompileSolution \"\n                 \"failed: \"\n              << ToString(status);\n          return false;\n        }\n\n        out_algorithms->emplace_back(\n            GetProfileResultFromConvSolution(solution));\n      }\n      break;\n    }\n    default: {\n      LOG(FATAL) << \"Unexpected convolution kind \" << static_cast<int>(kind);\n      return false;\n      break;\n    }\n  }\n\n  return true;\n}\n\nbool MIOpenSupport::GetMIOpenConvolveAlgorithmsFindMode(\n    dnn::ConvolutionKind kind, dnn::DataType element_type, Stream* stream,\n    const dnn::BatchDescriptor& input_descriptor, DeviceMemoryBase input_data,\n    const dnn::FilterDescriptor& filter_descriptor,\n    DeviceMemoryBase filter_data, const dnn::BatchDescriptor& output_descriptor,\n    DeviceMemoryBase output_data,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    ScratchAllocator* scratch_allocator,\n    std::vector<dnn::ProfileResult>* out_algorithms) {\n  auto miopen = miopen_->GetHandle(parent_, stream);\n\n  ScopedTensorDescriptor input_nd{input_descriptor,\n                                  ToMIOpenDataType(element_type)};\n  ScopedTensorDescriptor output_nd{output_descriptor,\n                                   ToMIOpenDataType(element_type)};\n  ScopedFilterDescriptor filter{filter_descriptor,\n                                ToMIOpenDataType(element_type)};\n  ScopedConvolutionDescriptor conv{convolution_descriptor,\n                                   ToMIOpenDataType(element_type)};\n\n  // Determine the workspace memory size that will need by the call to Find\n  size_t scratch_memory_size = 0;\n  switch (kind) {\n    case dnn::ConvolutionKind::FORWARD: {\n      auto status = wrap::miopenConvolutionForwardGetWorkSpaceSize(\n          miopen.handle(), filter.handle(), input_nd.handle(), conv.handle(),\n          output_nd.handle(), &scratch_memory_size);\n      if (status != miopenStatusSuccess) {\n        LOG(FATAL)\n            << \"call to miopenConvolutionForwardGetWorkspaceSize failed: \"\n            << ToString(status);\n        return false;\n      }\n      break;\n    }\n    case dnn::ConvolutionKind::BACKWARD_DATA: {\n      auto status = wrap::miopenConvolutionBackwardDataGetWorkSpaceSize(\n          miopen.handle(), output_nd.handle(), filter.handle(), conv.handle(),\n          input_nd.handle(), &scratch_memory_size);\n      if (status != miopenStatusSuccess) {\n        LOG(FATAL)\n            << \"call to miopenConvolutionBackwardDataGetWorkspaceSize failed: \"\n            << ToString(status);\n        return false;\n      }\n      break;\n    }\n    case dnn::ConvolutionKind::BACKWARD_FILTER: {\n      auto status = wrap::miopenConvolutionBackwardWeightsGetWorkSpaceSize(\n          miopen.handle(), output_nd.handle(), input_nd.handle(), conv.handle(),\n          filter.handle(), &scratch_memory_size);\n      if (status != miopenStatusSuccess) {\n        LOG(FATAL)\n            << \"call to miopenConvolutionBackwardWeightsGetWorkspaceSize \"\n               \"failed: \"\n            << ToString(status);\n        return false;\n      }\n      break;\n    }\n    default: {\n      LOG(FATAL) << \"Unexpected convolution kind \" << static_cast<int>(kind);\n      return false;\n      break;\n    }\n  }\n\n  // allocate scratch memory\n  DeviceMemory<uint8> scratch_memory;\n  if (scratch_memory_size != 0) {\n    if (scratch_allocator == nullptr) {\n      LOG(FATAL)\n          << \"An allocator must be specified when scratch memory is needed\";\n      return false;\n    }\n    auto allocated = scratch_allocator->AllocateBytes(scratch_memory_size);\n    if (allocated.ok()) {\n      scratch_memory = allocated.value();\n    } else {\n      LOG(FATAL)\n          << \"Failed to allocate scratch memory - \"\n          << allocated.status().error_message() << \"\\n\"\n          << \"\\tYou can set the env var TF_CUDNN_WORKSPACE_LIMIT_IN_MB to a \"\n             \"larger number (e.g. 8192) to increase the max memory limit.\\n\"\n          << \"\\tIncreasing the max memory limit might help resolve this \"\n             \"error\";\n      return false;\n    }\n  }\n\n  // Only get the best algorithm for Find Mode\n  size_t requestedAlgorithmCount = 1;\n\n  VLOG(kConvDebugVlogLevel)\n      << \"Number of conv algortihms to request: \" << requestedAlgorithmCount;\n\n  miopenConvAlgoPerf_t returnedAlgorithm;\n\n  int returnedAlgorithmCount = 0;\n  bool exhaustiveSearch = false;\n\n  switch (kind) {\n    case dnn::ConvolutionKind::FORWARD: {\n      auto status = wrap::miopenFindConvolutionForwardAlgorithm(\n          miopen.handle(), input_nd.handle(), input_data.opaque(),\n          filter.handle(), filter_data.opaque(), conv.handle(),\n          output_nd.handle(), output_data.opaque(), requestedAlgorithmCount,\n          &returnedAlgorithmCount, &returnedAlgorithm, scratch_memory.opaque(),\n          scratch_memory_size, exhaustiveSearch);\n      if (status != miopenStatusSuccess) {\n        LOG(FATAL) << \"call to miopenFindConvolutionForwardAlgorithm failed: \"\n                   << ToString(status);\n        return false;\n      }\n      break;\n    }\n    case dnn::ConvolutionKind::BACKWARD_DATA: {\n      auto status = wrap::miopenFindConvolutionBackwardDataAlgorithm(\n          miopen.handle(), output_nd.handle(), output_data.opaque(),\n          filter.handle(), filter_data.opaque(), conv.handle(),\n          input_nd.handle(), input_data.opaque(), requestedAlgorithmCount,\n          &returnedAlgorithmCount, &returnedAlgorithm, scratch_memory.opaque(),\n          scratch_memory_size, exhaustiveSearch);\n      if (status != miopenStatusSuccess) {\n        LOG(FATAL)\n            << \"call to miopenFindConvolutionBackwardDataAlgorithm failed: \"\n            << ToString(status);\n        return false;\n      }\n      break;\n    }\n    case dnn::ConvolutionKind::BACKWARD_FILTER: {\n      auto status = wrap::miopenFindConvolutionBackwardWeightsAlgorithm(\n          miopen.handle(), output_nd.handle(), output_data.opaque(),\n          input_nd.handle(), input_data.opaque(), conv.handle(),\n          filter.handle(), filter_data.opaque(), requestedAlgorithmCount,\n          &returnedAlgorithmCount, &returnedAlgorithm, scratch_memory.opaque(),\n          scratch_memory_size, exhaustiveSearch);\n      if (status != miopenStatusSuccess) {\n        LOG(FATAL) << \"call to miopenConvolutionBackwardWeightsAlgorithm \"\n                      \"failed: \"\n                   << ToString(status);\n        return false;\n      }\n      break;\n    }\n    default: {\n      LOG(FATAL) << \"Unexpected convolution kind \" << static_cast<int>(kind);\n      return false;\n      break;\n    }\n  }\n\n  out_algorithms->emplace_back(\n      GetProfileResultFromConvAlgoPerf(kind, returnedAlgorithm));\n\n  return true;\n}\n\nbool MIOpenSupport::GetRnnAlgorithms(\n    std::vector<dnn::AlgorithmDesc>* out_algorithms) {\n  // ROCM TODO: implement this with proper MIOpen API\n  return true;\n}\n\nbool MIOpenSupport::GetConvolveBackwardDataAlgorithms(\n    // ROCM TODO: refactor cc_major / cc_minor\n    CudaComputeCapability cuda_compute_capability, dnn::DataType input_type,\n    std::vector<dnn::AlgorithmDesc>* out_algorithms) {\n  out_algorithms->assign({\n      // clang-format off\n      dnn::AlgorithmDesc(miopenConvolutionBwdDataAlgoGEMM, false),\n      dnn::AlgorithmDesc(miopenConvolutionBwdDataAlgoDirect, false),\n      dnn::AlgorithmDesc(miopenConvolutionBwdDataAlgoFFT, false),\n      dnn::AlgorithmDesc(miopenConvolutionBwdDataAlgoWinograd, false),\n      // clang-format on\n  });\n  return true;\n}\n\nbool MIOpenSupport::GetConvolveBackwardFilterAlgorithms(\n    // ROCM TODO: refactor cc_major / cc_minor\n    CudaComputeCapability cuda_compute_capability, dnn::DataType input_type,\n    std::vector<dnn::AlgorithmDesc>* out_algorithms) {\n  out_algorithms->assign({\n      // clang-format off\n      dnn::AlgorithmDesc(miopenConvolutionBwdWeightsAlgoGEMM, false),\n      dnn::AlgorithmDesc(miopenConvolutionBwdWeightsAlgoDirect, false),\n      // clang-format on\n  });\n  return true;\n}\n\nbool MIOpenSupport::DoBatchNormalizationForward(\n    Stream* stream, const DeviceMemory<Eigen::half>& x,\n    const DeviceMemory<float>& scale, const DeviceMemory<float>& offset,\n    const DeviceMemory<float>& estimated_mean,\n    const DeviceMemory<float>& estimated_variance,\n    const DeviceMemory<Eigen::half>& side_input,\n    const dnn::BatchDescriptor& x_desc,\n    const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n    const double exponential_average_factor,\n    dnn::ActivationMode activation_mode, DeviceMemory<Eigen::half>* y,\n    DeviceMemory<float>* batch_mean, DeviceMemory<float>* batch_var,\n    DeviceMemory<float>* saved_mean, DeviceMemory<float>* saved_inv_var,\n    bool is_training, ScratchAllocator* reserve_space_allocator,\n    ScratchAllocator* workspace_allocator) {\n  return DoBatchNormalizationForwardImpl<Eigen::half, float>(\n      stream, dnn::DataType::kHalf, dnn::DataType::kFloat, x, scale, offset,\n      estimated_mean, estimated_variance, side_input, x_desc, scale_offset_desc,\n      epsilon, exponential_average_factor, activation_mode, y, batch_mean,\n      batch_var, saved_mean, saved_inv_var, is_training);\n}\n\nbool MIOpenSupport::DoBatchNormalizationForward(\n    Stream* stream, const DeviceMemory<float>& x,\n    const DeviceMemory<float>& scale, const DeviceMemory<float>& offset,\n    const DeviceMemory<float>& estimated_mean,\n    const DeviceMemory<float>& estimated_variance,\n    const DeviceMemory<float>& side_input, const dnn::BatchDescriptor& x_desc,\n    const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n    const double exponential_average_factor,\n    dnn::ActivationMode activation_mode, DeviceMemory<float>* y,\n    DeviceMemory<float>* batch_mean, DeviceMemory<float>* batch_var,\n    DeviceMemory<float>* saved_mean, DeviceMemory<float>* saved_inv_var,\n    bool is_training, ScratchAllocator* reserve_space_allocator,\n    ScratchAllocator* workspace_allocator) {\n  return DoBatchNormalizationForwardImpl<float, float>(\n      stream, dnn::DataType::kFloat, dnn::DataType::kFloat, x, scale, offset,\n      estimated_mean, estimated_variance, side_input, x_desc, scale_offset_desc,\n      epsilon, exponential_average_factor, activation_mode, y, batch_mean,\n      batch_var, saved_mean, saved_inv_var, is_training);\n}\n\ntemplate <class T, class U>\nbool MIOpenSupport::DoBatchNormalizationForwardImpl(\n    Stream* stream, dnn::DataType input_data_type,\n    dnn::DataType scale_data_type, const DeviceMemory<T>& x,\n    const DeviceMemory<U>& scale, const DeviceMemory<U>& offset,\n    const DeviceMemory<U>& estimated_mean,\n    const DeviceMemory<U>& estimated_variance,\n    const DeviceMemory<T>& side_input, const dnn::BatchDescriptor& x_desc,\n    const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n    const double exponential_average_factor,\n    dnn::ActivationMode activation_mode, DeviceMemory<T>* y,\n    DeviceMemory<U>* batch_mean, DeviceMemory<U>* batch_var,\n    DeviceMemory<U>* saved_mean, DeviceMemory<U>* saved_inv_var,\n    bool is_training) {\n  auto miopen = miopen_->GetHandle(parent_, stream);\n\n  ScopedTensorDescriptor x_descriptor{x_desc,\n                                      ToMIOpenDataType(input_data_type)};\n  ScopedTensorDescriptor scale_offset_descriptor{\n      scale_offset_desc, ToMIOpenDataType(scale_data_type)};\n  miopenBatchNormMode_t mode = miopenBNSpatial;\n  float one = 1.0;\n  float zero = 0.0;\n\n  auto status = miopenStatusInvalidValue;\n  if (is_training) {\n    status = wrap::miopenBatchNormalizationForwardTraining(\n        miopen.handle(), mode, &one, &zero, x_descriptor.handle(), x.opaque(),\n        x_descriptor.handle(), y->opaque(), scale_offset_descriptor.handle(),\n        const_cast<void*>(scale.opaque()), const_cast<void*>(offset.opaque()),\n        exponential_average_factor, batch_mean->opaque(), batch_var->opaque(),\n        epsilon, saved_mean->opaque(), saved_inv_var->opaque());\n  } else {\n    const void* maybe_inv_var = estimated_variance.opaque();\n    status = wrap::miopenBatchNormalizationForwardInference(\n        miopen.handle(), mode, &one, &zero, x_descriptor.handle(), x.opaque(),\n        x_descriptor.handle(), y->opaque(), scale_offset_descriptor.handle(),\n        const_cast<void*>(scale.opaque()), const_cast<void*>(offset.opaque()),\n        const_cast<void*>(estimated_mean.opaque()),\n        const_cast<void*>(maybe_inv_var), epsilon);\n  }\n  if (status != miopenStatusSuccess) {\n    LOG(ERROR) << \"failed to enqueue forward batch normalization on stream: \"\n               << ToString(status);\n    return false;\n  }\n  return true;\n}\n\nbool MIOpenSupport::DoBatchNormalizationBackward(\n    Stream* stream, const DeviceMemory<Eigen::half>& y_backprop,\n    const DeviceMemory<Eigen::half>& x, const DeviceMemory<float>& scale,\n    const DeviceMemory<float>& offset, const DeviceMemory<float>& mean,\n    const DeviceMemory<float>& inv_var, const DeviceMemory<Eigen::half>& y,\n    const dnn::BatchDescriptor& x_desc,\n    const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n    dnn::ActivationMode activation_mode, DeviceMemory<Eigen::half>* x_backprop,\n    DeviceMemory<float>* scale_backprop, DeviceMemory<float>* offset_backprop,\n    DeviceMemory<Eigen::half>* side_input_backprop,\n    DeviceMemory<uint8>* reserve_space_data,\n    ScratchAllocator* workspace_allocator) {\n  return DoBatchNormalizationBackwardImpl<Eigen::half, float>(\n      stream, miopenHalf, miopenFloat, y_backprop, x, scale, mean, inv_var,\n      x_desc, scale_offset_desc, epsilon, x_backprop, scale_backprop,\n      offset_backprop);\n}\n\nbool MIOpenSupport::DoBatchNormalizationBackward(\n    Stream* stream, const DeviceMemory<float>& y_backprop,\n    const DeviceMemory<float>& x, const DeviceMemory<float>& scale,\n    const DeviceMemory<float>& offset, const DeviceMemory<float>& mean,\n    const DeviceMemory<float>& variance, const DeviceMemory<float>& y,\n    const dnn::BatchDescriptor& x_desc,\n    const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n    dnn::ActivationMode activation_mode, DeviceMemory<float>* x_backprop,\n    DeviceMemory<float>* scale_backprop, DeviceMemory<float>* offset_backprop,\n    DeviceMemory<float>* side_input_backprop,\n    DeviceMemory<uint8>* reserve_space_data,\n    ScratchAllocator* workspace_allocator) {\n  return DoBatchNormalizationBackwardImpl<float, float>(\n      stream, miopenFloat, miopenFloat, y_backprop, x, scale, mean, variance,\n      x_desc, scale_offset_desc, epsilon, x_backprop, scale_backprop,\n      offset_backprop);\n}\n\ntemplate <class T, class U>\nbool MIOpenSupport::DoBatchNormalizationBackwardImpl(\n    Stream* stream, int miopen_input_type, int miopen_scale_type,\n    const DeviceMemory<T>& y_backprop, const DeviceMemory<T>& x,\n    const DeviceMemory<U>& scale, const DeviceMemory<U>& mean,\n    const DeviceMemory<U>& variance, const dnn::BatchDescriptor& x_desc,\n    const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n    DeviceMemory<T>* x_backprop, DeviceMemory<U>* scale_backprop,\n    DeviceMemory<U>* offset_backprop) {\n  auto miopen = miopen_->GetHandle(parent_, stream);\n  ScopedTensorDescriptor x_descriptor{\n      x_desc, static_cast<miopenDataType_t>(miopen_input_type)};\n  ScopedTensorDescriptor scale_offset_descriptor{\n      scale_offset_desc, static_cast<miopenDataType_t>(miopen_scale_type)};\n  miopenBatchNormMode_t mode = miopenBNSpatial;\n  float one = 1.0;\n  float zero = 0.0;\n\n  auto status = wrap::miopenBatchNormalizationBackward(\n      miopen.handle(), mode, &one, &zero, &one, &zero, x_descriptor.handle(),\n      x.opaque(), x_descriptor.handle(), y_backprop.opaque(),\n      x_descriptor.handle(), x_backprop->opaque(),\n      scale_offset_descriptor.handle(), scale.opaque(),\n      scale_backprop->opaque(), offset_backprop->opaque(), epsilon,\n      mean.opaque(), variance.opaque());\n  if (status != miopenStatusSuccess) {\n    LOG(ERROR) << \"failed to enqueue backward batch normalization on stream: \"\n               << ToString(status);\n    return false;\n  }\n  return true;\n}\n\ntsl::Status MIOpenSupport::DoFusedConvolve(\n    Stream* stream, dnn::DataType input_type, dnn::DataType side_input_type,\n    dnn::DataType bias_type, dnn::DataType output_type,\n    const dnn::BatchDescriptor& conv_input_descriptor,\n    DeviceMemoryBase conv_input_data, double conv_input_scale,\n    const dnn::FilterDescriptor& filter_descriptor,\n    DeviceMemoryBase filter_data,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    DeviceMemoryBase side_input_data, double side_input_scale,\n    const dnn::BatchDescriptor& bias_descriptor, DeviceMemoryBase biases,\n    dnn::ActivationMode activation_mode,\n    const dnn::BatchDescriptor& output_descriptor, DeviceMemoryBase output_data,\n    ScratchAllocator* scratch_allocator,\n    const dnn::AlgorithmConfig& algorithm_config,\n    dnn::ProfileResult* output_profile_result) {\n  return tsl::errors::Unimplemented(\"fused convolve not implemented yet\");\n}\n\nbool MIOpenSupport::DoTransformTensor(Stream* stream,\n                                      const dnn::BatchDescriptor& input_desc,\n                                      dnn::DataType input_type,\n                                      const DeviceMemoryBase& input_data,\n                                      const dnn::BatchDescriptor& output_desc,\n                                      dnn::DataType output_type, float scale,\n                                      DeviceMemoryBase* output_data) {\n  // ROCM TODO implement this operation\n  LOG(ERROR) << \"transform tensor not implemented yet\";\n  return false;\n}\n\nbool MIOpenSupport::DoMatMul(Stream* stream,\n                             const DeviceMemory<float>& input_data,\n                             const DeviceMemory<float>& weights,\n                             const dnn::BatchDescriptor& input_dimensions,\n                             const dnn::BatchDescriptor& output_dimensions,\n                             DeviceMemory<float>* output_data) {\n  if (input_dimensions.count() != output_dimensions.count()) {\n    LOG(ERROR) << \"MatMul input and output dimensions are not compatible.\";\n    return false;\n  }\n\n  // We do not permute the input or output, instead we just\n  // reinterpret the layout. We are working with row-major matrices\n  // and the rows of the input and output correspond to batch, so\n  // batch has to be outermost in both the input and output.\n  //\n  // By adding transposes to the BLAS gemm call we could perhaps make\n  // the kYXDepthBatch layout work as well, but there has been no need\n  // for that so far.\n  if (input_dimensions.layout() != dnn::DataLayout::kBatchYXDepth &&\n      input_dimensions.layout() != dnn::DataLayout::kBatchDepthYX) {\n    LOG(ERROR) << \"Unsupported MatMul input layout.\";\n    return false;\n  }\n  if (output_dimensions.layout() != dnn::DataLayout::kBatchYXDepth &&\n      output_dimensions.layout() != dnn::DataLayout::kBatchDepthYX) {\n    LOG(ERROR) << \"Unsupported MatMul output layout.\";\n    return false;\n  }\n\n  if (output_dimensions.width() == 1 && output_dimensions.height() == 1) {\n    // This is a fast path that also supports the kBatchYXDepth layout.\n\n    // The matrices here are in row-major format while BLAS expects\n    // column-major, i.e. our matrices are transposed as far as BLAS\n    // is concerned. So we need to compute output^T =\n    // input^T*weights^T. There is no parameter for transposing the\n    // output in BLAS gemm, but instead we can transpose both sides of\n    // the equality to see that this is equivalent to\n    // output=weights*input. So we only need to swap the order of\n    // weights and input in the matrix product to correct for the\n    // row-major versus column-major difference.\n    const int64_t m = output_dimensions.NodesAcrossFeatureMaps();\n    const int64_t n = input_dimensions.count();\n    const int64_t k = input_dimensions.NodesAcrossFeatureMaps();\n    if (!stream\n             ->ThenBlasGemm(blas::Transpose::kNoTranspose,\n                            blas::Transpose::kNoTranspose, m, n, k, weights, m,\n                            input_data, k, output_data, m,\n                            blas::kDefaultComputePrecision)\n             .ok()) {\n      return false;\n    }\n  } else {\n    // This is a slower and more complex path that supports output\n    // width() * height() > 1, though it only supports the\n    // kBatchYXDepth layout. Does support kBatchDepthYX if output\n    // feature_map_count() == 1, as then there is no difference\n    // between the two layouts.\n    //\n    // The operation here is the same as above, except that we have to\n    // do the matrix multiplication for each (y,x) output coordinate\n    // separately. We then interpret weights as containing K = width()\n    // * height() different matrices, which we all multiply onto the\n    // matrix from input_data, yielding K matrix products. We then\n    // combine these together into one matrix by concatenating all the\n    // first rows of these matrices, then all the seconds rows and so\n    // on. We can do this with a batched matrix multiplication, where\n    // the result is written to a different submatrix of the output\n    // for each matrix multiplication.\n    //\n    // The reason that we only support the kBatchYXDepth output layout\n    // is that we have to do something in the depth for each (y,x)\n    // coordinate. The kBatchYXDepth layout has the depth information\n    // for each point (y,x) in contiguous memory while the\n    // kBatchDepthYX layout does not.\n    //\n    // TODO(broune): Consider a special case for when output depth ==\n    // 1, as then possibly this could all be done as one matrix\n    // multiplication instead of a batched one, which should be\n    // faster. Another possibility would be to add a weights layout\n    // parameter and then support kBatchDepthYX for a different\n    // weights layout.\n    if (output_dimensions.layout() != dnn::DataLayout::kBatchYXDepth &&\n        !(output_dimensions.layout() == dnn::DataLayout::kBatchDepthYX &&\n          output_dimensions.feature_map_count() == 1)) {\n      LOG(ERROR) << \"Unsupported MatMul output layout.\";\n      return false;\n    }\n\n    const float alpha = 1.0f;  // Take the matrix product without scaling it.\n    const float beta = 0.0f;   // Ignore the original values in output_data.\n    const uint64_t m = output_dimensions.feature_map_count();\n    const uint64_t n = input_dimensions.count();\n    const uint64_t k = input_dimensions.NodesAcrossFeatureMaps();\n    const int lda = m;\n    const int ldb = k;\n    const int ldc = output_dimensions.NodesAcrossFeatureMaps();\n    const int batch_count = output_dimensions.NodesPerFeatureMap();\n\n    std::vector<DeviceMemory<float>> a(batch_count);\n    std::vector<DeviceMemory<float>> b(batch_count);\n    std::vector<DeviceMemory<float>> c(batch_count);\n    for (int i = 0; i < batch_count; ++i) {\n      const int weights_offset = i * input_dimensions.NodesAcrossFeatureMaps() *\n                                 output_dimensions.feature_map_count();\n      a[i] = DeviceMemory<float>::MakeFromByteSize(\n          const_cast<float*>(reinterpret_cast<const float*>(weights.opaque())) +\n              weights_offset,\n          weights.ElementCount() - weights_offset);\n\n      b[i] = input_data;\n\n      const int output_offset = i * output_dimensions.feature_map_count();\n      c[i] = DeviceMemory<float>::MakeFromByteSize(\n          const_cast<float*>(\n              reinterpret_cast<const float*>(output_data->opaque())) +\n              output_offset,\n          output_data->ElementCount() - output_offset);\n    }\n    const auto toPtrs = [](std::vector<DeviceMemory<float>>& v) {\n      std::vector<DeviceMemory<float>*> ptrs;\n      ptrs.reserve(v.size());\n      for (auto& mem : v) {\n        ptrs.push_back(&mem);\n      }\n      return ptrs;\n    };\n\n    stream->ThenBlasGemmBatched(blas::Transpose::kNoTranspose,\n                                blas::Transpose::kNoTranspose, m, n, k, alpha,\n                                toPtrs(a), lda, toPtrs(b), ldb, beta, toPtrs(c),\n                                ldc, batch_count);\n  }\n\n  return stream->ok();\n}\n\nbool MIOpenSupport::DoBiasAdd(Stream* stream,\n                              const DeviceMemory<float>& input_data,\n                              const DeviceMemory<float>& biases,\n                              const dnn::BatchDescriptor& dimensions,\n                              DeviceMemory<float>* output_data) {\n  ScopedTensorDescriptor input_descriptor{dimensions, miopenFloat};\n\n  BatchDescriptor bias_dimensions;\n  bias_dimensions.set_count(1)\n      .set_feature_map_count(dimensions.feature_map_count())\n      .set_height(1)\n      .set_width(1)\n      .set_layout(dnn::DataLayout::kBatchYXDepth);\n  ScopedTensorDescriptor bias_descriptor{bias_dimensions, miopenFloat};\n\n  if (input_data.opaque() != output_data->opaque()) {\n    stream->ThenMemcpy(output_data, input_data,\n                       dimensions.ElementCount() * sizeof(float));\n    if (!stream->ok()) {\n      LOG(ERROR)\n          << \"stream \" << stream\n          << \" could not enqueue a tensor copy as part of bias addition.\";\n      return false;\n    }\n  }\n\n  auto miopen = miopen_->GetHandle(parent_, stream);\n\n  const float alpha1 = 1.0f;\n  const float alpha2 = 0.0f;\n  const float beta = 1.0f;\n\n  auto status = wrap::miopenOpTensor(\n      miopen.handle(), miopenTensorOpAdd, &alpha1, bias_descriptor.handle(),\n      biases.opaque(), &alpha2, bias_descriptor.handle(), biases.opaque(),\n      &beta, input_descriptor.handle(), output_data->opaque());\n\n  if (status != miopenStatusSuccess) {\n    LOG(ERROR) << \"stream \" << stream << \" could not enqueue bias addition.\";\n    return false;\n  }\n\n  return true;\n}\n\nbool MIOpenSupport::DoActivate(Stream* stream,\n                               dnn::ActivationMode activation_mode,\n                               const dnn::BatchDescriptor& dimensions,\n                               const DeviceMemory<float>& input_data,\n                               DeviceMemory<float>* output_data,\n                               uint64_t options) {\n  LOG(ERROR) << \"miopen does not support activation yet\";\n  return false;\n}\n\ntsl::Status MIOpenSupport::DoPoolForward(\n    dnn::DataType element_type, Stream* stream,\n    const dnn::PoolingDescriptor& pooling_dimensions,\n    const dnn::BatchDescriptor& input_dimensions, DeviceMemoryBase input_data,\n    const dnn::BatchDescriptor& output_dimensions, DeviceMemoryBase output_data,\n    ScratchAllocator* workspace_allocator) {\n  if (element_type == dnn::DataType::kDouble) {\n    return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                       \"MIOpen does not support pooling for double type yet\");\n  }\n\n  auto miopen = miopen_->GetHandle(parent_, stream);\n  // Alpha is the scaling factor for input.\n  float alpha = 1.0;\n  // Beta is the scaling factor for output.\n  float beta = 0.0;\n\n  auto miopen_dtype =\n      element_type == dnn::DataType::kFloat ? miopenFloat : miopenHalf;\n\n  ScopedTensorDescriptor src_desc{input_dimensions, miopen_dtype};\n  ScopedTensorDescriptor dest_desc{output_dimensions, miopen_dtype};\n  ScopedPoolingDescriptor pooling_desc{pooling_dimensions};\n\n  bool do_backward = false;\n  uint8* workspace = nullptr;\n  size_t workspace_size = 0;\n  std::unique_ptr<TemporaryDeviceMemory<uint8>> wsp_mem;\n  if (m_pooling_cache_enabled && element_type == dnn::DataType::kFloat) {\n    do_backward = true;\n    auto status = wrap::miopenPoolingGetWorkSpaceSizeV2(\n        pooling_desc.handle(), dest_desc.handle(), &workspace_size);\n    if (status != miopenStatusSuccess) {\n      return tsl::errors::Internal(\n          \"Failed to obtain workspace size for backward pooling on stream: \",\n          ToString(status));\n    }\n    if (workspace_size != 0) {\n      PoolingWorkspaceDescriptor* pdesc = 0;\n      bool cache_hit =\n          m_pooling_cache_allowed &&\n          m_pooling_cache.find(input_data.opaque(), input_dimensions,\n                               output_dimensions, pooling_dimensions,\n                               miopenFloat, pdesc);\n      if (cache_hit) {\n        // reusing the same buffer\n        workspace = reinterpret_cast<uint8*>(\n            pdesc->workspace->mutable_device_memory()->opaque());\n      } else {\n        wsp_mem = stream->AllocateTemporaryArray<uint8>(workspace_size).value();\n        workspace = reinterpret_cast<uint8*>(\n            wsp_mem->mutable_device_memory()->opaque());\n        m_pooling_cache.insert(input_data.opaque(), input_dimensions,\n                               output_dimensions, pooling_dimensions,\n                               miopenFloat, wsp_mem, workspace_size,\n                               AsGpuStreamValue(stream));\n      }\n    }\n  }\n\n  auto status = wrap::miopenPoolingForward(\n      miopen.handle(), pooling_desc.handle(), &alpha, src_desc.handle(),\n      input_data.opaque(), &beta, dest_desc.handle(), output_data.opaque(),\n      do_backward, workspace, workspace_size);\n  if (status != miopenStatusSuccess) {\n    return tsl::errors::Internal(\n        \"Failed to enqueue forward pooling on stream: \", ToString(status));\n  }\n  return tsl::OkStatus();\n}\n\nbool PoolingWorkspaceDescriptor::IsSame(\n    const dnn::BatchDescriptor& input_dimensions,\n    const dnn::BatchDescriptor& output_dimensions,\n    const dnn::PoolingDescriptor& pooling_dimensions, int _type) {\n  return dtype == _type &&\n         input_dims ==\n             input_dimensions.full_dims(dnn::DataLayout::kBatchDepthYX) &&\n         output_dims ==\n             output_dimensions.full_dims(dnn::DataLayout::kBatchDepthYX) &&\n         op.mode() == pooling_dimensions.mode() &&\n         op.window() == pooling_dimensions.window() &&\n         op.padding() == pooling_dimensions.padding() &&\n         op.strides() == pooling_dimensions.strides();\n}\n\nbool PoolingWorkspaceCache::find(\n    const void* p, const dnn::BatchDescriptor& input_dimensions,\n    const dnn::BatchDescriptor& output_dimensions,\n    const dnn::PoolingDescriptor& pooling_dimensions, int _type,\n    PoolingWorkspaceDescriptor*& pdesc) {\n  pdesc = 0;\n  auto it = cache.find(p);\n  if (it == cache.end()) {\n    return false;\n  }\n  if (!it->second.IsSame(input_dimensions, output_dimensions,\n                         pooling_dimensions, _type)) {\n    return false;\n  }\n  pdesc = &it->second;\n  return true;\n}\n\nvoid PoolingWorkspaceCache::insert(\n    const void* p, const dnn::BatchDescriptor& input_dimensions,\n    const dnn::BatchDescriptor& output_dimensions,\n    const dnn::PoolingDescriptor& pooling_dimensions, int _type,\n    std::unique_ptr<TemporaryDeviceMemory<uint8>>& workspace, size_t wsp_size,\n    hipStream_t hip_stream) {\n  PoolingWorkspaceDescriptor* desc = 0;\n  auto it = cache.find(p);\n  if (it != cache.end()) {\n    // replacing an entry with the same pointer but different attributes\n    // (if everything matches, the caller is expected to reuse the entry)\n    desc = &it->second;\n    CHECK_EQ(hipStreamSynchronize(hip_stream), hipSuccess)\n        << \"Failed to sync hipStream\";\n    memory_used -= desc->workspace_size;\n  } else {\n    cache[p] = PoolingWorkspaceDescriptor();\n    desc = &cache[p];\n  }\n  desc->input_dims = input_dimensions.full_dims(dnn::DataLayout::kBatchDepthYX);\n  desc->output_dims =\n      output_dimensions.full_dims(dnn::DataLayout::kBatchDepthYX);\n  desc->op = pooling_dimensions;\n  desc->dtype = _type;\n  desc->timestamp = timestamp;\n  timestamp++;\n  desc->workspace = std::move(workspace);\n  desc->workspace_size = wsp_size;\n  memory_used += wsp_size;\n  trim(hip_stream);\n}\n\nvoid PoolingWorkspaceCache::trim(hipStream_t hip_stream) {\n  if (memory_used < memory_budget && cache.size() < trim_size) return;\n  bool must_sync = true;\n  while (true) {\n    int new_size = cache.size() - (cache.size() >> 2);\n    std::vector<const void*> old_entries;\n    for (auto& x : cache)\n      if (x.second.timestamp + new_size < timestamp)\n        old_entries.push_back(x.first);\n    if (old_entries.empty()) break;\n    if (must_sync)\n      CHECK_EQ(hipStreamSynchronize(hip_stream), hipSuccess)\n          << \"Failed to sync hipStream\";\n    must_sync = true;\n    for (auto x : old_entries) {\n      memory_used -= cache[x].workspace_size;\n      cache.erase(x);\n    }\n    if (memory_used < memory_budget || cache.size() < 10) break;\n  }\n}\n\ntsl::Status MIOpenSupport::DoPoolBackward(\n    dnn::DataType element_type, Stream* stream,\n    const dnn::PoolingDescriptor& pooling_dimensions,\n    const dnn::BatchDescriptor& input_dimensions, DeviceMemoryBase input_data,\n    const dnn::BatchDescriptor& output_dimensions, DeviceMemoryBase output_data,\n    DeviceMemoryBase input_diff_data, DeviceMemoryBase output_diff_data,\n    ScratchAllocator* workspace_allocator) {\n  if (element_type == dnn::DataType::kDouble) {\n    return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                       \"MIOpen does not support pooling for double type yet\");\n  }\n\n  auto miopen = miopen_->GetHandle(parent_, stream);\n  if (m_pooling_cache_allowed) m_pooling_cache_enabled = true;\n  // Alpha is the scaling factor for input.\n  float alpha = 1.0;\n  // Beta is the scaling factor for output.\n  float beta = 0.0;\n\n  auto miopen_dtype =\n      element_type == dnn::DataType::kFloat ? miopenFloat : miopenHalf;\n\n  ScopedTensorDescriptor src_desc{input_dimensions, miopen_dtype};\n  ScopedTensorDescriptor dest_desc{output_dimensions, miopen_dtype};\n  ScopedPoolingDescriptor pooling_desc{pooling_dimensions};\n\n  uint8* workspace_ptr = 0;\n  DeviceMemory<uint8> workspace;\n  PoolingWorkspaceDescriptor* pdesc = 0;\n\n  size_t workspace_size_in_bytes = 0;\n  auto status = wrap::miopenPoolingGetWorkSpaceSizeV2(\n      pooling_desc.handle(), dest_desc.handle(), &workspace_size_in_bytes);\n  if (status != miopenStatusSuccess) {\n    return tsl::errors::Internal(\n        \"Failed to obtain workspace size for backward pooling on stream: \",\n        ToString(status));\n  }\n\n  // Allocate the workspace.\n  if (workspace_size_in_bytes > 0) {\n    bool cache_hit = m_pooling_cache_allowed &&\n                     m_pooling_cache.find(input_data.opaque(), input_dimensions,\n                                          output_dimensions, pooling_dimensions,\n                                          miopen_dtype, pdesc);\n    if (cache_hit) {\n      assert(pdesc != 0);\n      workspace_ptr = reinterpret_cast<uint8*>(\n          pdesc->workspace->mutable_device_memory()->opaque());\n      VLOG(1) << \"Pooling cache hit\";\n    } else {\n      VLOG(1) << \"Pooling cache miss\";\n      assert(workspace_allocator);\n      auto allocated =\n          workspace_allocator->AllocateBytes(workspace_size_in_bytes);\n      if (!allocated.ok() || (workspace = allocated.value()) == nullptr) {\n        return tsl::errors::Internal(\n            \"Failed to allocate backward pooling workspace\");\n      }\n      DeviceMemory<uint8> dest2;  // duplicated dest from forward:\n      int64_t dest2_size = 0;\n\n      // miopen requires the strides and dims to be ordered as BDYX.\n      std::vector<int64_t> dims64 =\n          output_dimensions.full_dims(dnn::DataLayout::kBatchDepthYX);\n      // miopen does not use strides and must have 4D tensor.\n      // std::vector<int> dims(pooling_dimensions.ndims() + 2);\n\n      dest2_size = (element_type == dnn::DataType::kFloat)\n                       ? sizeof(float)\n                       : sizeof(Eigen::half);\n      for (auto& x : dims64) dest2_size *= x;\n\n      if (dest2_size > 0) {\n        assert(workspace_allocator);\n        auto allocated = workspace_allocator->AllocateBytes(dest2_size);\n        if (!allocated.ok() || (dest2 = allocated.value()) == nullptr) {\n          return tsl::errors::Internal(\n              \"Failed to allocate backward pooling workspace\");\n        }\n      } else {\n        LOG(ERROR) << \"Failed to calculate tensor size to chain forward and \"\n                      \"backward pooling\";\n      }\n\n      status = wrap::miopenPoolingForward(\n          miopen.handle(), pooling_desc.handle(), &alpha, src_desc.handle(),\n          input_data.opaque(), &beta, dest_desc.handle(), dest2.opaque(), true,\n          workspace.opaque(), workspace_size_in_bytes);\n\n      if (status != miopenStatusSuccess) {\n        return tsl::errors::Internal(\n            \"Failed to enqueue forward pooling (before backward) on stream: \",\n            ToString(status));\n      }\n      workspace_ptr = reinterpret_cast<uint8*>(workspace.opaque());\n    }\n  }\n\n  status = wrap::miopenPoolingBackward(\n      miopen.handle(), pooling_desc.handle(), &alpha, dest_desc.handle(),\n      output_data.opaque(), dest_desc.handle(), input_diff_data.opaque(),\n      src_desc.handle(), input_data.opaque(), &beta, src_desc.handle(),\n      output_diff_data.opaque(), workspace_ptr);\n\n  if (status != miopenStatusSuccess) {\n    return tsl::errors::Internal(\n        \"Failed to enqueue backward pooling on stream: \", ToString(status));\n  }\n  return tsl::OkStatus();\n}\n\nbool MIOpenSupport::DoNormalizeWithDimensions(\n    Stream* stream, const dnn::NormalizeDescriptor& normalize_descriptor,\n    const dnn::BatchDescriptor& dimensions,\n    const DeviceMemory<float>& input_data, DeviceMemory<float>* output_data) {\n  // Check for unsupported modes.\n  if (normalize_descriptor.wrap_around()) {\n    LOG(ERROR) << \"MIOpen LRN does not support wrap-around mode\";\n    return false;\n  }\n  if (normalize_descriptor.segment_size()) {\n    LOG(ERROR) << \"MIOpen LRN does not support segmentation\";\n    return false;\n  }\n\n  auto miopen = miopen_->GetHandle(parent_, stream);\n\n  // Launch the normalization.\n  ScopedTensorDescriptor dims{dimensions, miopenFloat};\n  ScopedNormalizeDescriptor normalize{normalize_descriptor};\n\n  // Alpha is the scaling factor for input.\n  float alpha = 1.0f;\n  // Beta is the scaling factor for output.\n  float beta = 0.0f;\n\n  auto status = wrap::miopenLRNForward(\n      miopen.handle(), normalize.handle(), &alpha, dims.handle(),\n      input_data.opaque(), &beta, dims.handle(), output_data->opaque(), false,\n      nullptr);\n  if (status != miopenStatusSuccess) {\n    LOG(ERROR) << \"failed to run miopenLRNForward\";\n    return false;\n  }\n  return true;\n}\n\nbool MIOpenSupport::DoNormalizeBackwardWithDimensions(\n    Stream* stream, const dnn::NormalizeDescriptor& normalize_descriptor,\n    const dnn::BatchDescriptor& dimensions, const DeviceMemory<float>& raw_data,\n    const DeviceMemory<float>& normalized_data,\n    const DeviceMemory<float>& normalized_variable_gradient,\n    DeviceMemory<float>* raw_variable_gradient,\n    ScratchAllocator* workspace_allocator) {\n  // Check for unsupported modes.\n  if (normalize_descriptor.wrap_around()) {\n    LOG(ERROR) << \"MIOpen LRN does not support wrap-around mode\";\n    return false;\n  }\n  if (normalize_descriptor.segment_size()) {\n    LOG(ERROR) << \"MIOpen LRN does not support segmentation\";\n    return false;\n  }\n\n  auto miopen = miopen_->GetHandle(parent_, stream);\n\n  ScopedTensorDescriptor dims{dimensions, miopenFloat};\n  ScopedNormalizeDescriptor normalize{normalize_descriptor};\n\n  float alpha = 1.0f;\n  float beta = 0.0f;\n\n  DeviceMemory<uint8> workspace;\n  size_t workspace_size_in_bytes = 0;\n  auto status =\n      wrap::miopenLRNGetWorkSpaceSize(dims.handle(), &workspace_size_in_bytes);\n\n  if (status != miopenStatusSuccess) {\n    LOG(ERROR) << \"failed to obtain workspace size for miopenLRNBackward\";\n    return false;\n  }\n\n  // Allocate the workspace.\n  if (workspace_size_in_bytes > 0) {\n    assert(workspace_allocator);\n    auto allocated =\n        workspace_allocator->AllocateBytes(workspace_size_in_bytes);\n    if (!allocated.ok() || (workspace = allocated.value()) == nullptr) {\n      LOG(ERROR) << \"Failed to allocate backward pooling workspace\";\n      return false;\n    }\n  }\n\n  DeviceMemory<uint8> dest2;  // duplicated dest from forward:\n  int dest2_size = 0;\n\n  // miopen requires the strides and dims to be ordered as BDYX.\n  std::vector<int64_t> dims64 =\n      dimensions.full_dims(dnn::DataLayout::kBatchDepthYX);\n\n  // miopen does not use strides and must have 4D tensor.\n  std::vector<int> dimsint(4);\n\n  std::transform(dims64.cbegin(), dims64.cend(), dimsint.begin(),\n                 &CheckedNarrowing<int64_t, int>);\n\n  dest2_size =\n      dimsint[0] * dimsint[1] * dimsint[2] * dimsint[3] * sizeof(float);\n\n  if (dest2_size > 0) {\n    assert(workspace_allocator);\n    auto allocated = workspace_allocator->AllocateBytes(dest2_size);\n    if (!allocated.ok() || (dest2 = allocated.value()) == nullptr) {\n      LOG(ERROR)\n          << \"Failed to allocate tensor to chain forward and backward LRN\";\n      return false;\n    }\n  } else {\n    LOG(ERROR) << \"Failed to calculate tensor size to chain forward and \"\n                  \"backward LRN\";\n  }\n\n  status = wrap::miopenLRNForward(miopen.handle(), normalize.handle(), &alpha,\n                                  dims.handle(), raw_data.opaque(), &beta,\n                                  dims.handle(), dest2.opaque(), true,\n                                  workspace.opaque());\n\n  if (status != miopenStatusSuccess) {\n    LOG(ERROR) << \"failed to run miopenLRNForward\";\n    return false;\n  }\n\n  status = wrap::miopenLRNBackward(\n      miopen.handle(), normalize.handle(), &alpha, dims.handle(),\n      normalized_data.opaque(), dims.handle(),\n      normalized_variable_gradient.opaque(), dims.handle(), raw_data.opaque(),\n      &beta, dims.handle(), raw_variable_gradient->opaque(),\n      workspace.opaque());\n\n  if (status != miopenStatusSuccess) {\n    LOG(ERROR) << \"failed to run miopenLRNBackward\";\n    return false;\n  }\n  return true;\n}\n\nbool MIOpenSupport::DoDepthConcatenate(\n    Stream* stream, absl::Span<const dnn::BatchDescriptor> input_dimensions,\n    absl::Span<const DeviceMemory<float>* const> input_data,\n    DeviceMemory<float>* output_data) {\n  CHECK_EQ(input_dimensions.size(), input_data.size());\n\n  for (const auto& dimensions : input_dimensions) {\n    if (dimensions.layout() != dnn::DataLayout::kBatchDepthYX) {\n      LOG(ERROR) << \"MIOpenSupport::DoDepthConcatenate currently only \"\n                    \"supports the kBatchDepthYX layout.\";\n      return false;\n    }\n  }\n\n  if (input_dimensions.empty()) {\n    return true;  // Nothing to do.\n  }\n\n  dnn::BatchDescriptor output_dimensions =\n      dnn::BatchDescriptor::DepthConcatenateOutputDescriptor(input_dimensions);\n\n  const int64_t area = output_dimensions.width() * output_dimensions.height();\n  const auto index = [area](int64_t batch, int64_t depth, int64_t yx,\n                            int64_t max_depth) {\n    return (batch * max_depth + depth) * area + yx;\n  };\n\n  std::vector<float> output_host(output_dimensions.ElementCount());\n  std::vector<float> tmp;\n  int64_t depth_sum = 0;\n  for (size_t i = 0; i < input_data.size(); ++i) {\n    const auto& dimensions = input_dimensions[i];\n    tmp.resize(dimensions.ElementCount());\n    stream->ThenMemcpyD2H<float>(*input_data[i], absl::MakeSpan(tmp));\n    tsl::Status block_status = stream->BlockHostUntilDone();\n    if (!block_status.ok()) {\n      LOG(ERROR) << \"BlockHostUntilDone failed: \" << block_status;\n      return false;\n    }\n\n    for (int64_t batch = 0; batch < output_dimensions.count(); ++batch) {\n      for (int64_t yx = 0; yx < area; ++yx) {\n        for (int64_t depth = 0; depth < dimensions.feature_map_count();\n             ++depth) {\n          LOG(INFO) << output_dimensions.ElementCount() << ' ' << batch << ' '\n                    << yx << ' ' << depth;\n          output_host[index(batch, depth + depth_sum, yx,\n                            output_dimensions.feature_map_count())] =\n              tmp[index(batch, depth, yx, dimensions.feature_map_count())];\n        }\n      }\n    }\n    depth_sum += dimensions.feature_map_count();\n  }\n  stream->ThenMemcpyH2D<float>(output_host, output_data);\n  return true;\n}\n\nbool MIOpenSupport::DoElementwiseOperate(\n    Stream* stream, dnn::ElementwiseOperation operation,\n    absl::Span<const dnn::BatchDescriptor> input_dimensions,\n    absl::Span<const DeviceMemory<float>* const> input_data,\n    const dnn::BatchDescriptor& output_dimensions,\n    DeviceMemory<float>* output_data) {\n  LOG(FATAL) << \"not yet implemented\";  // TODO(leary)\n  return false;\n}\n\nbool MIOpenSupport::DoXYPad(Stream* stream,\n                            const dnn::BatchDescriptor& dimensions,\n                            const DeviceMemory<float>& input_data,\n                            int64_t left_pad, int64_t right_pad,\n                            int64_t top_pad, int64_t bottom_pad,\n                            DeviceMemory<float>* output_data) {\n  LOG(FATAL) << \"not yet implemented\";  // TODO(leary)\n  return false;\n}\n\nbool MIOpenSupport::DoXYSlice(Stream* stream,\n                              const dnn::BatchDescriptor& dimensions,\n                              const DeviceMemory<float>& input_data,\n                              int64_t left_trim, int64_t right_trim,\n                              int64_t top_trim, int64_t bottom_trim,\n                              DeviceMemory<float>* output_data) {\n  LOG(FATAL) << \"not yet implemented\";  // TODO(leary)\n  return false;\n}\n\nbool MIOpenSupport::DoMemcpyD2HQuantized(\n    Stream* stream, const DeviceMemory<float>& gpu_unquantized_src,\n    dnn::QuantizedActivationMode mode, void* host_dst, int64_t size) {\n  LOG(ERROR) << \"quantized memcpy not supported by MIOpen\";\n  return false;\n}\n\nbool MIOpenSupport::DoMemcpyH2DQuantized(\n    Stream* stream, const void* host_src, int64_t size,\n    dnn::QuantizedActivationMode mode,\n    DeviceMemory<float>* gpu_unquantized_dst) {\n  LOG(ERROR) << \"quantized memcpy not supported by MIOpen\";\n  return false;\n}\n\nbool MIOpenSupport::DeriveOutputBatchDescriptor(\n    const BatchDescriptor& batch_descriptor,\n    const FilterDescriptor& filter_descriptor,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    dnn::BatchDescriptor* output_batch_descriptor) {\n  ScopedTensorDescriptor input_nd{batch_descriptor, miopenFloat};\n  ScopedFilterDescriptor filter{filter_descriptor, miopenFloat};\n  ScopedConvolutionDescriptor conv{convolution_descriptor, miopenFloat};\n\n  int dn = batch_descriptor.ndims() + 2;\n  std::vector<int> dims(dn);  // in BDYX\n  auto status = wrap::miopenGetConvolutionNdForwardOutputDim(\n      conv.handle(), input_nd.handle(), filter.handle(), &dn, dims.data());\n  if (status != miopenStatusSuccess) {\n    LOG(ERROR) << \"could not get output tensor for convolution: \"\n               << ToString(status);\n    return false;\n  }\n\n  output_batch_descriptor->set_count(dims[0])\n      .set_feature_map_count(dims[1])\n      .set_layout(batch_descriptor.layout());\n\n  for (int i = 0; i < batch_descriptor.ndims(); i++) {\n    output_batch_descriptor->set_spatial_dim(static_cast<dnn::DimIndex>(i),\n                                             dims.rbegin()[i]);\n  }\n\n  return true;\n}\n\ntemplate <typename T>\nbool MIOpenSupport::DoFusedConvolutionBiasActivationImpl(\n    Stream* stream,\n    int miopen_type,  // Actually miopenDataType_t.\n    const dnn::BatchDescriptor& conv_input_descriptor,\n    const DeviceMemory<T>& conv_input_data,\n    const dnn::FilterDescriptor& filter_descriptor,\n    const DeviceMemory<T>& filter_data,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    const dnn::BatchDescriptor& bias_descriptor,\n    const DeviceMemory<T>& bias_data, dnn::ActivationMode activation_mode,\n    const dnn::BatchDescriptor& output_descriptor, DeviceMemory<T>* output_data,\n    dnn::ProfileResult* output_profile_result) {\n  auto miopen = miopen_->GetHandle(parent_, stream);\n\n  ScopedTensorDescriptor conv_input_nd{\n      conv_input_descriptor, static_cast<miopenDataType_t>(miopen_type)};\n\n  ScopedTensorDescriptor bias_nd{bias_descriptor,\n                                 static_cast<miopenDataType_t>(miopen_type)};\n\n  ScopedTensorDescriptor output_nd{output_descriptor,\n                                   static_cast<miopenDataType_t>(miopen_type)};\n\n  ScopedConvolutionDescriptor conv{convolution_descriptor,\n                                   static_cast<miopenDataType_t>(miopen_type)};\n\n  ScopedFilterDescriptor filter{filter_descriptor,\n                                static_cast<miopenDataType_t>(miopen_type)};\n\n  ScopedActivationDescriptor activation_desc{activation_mode};\n\n  ScopedFusionPlanConvolutionBiasActivation fusion_plan{\n      miopen.handle(), conv_input_nd.handle(), filter.handle(),\n      conv.handle(),   bias_nd.handle(),       activation_desc};\n\n  bool retval = false;\n\n  if (fusion_plan.CompilationSucceeded()) {\n    const bool is_profiling = output_profile_result != nullptr;\n\n    std::unique_ptr<GpuTimer> timer;\n    if (is_profiling) {\n      timer.reset(new GpuTimer(parent_));\n      timer->Init();\n      timer->Start(AsGpuStream(stream));\n    }\n\n    miopenStatus_t status = miopenStatusSuccess;\n\n    if (status == miopenStatusSuccess) {\n      fusion_plan.SetConvolutionArgs(filter_data.opaque());\n    }\n\n    if (status == miopenStatusSuccess) {\n      status = fusion_plan.SetBiasArgs(bias_data.opaque());\n    }\n\n    if (status == miopenStatusSuccess) {\n      status = fusion_plan.SetActivationForwardArgs(activation_desc);\n    }\n\n    if (status == miopenStatusSuccess) {\n      status =\n          fusion_plan.Execute(conv_input_nd.handle(), conv_input_data.opaque(),\n                              output_nd.handle(), output_data->opaque());\n    }\n\n    if (is_profiling) {\n      timer->Stop(AsGpuStream(stream));\n      if (status == miopenStatusSuccess) {\n        output_profile_result->set_elapsed_time_in_ms(\n            timer->GetElapsedMilliseconds());\n      }\n      timer->Destroy();\n    }\n\n    if (status != miopenStatusSuccess) {\n      // Silently return when we are profiling.\n      if (!is_profiling) {\n        LOG(FATAL) << \"failed to enqueue fused-convolution on stream: \"\n                   << ToString(status);\n      }\n    }\n\n    retval = true;\n  }\n\n  return retval;\n}\n\nbool MIOpenSupport::DoFusedConvolutionBiasActivation(\n    Stream* stream, const dnn::BatchDescriptor& conv_input_descriptor,\n    const DeviceMemory<float>& conv_input_data,\n    const dnn::FilterDescriptor& filter_descriptor,\n    const DeviceMemory<float>& filter_data,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    const dnn::BatchDescriptor& bias_descriptor,\n    const DeviceMemory<float>& bias_data, dnn::ActivationMode activation_mode,\n    const dnn::BatchDescriptor& output_descriptor,\n    DeviceMemory<float>* output_data,\n    dnn::ProfileResult* output_profile_result) {\n  return DoFusedConvolutionBiasActivationImpl<float>(\n      stream, miopenFloat, conv_input_descriptor, conv_input_data,\n      filter_descriptor, filter_data, convolution_descriptor, bias_descriptor,\n      bias_data, activation_mode, output_descriptor, output_data,\n      output_profile_result);\n}\n\ntemplate <typename T, typename U>\nbool MIOpenSupport::DoFusedBatchNormActivationInferenceImpl(\n    Stream* stream,\n    int miopen_type,  // Actually miopenDataType_t.\n    const dnn::BatchDescriptor& x_descriptor, const DeviceMemory<T>& x_data,\n    const dnn::BatchDescriptor& scale_offset_mean_variance_descriptor,\n    const DeviceMemory<U>& scale_data, const DeviceMemory<U>& offset_data,\n    const DeviceMemory<U>& mean_data, const DeviceMemory<U>& variance_data,\n    double epsilon, dnn::ActivationMode activation_mode,\n    DeviceMemory<T>* y_data, dnn::ProfileResult* output_profile_result) {\n  auto miopen = miopen_->GetHandle(parent_, stream);\n\n  ScopedTensorDescriptor x_nd{x_descriptor,\n                              static_cast<miopenDataType_t>(miopen_type)};\n\n  ScopedTensorDescriptor scale_offset_mean_variance_nd{\n      scale_offset_mean_variance_descriptor,\n      static_cast<miopenDataType_t>(miopen_type)};\n\n  ScopedActivationDescriptor activation_desc{activation_mode};\n\n  ScopedFusionPlanBatchNormActivationInference fusion_plan{\n      miopen.handle(), x_nd.handle(), scale_offset_mean_variance_nd.handle(),\n      activation_desc};\n\n  bool retval = false;\n\n  if (fusion_plan.CompilationSucceeded()) {\n    const bool is_profiling = output_profile_result != nullptr;\n\n    std::unique_ptr<GpuTimer> timer;\n    if (is_profiling) {\n      timer.reset(new GpuTimer(parent_));\n      timer->Init();\n      timer->Start(AsGpuStream(stream));\n    }\n\n    miopenStatus_t status = miopenStatusSuccess;\n\n    if (status == miopenStatusSuccess) {\n      fusion_plan.SetBatchNormInferenceArgs(\n          scale_data.opaque(), offset_data.opaque(), mean_data.opaque(),\n          variance_data.opaque(), epsilon);\n    }\n\n    if (status == miopenStatusSuccess) {\n      status = fusion_plan.SetActivationForwardArgs(activation_desc);\n    }\n\n    if (status == miopenStatusSuccess) {\n      status = fusion_plan.Execute(x_nd.handle(), x_data.opaque(),\n                                   x_nd.handle(), y_data->opaque());\n    }\n\n    if (is_profiling) {\n      timer->Stop(AsGpuStream(stream));\n      if (status == miopenStatusSuccess) {\n        output_profile_result->set_elapsed_time_in_ms(\n            timer->GetElapsedMilliseconds());\n      }\n      timer->Destroy();\n    }\n\n    if (status != miopenStatusSuccess) {\n      // Silently return when we are profiling.\n      if (!is_profiling) {\n        LOG(FATAL) << \"failed to enqueue fused-convolution on stream: \"\n                   << ToString(status);\n      }\n    }\n\n    retval = true;\n  }\n\n  return retval;\n}\n\nbool MIOpenSupport::DoFusedBatchNormActivationInference(\n    Stream* stream, const dnn::BatchDescriptor& x_descriptor,\n    const DeviceMemory<float>& x_data,\n    const dnn::BatchDescriptor& scale_offset_mean_variance_descriptor,\n    const DeviceMemory<float>& scale_data,\n    const DeviceMemory<float>& offset_data,\n    const DeviceMemory<float>& mean_data,\n    const DeviceMemory<float>& variance_data, double epsilon,\n    dnn::ActivationMode activation_mode, DeviceMemory<float>* y_data,\n    dnn::ProfileResult* output_profile_result) {\n  return DoFusedBatchNormActivationInferenceImpl<float, float>(\n      stream, miopenFloat, x_descriptor, x_data,\n      scale_offset_mean_variance_descriptor, scale_data, offset_data, mean_data,\n      variance_data, epsilon, activation_mode, y_data, output_profile_result);\n}\n\nbool MIOpenSupport::DoFusedBatchNormActivationInference(\n    Stream* stream, const dnn::BatchDescriptor& x_descriptor,\n    const DeviceMemory<Eigen::half>& x_data,\n    const dnn::BatchDescriptor& scale_offset_mean_variance_descriptor,\n    const DeviceMemory<float>& scale_data,\n    const DeviceMemory<float>& offset_data,\n    const DeviceMemory<float>& mean_data,\n    const DeviceMemory<float>& variance_data, double epsilon,\n    dnn::ActivationMode activation_mode, DeviceMemory<Eigen::half>* y_data,\n    dnn::ProfileResult* output_profile_result) {\n  return DoFusedBatchNormActivationInferenceImpl<Eigen::half, float>(\n      stream, miopenHalf, x_descriptor, x_data,\n      scale_offset_mean_variance_descriptor, scale_data, offset_data, mean_data,\n      variance_data, epsilon, activation_mode, y_data, output_profile_result);\n}\n\ntemplate <typename T, typename U>\nbool MIOpenSupport::DoFusedBatchNormActivationForwardImpl(\n    Stream* stream,\n    int miopen_type,  // Actually miopenDataType_t.\n    const dnn::BatchDescriptor& x_descriptor, const DeviceMemory<T>& x_data,\n    const dnn::BatchDescriptor& scale_offset_mean_variance_descriptor,\n    const DeviceMemory<U>& scale_data, const DeviceMemory<U>& offset_data,\n    double epsilon, dnn::ActivationMode activation_mode,\n    DeviceMemory<T>* y_data, DeviceMemory<U>* batch_mean_data,\n    DeviceMemory<U>* batch_var_data, DeviceMemory<U>* saved_mean_data,\n    DeviceMemory<U>* saved_var_data,\n    dnn::ProfileResult* output_profile_result) {\n  auto miopen = miopen_->GetHandle(parent_, stream);\n\n  ScopedTensorDescriptor x_nd{x_descriptor,\n                              static_cast<miopenDataType_t>(miopen_type)};\n\n  ScopedTensorDescriptor scale_offset_mean_variance_nd{\n      scale_offset_mean_variance_descriptor,\n      static_cast<miopenDataType_t>(miopen_type)};\n\n  ScopedActivationDescriptor activation_desc{activation_mode};\n\n  ScopedFusionPlanBatchNormActivationForward fusion_plan{\n      miopen.handle(), x_nd.handle(), scale_offset_mean_variance_nd.handle(),\n      activation_desc};\n\n  bool retval = false;\n\n  if (fusion_plan.CompilationSucceeded()) {\n    const bool is_profiling = output_profile_result != nullptr;\n\n    std::unique_ptr<GpuTimer> timer;\n    if (is_profiling) {\n      timer.reset(new GpuTimer(parent_));\n      timer->Init();\n      timer->Start(AsGpuStream(stream));\n    }\n\n    miopenStatus_t status = miopenStatusSuccess;\n\n    if (status == miopenStatusSuccess) {\n      fusion_plan.SetBatchNormForwardArgs(\n          scale_data.opaque(), offset_data.opaque(), batch_mean_data->opaque(),\n          batch_var_data->opaque(), saved_mean_data->opaque(),\n          saved_var_data->opaque(), epsilon);\n    }\n\n    if (status == miopenStatusSuccess) {\n      status = fusion_plan.SetActivationForwardArgs(activation_desc);\n    }\n\n    if (status == miopenStatusSuccess) {\n      status = fusion_plan.Execute(x_nd.handle(), x_data.opaque(),\n                                   x_nd.handle(), y_data->opaque());\n    }\n\n    if (is_profiling) {\n      timer->Stop(AsGpuStream(stream));\n      if (status == miopenStatusSuccess) {\n        output_profile_result->set_elapsed_time_in_ms(\n            timer->GetElapsedMilliseconds());\n      }\n      timer->Destroy();\n    }\n\n    if (status != miopenStatusSuccess) {\n      // Silently return when we are profiling.\n      if (!is_profiling) {\n        LOG(FATAL) << \"failed to enqueue fused-convolution on stream: \"\n                   << ToString(status);\n      }\n    }\n\n    retval = true;\n  }\n\n  return retval;\n}\n\nbool MIOpenSupport::DoFusedBatchNormActivationForward(\n    Stream* stream, const dnn::BatchDescriptor& x_descriptor,\n    const DeviceMemory<float>& x_data,\n    const dnn::BatchDescriptor& scale_offset_mean_variance_descriptor,\n    const DeviceMemory<float>& scale_data,\n    const DeviceMemory<float>& offset_data, double epsilon,\n    dnn::ActivationMode activation_mode, DeviceMemory<float>* y_data,\n    DeviceMemory<float>* batch_mean_data, DeviceMemory<float>* batch_var_data,\n    DeviceMemory<float>* saved_mean_data, DeviceMemory<float>* saved_var_data,\n    dnn::ProfileResult* output_profile_result) {\n  return DoFusedBatchNormActivationForwardImpl<float, float>(\n      stream, miopenFloat, x_descriptor, x_data,\n      scale_offset_mean_variance_descriptor, scale_data, offset_data, epsilon,\n      activation_mode, y_data, batch_mean_data, batch_var_data, saved_mean_data,\n      saved_var_data, output_profile_result);\n}\n\nbool MIOpenSupport::DoFusedBatchNormActivationForward(\n    Stream* stream, const dnn::BatchDescriptor& x_descriptor,\n    const DeviceMemory<Eigen::half>& x_data,\n    const dnn::BatchDescriptor& scale_offset_mean_variance_descriptor,\n    const DeviceMemory<float>& scale_data,\n    const DeviceMemory<float>& offset_data, double epsilon,\n    dnn::ActivationMode activation_mode, DeviceMemory<Eigen::half>* y_data,\n    DeviceMemory<float>* batch_mean_data, DeviceMemory<float>* batch_var_data,\n    DeviceMemory<float>* saved_mean_data, DeviceMemory<float>* saved_var_data,\n    dnn::ProfileResult* output_profile_result) {\n  return DoFusedBatchNormActivationForwardImpl<Eigen::half, float>(\n      stream, miopenHalf, x_descriptor, x_data,\n      scale_offset_mean_variance_descriptor, scale_data, offset_data, epsilon,\n      activation_mode, y_data, batch_mean_data, batch_var_data, saved_mean_data,\n      saved_var_data, output_profile_result);\n}\n\ntemplate <typename T, typename U>\nbool MIOpenSupport::DoFusedBatchNormActivationBackwardImpl(\n    Stream* stream,\n    int miopen_type,  // Actually miopenDataType_t.\n    const dnn::BatchDescriptor& y_act_backprop_descriptor,\n    const DeviceMemory<T>& y_act_backprop_data,\n    const DeviceMemory<T>& y_act_data, dnn::ActivationMode activation_mode,\n    const DeviceMemory<T>& x_bn_data,\n    const dnn::BatchDescriptor& scale_offset_mean_variance_descriptor,\n    const DeviceMemory<U>& scale_data, const DeviceMemory<U>& offset_data,\n    const DeviceMemory<U>& saved_mean_data,\n    const DeviceMemory<U>& saved_var_data, DeviceMemory<T>* x_bn_backprop_data,\n    DeviceMemory<U>* scale_backprop_data, DeviceMemory<U>* offset_backprop_data,\n    dnn::ProfileResult* output_profile_result) {\n  auto miopen = miopen_->GetHandle(parent_, stream);\n\n  ScopedTensorDescriptor y_act_backprop_nd{\n      y_act_backprop_descriptor, static_cast<miopenDataType_t>(miopen_type)};\n\n  ScopedTensorDescriptor scale_offset_mean_variance_nd{\n      scale_offset_mean_variance_descriptor,\n      static_cast<miopenDataType_t>(miopen_type)};\n\n  ScopedActivationDescriptor activation_desc{activation_mode};\n\n  ScopedFusionPlanBatchNormActivationBackward fusion_plan{\n      miopen.handle(), y_act_backprop_nd.handle(),\n      scale_offset_mean_variance_nd.handle(), activation_desc};\n\n  bool retval = false;\n\n  if (fusion_plan.CompilationSucceeded()) {\n    const bool is_profiling = output_profile_result != nullptr;\n\n    std::unique_ptr<GpuTimer> timer;\n    if (is_profiling) {\n      timer.reset(new GpuTimer(parent_));\n      timer->Init();\n      timer->Start(AsGpuStream(stream));\n    }\n\n    miopenStatus_t status = miopenStatusSuccess;\n\n    if (status == miopenStatusSuccess) {\n      fusion_plan.SetBatchNormBackwardArgs(\n          x_bn_data.opaque(), scale_data.opaque(), offset_data.opaque(),\n          saved_mean_data.opaque(), saved_var_data.opaque(),\n          scale_backprop_data->opaque(), offset_backprop_data->opaque());\n    }\n\n    if (status == miopenStatusSuccess) {\n      status = fusion_plan.SetActivationBackwardArgs(activation_desc,\n                                                     y_act_data.opaque());\n    }\n\n    if (status == miopenStatusSuccess) {\n      status = fusion_plan.Execute(\n          y_act_backprop_nd.handle(), y_act_backprop_data.opaque(),\n          y_act_backprop_nd.handle(), x_bn_backprop_data->opaque());\n    }\n\n    if (is_profiling) {\n      timer->Stop(AsGpuStream(stream));\n      if (status == miopenStatusSuccess) {\n        output_profile_result->set_elapsed_time_in_ms(\n            timer->GetElapsedMilliseconds());\n      }\n      timer->Destroy();\n    }\n\n    if (status != miopenStatusSuccess) {\n      // Silently return when we are profiling.\n      if (!is_profiling) {\n        LOG(FATAL) << \"failed to enqueue fused-convolution on stream: \"\n                   << ToString(status);\n      }\n    }\n\n    retval = true;\n  }\n\n  return retval;\n}\n\nbool MIOpenSupport::DoFusedBatchNormActivationBackward(\n    Stream* stream, const dnn::BatchDescriptor& y_act_backprop_descriptor,\n    const DeviceMemory<float>& y_act_backprop_data,\n    const DeviceMemory<float>& y_act_data, dnn::ActivationMode activation_mode,\n    const DeviceMemory<float>& x_bn_data,\n    const dnn::BatchDescriptor& scale_offset_mean_variance_descriptor,\n    const DeviceMemory<float>& scale_data,\n    const DeviceMemory<float>& offset_data,\n    const DeviceMemory<float>& saved_mean_data,\n    const DeviceMemory<float>& saved_var_data,\n    DeviceMemory<float>* x_bn_backprop_data,\n    DeviceMemory<float>* scale_backprop_data,\n    DeviceMemory<float>* offset_backprop_data,\n    dnn::ProfileResult* output_profile_result) {\n  return DoFusedBatchNormActivationBackwardImpl<float, float>(\n      stream, miopenFloat, y_act_backprop_descriptor, y_act_backprop_data,\n      y_act_data, activation_mode, x_bn_data,\n      scale_offset_mean_variance_descriptor, scale_data, offset_data,\n      saved_mean_data, saved_var_data, x_bn_backprop_data, scale_backprop_data,\n      offset_backprop_data, output_profile_result);\n}\n\nbool MIOpenSupport::DoFusedBatchNormActivationBackward(\n    Stream* stream, const dnn::BatchDescriptor& y_act_backprop_descriptor,\n    const DeviceMemory<Eigen::half>& y_act_backprop_data,\n    const DeviceMemory<Eigen::half>& y_act_data,\n    dnn::ActivationMode activation_mode,\n    const DeviceMemory<Eigen::half>& x_bn_data,\n    const dnn::BatchDescriptor& scale_offset_mean_variance_descriptor,\n    const DeviceMemory<float>& scale_data,\n    const DeviceMemory<float>& offset_data,\n    const DeviceMemory<float>& saved_mean_data,\n    const DeviceMemory<float>& saved_var_data,\n    DeviceMemory<Eigen::half>* x_bn_backprop_data,\n    DeviceMemory<float>* scale_backprop_data,\n    DeviceMemory<float>* offset_backprop_data,\n    dnn::ProfileResult* output_profile_result) {\n  return DoFusedBatchNormActivationBackwardImpl<Eigen::half, float>(\n      stream, miopenHalf, y_act_backprop_descriptor, y_act_backprop_data,\n      y_act_data, activation_mode, x_bn_data,\n      scale_offset_mean_variance_descriptor, scale_data, offset_data,\n      saved_mean_data, saved_var_data, x_bn_backprop_data, scale_backprop_data,\n      offset_backprop_data, output_profile_result);\n}\n\n}  // namespace gpu\n\nvoid initialize_miopen() {\n  auto miopenAlreadyRegistered = PluginRegistry::Instance()->HasFactory(\n      rocm::kROCmPlatformId, PluginKind::kDnn, gpu::kMIOpenPlugin);\n\n  if (!miopenAlreadyRegistered) {\n    tsl::Status status =\n        PluginRegistry::Instance()->RegisterFactory<PluginRegistry::DnnFactory>(\n            rocm::kROCmPlatformId, gpu::kMIOpenPlugin, \"MIOpen\",\n            [](internal::StreamExecutorInterface* parent) -> dnn::DnnSupport* {\n              gpu::GpuExecutor* rocm_executor =\n                  dynamic_cast<gpu::GpuExecutor*>(parent);\n              if (rocm_executor == nullptr) {\n                LOG(ERROR)\n                    << \"Attempting to initialize an instance of the MIOpen \"\n                    << \"support library with a non-ROCM StreamExecutor\";\n                return nullptr;\n              }\n\n              gpu::MIOpenSupport* dnn = new gpu::MIOpenSupport(rocm_executor);\n              if (!dnn->Init().ok()) {\n                // Note: Init() will log a more specific error.\n                delete dnn;\n                return nullptr;\n              }\n              return dnn;\n            });\n\n    if (!status.ok()) {\n      LOG(ERROR) << \"Unable to register MIOpen factory: \"\n                 << status.error_message();\n    }\n\n    PluginRegistry::Instance()->SetDefaultFactory(\n        rocm::kROCmPlatformId, PluginKind::kDnn, gpu::kMIOpenPlugin);\n  }\n}\n\n}  // namespace stream_executor\n\nREGISTER_MODULE_INITIALIZER(register_miopen,\n                            { stream_executor::initialize_miopen(); });"