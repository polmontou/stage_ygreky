"/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#include \"tensorflow/core/kernels/data/iterator_ops.h\"\n\n#include <cstdint>\n#include <functional>\n#include <memory>\n#include <string>\n#include <utility>\n#include <vector>\n\n#include \"absl/time/time.h\"\n#include \"tensorflow/core/activity_watcher/activity.h\"\n#include \"tensorflow/core/activity_watcher/activity_utils.h\"\n#include \"tensorflow/core/common_runtime/graph_runner.h\"\n#include \"tensorflow/core/common_runtime/renamed_device.h\"\n#include \"tensorflow/core/data/dataset_utils.h\"\n#include \"tensorflow/core/data/finalization_utils.h\"\n#include \"tensorflow/core/data/metric_utils.h\"\n#include \"tensorflow/core/data/serialization_utils.h\"\n#include \"tensorflow/core/data/tfdataz_metrics.h\"\n#include \"tensorflow/core/framework/cancellation.h\"\n#include \"tensorflow/core/framework/dataset_options.pb.h\"\n#include \"tensorflow/core/framework/function.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/framework/variant_op_registry.h\"\n#include \"tensorflow/core/framework/variant_tensor_data.h\"\n#include \"tensorflow/core/kernels/data/optional_ops.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/lib/core/refcount.h\"\n#include \"tensorflow/core/lib/core/threadpool.h\"\n#include \"tensorflow/core/lib/gtl/cleanup.h\"\n#include \"tensorflow/core/platform/env.h\"\n#include \"tensorflow/core/platform/errors.h\"\n#include \"tensorflow/core/platform/mutex.h\"\n#include \"tensorflow/core/platform/refcount.h\"\n#include \"tensorflow/core/platform/resource.h\"\n#include \"tensorflow/core/profiler/lib/traceme.h\"\n#include \"tensorflow/core/profiler/lib/traceme_encode.h\"\n\nnamespace tensorflow {\nnamespace data {\nnamespace {\n\n// See documentation in ../../ops/dataset_ops.cc for a high-level\n// description of the following ops.\n\nconst char kAnonymousIterator[] = \"AnonymousIterator\";\nconst char kAnonymousIteratorV2[] = \"AnonymousIteratorV2\";\nconst char kAnonymousIteratorV3[] = \"AnonymousIteratorV3\";\nconst char kIteratorVariantTypeName[] = \"tensorflow::Iterator\";\nconst char kOutputShapes[] = \"output_shapes\";\nconst char kOutputTypes[] = \"output_types\";\n\nbool SymbolicCheckpointEnabled(const Options& options) {\n  return options.optional_symbolic_checkpoint_case() ==\n             Options::kSymbolicCheckpoint &&\n         options.symbolic_checkpoint();\n}\n\n}  // namespace\n\n/* static */ constexpr const char* const\n    SerializeIteratorOp::kExternalStatePolicy;\n\nIteratorResource::IteratorResource(\n    Env* env, const DataTypeVector& output_dtypes,\n    const std::vector<PartialTensorShape>& output_shapes,\n    std::unique_ptr<DeviceMgr> device_mgr,\n    std::unique_ptr<FunctionLibraryDefinition> flib_def,\n    std::unique_ptr<ProcessFunctionLibraryRuntime> pflr,\n    FunctionLibraryRuntime* flr)\n    : metrics_collector_(flr->device()->device_type(), *env),\n      unbounded_thread_pool_(env, \"tf_data_iterator_resource\"),\n      env_(*env),\n      device_mgr_(std::move(device_mgr)),\n      iterator_state_(std::make_shared<State>(std::move(flib_def),\n                                              std::move(pflr), flr,\n                                              /*iterator=*/nullptr)),\n      output_dtypes_(output_dtypes),\n      output_shapes_(output_shapes) {\n  tf_dataz_metrics_collector_ = std::make_shared<TfDatazMetricsCollector>(*env);\n  TfDatazMetricsRegistry::Register(tf_dataz_metrics_collector_);\n  VLOG(2) << \"creating iterator resource\";\n}\n\nIteratorResource::~IteratorResource() {\n  TfDatazMetricsRegistry::Deregister(tf_dataz_metrics_collector_);\n  VLOG(2) << \"destroying iterator resource\";\n}\n\nStatus IteratorResource::GetNext(OpKernelContext* ctx,\n                                 std::vector<Tensor>* out_tensors,\n                                 bool* end_of_sequence) {\n  std::shared_ptr<State> captured_state;\n  {\n    tf_shared_lock l(mu_);\n    captured_state = iterator_state_;\n  }\n  auto iterator = captured_state->iterator();\n  if (!iterator) {\n    return errors::FailedPrecondition(\n        \"GetNext() failed because the iterator has not been initialized. \"\n        \"Ensure that you have run the initializer operation for this iterator \"\n        \"before getting the next element.\");\n  }\n  auto* dataset = captured_state->dataset();\n  IteratorContext::Params params(ctx);\n  params.cancellation_manager = captured_state->cancellation_manager();\n  params.flr = captured_state->flr();\n  params.function_handle_cache = captured_state->function_handle_cache();\n  params.resource_mgr = captured_state->resource_mgr();\n  params.symbolic_checkpoint = SymbolicCheckpointEnabled(dataset->options());\n  params.thread_factory = unbounded_thread_pool_.get_thread_factory();\n  params.thread_pool = &unbounded_thread_pool_;\n  std::function<void()> deregister_fn;\n  TF_RETURN_IF_ERROR(RegisterCancellationCallback(\n      ctx->cancellation_manager(),\n      [cm = params.cancellation_manager]() { cm->StartCancel(); },\n      &deregister_fn));\n  auto cleanup = gtl::MakeCleanup(std::move(deregister_fn));\n  IteratorContext iter_ctx(std::move(params));\n  const absl::Time start_time = metrics_collector_.RecordStart();\n  auto status = iterator->GetNext(&iter_ctx, out_tensors, end_of_sequence);\n  metrics_collector_.RecordStop(start_time, *out_tensors);\n  const int64_t get_next_latency_micros =\n      env_.NowMicros() - absl::ToUnixMicros(start_time);\n  tf_dataz_metrics_collector_->RecordGetNextLatency(get_next_latency_micros);\n  captured_state->MergeCheckpoint(iter_ctx.checkpoint());\n  return status;\n}\n\nStatus IteratorResource::Save(OpKernelContext* ctx,\n                              ExternalStatePolicy external_state_policy,\n                              IteratorStateWriter* writer) {\n  std::shared_ptr<State> captured_state;\n  {\n    tf_shared_lock l(mu_);\n    captured_state = iterator_state_;\n  }\n  auto iterator = captured_state->iterator();\n  if (!iterator) {\n    return errors::FailedPrecondition(\n        \"Save() failed because the iterator has not been initialized. Ensure \"\n        \"that you have run the initializer operation for this iterator before \"\n        \"saving it.\");\n  }\n  auto* dataset = captured_state->dataset();\n  if (SymbolicCheckpointEnabled(dataset->options())) {\n    const auto& checkpoint = captured_state->checkpoint();\n    if (!checkpoint.GetStatus().ok()) {\n      return checkpoint.GetStatus();\n    }\n    TF_RETURN_IF_ERROR(checkpoint.Save(writer));\n    return OkStatus();\n  }\n  SerializationContext::Params params(ctx);\n  params.external_state_policy = external_state_policy;\n  params.symbolic_checkpoint = SymbolicCheckpointEnabled(dataset->options());\n  SerializationContext serialization_ctx(params);\n  return iterator->Save(&serialization_ctx, writer);\n}\n\nStatus IteratorResource::Restore(OpKernelContext* ctx,\n                                 IteratorStateReader* reader) {\n  const DatasetBase* dataset;\n  std::shared_ptr<State> new_state;\n  const DatasetBase* input_dataset;\n  {\n    tf_shared_lock l(mu_);\n    auto iterator = iterator_state_->iterator();\n    if (!iterator) {\n      return errors::FailedPrecondition(\n          \"Restore() failed because the iterator has not been initialized. \"\n          \"Ensure that you have run the initializer operation for this \"\n          \"iterator before restoring it.\");\n    }\n    dataset = iterator->dataset();\n    // Hang onto a reference until we've created the new iterator, which will\n    // then hold its own reference to keep the dataset alive.\n    dataset->Ref();\n    new_state =\n        std::make_shared<State>(iterator_state_->flib_def(),\n                                iterator_state_->pflr(), iterator_state_->flr(),\n                                /*iterator=*/nullptr);\n    input_dataset = iterator_state_->dataset();\n  }\n  core::ScopedUnref scoped_unref(dataset);\n  IteratorContext::Params params(ctx);\n  params.cancellation_manager = new_state->cancellation_manager();\n  params.flr = new_state->flr();\n  params.function_handle_cache = new_state->function_handle_cache();\n  params.resource_mgr = new_state->resource_mgr();\n  params.symbolic_checkpoint =\n      SymbolicCheckpointEnabled(input_dataset->options());\n  params.thread_factory = unbounded_thread_pool_.get_thread_factory();\n  params.thread_pool = &unbounded_thread_pool_;\n  std::function<void()> deregister_fn;\n  TF_RETURN_IF_ERROR(RegisterCancellationCallback(\n      ctx->cancellation_manager(),\n      [cm = params.cancellation_manager]() { cm->StartCancel(); },\n      &deregister_fn));\n  auto cleanup = gtl::MakeCleanup(std::move(deregister_fn));\n  IteratorContext iter_ctx(IteratorContext(std::move(params)));\n  std::unique_ptr<IteratorBase> iterator_base;\n  TF_RETURN_IF_ERROR(dataset->MakeIteratorFromCheckpoint(\n      &iter_ctx, \"Iterator\", reader, &iterator_base));\n  new_state->DowncastAndSetIteratorAndDataset(std::move(iterator_base),\n                                              input_dataset);\n  new_state->MergeCheckpoint(iter_ctx.checkpoint());\n  mutex_lock l(mu_);\n  std::swap(iterator_state_, new_state);\n  return OkStatus();\n}\n\nStatus IteratorResource::SetIteratorFromDataset(OpKernelContext* ctx,\n                                                const DatasetBase* dataset) {\n  std::shared_ptr<State> new_state;\n  {\n    tf_shared_lock l(mu_);\n    new_state =\n        std::make_shared<State>(iterator_state_->flib_def(),\n                                iterator_state_->pflr(), iterator_state_->flr(),\n                                /*iterator=*/nullptr);\n  }\n\n  // Create new iterator.\n  IteratorContext::Params params(ctx);\n  params.cancellation_manager = new_state->cancellation_manager();\n  params.flr = new_state->flr();\n  params.function_handle_cache = new_state->function_handle_cache();\n  params.resource_mgr = new_state->resource_mgr();\n  params.symbolic_checkpoint = SymbolicCheckpointEnabled(dataset->options());\n  params.thread_factory = unbounded_thread_pool_.get_thread_factory();\n  params.thread_pool = &unbounded_thread_pool_;\n  std::function<void()> deregister_fn;\n  TF_RETURN_IF_ERROR(RegisterCancellationCallback(\n      ctx->cancellation_manager(),\n      [cm = params.cancellation_manager]() { cm->StartCancel(); },\n      &deregister_fn));\n  auto cleanup = gtl::MakeCleanup(std::move(deregister_fn));\n  IteratorContext iter_ctx(IteratorContext(std::move(params)));\n  std::unique_ptr<IteratorBase> iterator;\n  if (ctx->function_library()->device()->device_type() == DEVICE_CPU) {\n    DatasetBase* finalized_dataset;\n    TF_ASSIGN_OR_RETURN(finalized_dataset, GetFinalizedDataset(ctx, dataset));\n    TF_RETURN_IF_ERROR(finalized_dataset->MakeIterator(&iter_ctx,\n                                                       /*parent=*/nullptr,\n                                                       \"Iterator\", &iterator));\n  } else {\n    TF_RETURN_IF_ERROR(dataset->MakeIterator(&iter_ctx,\n                                             /*parent=*/nullptr, \"Iterator\",\n                                             &iterator));\n  }\n  TF_RETURN_IF_ERROR(\n      VerifyTypesMatch(output_dtypes_, iterator->output_dtypes()));\n  TF_RETURN_IF_ERROR(\n      VerifyShapesCompatible(output_shapes_, iterator->output_shapes()));\n  new_state->DowncastAndSetIteratorAndDataset(std::move(iterator), dataset);\n  new_state->MergeCheckpoint(iter_ctx.checkpoint());\n  mutex_lock l(mu_);\n  std::swap(iterator_state_, new_state);\n  return OkStatus();\n}\n\nvoid IteratorResource::State::DowncastAndSetIteratorAndDataset(\n    std::unique_ptr<IteratorBase> it, const DatasetBase* dataset) {\n  iterator_.reset(static_cast<DatasetBaseIterator*>(it.release()));\n  if (dataset) {\n    dataset->Ref();\n    dataset_.reset(const_cast<DatasetBase*>(dataset));\n  }\n}\n\nvoid IteratorResource::State::MergeCheckpoint(const MemoryCheckpoint& other) {\n  if (SymbolicCheckpointEnabled(dataset_->options())) {\n    checkpoint_.Merge(other);\n  }\n}\n\nnamespace {\n\n// A helper class that uses a list of IteratorStateVariant objects to represent\n// the state for an iterator resource. It exposes methods that help with\n// saving and restoring of this state. Sample usage\n// Saving:\n//   IteratorVariantSerializer serializer;\n//   serializer.InitializeFromIterator(iterator_resource);\n//   Tensor serialized_t;\n//   serializer.Serialize(&serialized_t);\n//\n// Restoring:\n//   IteratorVariantSerializer serializer;\n//   serializer.InitFromTensor(ctx->input(0));\n//   IteratorStateReader* reader = serializer.GetReader();\n//   iterator_resource->Restore(ctx, reader);\nclass IteratorVariantSerializer {\n public:\n  IteratorVariantSerializer() = default;\n\n  // Calls `Save` on the iterator_resource to build up the list of\n  // IteratorStateVariant objects.\n  Status InitializeFromIterator(OpKernelContext* ctx,\n                                ExternalStatePolicy external_state_policy,\n                                IteratorResource* iterator_resource) {\n    VariantTensorDataWriter writer;\n    TF_RETURN_IF_ERROR(\n        iterator_resource->Save(ctx, external_state_policy, &writer));\n    std::vector<std::unique_ptr<VariantTensorData>> data;\n    writer.ReleaseData(&data);\n    variants_.clear();\n    variants_.reserve(data.size());\n    for (auto& it : data) {\n      IteratorStateVariant v;\n      TF_RETURN_IF_ERROR(v.InitializeFromVariantData(std::move(it)));\n      variants_.push_back(v);\n    }\n    num_tensors_ = variants_.size();\n    can_serialize_ = true;\n    return OkStatus();\n  }\n\n  // Initializes `this` from `serialized_t` while restoring the iterator state.\n  Status InitFromTensor(const Tensor* serialized_t) {\n    int64_t num_tensors = serialized_t->dim_size(0);\n    auto serialized_vec = serialized_t->vec<Variant>();\n    std::vector<const VariantTensorData*> data;\n    data.reserve(num_tensors);\n    for (int i = 0; i < num_tensors; ++i) {\n      auto* w = serialized_vec(i).get<IteratorStateVariant>();\n      if (!w) {\n        return errors::Internal(\n            \"Cannot initialize an iterator from tensor \",\n            serialized_vec(i).DebugString(),\n            \". Expected a variant tensor of type IteratorStateVariant\");\n      }\n      data.push_back(w->GetData());\n    }\n    reader_ = std::make_unique<VariantTensorDataReader>(data);\n    num_tensors_ = data.size();\n    return OkStatus();\n  }\n\n  int64_t NumTensors() { return num_tensors_; }\n\n  // Stores the IteratorStateVariant list into a pre-allocated tensor. Expects\n  // that InitializeFromIterator was called before.\n  Status Serialize(Tensor* serialized) {\n    if (!can_serialize_) {\n      return errors::InvalidArgument(\n          \"Please call InitializeFromIterator before calling Serialize.\");\n    }\n    int64_t size = variants_.size();\n    for (int64_t i = 0; i < size; ++i) {\n      if (variants_[i].GetData() == nullptr) {\n        return errors::Internal(\n            \"Cannot serialize an empty IteratorStateVariant\");\n      }\n      serialized->vec<Variant>()(i) = variants_[i];\n    }\n    return OkStatus();\n  }\n\n  // Returns an IteratorStateReader to restore iterator state. Expects that\n  // InitFromTensor was called before.\n  IteratorStateReader* GetReader() { return reader_.get(); }\n\n private:\n  bool can_serialize_ = false;\n  int64_t num_tensors_;\n  std::vector<IteratorStateVariant> variants_;\n  std::unique_ptr<IteratorStateReader> reader_;\n};\n\n}  // namespace\n\n// Note that IteratorHandleOp holds a reference to the resource it creates. If\n// cleaning up resources with DestroyResourceOp is important, consider creating\n// resource containers with AnonymousIteratorHandleOp instead.\nIteratorHandleOp::IteratorHandleOp(OpKernelConstruction* ctx)\n    : OpKernel(ctx), graph_def_version_(ctx->graph_def_version()) {\n  OP_REQUIRES_OK(ctx, ctx->GetAttr(kOutputTypes, &output_dtypes_));\n  OP_REQUIRES_OK(ctx, ctx->GetAttr(kOutputShapes, &output_shapes_));\n  OP_REQUIRES_OK(ctx, ctx->GetAttr(\"shared_name\", &name_));\n}\n\n// The resource is deleted from the resource manager only when it is private\n// to kernel. Ideally the resource should be deleted when it is no longer held\n// by anyone, but it would break backward compatibility.\nIteratorHandleOp::~IteratorHandleOp() {\n  if (resource_ != nullptr) {\n    resource_->Unref();\n    if (cinfo_.resource_is_private_to_kernel()) {\n      if (!cinfo_.resource_manager()\n               ->template Delete<IteratorResource>(cinfo_.container(),\n                                                   cinfo_.name())\n               .ok()) {\n        // Do nothing; the resource can have been deleted by session resets.\n      }\n    }\n  }\n}\n\nvoid IteratorHandleOp::Compute(OpKernelContext* context)\n    TF_LOCKS_EXCLUDED(mu_) {\n  {\n    mutex_lock l(mu_);\n    if (resource_ == nullptr) {\n      FunctionLibraryRuntime* flr;\n      std::unique_ptr<DeviceMgr> device_mgr(nullptr);\n      std::unique_ptr<FunctionLibraryDefinition> flib_def(nullptr);\n      std::unique_ptr<ProcessFunctionLibraryRuntime> pflr(nullptr);\n      // If the iterator is shared then we construct a new FLR, and pass that\n      // in. NOTE(mrry,rohanj): In this case it is not possible to call remote\n      // functions from the iterator. We may add this functionality if there\n      // is sufficient demand, but it will require a significant refactoring.\n      if (!name_.empty()) {\n        flr = CreatePrivateFLR(context, &device_mgr, &flib_def, &pflr);\n      } else {\n        OP_REQUIRES_OK(context, context->function_library()->Clone(\n                                    &flib_def, &pflr, &flr, true));\n      }\n\n      ResourceMgr* mgr = context->resource_manager();\n      OP_REQUIRES_OK(context, cinfo_.Init(mgr, def()));\n\n      IteratorResource* resource;\n      OP_REQUIRES_OK(\n          context,\n          mgr->LookupOrCreate<IteratorResource>(\n              cinfo_.container(), cinfo_.name(), &resource,\n              [context, flr, &device_mgr, &flib_def, &pflr,\n               this](IteratorResource** ret) TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {\n                *ret = new IteratorResource(\n                    context->env(), output_dtypes_, output_shapes_,\n                    std::move(device_mgr), std::move(flib_def), std::move(pflr),\n                    flr);\n                return OkStatus();\n              }));\n\n      Status s = VerifyResource(resource);\n      if (TF_PREDICT_FALSE(!s.ok())) {\n        resource->Unref();\n        context->SetStatus(s);\n        return;\n      }\n\n      resource_ = resource;\n    }\n  }\n  OP_REQUIRES_OK(context, MakeResourceHandleToOutput(\n                              context, 0, cinfo_.container(), cinfo_.name(),\n                              TypeIndex::Make<IteratorResource>()));\n}\n\nStatus IteratorHandleOp::VerifyResource(IteratorResource* resource) {\n  TF_RETURN_IF_ERROR(\n      VerifyTypesMatch(output_dtypes_, resource->output_dtypes()));\n  TF_RETURN_IF_ERROR(\n      VerifyShapesCompatible(output_shapes_, resource->output_shapes()));\n  return OkStatus();\n}\n\nFunctionLibraryRuntime* IteratorHandleOp::CreatePrivateFLR(\n    OpKernelContext* ctx, std::unique_ptr<DeviceMgr>* device_mgr,\n    std::unique_ptr<FunctionLibraryDefinition>* flib_def,\n    std::unique_ptr<ProcessFunctionLibraryRuntime>* pflr) {\n  // Wrap the existing device in order to see any captured resources\n  // in its resource manager. The existing device will outlive the\n  // IteratorResource, because we are storing the IteratorResource\n  // in that device's resource manager.\n\n  *device_mgr =\n      std::make_unique<StaticDeviceMgr>(RenamedDevice::NewRenamedDevice(\n          ctx->device()->name(), down_cast<Device*>(ctx->device()),\n          false /* owns_underlying */, false /* isolate_session_state */));\n  *flib_def = std::make_unique<FunctionLibraryDefinition>(\n      *ctx->function_library()->GetFunctionLibraryDefinition());\n  const auto* config = ctx->function_library()->config_proto();\n  *pflr = std::make_unique<ProcessFunctionLibraryRuntime>(\n      device_mgr->get(), ctx->env(),\n      /*config=*/config, graph_def_version_, flib_def->get(),\n      config->graph_options().optimizer_options());\n\n  return (*pflr)->GetFLR(ctx->device()->name());\n}\n\n// Like IteratorHandleOp, but creates handles which are never shared, and does\n// not hold a reference to these handles. The latter is important for eager\n// execution, since OpKernel instances generally live as long as the program\n// running them.\nAnonymousIteratorHandleOp::AnonymousIteratorHandleOp(\n    OpKernelConstruction* context)\n    : AnonymousResourceOp<IteratorResource>(\n          context,\n          /* ref_counting */\n          // Only enable this for V2 (via Python's iter protocol),\n          // AnonymousIteratorV1 requires IteratorToStringHandle, which is\n          // undefined on Refcounting ResourceHandle.\n          context->def().op() == kAnonymousIteratorV2 ||\n              context->def().op() == kAnonymousIteratorV3,\n          // V1 does not return a deleter.\n          /* return_deleter */\n          context->def().op() == kAnonymousIteratorV2),\n      graph_def_version_(context->graph_def_version()) {\n  OP_REQUIRES_OK(context, context->GetAttr(kOutputTypes, &output_dtypes_));\n  OP_REQUIRES_OK(context, context->GetAttr(kOutputShapes, &output_shapes_));\n}\n\nstring AnonymousIteratorHandleOp::name() { return kAnonymousIterator; }\n\nStatus AnonymousIteratorHandleOp::CreateResource(\n    OpKernelContext* ctx, std::unique_ptr<FunctionLibraryDefinition> flib_def,\n    std::unique_ptr<ProcessFunctionLibraryRuntime> pflr,\n    FunctionLibraryRuntime* lib, IteratorResource** resource) {\n  std::unique_ptr<DeviceMgr> device_mgr(nullptr);\n  *resource = new IteratorResource(ctx->env(), output_dtypes_, output_shapes_,\n                                   std::move(device_mgr), std::move(flib_def),\n                                   std::move(pflr), lib);\n  return OkStatus();\n}\n\nHybridAsyncOpKernel::HybridAsyncOpKernel(OpKernelConstruction* ctx,\n                                         const char* background_worker_name)\n    : AsyncOpKernel(ctx),\n      background_worker_(ctx->env(), background_worker_name) {}\n\nvoid HybridAsyncOpKernel::ComputeAsync(OpKernelContext* ctx,\n                                       DoneCallback done) {\n  background_worker_.Schedule([this, ctx, done = std::move(done)]() {\n    ctx->SetStatus(DoCompute(ctx));\n    done();\n  });\n}\n\nvoid HybridAsyncOpKernel::Compute(OpKernelContext* ctx) {\n  ctx->SetStatus(DoCompute(ctx));\n}\n\nStatus MakeIteratorOp::DoCompute(OpKernelContext* ctx) {\n  tensorflow::ResourceTagger tag(kTFDataResourceTag,\n                                 ctx->op_kernel().type_string());\n  DatasetBase* dataset;\n  TF_RETURN_IF_ERROR(GetDatasetFromVariantTensor(ctx->input(0), &dataset));\n  IteratorResource* iterator_resource;\n  TF_RETURN_IF_ERROR(\n      LookupResource(ctx, HandleFromInput(ctx, 1), &iterator_resource));\n  core::ScopedUnref unref_iterator(iterator_resource);\n  return iterator_resource->SetIteratorFromDataset(ctx, dataset);\n}\n\nStatus DeleteIteratorOp::DoCompute(OpKernelContext* ctx) {\n  tensorflow::ResourceTagger tag(kTFDataResourceTag,\n                                 ctx->op_kernel().type_string());\n  const ResourceHandle& handle = ctx->input(0).flat<ResourceHandle>()(0);\n  // The iterator resource is guaranteed to exist because the variant tensor\n  // wrapping the deleter is provided as an unused input to this op, which\n  // guarantees that it has not run yet.\n  return DeleteResource(ctx, handle);\n}\n\nnamespace {\n\nclass ToSingleElementOp : public AsyncOpKernel {\n public:\n  explicit ToSingleElementOp(OpKernelConstruction* ctx)\n      : AsyncOpKernel(ctx),\n        metrics_collector_(ctx->device()->attributes().device_type(),\n                           *ctx->env()),\n        unbounded_threadpool_(ctx->env(), \"tf_data_to_single_element\") {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"output_types\", &output_types_));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"output_shapes\", &output_shapes_));\n  }\n\n  void ComputeAsync(OpKernelContext* ctx, DoneCallback done) override {\n    unbounded_threadpool_.Schedule([this, ctx, done = std::move(done)]() {\n      ctx->SetStatus(DoCompute(ctx));\n      done();\n    });\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    ctx->SetStatus(DoCompute(ctx));\n  }\n\n private:\n  Status DoCompute(OpKernelContext* ctx) {\n    profiler::TraceMe traceme(\n        [&] {\n          return profiler::TraceMeEncode(\"ToSingleElementOp::DoCompute\",\n                                         {{\"id\", ctx->step_id()}});\n        },\n        profiler::kInfo);\n    tensorflow::ResourceTagger tag(kTFDataResourceTag,\n                                   ctx->op_kernel().type_string());\n    metrics::RecordTFDataFetchOp(\"ToSingleElementOp\");\n    DatasetBase* dataset;\n    TF_RETURN_IF_ERROR(GetDatasetFromVariantTensor(ctx->input(0), &dataset));\n\n    IteratorContext::Params params(ctx);\n    ResourceMgr resource_mgr;\n    params.resource_mgr = &resource_mgr;\n    CancellationManager cancellation_manager(ctx->cancellation_manager());\n    params.cancellation_manager = &cancellation_manager;\n\n    IteratorContext iter_ctx(std::move(params));\n    std::unique_ptr<IteratorBase> iterator;\n    TF_RETURN_IF_ERROR(dataset->MakeIterator(\n        &iter_ctx, /*parent=*/nullptr, \"SingleElementIterator\", &iterator));\n\n    std::vector<Tensor> components;\n    components.reserve(dataset->output_dtypes().size());\n    bool end_of_sequence = false;\n\n    const absl::Time start_time = metrics_collector_.RecordStart();\n    TF_RETURN_IF_ERROR(\n        iterator->GetNext(&iter_ctx, &components, &end_of_sequence));\n    metrics_collector_.RecordStop(start_time, components);\n\n    if (end_of_sequence) {\n      return errors::InvalidArgument(\"Dataset was empty.\");\n    }\n    TF_RETURN_IF_ERROR(VerifyTypesMatch(output_types_, components));\n    TF_RETURN_IF_ERROR(VerifyShapesCompatible(output_shapes_, components));\n    for (int i = 0; i < components.size(); ++i) {\n      ctx->set_output(i, components[i]);\n    }\n\n    components.clear();\n    TF_RETURN_IF_ERROR(\n        iterator->GetNext(&iter_ctx, &components, &end_of_sequence));\n    if (!end_of_sequence) {\n      return errors::InvalidArgument(\"Dataset had more than one element.\");\n    }\n    return OkStatus();\n  }\n\n  IteratorMetricsCollector metrics_collector_;\n  UnboundedThreadPool unbounded_threadpool_;\n  DataTypeVector output_types_;\n  std::vector<PartialTensorShape> output_shapes_;\n};\n\nclass OneShotIteratorOp : public AsyncOpKernel {\n public:\n  explicit OneShotIteratorOp(OpKernelConstruction* ctx)\n      : AsyncOpKernel(ctx),\n        background_worker_(ctx->env(), \"tf_data_one_shot_iterator\"),\n        graph_def_version_(ctx->graph_def_version())\n\n  {\n    string shared_name;\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"shared_name\", &shared_name));\n    OP_REQUIRES(ctx, shared_name.empty(),\n                errors::InvalidArgument(\"OneShotIteratorOp does not currently \"\n                                        \"support the 'shared_name' attr.\"));\n    OP_REQUIRES_OK(ctx,\n                   ctx->GetAttr(\"dataset_factory\", &dataset_factory_func_));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(kOutputTypes, &output_dtypes_));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(kOutputShapes, &output_shapes_));\n  }\n\n  ~OneShotIteratorOp() override {\n    if (iterator_resource_ != nullptr) {\n      iterator_resource_->Unref();\n      if (!cinfo_.resource_manager()\n               ->Delete<IteratorResource>(cinfo_.container(), cinfo_.name())\n               .ok()) {\n        // Do nothing; the resource can have been deleted by session resets.\n      }\n    }\n  }\n\n  // NOTE(mrry): This is based on `ResourceOpKernel<T>::Compute()`,\n  // but due to the fact that `ResourceOpKernel<T>::CreateResource()`\n  // does not provide access to the `OpKernelContext*` and we need\n  // this to invoke the factory function, it's not possible to\n  // implement this kernel by implementing `CreateResource()`.\n  // Furthermore, due to the fact that this kernel might block when\n  // running the initialization function, we must implement this\n  // kernel as an async kernel.\n  void ComputeAsync(OpKernelContext* ctx, DoneCallback done) override {\n    tensorflow::ResourceTagger tag(kTFDataResourceTag,\n                                   ctx->op_kernel().type_string());\n    {\n      mutex_lock l(mu_);\n      if (iterator_resource_ == nullptr && initialization_status_.ok()) {\n        // The initialization thread will call `done`.\n        if (!initialization_started_) {\n          // TODO(mrry): Convert the initialization code to use\n          // callbacks instead of wasting a thread.\n          background_worker_.Schedule([this, ctx, done]() { Init(ctx, done); });\n          initialization_started_ = true;\n        } else {\n          done_callbacks_.emplace_back(ctx, std::move(done));\n        }\n        return;\n      }\n    }\n    ProduceOutput(ctx, done);\n  }\n\n private:\n  void Init(OpKernelContext* ctx, const DoneCallback& done) {\n    IteratorResource* iterator = nullptr;\n    ContainerInfo cinfo;\n    Status s = TryInit(ctx, &iterator, &cinfo);\n\n    std::vector<std::pair<OpKernelContext*, DoneCallback>> callbacks_to_run;\n    {\n      mutex_lock l(mu_);\n      if (s.ok()) {\n        iterator_resource_ = iterator;\n        cinfo_ = cinfo;\n      }\n      initialization_status_ = s;\n      std::swap(done_callbacks_, callbacks_to_run);\n    }\n\n    for (auto&& ctx_done : callbacks_to_run) {\n      ProduceOutput(ctx_done.first, ctx_done.second);\n    }\n    ProduceOutput(ctx, done);\n  }\n\n  Status TryInit(OpKernelContext* ctx, IteratorResource** iterator,\n                 ContainerInfo* cinfo) {\n    TF_RETURN_IF_ERROR(cinfo->Init(ctx->resource_manager(), def()));\n\n    FunctionLibraryRuntime* flr;\n    std::unique_ptr<FunctionLibraryDefinition> flib_def(nullptr);\n    std::unique_ptr<ProcessFunctionLibraryRuntime> pflr(nullptr);\n    TF_RETURN_IF_ERROR(\n        ctx->function_library()->Clone(&flib_def, &pflr, &flr, true));\n\n    // Create an IteratorResource that will hold the iterator for this op.\n    TF_RETURN_IF_ERROR(\n        ctx->resource_manager()->LookupOrCreate<IteratorResource>(\n            cinfo->container(), cinfo->name(), iterator,\n            [ctx, flr, this, &flib_def, &pflr](IteratorResource** ret)\n                TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {\n                  *ret = new IteratorResource(\n                      ctx->env(), output_dtypes_, output_shapes_,\n                      /*device_mgr=*/nullptr, std::move(flib_def),\n                      std::move(pflr), flr);\n                  return OkStatus();\n                }));\n\n    core::ScopedUnref unref_iterator(*iterator);\n\n    TF_RETURN_IF_ERROR(\n        VerifyTypesMatch(output_dtypes_, (*iterator)->output_dtypes()));\n    TF_RETURN_IF_ERROR(\n        VerifyShapesCompatible(output_shapes_, (*iterator)->output_shapes()));\n\n    // Call the dataset_factory_func_ to create a new dataset,\n    // over which this op will iterate.\n    FunctionLibraryRuntime::Handle f_handle;\n    TF_RETURN_IF_ERROR(ctx->function_library()->Instantiate(\n        dataset_factory_func_.name(), AttrSlice(&dataset_factory_func_.attr()),\n        &f_handle));\n    FunctionLibraryRuntime::Options opts;\n    opts.cancellation_manager = ctx->cancellation_manager();\n    ScopedStepContainer step_container(opts.step_id, [ctx](const string& name) {\n      ctx->resource_manager()->Cleanup(name).IgnoreError();\n    });\n    opts.step_container = &step_container;\n    opts.runner = ctx->runner();\n    opts.run_all_kernels_inline = ctx->run_all_kernels_inline();\n    std::vector<Tensor> return_values;\n    TF_RETURN_IF_ERROR(ctx->function_library()->RunSync(\n        std::move(opts), f_handle, {}, &return_values));\n    if (return_values.size() != 1 || return_values[0].dtype() != DT_VARIANT ||\n        !TensorShapeUtils::IsScalar(return_values[0].shape())) {\n      return errors::InvalidArgument(\n          \"The `dataset_factory` function must return \"\n          \"a single scalar of dtype DT_VARIANT.\");\n    }\n\n    // Create an iterator for the dataset that was created in the\n    // factory function.\n    DatasetBase* dataset;\n    TF_RETURN_IF_ERROR(GetDatasetFromVariantTensor(return_values[0], &dataset));\n    TF_RETURN_IF_ERROR((*iterator)->SetIteratorFromDataset(ctx, dataset));\n    (*iterator)->Ref();\n    return OkStatus();\n  }\n\n  void ProduceOutput(OpKernelContext* ctx, const DoneCallback& done) {\n    Tensor* handle;\n    OP_REQUIRES_OK_ASYNC(ctx, ctx->allocate_output(0, TensorShape({}), &handle),\n                         done);\n    Status s;\n    {\n      mutex_lock l(mu_);\n      s = initialization_status_;\n      if (s.ok()) {\n        handle->scalar<ResourceHandle>()() =\n            MakeResourceHandle<IteratorResource>(ctx, cinfo_.container(),\n                                                 cinfo_.name());\n      }\n    }\n    OP_REQUIRES_OK_ASYNC(ctx, s, done);\n    done();\n  }\n\n  NameAttrList dataset_factory_func_;\n  DataTypeVector output_dtypes_;\n  std::vector<PartialTensorShape> output_shapes_;\n\n  BackgroundWorker background_worker_;\n\n  mutex mu_;\n  ContainerInfo cinfo_ TF_GUARDED_BY(mu_);\n  IteratorResource* iterator_resource_ TF_GUARDED_BY(mu_) = nullptr;\n\n  bool initialization_started_ TF_GUARDED_BY(mu_) = false;\n  Status initialization_status_ TF_GUARDED_BY(mu_);\n  std::vector<std::pair<OpKernelContext*, DoneCallback>> done_callbacks_\n      TF_GUARDED_BY(mu_);\n  const int graph_def_version_;\n};\n\n}  // namespace\n\nAsyncOpKernel* IteratorGetNextOp::AsAsync() {\n  return type_string() == \"IteratorGetNextSync\" ? nullptr : this;\n}\n\nvoid RecordElementSize(const std::vector<Tensor> element,\n                       profiler::TraceMe* traceme) {\n  traceme->AppendMetadata([&]() {\n    int64_t element_size = 0;\n    for (const auto& component : element) {\n      element_size += component.TotalBytes();\n    }\n    return profiler::TraceMeEncode({{\"element_size\", element_size}});\n  });\n}\n\nStatus IteratorGetNextOp::DoCompute(OpKernelContext* ctx) {\n  VLOG(3) << \"IteratorGetNextOp enter. iter_id=\" << ctx->frame_iter().iter_id;\n  auto cleanup = gtl::MakeCleanup([ctx] {\n    VLOG(3) << \"IteratorGetNextOp exit. iter_id=\" << ctx->frame_iter().iter_id;\n  });\n  activity_watcher::ActivityScope activity_scope([ctx = ctx]() {\n    return activity_watcher::ActivityFromContext(\n        ctx, \"IteratorGetNextOp::DoCompute\",\n        activity_watcher::ActivityCategory::kDatasetOp);\n  });\n  profiler::TraceMe traceme(\n      [&] {\n        return profiler::TraceMeEncode(\n            \"IteratorGetNextOp::DoCompute\",\n            {{\"id\", ctx->step_id()}, {\"iter_num\", ctx->frame_iter().iter_id}});\n      },\n      profiler::kInfo);\n  tensorflow::ResourceTagger tag(kTFDataResourceTag,\n                                 ctx->op_kernel().type_string());\n  metrics::RecordTFDataFetchOp(\"IteratorGetNextOp\");\n  IteratorResource* iterator;\n  TF_RETURN_IF_ERROR(LookupResource(ctx, HandleFromInput(ctx, 0), &iterator));\n  core::ScopedUnref unref_iterator(iterator);\n  std::vector<Tensor> components;\n  bool end_of_sequence = false;\n\n  TF_RETURN_IF_ERROR(iterator->GetNext(ctx, &components, &end_of_sequence));\n  if (end_of_sequence) {\n    return errors::OutOfRange(\"End of sequence\");\n  }\n  TF_RETURN_IF_ERROR(VerifyTypesMatch(output_types_, components));\n  TF_RETURN_IF_ERROR(VerifyShapesCompatible(output_shapes_, components));\n  RecordElementSize(components, &traceme);\n  for (int i = 0; i < components.size(); ++i) {\n    ctx->set_output(i, components[i]);\n  }\n  return OkStatus();\n}\n\nStatus IteratorGetNextAsOptionalOp::DoCompute(OpKernelContext* ctx) {\n  VLOG(3) << \"IteratorGetNextAsOptionalOp enter. iter_id=\"\n          << ctx->frame_iter().iter_id;\n  auto cleanup = gtl::MakeCleanup([ctx] {\n    VLOG(3) << \"IteratorGetNextAsOptionalOp exit. iter_id=\"\n            << ctx->frame_iter().iter_id;\n  });\n  activity_watcher::ActivityScope activity_scope([ctx = ctx]() {\n    return activity_watcher::ActivityFromContext(\n        ctx, \"IteratorGetNextAsOptionalOp::DoCompute\",\n        activity_watcher::ActivityCategory::kDatasetOp);\n  });\n  profiler::TraceMe traceme(\n      [&] {\n        return profiler::TraceMeEncode(\n            \"IteratorGetNextAsOptionalOp::DoCompute\",\n            {{\"id\", ctx->step_id()}, {\"iter_num\", ctx->frame_iter().iter_id}});\n      },\n      profiler::kInfo);\n  tensorflow::ResourceTagger tag(kTFDataResourceTag,\n                                 ctx->op_kernel().type_string());\n  metrics::RecordTFDataFetchOp(\"IteratorGetNextAsOptionalOp\");\n  IteratorResource* iterator;\n  TF_RETURN_IF_ERROR(LookupResource(ctx, HandleFromInput(ctx, 0), &iterator));\n  core::ScopedUnref unref_iterator(iterator);\n  std::vector<Tensor> components;\n  bool end_of_sequence = false;\n\n  TF_RETURN_IF_ERROR(iterator->GetNext(ctx, &components, &end_of_sequence));\n\n  if (end_of_sequence) {\n    return WriteOptionalNoneToOutput(ctx, 0);\n  } else {\n    RecordElementSize(components, &traceme);\n    for (int i = 0; i < components.size(); ++i) {\n      if (components[i].dtype() != output_types_[i]) {\n        return errors::InvalidArgument(\n            \"The given optional does not match the expected type for \"\n            \"component \",\n            i, \". Expected: \", DataTypeString(output_types_[i]),\n            \". Actual: \", DataTypeString(components[i].dtype()), \".\");\n      }\n      if (!output_shapes_[i].IsCompatibleWith(components[i].shape())) {\n        return errors::InvalidArgument(\n            \"The given optional does not match the expected shape \"\n            \"for component \",\n            i, \". Expected: \", output_shapes_[i].DebugString(),\n            \". Actual: \", components[i].shape().DebugString(), \".\");\n      }\n    }\n    return WriteOptionalWithValueToOutput(ctx, 0, std::move(components));\n  }\n}\n\nvoid IteratorToStringHandleOp::Compute(OpKernelContext* ctx) {\n  const Tensor& resource_handle_t = ctx->input(0);\n  OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(resource_handle_t.shape()),\n              errors::InvalidArgument(\"resource_handle must be a scalar\"));\n\n  // Validate that the handle corresponds to a real resource, and\n  // that it is an IteratorResource.\n  IteratorResource* iterator_resource;\n  OP_REQUIRES_OK(\n      ctx, LookupResource(ctx, HandleFromInput(ctx, 0), &iterator_resource));\n  iterator_resource->Unref();\n\n  Tensor* string_handle_t;\n  OP_REQUIRES_OK(ctx,\n                 ctx->allocate_output(0, TensorShape({}), &string_handle_t));\n  string_handle_t->scalar<tstring>()() =\n      resource_handle_t.scalar<ResourceHandle>()().SerializeAsString();\n}\n\nIteratorFromStringHandleOp::IteratorFromStringHandleOp(\n    OpKernelConstruction* ctx)\n    : OpKernel(ctx) {\n  OP_REQUIRES_OK(ctx, ctx->GetAttr(kOutputTypes, &output_dtypes_));\n  OP_REQUIRES_OK(ctx, ctx->GetAttr(kOutputShapes, &output_shapes_));\n  OP_REQUIRES(\n      ctx,\n      output_dtypes_.empty() || output_shapes_.empty() ||\n          output_dtypes_.size() == output_shapes_.size(),\n      errors::InvalidArgument(\"If both 'output_types' and 'output_shapes' \"\n                              \"are set, they must have the same length.\"));\n}\n\nvoid IteratorFromStringHandleOp::Compute(OpKernelContext* ctx) {\n  const Tensor& string_handle_t = ctx->input(0);\n  OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(string_handle_t.shape()),\n              errors::InvalidArgument(\"string_handle must be a scalar\"));\n\n  ResourceHandle resource_handle;\n  OP_REQUIRES(\n      ctx, resource_handle.ParseFromString(string_handle_t.scalar<tstring>()()),\n      errors::InvalidArgument(\n          \"Could not parse string_handle as a valid ResourceHandle\"));\n\n  OP_REQUIRES(\n      ctx, resource_handle.device() == ctx->device()->attributes().name(),\n      errors::InvalidArgument(\"Attempted create an iterator on device \\\"\",\n                              ctx->device()->attributes().name(),\n                              \"\\\" from handle defined on device \\\"\",\n                              resource_handle.device(), \"\\\"\"));\n\n  // Validate that the handle corresponds to a real resource, and\n  // that it is an IteratorResource.\n  IteratorResource* iterator_resource;\n  OP_REQUIRES_OK(ctx, LookupResource(ctx, resource_handle, &iterator_resource));\n  core::ScopedUnref unref_iterator(iterator_resource);\n  if (!output_dtypes_.empty()) {\n    OP_REQUIRES_OK(ctx, VerifyTypesMatch(output_dtypes_,\n                                         iterator_resource->output_dtypes()));\n  }\n  if (!output_shapes_.empty()) {\n    OP_REQUIRES_OK(ctx,\n                   VerifyShapesCompatible(output_shapes_,\n                                          iterator_resource->output_shapes()));\n  }\n\n  Tensor* resource_handle_t;\n  OP_REQUIRES_OK(ctx,\n                 ctx->allocate_output(0, TensorShape({}), &resource_handle_t));\n  resource_handle_t->scalar<ResourceHandle>()() = resource_handle;\n}\n\nSerializeIteratorOp::SerializeIteratorOp(OpKernelConstruction* ctx)\n    : OpKernel(ctx) {\n  if (ctx->HasAttr(kExternalStatePolicy)) {\n    int64_t external_state_policy;\n    OP_REQUIRES_OK(ctx,\n                   ctx->GetAttr(kExternalStatePolicy, &external_state_policy));\n    external_state_policy_ = ExternalStatePolicy(external_state_policy);\n  }\n}\n\nvoid SerializeIteratorOp::Compute(OpKernelContext* ctx) {\n  tensorflow::ResourceTagger tag(kTFDataResourceTag,\n                                 ctx->op_kernel().type_string());\n  const Tensor& resource_handle_t = ctx->input(0);\n  OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(resource_handle_t.shape()),\n              errors::InvalidArgument(\"resource_handle must be a scalar\"));\n  // Validate that the handle corresponds to a real resource, and\n  // that it is an IteratorResource.\n  IteratorResource* iterator_resource;\n  OP_REQUIRES_OK(\n      ctx, LookupResource(ctx, HandleFromInput(ctx, 0), &iterator_resource));\n  core::ScopedUnref unref_iterator(iterator_resource);\n  IteratorVariantSerializer serializer;\n  OP_REQUIRES_OK(ctx, serializer.InitializeFromIterator(\n                          ctx, external_state_policy_, iterator_resource));\n  Tensor* serialized_t;\n  OP_REQUIRES_OK(ctx,\n                 ctx->allocate_output(0, TensorShape({serializer.NumTensors()}),\n                                      &serialized_t));\n  OP_REQUIRES_OK(ctx, serializer.Serialize(serialized_t));\n}\n\nvoid DeserializeIteratorOp::Compute(OpKernelContext* ctx) {\n  tensorflow::ResourceTagger tag(kTFDataResourceTag,\n                                 ctx->op_kernel().type_string());\n  // Validate that the handle corresponds to a real resource, and\n  // that it is an IteratorResource.\n  IteratorResource* iterator_resource;\n  OP_REQUIRES_OK(\n      ctx, LookupResource(ctx, HandleFromInput(ctx, 0), &iterator_resource));\n  core::ScopedUnref unref_iterator(iterator_resource);\n  const Tensor* serialized_t;\n  OP_REQUIRES_OK(ctx, ctx->input(\"serialized\", &serialized_t));\n  IteratorVariantSerializer serializer;\n  OP_REQUIRES_OK(ctx, serializer.InitFromTensor(serialized_t));\n  Status s = iterator_resource->Restore(ctx, serializer.GetReader());\n  if (!s.ok()) {\n    OP_REQUIRES_OK(\n        ctx,\n        errors::CreateWithUpdatedMessage(\n            s, absl::StrCat(\n                   \"Failed to restore dataset iterator from checkpoint: \",\n                   s.error_message(),\n                   \". Make sure the dataset definition has not changed between \"\n                   \"the process that saved the checkpoint and the process that \"\n                   \"is restoring it.\")));\n  }\n}\n\nnamespace {\n\nREGISTER_KERNEL_BUILDER(Name(\"Iterator\").Device(DEVICE_CPU), IteratorHandleOp);\nREGISTER_KERNEL_BUILDER(Name(\"IteratorV2\").Device(DEVICE_CPU).Priority(2),\n                        IteratorHandleOp);\nREGISTER_KERNEL_BUILDER(Name(\"IteratorV2\").Device(DEVICE_GPU).Priority(1),\n                        IteratorHandleOp);\nREGISTER_KERNEL_BUILDER(Name(\"MakeIterator\").Device(DEVICE_CPU).Priority(2),\n                        MakeIteratorOp);\nREGISTER_KERNEL_BUILDER(\n    Name(\"MakeIterator\").Device(DEVICE_GPU).Priority(1).HostMemory(\"dataset\"),\n    MakeIteratorOp);\nREGISTER_KERNEL_BUILDER(Name(\"DeleteIterator\").Device(DEVICE_CPU).Priority(2),\n                        DeleteIteratorOp);\nREGISTER_KERNEL_BUILDER(Name(\"DeleteIterator\").Device(DEVICE_GPU).Priority(1),\n                        DeleteIteratorOp);\nREGISTER_KERNEL_BUILDER(\n    Name(\"AnonymousIterator\").Device(DEVICE_CPU).Priority(2),\n    AnonymousIteratorHandleOp);\nREGISTER_KERNEL_BUILDER(\n    Name(\"AnonymousIterator\").Device(DEVICE_GPU).Priority(1),\n    AnonymousIteratorHandleOp);\nREGISTER_KERNEL_BUILDER(\n    Name(\"AnonymousIteratorV2\").Device(DEVICE_CPU).Priority(2),\n    AnonymousIteratorHandleOp);\nREGISTER_KERNEL_BUILDER(\n    Name(\"AnonymousIteratorV2\").Device(DEVICE_GPU).Priority(1),\n    AnonymousIteratorHandleOp);\nREGISTER_KERNEL_BUILDER(\n    Name(\"AnonymousIteratorV3\").Device(DEVICE_CPU).Priority(2),\n    AnonymousIteratorHandleOp);\nREGISTER_KERNEL_BUILDER(\n    Name(\"AnonymousIteratorV3\").Device(DEVICE_GPU).Priority(1),\n    AnonymousIteratorHandleOp);\nREGISTER_KERNEL_BUILDER(Name(\"DatasetToSingleElement\").Device(DEVICE_CPU),\n                        ToSingleElementOp);\nREGISTER_KERNEL_BUILDER(Name(\"OneShotIterator\").Device(DEVICE_CPU),\n                        OneShotIteratorOp);\nREGISTER_KERNEL_BUILDER(Name(\"IteratorGetNext\").Device(DEVICE_CPU).Priority(2),\n                        IteratorGetNextOp);\nREGISTER_KERNEL_BUILDER(Name(\"IteratorGetNext\").Device(DEVICE_GPU).Priority(1),\n                        IteratorGetNextOp);\nREGISTER_KERNEL_BUILDER(\n    Name(\"IteratorGetNextSync\").Device(DEVICE_CPU).Priority(2),\n    IteratorGetNextOp);\nREGISTER_KERNEL_BUILDER(\n    Name(\"IteratorGetNextSync\").Device(DEVICE_GPU).Priority(1),\n    IteratorGetNextOp);\nREGISTER_KERNEL_BUILDER(\n    Name(\"IteratorGetNextAsOptional\").Device(DEVICE_CPU).Priority(2),\n    IteratorGetNextAsOptionalOp);\nREGISTER_KERNEL_BUILDER(\n    Name(\"IteratorGetNextAsOptional\").Device(DEVICE_GPU).Priority(1),\n    IteratorGetNextAsOptionalOp);\nREGISTER_KERNEL_BUILDER(\n    Name(\"IteratorToStringHandle\").Device(DEVICE_CPU).Priority(2),\n    IteratorToStringHandleOp);\nREGISTER_KERNEL_BUILDER(Name(\"IteratorToStringHandle\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"string_handle\")\n                            .Priority(1),\n                        IteratorToStringHandleOp);\nREGISTER_KERNEL_BUILDER(Name(\"IteratorFromStringHandle\").Device(DEVICE_CPU),\n                        IteratorFromStringHandleOp);\nREGISTER_KERNEL_BUILDER(\n    Name(\"IteratorFromStringHandleV2\").Device(DEVICE_CPU).Priority(2),\n    IteratorFromStringHandleOp);\nREGISTER_KERNEL_BUILDER(Name(\"IteratorFromStringHandleV2\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"string_handle\")\n                            .Priority(1),\n                        IteratorFromStringHandleOp);\nREGISTER_KERNEL_BUILDER(Name(\"SerializeIterator\").Device(DEVICE_CPU),\n                        SerializeIteratorOp);\nREGISTER_KERNEL_BUILDER(Name(\"DeserializeIterator\").Device(DEVICE_CPU),\n                        DeserializeIteratorOp);\n\n}  // namespace\n}  // namespace data\n}  // namespace tensorflow"