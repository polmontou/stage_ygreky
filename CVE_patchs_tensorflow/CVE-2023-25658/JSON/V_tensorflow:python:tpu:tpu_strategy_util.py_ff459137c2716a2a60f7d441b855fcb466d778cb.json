"# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"TPU specific APIs to be used in conjunction with TPU Strategy.\"\"\"\n\nimport gc\n\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.python.client import session as session_lib\nfrom tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver import TPUClusterResolver\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import monitoring\nfrom tensorflow.python.eager.def_function import function\nfrom tensorflow.python.framework import device\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.tpu import topology\nfrom tensorflow.python.tpu import tpu\nfrom tensorflow.python.util import compat\nfrom tensorflow.python.util.tf_export import tf_export\n\n\n_INITIALIZED_TPU_SYSTEMS = {}\n_LOCAL_MASTERS = (\"\", \"local\")\n\n\n_tpu_worker_address = monitoring.StringGauge(\n    \"/tensorflow/tpu/worker_address\",\n    \"The worker address that the coordinator/client connects to.\", \"address\")\n\n\n@tf_export(\"tpu.experimental.initialize_tpu_system\")\ndef initialize_tpu_system(cluster_resolver=None):\n  \"\"\"Initialize the TPU devices.\n\n  Args:\n    cluster_resolver: A tf.distribute.cluster_resolver.TPUClusterResolver,\n        which provides information about the TPU cluster.\n  Returns:\n    The tf.tpu.Topology object for the topology of the TPU cluster. If called\n    inside tf.function, it returns the serialized topology object instead.\n\n  Raises:\n    RuntimeError: If running inside a tf.function.\n    NotFoundError: If no TPU devices found in eager mode.\n  \"\"\"\n\n  # Deallocate all TPU buffers by clearing out eager context caches and\n  # triggering garbage collection to avoid keeping invalid tpu buffer around\n  # after reinitialized tpu system.\n  logging.info(\"Deallocate tpu buffers before initializing tpu system.\")\n  context.context()._clear_caches()  # pylint: disable=protected-access\n  context.context().clear_kernel_cache()\n  gc.collect()\n\n  job = None\n  if cluster_resolver is None:\n    # If no cluster resolver is specified, and running eagerly, execute the init\n    # ops in the current device scope.\n    if context.executing_eagerly():\n      curr_device = device.DeviceSpec.from_string(context.context().device_name)\n      if curr_device.job is not None:\n        job = \"{}/replica:0/task:0\".format(curr_device.job)\n\n    cluster_resolver = TPUClusterResolver(\"\")\n  assert isinstance(cluster_resolver, TPUClusterResolver)\n\n  tpu_name = compat.as_text(cluster_resolver._tpu)  # pylint: disable=protected-access\n  if tpu_name in _INITIALIZED_TPU_SYSTEMS:\n    logging.warning(\n        \"TPU system %s has already been initialized. \"\n        \"Reinitializing the TPU can cause previously created \"\n        \"variables on TPU to be lost.\", tpu_name)\n\n  logging.info(\"Initializing the TPU system: %s\", tpu_name)\n\n  # This function looks as it is for the following non-intuitive reasons.\n  # tpu.initialize_system creates a dummy op whose sole purpose is to trigger\n  # DistributedTPURewritePass. This pass actually adds real ops that\n  # initialize the TPU system. Thus, we can't simply run tpu.initialize_system\n  # eagerly. We need to wrap it in defun and trigger the rewrite passes on it.\n  if tpu_name not in _LOCAL_MASTERS:\n    # Explicitly place the tpu.initialize_system in the first worker to\n    # avoid the output node match multiple devices error.\n    job = \"{}/replica:0/task:0\".format(cluster_resolver.get_job_name())\n\n  if context.executing_eagerly():\n    @function\n    def _tpu_init_fn():\n      # In TF1, we usually close chips when compilation fails to clear the data\n      # in infeed. In TF2, we don't need to do this because infeed is no longer\n      # used, so user can recover from TPU compilation failures more smoothly.\n      # Same for the cancellation of a TPU excution.\n      return tpu.initialize_system(\n          job=job,\n          compilation_failure_closes_chips=False,\n          tpu_cancellation_closes_chips=False)\n\n    # The TPU_SYSTEM device must match the device used in tpu.initialize_system\n    # exactly, otherwise you can get errors if there are multiple TPU_SYSTEM\n    # devices available.\n    try:\n      with ops.device(tpu._tpu_system_device_name(job)):  # pylint: disable=protected-access\n        output = _tpu_init_fn()\n      context.async_wait()\n    except errors.InvalidArgumentError as e:\n      raise errors.NotFoundError(\n          None, None,\n          \"TPUs not found in the cluster. Failed in initialization: \"\n          + str(e))\n\n    # Clear out the eager context caches since the memory is invalid now.\n    context.context()._initialize_logical_devices()  # pylint: disable=protected-access\n\n    serialized_topology = output.numpy()\n  elif not ops.executing_eagerly_outside_functions():\n    master = cluster_resolver.master()\n    cluster_spec = cluster_resolver.cluster_spec()\n\n    session_config = config_pb2.ConfigProto(allow_soft_placement=True)\n    if cluster_spec:\n      session_config.cluster_def.CopyFrom(cluster_spec.as_cluster_def())\n\n    with ops.Graph().as_default():\n      with session_lib.Session(config=session_config, target=master) as sess:\n        serialized_topology = sess.run(tpu.initialize_system())\n  else:\n    with ops.device(tpu._tpu_system_device_name(job)):  # pylint: disable=protected-access\n      serialized_topology = tpu.initialize_system(\n          job=job, compilation_failure_closes_chips=False)\n      # If initialize_tpu_system is called inside tf.function, we only return\n      # the serialized topology object as the tf.tpu.Topology object has to be\n      # constructed in eager mode.\n      return serialized_topology\n\n  logging.info(\"Finished initializing TPU system.\")\n  tpu_topology = topology.Topology(serialized=serialized_topology)\n  cluster_resolver.set_tpu_topology(serialized_topology)\n  _INITIALIZED_TPU_SYSTEMS[tpu_name] = tpu_topology\n\n  # Record the address of the TPU worker-0 that the coordinator connects to.\n  # This can be used to associate the TPU worker with the right coordinator when\n  # aggregating the metrics for the application. An example of the address:\n  # /bns/mb/borg/mb/bns/chienchunh/chienchunh_group_49640234.1.tfm_train_tpu_worker/0\n  _tpu_worker_address.get_cell(\"address\").set(cluster_resolver.get_master())\n\n  return tpu_topology\n\n\ndef get_initialized_tpu_systems():\n  \"\"\"Returns all currently initialized tpu systems.\n\n  Returns:\n     A dictionary, with tpu name as the key and the tpu topology as the value.\n  \"\"\"\n  return _INITIALIZED_TPU_SYSTEMS.copy()\n\n\n@tf_export(\"tpu.experimental.shutdown_tpu_system\")\ndef shutdown_tpu_system(cluster_resolver=None):\n  \"\"\"Shuts down the TPU devices.\n\n  This will clear all caches, even those that are maintained through sequential\n  calls to tf.tpu.experimental.initialize_tpu_system, such as the compilation\n  cache.\n\n  Args:\n    cluster_resolver: A tf.distribute.cluster_resolver.TPUClusterResolver,\n        which provides information about the TPU cluster.\n\n  Raises:\n    RuntimeError: If no TPU devices found for eager execution or if run in a\n        tf.function.\n  \"\"\"\n  job = None\n  if cluster_resolver is None:\n    # If no cluster resolver is specified, and running eagerly, execute the init\n    # ops in the current device scope.\n    if context.executing_eagerly():\n      curr_device = device.DeviceSpec.from_string(context.context().device_name)\n      if curr_device.job is not None:\n        job = \"{}/replica:0/task:0\".format(curr_device.job)\n\n    cluster_resolver = TPUClusterResolver(\"\")\n  assert isinstance(cluster_resolver, TPUClusterResolver)\n\n  tpu_name = compat.as_text(cluster_resolver._tpu)  # pylint: disable=protected-access\n  if tpu_name not in _INITIALIZED_TPU_SYSTEMS:\n    logging.warning(\"You are shutting down a TPU system %s that has not been \"\n                    \"initialized.\" % tpu_name)\n\n  logging.info(\"Shutting down the TPU system: %s\", tpu_name)\n\n  if context.executing_eagerly():\n    # This function looks as it is for the following non-intuitive reasons.\n    # tpu.shutdown_system creates a dummy op whose sole purpose is to trigger\n    # DistributedTPURewritePass. This pass actually adds real ops that\n    # shutdown the TPU system. Thus, we can't simply run tpu.shutdown_system\n    # eagerly. We need to wrap it in defun and trigger the rewrite passes on it.\n    if tpu_name not in _LOCAL_MASTERS:\n      # Explicitly place the tpu.shutdown_system in the first worker to\n      # avoid the output node match multiple devices error.\n      job = \"{}/replica:0/task:0\".format(cluster_resolver.get_job_name())\n\n    @function\n    def _tpu_shutdown_fn():\n      tpu.shutdown_system(job=job)\n\n    # The TPU_SYSTEM device must match the device used in tpu.shutdown_system\n    # exactly, otherwise you can get errors if there are multiple TPU_SYSTEM\n    # devices available.\n    with ops.device(tpu._tpu_system_device_name(job)):  # pylint: disable=protected-access\n      _tpu_shutdown_fn()\n\n    # Clear out the eager context caches since the memory is invalid now.\n    logging.info(\"Clearing out eager caches\")\n    context.context()._clear_caches()  # pylint: disable=protected-access\n    context.context().clear_kernel_cache()\n  elif not ops.executing_eagerly_outside_functions():\n    master = cluster_resolver.master()\n    cluster_spec = cluster_resolver.cluster_spec()\n\n    session_config = config_pb2.ConfigProto(allow_soft_placement=True)\n    if cluster_spec:\n      session_config.cluster_def.CopyFrom(cluster_spec.as_cluster_def())\n\n    with ops.Graph().as_default():\n      with session_lib.Session(config=session_config, target=master) as sess:\n        sess.run(tpu.shutdown_system())\n  else:\n    raise RuntimeError(\n        \"initialize_tpu_system is not supported within \"\n        \"tf.functions.  You should call initialize_tpu_system outside of your tf.function. \"\n    )\n\n  logging.info(\"Finished shutting down TPU system.\")\n  if tpu_name in _INITIALIZED_TPU_SYSTEMS:\n    del _INITIALIZED_TPU_SYSTEMS[tpu_name]"