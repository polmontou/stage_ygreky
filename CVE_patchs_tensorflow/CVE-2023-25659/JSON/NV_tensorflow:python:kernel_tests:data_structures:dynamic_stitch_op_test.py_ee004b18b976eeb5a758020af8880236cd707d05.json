"# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for tensorflow.ops.data_flow_ops.{,parallel_}dynamic_stitch.\"\"\"\n\nimport numpy as np\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import data_flow_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import gradients_impl\nimport tensorflow.python.ops.data_flow_grad  # pylint: disable=unused-import\nfrom tensorflow.python.platform import test\n\n\nclass DynamicStitchTestBase(object):\n\n  def __init__(self, stitch_op):\n    self.stitch_op = stitch_op\n\n  def testScalar(self):\n    with test_util.use_gpu():\n      indices = [constant_op.constant(0), constant_op.constant(1)]\n      data = [constant_op.constant(40), constant_op.constant(60)]\n      for step in -1, 1:\n        stitched_t = self.stitch_op(indices[::step], data)\n        stitched_val = self.evaluate(stitched_t)\n        self.assertAllEqual([40, 60][::step], stitched_val)\n        # Dimension 0 is max(flatten(indices))+1.\n        self.assertEqual([2], stitched_t.get_shape().as_list())\n\n  @test_util.run_deprecated_v1\n  def testShapeInferenceForScalarWithNonConstantIndices(self):\n    with test_util.use_gpu():\n      indices = [\n          array_ops.placeholder(dtype=dtypes.int32),\n          constant_op.constant(1)\n      ]\n      data = [constant_op.constant(40), constant_op.constant(60)]\n      for step in -1, 1:\n        stitched_t = self.stitch_op(indices[::step], data)\n        # Dimension 0 is max(flatten(indices))+1, but the first indices input is\n        # not a constant tensor, so we can only infer it as a vector of unknown\n        # length.\n        self.assertEqual([None], stitched_t.get_shape().as_list())\n\n  @test_util.disable_tfrt(\"b/169901260\")\n  def testSimpleOneDimensional(self):\n    # Test various datatypes in the simple case to ensure that the op was\n    # registered under those types.\n    dtypes_to_test = [\n        dtypes.float32,\n        dtypes.float16,\n        dtypes.bfloat16,\n        dtypes.qint8,\n        dtypes.quint8,\n        dtypes.qint32,\n    ]\n    for dtype in dtypes_to_test:\n      indices = [\n          constant_op.constant([0, 4, 7]),\n          constant_op.constant([1, 6, 2, 3, 5])\n      ]\n      data = [\n          math_ops.cast(constant_op.constant([0, 40, 70]), dtype=dtype),\n          math_ops.cast(\n              constant_op.constant([10, 60, 20, 30, 50]), dtype=dtype)\n      ]\n      stitched_t = self.stitch_op(indices, data)\n      stitched_val = self.evaluate(stitched_t)\n      self.assertAllEqual([0, 10, 20, 30, 40, 50, 60, 70], stitched_val)\n      # Dimension 0 is max(flatten(indices))+1.\n      self.assertEqual([8], stitched_t.get_shape().as_list())\n\n  def testOneListOneDimensional(self):\n    indices = [constant_op.constant([1, 6, 2, 3, 5, 0, 4, 7])]\n    data = [constant_op.constant([10, 60, 20, 30, 50, 0, 40, 70])]\n    stitched_t = self.stitch_op(indices, data)\n    stitched_val = self.evaluate(stitched_t)\n    self.assertAllEqual([0, 10, 20, 30, 40, 50, 60, 70], stitched_val)\n    # Dimension 0 is max(flatten(indices))+1.\n    self.assertEqual([8], stitched_t.get_shape().as_list())\n\n  def testSimpleTwoDimensional(self):\n    indices = [\n        constant_op.constant([0, 4, 7]),\n        constant_op.constant([1, 6]),\n        constant_op.constant([2, 3, 5])\n    ]\n    data = [\n        constant_op.constant([[0, 1], [40, 41], [70, 71]]),\n        constant_op.constant([[10, 11], [60, 61]]),\n        constant_op.constant([[20, 21], [30, 31], [50, 51]])\n    ]\n    stitched_t = self.stitch_op(indices, data)\n    stitched_val = self.evaluate(stitched_t)\n    self.assertAllEqual([[0, 1], [10, 11], [20, 21], [30, 31], [40, 41],\n                         [50, 51], [60, 61], [70, 71]], stitched_val)\n    # Dimension 0 is max(flatten(indices))+1.\n    self.assertEqual([8, 2], stitched_t.get_shape().as_list())\n\n  def testZeroSizeTensor(self):\n    indices = [\n        constant_op.constant([0, 4, 7]),\n        constant_op.constant([1, 6]),\n        constant_op.constant([2, 3, 5]),\n        array_ops.zeros([0], dtype=dtypes.int32)\n    ]\n    data = [\n        constant_op.constant([[0, 1], [40, 41], [70, 71]]),\n        constant_op.constant([[10, 11], [60, 61]]),\n        constant_op.constant([[20, 21], [30, 31], [50, 51]]),\n        array_ops.zeros([0, 2], dtype=dtypes.int32)\n    ]\n    stitched_t = self.stitch_op(indices, data)\n    stitched_val = self.evaluate(stitched_t)\n    self.assertAllEqual([[0, 1], [10, 11], [20, 21], [30, 31], [40, 41],\n                         [50, 51], [60, 61], [70, 71]], stitched_val)\n    # Dimension 0 is max(flatten(indices))+1.\n    self.assertEqual([8, 2], stitched_t.get_shape().as_list())\n\n  def testAllZeroSizeTensor(self):\n    indices = [\n        array_ops.zeros([0], dtype=dtypes.int32),\n        array_ops.zeros([0], dtype=dtypes.int32)\n    ]\n    data = [\n        array_ops.zeros([0, 2], dtype=dtypes.int32),\n        array_ops.zeros([0, 2], dtype=dtypes.int32)\n    ]\n    stitched_t = self.stitch_op(indices, data)\n    stitched_val = self.evaluate(stitched_t)\n    self.assertAllEqual(np.zeros((0, 2)), stitched_val)\n    self.assertEqual([0, 2], stitched_t.get_shape().as_list())\n\n  @test_util.run_deprecated_v1\n  def testHigherRank(self):\n    indices = [\n        constant_op.constant(6),\n        constant_op.constant([4, 1]),\n        constant_op.constant([[5, 2], [0, 3]])\n    ]\n    data = [\n        constant_op.constant([61., 62.]),\n        constant_op.constant([[41., 42.], [11., 12.]]),\n        constant_op.constant([[[51., 52.], [21., 22.]],\n                              [[1., 2.], [31., 32.]]])\n    ]\n    stitched_t = self.stitch_op(indices, data)\n    stitched_val = self.evaluate(stitched_t)\n    correct = 10. * np.arange(7)[:, None] + [1., 2.]\n    self.assertAllEqual(correct, stitched_val)\n    self.assertEqual([7, 2], stitched_t.get_shape().as_list())\n    # Test gradients\n    stitched_grad = 7. * stitched_val\n    grads = gradients_impl.gradients(stitched_t, indices + data,\n                                     stitched_grad)\n    self.assertEqual(grads[:3], [None] * 3)  # Indices have no gradients\n    for datum, grad in zip(data, self.evaluate(grads[3:])):\n      self.assertAllEqual(7. * self.evaluate(datum), grad)\n\n  @test_util.run_deprecated_v1\n  def testErrorIndicesMultiDimensional(self):\n    indices = [\n        constant_op.constant([0, 4, 7]),\n        constant_op.constant([[1, 6, 2, 3, 5]])\n    ]\n    data = [\n        constant_op.constant([[0, 40, 70]]),\n        constant_op.constant([10, 60, 20, 30, 50])\n    ]\n    with self.assertRaises(ValueError):\n      self.stitch_op(indices, data)\n\n  @test_util.run_deprecated_v1\n  def testErrorDataNumDimsMismatch(self):\n    indices = [\n        constant_op.constant([0, 4, 7]),\n        constant_op.constant([1, 6, 2, 3, 5])\n    ]\n    data = [\n        constant_op.constant([0, 40, 70]),\n        constant_op.constant([[10, 60, 20, 30, 50]])\n    ]\n    with self.assertRaises(ValueError):\n      self.stitch_op(indices, data)\n\n  @test_util.run_deprecated_v1\n  def testErrorDataDimSizeMismatch(self):\n    indices = [\n        constant_op.constant([0, 4, 5]),\n        constant_op.constant([1, 6, 2, 3])\n    ]\n    data = [\n        constant_op.constant([[0], [40], [70]]),\n        constant_op.constant([[10, 11], [60, 61], [20, 21], [30, 31]])\n    ]\n    with self.assertRaises(ValueError):\n      self.stitch_op(indices, data)\n\n  @test_util.run_deprecated_v1\n  def testErrorDataAndIndicesSizeMismatch(self):\n    indices = [\n        constant_op.constant([0, 4, 7]),\n        constant_op.constant([1, 6, 2, 3, 5])\n    ]\n    data = [\n        constant_op.constant([0, 40, 70]),\n        constant_op.constant([10, 60, 20, 30])\n    ]\n    with self.assertRaises(ValueError):\n      self.stitch_op(indices, data)\n\n  def testOutOfBoundsIndexRaisesInvalidArgument(self):\n    with self.assertRaisesRegex(errors.InvalidArgumentError, \"out of range\"):\n      indices = [[-1000], [405], [519], [758], [1015]]\n      data = [\n          [110.27793884277344],\n          [120.29475402832031],\n          [157.2418212890625],\n          [157.2626953125],\n          [188.45382690429688],\n      ]\n\n      self.evaluate(self.stitch_op(indices, data))\n\n\nclass DynamicStitchTest(DynamicStitchTestBase, test.TestCase):\n\n  def __init__(self, *test_case_args):\n    test.TestCase.__init__(self, *test_case_args)\n    DynamicStitchTestBase.__init__(self, data_flow_ops.dynamic_stitch)\n\n\nclass ParallelDynamicStitchTest(DynamicStitchTestBase, test.TestCase):\n\n  def __init__(self, *test_case_args):\n    test.TestCase.__init__(self, *test_case_args)\n    DynamicStitchTestBase.__init__(self, data_flow_ops.parallel_dynamic_stitch)\n\n  def testScalar(self):\n    with test_util.use_gpu():\n      indices = [constant_op.constant(0), constant_op.constant(1)]\n      data = [constant_op.constant(40.0), constant_op.constant(60.0)]\n      for step in -1, 1:\n        stitched_t = data_flow_ops.dynamic_stitch(indices[::step], data)\n        stitched_val = self.evaluate(stitched_t)\n        self.assertAllEqual([40.0, 60.0][::step], stitched_val)\n        # Dimension 0 is max(flatten(indices))+1.\n        self.assertEqual([2], stitched_t.get_shape().as_list())\n\n  @test_util.run_deprecated_v1\n  def testHigherRank(self):\n    indices = [\n        constant_op.constant(6),\n        constant_op.constant([4, 1]),\n        constant_op.constant([[5, 2], [0, 3]])\n    ]\n    data = [\n        constant_op.constant([61, 62], dtype=dtypes.float32),\n        constant_op.constant([[41, 42], [11, 12]], dtype=dtypes.float32),\n        constant_op.constant(\n            [[[51, 52], [21, 22]], [[1, 2], [31, 32]]], dtype=dtypes.float32)\n    ]\n    stitched_t = data_flow_ops.dynamic_stitch(indices, data)\n    stitched_val = self.evaluate(stitched_t)\n    correct = 10 * np.arange(7)[:, None] + [1.0, 2.0]\n    self.assertAllEqual(correct, stitched_val)\n    self.assertEqual([7, 2], stitched_t.get_shape().as_list())\n    # Test gradients\n    stitched_grad = 7 * stitched_val\n    grads = gradients_impl.gradients(stitched_t, indices + data,\n                                     stitched_grad)\n    self.assertEqual(grads[:3], [None] * 3)  # Indices have no gradients\n    for datum, grad in zip(data, self.evaluate(grads[3:])):\n      self.assertAllEqual(7.0 * self.evaluate(datum), grad)\n\n  # GPU version unit tests\n  def testScalarGPU(self):\n    indices = [constant_op.constant(0), constant_op.constant(1)]\n    data = [constant_op.constant(40.0), constant_op.constant(60.0)]\n    for step in -1, 1:\n      stitched_t = data_flow_ops.dynamic_stitch(indices[::step], data)\n      stitched_val = self.evaluate(stitched_t)\n      self.assertAllEqual([40.0, 60.0][::step], stitched_val)\n      # Dimension 0 is max(flatten(indices))+1.\n      self.assertEqual([2], stitched_t.get_shape().as_list())\n\n  @test_util.run_deprecated_v1\n  def testHigherRankGPU(self):\n    indices = [\n        constant_op.constant(6),\n        constant_op.constant([4, 1]),\n        constant_op.constant([[5, 2], [0, 3]])\n    ]\n    data = [\n        constant_op.constant([61, 62], dtype=dtypes.float32),\n        constant_op.constant([[41, 42], [11, 12]], dtype=dtypes.float32),\n        constant_op.constant(\n            [[[51, 52], [21, 22]], [[1, 2], [31, 32]]], dtype=dtypes.float32)\n    ]\n    stitched_t = data_flow_ops.dynamic_stitch(indices, data)\n    stitched_val = self.evaluate(stitched_t)\n    correct = 10 * np.arange(7)[:, None] + [1.0, 2.0]\n    self.assertAllEqual(correct, stitched_val)\n    self.assertEqual([7, 2], stitched_t.get_shape().as_list())\n    # Test gradients\n    stitched_grad = 7 * stitched_val\n    grads = gradients_impl.gradients(stitched_t, indices + data,\n                                     stitched_grad)\n    self.assertEqual(grads[:3], [None] * 3)  # Indices have no gradients\n    for datum, grad in zip(data, self.evaluate(grads[3:])):\n      self.assertAllEqual(7.0 * self.evaluate(datum), grad)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testMismatchedDataAndIndexListSizes(self):\n    indices = [\n        constant_op.constant([2]),\n        constant_op.constant([1]),\n        constant_op.constant([0]),\n        constant_op.constant([3]),\n    ]\n    data = [\n        constant_op.constant([1.0]),\n        constant_op.constant([2.0]),\n        constant_op.constant([3.0]),\n        constant_op.constant([4.0])\n    ]\n    with self.assertRaisesRegex(\n        (ValueError, errors.InvalidArgumentError),\n        \"expected inputs .* do not match|List argument .* must match\"):\n      self.evaluate(data_flow_ops.dynamic_stitch(indices[0:2], data))\n\n    with self.assertRaisesRegex(\n        (ValueError, errors.InvalidArgumentError),\n        \"expected inputs .* do not match|List argument .* must match\"):\n      self.evaluate(data_flow_ops.dynamic_stitch(indices, data[0:2]))\n\nif __name__ == \"__main__\":\n  test.main()"