"# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for tensorflow.ops.tensor_array_ops.\"\"\"\n\nimport numpy as np\n\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.python.client import session as session_lib\nfrom tensorflow.python.data.ops import dataset_ops\nfrom tensorflow.python.eager import backprop\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_spec\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import control_flow_util\nfrom tensorflow.python.ops import data_flow_ops\nfrom tensorflow.python.ops import gen_data_flow_ops\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import tensor_array_grad\nfrom tensorflow.python.ops import tensor_array_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\nimport tensorflow.python.ops.nn_grad  # pylint: disable=unused-import\nfrom tensorflow.python.platform import test\n\n\ndef _make_converter(tf_dtype):\n  def _converter(x):\n    if tf_dtype == dtypes.string:\n      # In Python3, np.str_ is unicode, while we always want bytes\n      return np.asarray(x).astype(\"|S\")\n    x = np.asarray(x).astype(tf_dtype.as_numpy_dtype)\n    if tf_dtype.is_complex:\n      # Add a non-zero imaginary component to x.\n      x -= 1j * x\n    return x\n  return _converter\n\n\ndef _make_ta(size, name, dtype=dtypes.float32, infer_shape=False):\n  return tensor_array_ops.TensorArray(\n      dtype=dtype, tensor_array_name=name, size=size, infer_shape=infer_shape)\n\n\n@test_util.run_all_in_graph_and_eager_modes\n@test_util.with_control_flow_v2\nclass TensorArrayTest(test.TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n    super(TensorArrayTest, cls).setUpClass()\n    cls._workers, _ = test.create_local_cluster(num_workers=3, num_ps=0)\n\n  @classmethod\n  def tearDownClass(cls):\n    super(TensorArrayTest, cls).tearDownClass()\n    session_lib.Session.reset(cls._workers[0].target)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testTensorArrayWriteRead(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=3,\n          infer_shape=False)\n\n      w0 = ta.write(0, [[4.0, 5.0]])\n      w1 = w0.write(1, [[1.0]])\n      w2 = w1.write(2, -3.0)\n\n      r0 = w2.read(0)\n      r1 = w2.read(1)\n      r2 = w2.read(2)\n\n      d0, d1, d2 = self.evaluate([r0, r1, r2])\n      self.assertAllEqual([[4.0, 5.0]], d0)\n      self.assertAllEqual([[1.0]], d1)\n      self.assertAllEqual(-3.0, d2)\n\n  def _testTensorArrayWritePack(self, tf_dtype):\n    with self.cached_session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=tf_dtype, tensor_array_name=\"foo\", size=3)\n\n      convert = _make_converter(tf_dtype)\n\n      w0 = ta.write(0, convert([[4.0, 5.0]]))\n      w1 = w0.write(1, convert([[6.0, 7.0]]))\n      w2 = w1.write(2, convert([[8.0, 9.0]]))\n\n      c0 = w2.stack()\n\n      c0 = self.evaluate(c0)\n      self.assertAllEqual(\n          convert([[[4.0, 5.0]], [[6.0, 7.0]], [[8.0, 9.0]]]), c0)\n\n  def _testTensorArrayWritePackMaybeLegacy(self):\n    self._testTensorArrayWritePack(dtypes.float32)\n    self._testTensorArrayWritePack(dtypes.float64)\n    self._testTensorArrayWritePack(dtypes.int32)\n    self._testTensorArrayWritePack(dtypes.int64)\n    self._testTensorArrayWritePack(dtypes.complex64)\n    self._testTensorArrayWritePack(dtypes.complex128)\n    self._testTensorArrayWritePack(dtypes.string)\n\n  def testTensorArrayWritePack(self):\n    self._testTensorArrayWritePackMaybeLegacy()\n\n  def testEmptyTensorArrayPack(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, tensor_array_name=\"foo\", size=3)\n\n      empty_element = np.zeros((0, 1), dtype=np.float32)\n      w0 = ta.write(0, empty_element)\n      w1 = w0.write(1, empty_element)\n      w2 = w1.write(2, empty_element)\n\n      c0 = w2.stack()\n\n      c0 = self.evaluate(c0)\n      self.assertAllEqual([3, 0, 1], c0.shape)\n\n  def testTensorArrayWriteConcatInParallel(self):\n    with self.session():\n\n      def _concat_1():\n        ta = tensor_array_ops.TensorArray(\n            dtype=dtypes.int32, size=2, infer_shape=False)\n        w0 = ta.write(0, constant_op.constant([1]))\n        w1 = w0.write(1, constant_op.constant([],\n                                              shape=(0,),\n                                              dtype=dtypes.int32))\n        return w1.concat()\n\n      def _concat_2():\n        ta = tensor_array_ops.TensorArray(\n            dtype=dtypes.int32, size=3, infer_shape=False)\n        w0 = ta.write(0, constant_op.constant([8]))\n        w1 = w0.write(1, constant_op.constant([],\n                                              shape=(0,),\n                                              dtype=dtypes.int32))\n        w2 = w1.write(2, constant_op.constant([9]))\n        return w2.concat()\n\n      def _write(index, output):\n        elements = control_flow_ops.cond(\n            math_ops.less(index, 3), _concat_1, _concat_2)\n        return (index + 1, output.write(index, elements))\n\n      num_iterations = 6\n      init_state = (0,\n                    tensor_array_ops.TensorArray(\n                        dtype=dtypes.int32,\n                        size=num_iterations,\n                        infer_shape=False))\n      _, final_state = control_flow_ops.while_loop(\n          lambda i, _: i < num_iterations, _write, init_state)\n\n      c0 = final_state.concat()\n\n      c0 = self.evaluate(c0)\n      self.assertAllEqual([1, 1, 1, 8, 9, 8, 9, 8, 9], c0)\n\n  def _testTensorArrayWriteConcat(self, tf_dtype):\n    with self.cached_session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=tf_dtype, tensor_array_name=\"foo\", size=3, infer_shape=False)\n\n      convert = _make_converter(tf_dtype)\n\n      w0 = ta.write(0, convert([[4.0, 5.0], [104.0, 105.0], [204.0, 205.0]]))\n      w1 = w0.write(1, convert([[6.0, 7.0], [106.0, 107.0]]))\n      w2 = w1.write(2, convert([[8.0, 9.0]]))\n\n      c0 = w2.concat()\n\n      c0 = self.evaluate(c0)\n      self.assertAllEqual(\n          convert([[4.0, 5.0], [104.0, 105.0], [204.0, 205.0], [6.0, 7.0],\n                   [106.0, 107.0], [8.0, 9.0]]), c0)\n\n  @test_util.deprecated_graph_mode_only\n  def testTensorArrayWriteConcat(self):\n    self._testTensorArrayWriteConcat(dtypes.float32)\n    self._testTensorArrayWriteConcat(dtypes.float64)\n    self._testTensorArrayWriteConcat(dtypes.int32)\n    self._testTensorArrayWriteConcat(dtypes.int64)\n    self._testTensorArrayWriteConcat(dtypes.complex64)\n    self._testTensorArrayWriteConcat(dtypes.complex128)\n    self._testTensorArrayWriteConcat(dtypes.string)\n\n  def _testTensorArrayReadOrPackNotAllValuesAvailableFillsZeros(self):\n    with self.cached_session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=3,\n          element_shape=tensor_shape.TensorShape([1, 2]))\n      self.assertAllEqual([[0.0, 0.0]], self.evaluate(ta.read(0)))\n      self.assertAllEqual([[[0.0, 0.0]], [[4.0, 5.0]], [[0.0, 0.0]]],\n                          self.evaluate(ta.write(1, [[4.0, 5.0]]).stack()))\n      self.assertAllEqual([[0.0, 0.0], [4.0, 5.0], [0.0, 0.0]],\n                          self.evaluate(ta.write(1, [[4.0, 5.0]]).concat()))\n\n  @test_util.run_v1_only(\"b/122324791\")\n  def testTensorArrayReadOrPackNotAllValuesAvailableFillsZeros(self):\n    self._testTensorArrayReadOrPackNotAllValuesAvailableFillsZeros()\n\n  def _testTensorArrayReadOrPackNotAllValuesAvailableInferShapeFillsZeros(self):\n    ta = tensor_array_ops.TensorArray(\n        dtype=dtypes.float32,\n        tensor_array_name=\"foo\",\n        size=3)\n    self.assertAllEqual(\n        [[0.0, 0.0]], self.evaluate(ta.write(1, [[4.0, 5.0]]).read(0)))\n    self.assertAllEqual([[[0.0, 0.0]], [[4.0, 5.0]], [[0.0, 0.0]]],\n                        self.evaluate(ta.write(1, [[4.0, 5.0]]).stack()))\n    self.assertAllEqual([[0.0, 0.0], [4.0, 5.0], [0.0, 0.0]],\n                        self.evaluate(ta.write(1, [[4.0, 5.0]]).concat()))\n\n  @test_util.run_v1_only(\"b/122324791\")\n  def testTensorArrayReadOrPackNotAllValuesAvailableInferShapeFillsZeros(self):\n    self._testTensorArrayReadOrPackNotAllValuesAvailableInferShapeFillsZeros()\n\n  @test_util.run_v1_only(\"Uses placeholders\")\n  def testSkipEagerTensorArrayReadUninitializedInferShapeFillsZeros(self):\n    with self.cached_session() as sess:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=3)\n      val = array_ops.placeholder(dtypes.float32)\n      self.assertAllEqual(\n          [[0.0, 0.0]], sess.run(ta.write(1, val).read(0), {val: [[4.0, 5.0]]}))\n\n  def _testTensorArrayUnpackRead(self, tf_dtype):\n    with self.cached_session():\n      convert = _make_converter(tf_dtype)\n\n      ta = _make_ta(3, \"foo\", dtype=tf_dtype)\n      # Unpack a vector into scalars\n      w0 = ta.unstack(convert([1.0, 2.0, 3.0]))\n      r0 = w0.read(0)\n      r1 = w0.read(1)\n      r2 = w0.read(2)\n\n      d0, d1, d2 = self.evaluate([r0, r1, r2])\n      self.assertAllEqual(convert(1.0), d0)\n      self.assertAllEqual(convert(2.0), d1)\n      self.assertAllEqual(convert(3.0), d2)\n\n      # Unpack a matrix into vectors\n      w1 = ta.unstack(convert([[1.0, 1.1], [2.0, 2.1], [3.0, 3.1]]))\n      r0 = w1.read(0)\n      r1 = w1.read(1)\n      r2 = w1.read(2)\n\n      d0, d1, d2 = self.evaluate([r0, r1, r2])\n      self.assertAllEqual(convert([1.0, 1.1]), d0)\n      self.assertAllEqual(convert([2.0, 2.1]), d1)\n      self.assertAllEqual(convert([3.0, 3.1]), d2)\n\n      # Try unpacking an empty matrix, which should not cause an error.\n      w2 = ta.unstack(convert([[], [], []]))\n      r0 = w2.read(0)\n      r1 = w2.read(1)\n      r2 = w2.read(2)\n\n      d0, d1, d2 = self.evaluate([r0, r1, r2])\n      self.assertAllEqual(convert([]), d0)\n      self.assertAllEqual(convert([]), d1)\n      self.assertAllEqual(convert([]), d2)\n\n  def _testTensorArrayUnpackReadMaybeLegacy(self):\n    self._testTensorArrayUnpackRead(dtypes.float32)\n    self._testTensorArrayUnpackRead(dtypes.float64)\n    self._testTensorArrayUnpackRead(dtypes.int32)\n    self._testTensorArrayUnpackRead(dtypes.int64)\n    self._testTensorArrayUnpackRead(dtypes.complex64)\n    self._testTensorArrayUnpackRead(dtypes.complex128)\n    self._testTensorArrayUnpackRead(dtypes.string)\n    self._testTensorArrayUnpackRead(dtypes.bfloat16)\n\n  def testTensorArrayUnpackRead(self):\n    self._testTensorArrayUnpackReadMaybeLegacy()\n\n  def _testTensorArraySplitRead(self, tf_dtype):\n    with self.cached_session():\n      convert = _make_converter(tf_dtype)\n\n      # Split an empty vector\n      ta = _make_ta(3, \"foo\", dtype=tf_dtype)\n      lengths = constant_op.constant([0, 0, 0])\n      w0 = ta.split(convert([]), lengths=lengths)\n      r0 = w0.read(0)\n      r1 = w0.read(1)\n      r2 = w0.read(2)\n\n      d0, d1, d2 = self.evaluate([r0, r1, r2])\n      self.assertAllEqual(convert([]), d0)\n      self.assertAllEqual(convert([]), d1)\n      self.assertAllEqual(convert([]), d2)\n\n      # Split a vector\n      lengths = constant_op.constant([2, 0, 1])\n      w0 = ta.split(convert([1.0, 2.0, 3.0]), lengths=lengths)\n      r0 = w0.read(0)\n      r1 = w0.read(1)\n      r2 = w0.read(2)\n\n      d0, d1, d2 = self.evaluate([r0, r1, r2])\n      self.assertAllEqual(convert([1.0, 2.0]), d0)\n      self.assertAllEqual(convert([]), d1)\n      self.assertAllEqual(convert([3.0]), d2)\n\n      # Split a matrix\n      lengths = constant_op.constant([2, 0, 1])\n      w0 = ta.split(\n          convert([[1.0, 101.0], [2.0, 201.0], [3.0, 301.0]]), lengths=lengths)\n      r0 = w0.read(0)\n      r1 = w0.read(1)\n      r2 = w0.read(2)\n\n      d0, d1, d2 = self.evaluate([r0, r1, r2])\n      self.assertAllEqual(convert([[1.0, 101.0], [2.0, 201.0]]), d0)\n      self.assertAllEqual(convert([]).reshape(0, 2), d1)\n      self.assertAllEqual(convert([[3.0, 301.0]]), d2)\n\n  @test_util.deprecated_graph_mode_only\n  def testTensorArraySplitRead(self):\n    self._testTensorArraySplitRead(dtypes.float32)\n    self._testTensorArraySplitRead(dtypes.float64)\n    self._testTensorArraySplitRead(dtypes.int32)\n    self._testTensorArraySplitRead(dtypes.int64)\n    self._testTensorArraySplitRead(dtypes.complex64)\n    self._testTensorArraySplitRead(dtypes.complex128)\n    self._testTensorArraySplitRead(dtypes.string)\n    self._testTensorArraySplitRead(dtypes.bfloat16)\n\n  @test_util.disable_control_flow_v2(\"v2 does not support TensorArray.grad.\")\n  @test_util.run_v1_only(\"v2 does not support TensorArray.grad.\")\n  def testSkipEagerTensorGradArrayWriteRead(self):\n    with self.session() as session:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=3,\n          infer_shape=False)\n      g_ta = ta.grad(\"grad\")\n\n      w0 = ta.write(0, [[4.0, 5.0]])\n      w1 = w0.write(1, [[1.0]])\n      w2 = w1.write(2, -3.0)\n\n      g_w0 = g_ta.write(0, [[5.0, 6.0]])\n      g_w1 = g_w0.write(1, [[2.0]])\n      g_w2 = g_w1.write(2, -2.0)\n\n      r0 = w2.read(0)\n      r1 = w2.read(1)\n      r2 = w2.read(2)\n\n      g_r0 = g_w2.read(0)\n      g_r1 = g_w2.read(1)\n      g_r2 = g_w2.read(2)\n\n      d0, d1, d2, g_d0, g_d1, g_d2 = session.run([r0, r1, r2, g_r0, g_r1, g_r2])\n      self.assertAllEqual([[4.0, 5.0]], d0)\n      self.assertAllEqual([[1.0]], d1)\n      self.assertAllEqual(-3.0, d2)\n      self.assertAllEqual([[5.0, 6.0]], g_d0)\n      self.assertAllEqual([[2.0]], g_d1)\n      self.assertAllEqual(-2.0, g_d2)\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerTensorArrayGradGrad(self):\n    if not control_flow_util.ENABLE_CONTROL_FLOW_V2:\n      self.skipTest(\"Legacy TensorArray does not support double derivatives.\")\n    with self.test_session() as session:\n      x = constant_op.constant(4.0)\n\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=1,\n          infer_shape=False)\n      w0 = ta.write(0, x)\n      r0 = w0.read(0)\n      y = r0 * r0\n\n      g1 = gradients_impl.gradients(ys=[y], xs=[x])\n      g2 = gradients_impl.gradients(ys=[g1], xs=[x])\n      self.assertAllEqual([2.0], session.run(g2))\n\n  @test_util.disable_control_flow_v2(\"v2 does not support TensorArray.grad.\")\n  @test_util.run_v1_only(\"v2 does not support TensorArray.grad.\")\n  def testSkipEagerTensorGradArrayDynamicWriteRead(self):\n    with self.session() as session:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=0,\n          dynamic_size=True,\n          infer_shape=False)\n\n      w0 = ta.write(0, [[4.0, 5.0]])\n      w1 = w0.write(1, [[1.0]])\n      w2 = w1.write(2, -3.0)\n\n      g_ta = w2.grad(\"grad\")  # Get gradient array here so we know the shape\n\n      s = w2.size()\n      g_s = g_ta.size()\n\n      g_w0 = g_ta.write(0, [[5.0, 6.0]])\n      g_w1 = g_w0.write(1, [[2.0]])\n      g_w2 = g_w1.write(2, -2.0)\n\n      r0 = w2.read(0)\n      r1 = w2.read(1)\n      r2 = w2.read(2)\n\n      g_r0 = g_w2.read(0)\n      g_r1 = g_w2.read(1)\n      g_r2 = g_w2.read(2)\n\n      d0, d1, d2, g_d0, g_d1, g_d2, vs, g_vs = session.run(\n          [r0, r1, r2, g_r0, g_r1, g_r2, s, g_s])\n      self.assertAllEqual([[4.0, 5.0]], d0)\n      self.assertAllEqual([[1.0]], d1)\n      self.assertAllEqual(-3.0, d2)\n      self.assertAllEqual([[5.0, 6.0]], g_d0)\n      self.assertAllEqual([[2.0]], g_d1)\n      self.assertAllEqual(-2.0, g_d2)\n      self.assertAllEqual(3, vs)\n      self.assertAllEqual(3, g_vs)\n\n  @test_util.disable_control_flow_v2(\"v2 does not support TensorArray.grad.\")\n  @test_util.run_v1_only(\"v2 does not support TensorArray.grad.\")\n  def testSkipEagerTensorGradAccessTwiceReceiveSameObject(self):\n    with self.session() as session:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, tensor_array_name=\"foo\", size=3)\n      g_ta_0 = ta.grad(\"grad\")\n      g_ta_1 = ta.grad(\"grad\")\n\n      with ops.control_dependencies([g_ta_0.write(0, [[4.0, 5.0]]).flow]):\n        # Write with one gradient handle, read with another copy of it\n        r1_0 = g_ta_1.read(0)\n\n      t_g_ta_0, t_g_ta_1, d_r1_0 = session.run(\n          [g_ta_0.handle.op, g_ta_1.handle.op, r1_0])\n      self.assertAllEqual(t_g_ta_0, t_g_ta_1)\n      self.assertAllEqual([[4.0, 5.0]], d_r1_0)\n\n  def testTensorArrayWriteWrongIndexOrDataTypeFails(self):\n    with self.session():\n      ta = _make_ta(3, \"foo\", dtype=dtypes.float32)\n      # TODO(b/129870929): Remove the last 2 checks (runtime checks) after\n      # back back from preferred_dtype= to dtype= in convert_to_tensor.  Also\n      # restrict error check to only TypeError.\n      error_msg_regex = (\n          \"(\"\n          \"Expected float32, got 'wrong_type_scalar' of type 'str' instead.\"\n          \"|\"\n          \"Cannot convert provided value to EagerTensor. Provided value: \"\n          \"wrong_type_scalar Requested dtype: float\"\n          \"|\"\n          \"TensorArray dtype is float.* but Op is trying to write dtype string\"\n          \"|\"\n          \"Invalid data types; op elements string but list elements float\"\n          \")\")\n      with self.assertRaisesRegex((TypeError, errors.InvalidArgumentError),\n                                  error_msg_regex):\n        self.evaluate(ta.write(0, \"wrong_type_scalar\").flow)\n\n      if (control_flow_util.ENABLE_CONTROL_FLOW_V2 and\n          not context.executing_eagerly()):\n        error_msg = \"Trying to modify element -1 in a list with 3 elements.\"\n      else:\n        error_msg = \"index -1\"\n      with self.assertRaisesOpError(error_msg):\n        self.evaluate(ta.write(-1, 3.0).flow)\n\n      if (control_flow_util.ENABLE_CONTROL_FLOW_V2 and\n          not context.executing_eagerly()):\n        error_msg = \"Trying to modify element 3 in a list with 3 elements\"\n      else:\n        error_msg = (\"Tried to write to index 3 but array is not \"\n                     \"resizeable and size is: 3\")\n      # Test reading from too large an index\n      with self.assertRaisesOpError(error_msg):\n        self.evaluate(ta.write(3, 3.0).flow)\n\n  def testTensorArrayReadWrongIndexOrDataTypeFails(self):\n    with self.session():\n      ta = _make_ta(3, \"foo\", dtype=dtypes.float32)\n\n      w0 = ta.write(0, [[4.0, 5.0]])\n\n      # Test reading wrong datatype (only possible when constructing graphs).\n      if (not context.executing_eagerly() and\n          not control_flow_util.ENABLE_CONTROL_FLOW_V2):\n        r0_bad = gen_data_flow_ops.tensor_array_read_v3(\n            handle=w0.handle, index=0, dtype=dtypes.float64, flow_in=w0.flow)\n        with self.assertRaisesOpError(\n            \"TensorArray dtype is float but Op requested dtype double.\"):\n          self.evaluate(r0_bad)\n\n      if (control_flow_util.ENABLE_CONTROL_FLOW_V2 and\n          not context.executing_eagerly()):\n        error_msg = \"Trying to access element -1 in a list with 3 elements.\"\n      else:\n        error_msg = \"index -1\"\n      # Test reading from a negative index, which is not allowed\n      with self.assertRaisesOpError(error_msg):\n        self.evaluate(ta.read(-1))\n\n      if (control_flow_util.ENABLE_CONTROL_FLOW_V2 and\n          not context.executing_eagerly()):\n        error_msg = \"Trying to access element 3 in a list with 3 elements.\"\n      else:\n        error_msg = \"Tried to read from index 3 but array size is: 3\"\n      # Test reading from too large an index\n      with self.assertRaisesOpError(error_msg):\n        self.evaluate(ta.read(3))\n\n  @test_util.disable_control_flow_v2(\"v2 allows multiple writes.\")\n  @test_util.run_v1_only(\"v2 allows multiple writes.\")\n  def testSkipEagerTensorArrayWriteMultipleFails(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, tensor_array_name=\"foo\", size=3)\n\n      with self.assertRaisesOpError(\n          \"Could not write to TensorArray index 2 because \"\n          \"it has already been written to.\"):\n        self.evaluate(ta.write(2, 3.0).write(2, 3.0).flow)\n\n  def testTensorArrayConcatIncompatibleShapesFails(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=3,\n          infer_shape=False)\n\n      w1 = ta.write(0, 3.0)\n      w2 = w1.write(1, 4.0)\n      w3 = w2.write(2, [3.0])\n\n      with self.assertRaisesOpError(\n          \"Concat saw a scalar shape at index 0 but requires at least vectors\"):\n        self.evaluate(w3.concat())\n\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=3,\n          infer_shape=False)\n\n      w1 = ta.write(0, [3.0])\n      w2 = w1.write(1, [4.0])\n      w3 = w2.write(2, [[3.0]])\n\n      # The exact error messages differ between eager execution and graph\n      # construction as the former bubbles up the error from array_op.concat.\n      error_msg = (\"Incompatible ranks\"\n                   if control_flow_util.ENABLE_CONTROL_FLOW_V2 and\n                   not context.executing_eagerly() else \"shape\")\n      with self.assertRaisesRegex(errors.InvalidArgumentError, error_msg):\n        self.evaluate(w3.concat())\n\n  def testTensorArraySplitIncompatibleShapesFails(self):\n    with self.session():\n      in_eager_mode = context.executing_eagerly()\n      ta = _make_ta(3, \"foo\")\n      with self.assertRaisesOpError(\n          r\"Expected lengths to be a vector, received shape: \\[\\]\"):\n        if in_eager_mode:\n          self.evaluate(ta.split([1.0, 2.0, 3.0], 1))\n        else:\n          lengths = array_ops.placeholder(dtypes.int64)\n          ta.split([1.0, 2.0, 3.0], lengths).flow.eval(feed_dict={lengths: 1})\n\n      error_msg = (\"Unused values in tensor. Length of tensor: 3 Values used: 1\"\n                   if control_flow_util.ENABLE_CONTROL_FLOW_V2 and\n                   not in_eager_mode else\n                   r\"Expected sum of lengths to be equal to values.shape\\[0\\], \"\n                   r\"but sum of lengths is 1 and value's shape is: \\[3\\]\")\n      with self.assertRaisesOpError(error_msg):\n        self.evaluate(ta.split([1.0, 2.0, 3.0], [1]).flow)\n\n      ta = _make_ta(1, \"baz\")\n      if control_flow_util.ENABLE_CONTROL_FLOW_V2 and not in_eager_mode:\n        with self.assertRaisesRegex(\n            ValueError, \"Shape must be at least rank 1 but is rank 0\"):\n          self.evaluate(ta.split(1.0, [1]).flow)\n      else:\n        with self.assertRaisesOpError(\n            r\"Expected value to be at least a vector, but received shape: \\[\\]\"\n        ):\n          self.evaluate(ta.split(1.0, [1]).flow)\n\n      if not control_flow_util.ENABLE_CONTROL_FLOW_V2 or in_eager_mode:\n        ta = _make_ta(2, \"buz\")\n        with self.assertRaisesOpError(\n            r\"TensorArray's size is not equal to the size of lengths \"\n            r\"\\(2 vs. 1\\), and the TensorArray is not marked as \"\n            r\"dynamically resizeable\"):\n          self.evaluate(ta.split([1.0], [1]).flow)\n\n  def _testTensorArrayWriteGradientAddMultipleAdds(self, dtype):\n    with self.cached_session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtype, tensor_array_name=\"foo\", size=3, infer_shape=False)\n      ta_grad = ta.grad(\"grad\")\n\n      c = lambda x: np.asarray(x, dtype=dtype.as_numpy_dtype)\n\n      w0 = ta.write(2, c(3.0))\n      w1 = w0.write(2, c(4.0))\n\n      w0_grad = ta_grad.write(2, c(3.0))\n      w1_grad = w0_grad.write(2, c(4.0))\n      w2_grad = w1_grad.write(2, c(5.0))\n\n      # Assert that aggregation works correctly\n      self.assertAllEqual(c(12.00), w2_grad.read(2))\n\n      # Assert that if multiple_writes_aggregate is not enabled,\n      # multiple writes raise an exception.\n      with self.assertRaisesOpError(\n          r\"TensorArray foo_.*: Could not write to TensorArray index 2 because \"\n          r\"it has already been written to.\"):\n        self.evaluate(w1.flow)\n\n      # Using differing shapes causes an exception\n      wb0_grad = ta_grad.write(1, c(1.0))\n      wb1_grad = wb0_grad.write(1, c([1.0]))\n\n      with self.assertRaisesOpError(\n          r\"Could not aggregate to TensorArray index 1 because the \"\n          r\"existing shape is \\[\\] but the new input shape is \\[1\\]\"):\n        self.evaluate(wb1_grad.flow)\n\n  @test_util.disable_control_flow_v2(\"v2 does not support TensorArray.grad.\")\n  @test_util.run_v1_only(\"v2 does not support TensorArray.grad.\")\n  def testSkipEagerTensorArrayWriteGradientAddMultipleAdds(self):\n    for dtype in (dtypes.int32, dtypes.int64, dtypes.float32, dtypes.float64,\n                  dtypes.complex64, dtypes.complex128):\n      self._testTensorArrayWriteGradientAddMultipleAdds(dtype)\n\n  @test_util.disable_control_flow_v2(\"Low level legacy TA op test.\")\n  @test_util.run_v1_only(\"Low level legacy TA op test.\")\n  def testSkipEagerTensorArrayGradWithShapeKnownElementShape(self):\n    with self.session() as sess:\n      ta = tensor_array_ops.TensorArray(\n          size=3,\n          dtype=dtypes.float32,\n          element_shape=tensor_shape.TensorShape([2, 3]))\n      handle, flow = data_flow_ops.tensor_array_grad_with_shape(\n          handle=ta.handle,\n          flow_in=ta.flow,\n          shape_to_prepend=tensor_shape.TensorShape([4, 5]),\n          source=\"source\")\n      ta_grad = tensor_array_ops.TensorArray(\n          dtypes.float32, handle=handle, flow=flow)\n      value = array_ops.placeholder(dtypes.float32)\n      ta_grad = ta_grad.write(0, value)\n      read_value = ta_grad.read(0)\n\n      # Make sure shape inference worked.\n      self.assertAllEqual([None, None, 2, 3], read_value.shape.as_list())\n      # Writing with wrong shape should not work.\n      with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                  \"Could not write to TensorArray\"):\n        fed_value = np.random.random([2, 3])\n        sess.run(read_value, feed_dict={value: fed_value})\n      # Writing with correct shape should work.\n      fed_value = np.random.random([4, 5, 2, 3])\n      self.assertAllClose(fed_value,\n                          sess.run(read_value, feed_dict={value: fed_value}))\n\n  @test_util.disable_control_flow_v2(\"Low level legacy TA op test.\")\n  @test_util.run_v1_only(\"Low level legacy TA op test.\")\n  def testSkipEagerTensorArrayGradWithShapeUnknownElementShape(self):\n    with self.session() as sess:\n      ta = tensor_array_ops.TensorArray(\n          size=3, dtype=dtypes.float32,\n          element_shape=None)  # Note that element_shape is unknown\n      handle, flow = data_flow_ops.tensor_array_grad_with_shape(\n          handle=ta.handle,\n          flow_in=ta.flow,\n          shape_to_prepend=tensor_shape.TensorShape([4, 5]),\n          source=\"source\")\n      ta_grad = tensor_array_ops.TensorArray(\n          dtypes.float32, handle=handle, flow=flow)\n      value = array_ops.placeholder(dtypes.float32)\n      ta_grad = ta_grad.write(0, value)\n      read_value = ta_grad.read(0)\n\n      # Make sure shape inference worked.\n      self.assertIsNone(read_value.shape.ndims)\n      # Write with some shape and check read value.\n      fed_value = np.random.random([4, 5, 7])\n      self.assertAllClose(fed_value,\n                          sess.run(read_value, feed_dict={value: fed_value}))\n\n  def testMultiTensorArray(self):\n    with self.session():\n      h1 = tensor_array_ops.TensorArray(\n          size=1, dtype=dtypes.float32, tensor_array_name=\"foo\")\n      w1 = h1.write(0, 4.0)\n      r1 = w1.read(0)\n\n      h2 = tensor_array_ops.TensorArray(\n          size=1, dtype=dtypes.float32, tensor_array_name=\"bar\")\n\n      w2 = h2.write(0, 5.0)\n      r2 = w2.read(0)\n      r = r1 + r2\n      val = self.evaluate(r)\n      self.assertAllClose(9.0, val)\n\n  def _testTensorArrayGradientWriteReadType(self, dtype):\n    with self.cached_session() as session:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.as_dtype(dtype),\n          tensor_array_name=\"foo\",\n          size=3,\n          infer_shape=False)\n\n      c = lambda x: np.array(x, dtype=dtype)\n\n      value_0 = constant_op.constant(c([[4.0, 5.0]]))\n      value_1 = constant_op.constant(c(3.0))\n\n      w0 = ta.write(0, value_0)\n      w1 = w0.write(1, value_1)\n      r0 = w1.read(0)\n      r1 = w1.read(1)\n      r0_2 = w1.read(0)\n\n      # Test individual components' gradients\n      grad_just_r0 = gradients_impl.gradients(\n          ys=[r0], xs=[value_0], grad_ys=[c([[2.0, 3.0]])])\n      grad_just_r0_vals = session.run(grad_just_r0)\n      self.assertAllEqual(c([[2.0, 3.0]]), grad_just_r0_vals[0])\n\n      grad_r0_r0_2 = gradients_impl.gradients(\n          ys=[r0, r0_2],\n          xs=[value_0],\n          grad_ys=[c([[2.0, 3.0]]), c([[1.0, -1.0]])])\n      grad_r0_r0_2_vals = session.run(grad_r0_r0_2)\n      self.assertAllEqual(c([[3.0, 2.0]]), grad_r0_r0_2_vals[0])\n\n      grad_just_r1 = gradients_impl.gradients(\n          ys=[r1], xs=[value_1], grad_ys=[c(-2.0)])\n      grad_just_r1_vals = session.run(grad_just_r1)\n      self.assertAllEqual(c(-2.0), grad_just_r1_vals[0])\n\n      # Test combined gradients\n      grad = gradients_impl.gradients(\n          ys=[r0, r0_2, r1],\n          xs=[value_0, value_1],\n          grad_ys=[c([[2.0, 3.0]]), c([[1.0, -1.0]]), c(-2.0)])\n      grad_vals = session.run(grad)\n      self.assertEqual(len(grad_vals), 2)\n      self.assertAllEqual(c([[3.0, 2.0]]), grad_vals[0])\n      self.assertAllEqual(c(-2.0), grad_vals[1])\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerTensorArrayGradientWriteRead(self):\n    for dtype in (np.float32, np.float64, np.complex64, np.complex128):\n      self._testTensorArrayGradientWriteReadType(dtype)\n\n  def _testTensorArrayGradientWritePackConcatAndRead(self):\n    with self.cached_session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=2,\n          clear_after_read=False)\n\n      value_0 = constant_op.constant([-1.0, 1.0])\n      value_1 = constant_op.constant([-10.0, 10.0])\n\n      w0 = ta.write(0, value_0)\n      w1 = w0.write(1, value_1)\n      p0 = w1.stack()\n      r0 = w1.read(0)\n      s0 = w1.concat()\n\n      # Test gradient accumulation between read(0), pack(), and concat()\n      with ops.control_dependencies([p0, r0, s0]):\n        grad_r = gradients_impl.gradients(\n            ys=[p0, r0, s0],\n            xs=[value_0, value_1],\n            grad_ys=[\n                [[2.0, 3.0], [4.0, 5.0]],  # pack gradient\n                [-0.5, 1.5],  # read(0) gradient\n                [20.0, 30.0, 40.0, 50.0]\n            ])  # concat gradient\n      grad_vals = self.evaluate(grad_r)  # 2 + 2 entries\n\n      self.assertAllClose([2.0 - 0.5 + 20.0, 3.0 + 1.5 + 30.0], grad_vals[0])\n      self.assertAllEqual([4.0 + 40.0, 5.0 + 50.0], grad_vals[1])\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerTensorArrayGradientWritePackConcatAndRead(self):\n    self._testTensorArrayGradientWritePackConcatAndRead()\n\n  @test_util.disable_control_flow_v2(\"v2 does not support clear_after_read.\")\n  @test_util.run_v1_only(\"v2 does not support clear_after_read.\")\n  def testTensorArrayReadTwice(self):\n    with self.session():\n      value = constant_op.constant([[1.0, -1.0], [10.0, -10.0]])\n\n      ta_readonce = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, tensor_array_name=\"foo\", size=2)\n\n      w_readonce = ta_readonce.unstack(value)\n      r0_readonce = w_readonce.read(0)\n\n      with self.assertRaisesOpError(\n          r\"Could not read index 0 twice because it was cleared after a \"\n          r\"previous read \\(perhaps try setting clear_after_read = false\\?\\)\"):\n        with ops.control_dependencies([r0_readonce]):\n          self.evaluate(w_readonce.read(0))\n\n      ta_readtwice = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=2,\n          clear_after_read=False)\n      w_readtwice = ta_readtwice.unstack(value)\n      r0_readtwice = w_readtwice.read(0)\n      with ops.control_dependencies([r0_readtwice]):\n        r1_readtwice = w_readtwice.read(0)\n\n      self.assertAllEqual([1.0, -1.0], self.evaluate(r1_readtwice))\n\n  def _testTensorArrayGradientUnpackRead(self):\n    with self.cached_session() as session:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=2,\n          clear_after_read=False)\n\n      value = constant_op.constant([[1.0, -1.0], [10.0, -10.0]])\n\n      w = ta.unstack(value)\n      r0 = w.read(0)\n      r0_1 = w.read(0)\n      r1 = w.read(1)\n\n      # Test combined gradients + aggregation of read(0)\n      grad = gradients_impl.gradients(\n          ys=[r0, r0_1, r1],\n          xs=[value],\n          grad_ys=[[2.0, 3.0], [-1.5, 1.5], [4.0, 5.0]])\n      grad_vals = session.run(grad)\n\n      self.assertEqual(len(grad_vals), 1)\n      self.assertAllEqual([[2.0 - 1.5, 3.0 + 1.5], [4.0, 5.0]], grad_vals[0])\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerTensorArrayGradientUnpackRead(self):\n    self._testTensorArrayGradientUnpackRead()\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerTensorArrayGradientSplitConcat(self):\n    with self.session() as session:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, tensor_array_name=\"foo\", size=2,\n          infer_shape=False)\n\n      value = constant_op.constant(\n          [[1.0, -1.0], [10.0, -10.0], [100.0, -100.0]])\n\n      w = ta.split(value, [2, 1])\n      r = w.concat()\n\n      # Test combined gradients\n      grad = gradients_impl.gradients(\n          ys=[r],\n          xs=[value],\n          grad_ys=[[[2.0, -2.0], [20.0, -20.0], [200.0, -200.0]]])\n      grad_vals = session.run(grad)\n\n      self.assertEqual(len(grad_vals), 1)\n      self.assertAllEqual([[2.0, -2.0], [20.0, -20.0], [200.0, -200.0]],\n                          grad_vals[0])\n\n  def _testTensorArrayGradientDynamicUnpackRead(self):\n    with self.cached_session() as session:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=0,\n          dynamic_size=True)\n\n      value = constant_op.constant([[1.0, -1.0], [10.0, -10.0]])\n\n      w = ta.unstack(value)\n      r0 = w.read(0)\n      r1 = w.read(1)\n\n      # Test combined gradients + aggregation of read(0)\n      grad = gradients_impl.gradients(\n          ys=[r0, r1], xs=[value], grad_ys=[[2.0, 3.0], [4.0, 5.0]])\n      grad_vals = session.run(grad)\n\n      self.assertEqual(len(grad_vals), 1)\n      self.assertAllEqual([[2.0, 3.0], [4.0, 5.0]], grad_vals[0])\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerTensorArrayGradientDynamicUnpackRead(self):\n    self._testTensorArrayGradientDynamicUnpackRead()\n\n  def testCloseTensorArray(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, tensor_array_name=\"foo\", size=3)\n      self.evaluate(ta.close())\n\n  def testSizeTensorArray(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, tensor_array_name=\"foo\", size=3)\n      s = ta.size()\n      self.assertAllEqual(3, self.evaluate(s))\n\n  def testWriteCloseTensorArray(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=3,\n          infer_shape=False)\n      w0 = ta.write(0, [[4.0, 5.0]])\n      w1 = w0.write(1, [3.0])\n      self.evaluate(w1.close())  # Expected to run without problems\n\n  def _testWhileLoopWritePackGradients(self, dynamic_size, dtype):\n    np_dtype = dtype.as_numpy_dtype\n    with self.cached_session():\n\n      def func(v0, state0, var):\n        ta = tensor_array_ops.TensorArray(\n            dtype=dtype,\n            tensor_array_name=\"foo\",\n            size=0 if dynamic_size else 3,\n            dynamic_size=dynamic_size)\n        time_0 = array_ops.identity(0)\n\n        def body(time, ta_t, state):\n          sliced = array_ops.slice(\n              v0, begin=array_ops.stack([time, 0]), size=[1, -1])\n          sliced = array_ops.squeeze(sliced)\n          out = sliced + var + state\n          state += sliced\n          ta_t = ta_t.write(time, out)\n          return (time + 1, ta_t, state)\n\n        (unused_0, h_final, unused_2) = control_flow_ops.while_loop(\n            cond=lambda time, unused_1, unused_2: time < 3,\n            body=body,\n            loop_vars=(time_0, ta, state0),\n            shape_invariants=(time_0.get_shape(), tensor_shape.unknown_shape(),\n                              tensor_shape.unknown_shape()),\n            parallel_iterations=3)\n        vout = h_final.stack()\n        return vout\n\n      v0 = array_ops.identity(np.arange(3 * 5, dtype=np_dtype).reshape(3, 5))\n      state0 = array_ops.identity(np.array([1] * 5, dtype=np_dtype))\n      init_val = np.arange(100, 105, dtype=np_dtype)\n      var = variable_scope.get_variable(\n          \"var\",\n          shape=init_val.shape,\n          dtype=np_dtype,\n          initializer=init_ops.constant_initializer(init_val))\n\n      vout = func(v0, state0, var)\n      grad_val = -np.arange(3 * 5, dtype=np_dtype).reshape(3, 5)\n      if context.executing_eagerly():\n        grad_fn = backprop.gradients_function(func)\n        v0_grad, state0_grad, var_grad = grad_fn(v0, state0, var, dy=grad_val)\n      else:\n        v0_grad = gradients_impl.gradients([vout], [v0], [grad_val])[0]\n        state0_grad = gradients_impl.gradients([vout], [state0], [grad_val])[0]\n        var_grad = gradients_impl.gradients([vout], [var], [grad_val])[0]\n        self.evaluate(variables.global_variables_initializer())\n\n      state0_t, var_t, v0_t, vout_t, v0_grad_t, var_grad_t, state0_grad_t = (\n          self.evaluate(\n              ([state0, var, v0, vout, v0_grad, var_grad, state0_grad])))\n      just_v0_grad_t = self.evaluate(v0_grad)\n\n      # state = [ state0 | state0 + v0[0] | state0 + v0[0] + v0[1] ]\n      # vout = [ v0[0] + var + state[0] |\n      #          v0[1] + var + state[1] |\n      #          v0[2] + var + state[2] ]\n      #      = [ v0[0] + var + state0 |\n      #          v0[1] + var + state0 + v0[0] |\n      #          v0[2] + var + state0 + v0[0] + v0[1] ]\n      #\n      # d(vout[0])/d(v0) = [1 | 0 | 0 ]\n      # d(vout[1])/d(v0) = [1 | 1 | 0 ]\n      # d(vout[2])/d(v0) = [1 | 1 | 1 ]\n      # d(vout)/d(var) = [1 | 1 | 1]\n      # d(vout)/d(state0) = [ 1 | 1 | 1 ]\n\n      state_per_time = np.array(\n          [state0_t, state0_t + v0_t[0, :], state0_t + v0_t[0, :] + v0_t[1, :]])\n\n      # Compare forward prop\n      self.assertAllClose(v0_t + var_t + state_per_time, vout_t)\n\n      # Compare backward prop\n      expected_v0_grad_t = np.array([\n          grad_val[0, :] + grad_val[1, :] + grad_val[2, :],\n          grad_val[1, :] + grad_val[2, :], grad_val[2, :]\n      ])\n\n      self.assertAllEqual(expected_v0_grad_t, v0_grad_t)\n      self.assertAllEqual(expected_v0_grad_t, just_v0_grad_t)\n      self.assertAllClose(grad_val.sum(axis=0), var_grad_t)\n      self.assertAllClose(grad_val.sum(axis=0), state0_grad_t)\n\n  def testWhileLoopWritePackGradients(self):\n    self._testWhileLoopWritePackGradients(\n        dynamic_size=False, dtype=dtypes.float32)\n    # TODO(ebrevdo): re-enable when While supports non-float32 gradients.\n    # self._testWhileLoopWritePackGradients(\n    #     dynamic_size=False, dtype=tf.int64)\n\n  @test_util.run_deprecated_v1\n  def testSkipEagerWhileLoopDynamicWritePackGradients(self):\n    self._testWhileLoopWritePackGradients(\n        dynamic_size=True, dtype=dtypes.float32)\n\n  def testGradSerialTwoLoops(self):\n    with self.session():\n\n      def loop(x):\n        num_steps = 100\n        acc = tensor_array_ops.TensorArray(\n            dtype=dtypes.float32,\n            size=num_steps,\n            clear_after_read=False,\n            element_shape=tensor_shape.TensorShape([]))\n        i = constant_op.constant(0, name=\"i\")\n\n        c = lambda i, acc: i < 5\n\n        def b(i, acc):\n          x1 = control_flow_ops.cond(\n              math_ops.equal(i, 0), lambda: x,\n              lambda: math_ops.multiply(acc.read(i - 1), 2.0))\n          return i + 1, acc.write(i, x1)\n\n        i1, acc1 = control_flow_ops.while_loop(c, b, [i, acc])\n\n        z = constant_op.constant(0.0)\n\n        def fn(i, acc):\n          return i + 1, acc.write(i, z)\n\n        _, acc2 = control_flow_ops.while_loop(lambda i, acc: i < num_steps, fn,\n                                              [i1, acc1])\n\n        r = acc2.stack()\n        return r\n\n      x = constant_op.constant(2.0, name=\"x\")\n      if context.executing_eagerly():\n        grad = backprop.gradients_function(loop)(x)[0]\n      else:\n        grad = gradients_impl.gradients(loop(x), [x])[0]\n      self.assertAllClose(31.0, self.evaluate(grad))\n\n  def testShapeAfterWhileLoop(self):\n    size = 10\n    ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=size)\n    _, ta = control_flow_ops.while_loop(\n        lambda i, _: i < size,\n        lambda i, ta: (i + 1, ta.write(i, [[0.]])), [0, ta],\n        parallel_iterations=1)\n    self.assertIsNotNone(ta.element_shape.dims)\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerSumOfTwoReadVariablesWithoutRepeatGrad(self):\n    with self.session() as session:\n      a = array_ops.identity(\n          np.arange(\n              3 * 5, dtype=np.float32).reshape(3, 5) + 1)\n      b = array_ops.identity(\n          np.arange(\n              3 * 5, dtype=np.float32).reshape(3, 5) + 1 + 3 * 5)\n      ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=2)\n      ta = ta.write(0, a, name=\"write_a\")\n      ta = ta.write(1, b, name=\"write_b\")\n      c = (\n          ta.read(\n              0, name=\"read_a_0\") +  # a + b\n          ta.read(\n              1, name=\"read_b_0\"))\n      g0 = -(np.arange(3 * 5, dtype=np.float32).reshape(3, 5) + 1)\n      grad_a = gradients_impl.gradients([c], [a], [g0])[0]  # d(a+b)/da = 1\n      grad_b = gradients_impl.gradients([c], [b], [g0])[0]  # d(a+b)/db = 1\n\n      # Test gradients calculated individually\n      grad_a_t, = session.run([grad_a])\n      self.assertAllEqual(grad_a_t, g0)\n\n      grad_b_t, = session.run([grad_b])\n      self.assertAllEqual(grad_b_t, g0)\n\n      # Test gradients calculated jointly\n      joint_grad_a_t, joint_grad_b_t = session.run([grad_a, grad_b])\n      self.assertAllEqual(joint_grad_a_t, g0)\n      self.assertAllEqual(joint_grad_b_t, g0)\n\n  def _grad_source_for_name(self, name):\n    return tensor_array_grad._GetGradSource(constant_op.constant(0, name=name))\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerGetGradSource_Invalid(self):\n    with self.assertRaises(ValueError):\n      self._grad_source_for_name(\"\")\n    with self.assertRaises(ValueError):\n      self._grad_source_for_name(\"foo\")\n    with self.assertRaises(ValueError):\n      self._grad_source_for_name(\"foo/bar\")\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerGetGradSource_NoEnclosingScope(self):\n    self.assertEqual(\"gradients:0\", self._grad_source_for_name(\"gradients\"))\n    self.assertEqual(\"gradients_0:0\", self._grad_source_for_name(\"gradients_0\"))\n    self.assertEqual(\"gradients\", self._grad_source_for_name(\"gradients/foo\"))\n    self.assertEqual(\"gradients_0\",\n                     self._grad_source_for_name(\"gradients_0/foo\"))\n    self.assertEqual(\"gradients\",\n                     self._grad_source_for_name(\"gradients/foo/bar\"))\n    self.assertEqual(\"gradients_0\",\n                     self._grad_source_for_name(\"gradients_0/foo/bar\"))\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerGetGradSource_EnclosingScope(self):\n    self.assertEqual(\"foo/gradients:0\",\n                     self._grad_source_for_name(\"foo/gradients\"))\n    self.assertEqual(\"foo/gradients_0:0\",\n                     self._grad_source_for_name(\"foo/gradients_0\"))\n    self.assertEqual(\"foo/gradients\",\n                     self._grad_source_for_name(\"foo/gradients/bar\"))\n    self.assertEqual(\"foo/gradients_0\",\n                     self._grad_source_for_name(\"foo/gradients_0/bar\"))\n    self.assertEqual(\"foo/bar/gradients\",\n                     self._grad_source_for_name(\"foo/bar/gradients/baz\"))\n    self.assertEqual(\"foo/bar/gradients_0\",\n                     self._grad_source_for_name(\"foo/bar/gradients_0/baz\"))\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerGetGradSource_NestedUsesInnermost(self):\n    self.assertEqual(\n        \"foo/gradients/bar/gradients_0\",\n        self._grad_source_for_name(\"foo/gradients/bar/gradients_0/baz\"))\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerWriteShape(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, tensor_array_name=\"foo\", size=3)\n      c0 = constant_op.constant([4.0, 5.0])\n      w0 = ta.write(0, c0)\n      r0 = w0.read(0)\n      self.assertAllEqual(c0.get_shape(), r0.get_shape())\n\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, tensor_array_name=\"foo\", size=3)\n      c1 = constant_op.constant([6.0, 7.0])\n      w1 = w0.write(1, c1)\n      r0 = w1.read(0)\n      r1 = w1.read(1)\n      self.assertAllEqual(c0.get_shape(), r0.get_shape())\n      self.assertAllEqual(c1.get_shape(), r1.get_shape())\n\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, tensor_array_name=\"foo\", size=3)\n      c2 = constant_op.constant([4.0, 5.0, 6.0])\n      with self.assertRaises(ValueError):\n        w0.write(0, c2)\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerPartlyUnknownShape(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, tensor_array_name=\"foo\", size=6)\n\n      c0 = array_ops.placeholder(dtypes.float32, [None, None, None, 3])\n      w0 = ta.write(0, c0)\n      r0 = w0.read(0)\n      self.assertAllEqual([None, None, None, 3], r0.get_shape().as_list())\n\n      c1 = array_ops.placeholder(dtypes.float32, [None, None, None, 3])\n      w1 = w0.write(1, c1)\n      r1 = w1.read(0)\n      self.assertAllEqual([None, None, None, 3], r1.get_shape().as_list())\n\n      # Writing less specific shape (doesn't change type.)\n      c2 = array_ops.placeholder(dtypes.float32, [None, None, None, None])\n      w2 = w1.write(2, c2)\n      r2 = w2.read(0)\n      self.assertAllEqual([None, None, None, 3], r2.get_shape().as_list())\n\n      # Writing more specific shape in one dimension and less specific in\n      # another.\n      c3 = array_ops.placeholder(dtypes.float32, [None, None, 2, None])\n      w3 = w2.write(3, c3)\n      r3 = w3.read(0)\n      self.assertAllEqual([None, None, 2, 3], r3.get_shape().as_list())\n\n      # Writing partly defined shape using TensorArray.scatter.\n      c4 = array_ops.placeholder(dtypes.float32, [2, None, 4, 2, 3])\n      w4 = w3.scatter([4, 5], c4)\n      r4 = w4.read(0)\n      self.assertAllEqual([None, 4, 2, 3], r4.get_shape().as_list())\n\n      # Writing fully defined shape using TensorArray.split.\n      c5 = array_ops.placeholder(dtypes.float32, [10, 4, 2, 3])\n      w5 = w4.split(c5, constant_op.constant([5, 5]))\n      r5 = w5.read(0)\n      self.assertAllEqual([5, 4, 2, 3], r5.get_shape().as_list())\n\n  def _testUnpackShape(self):\n    with self.cached_session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=0,\n          dynamic_size=True,\n          infer_shape=True)\n      value = constant_op.constant(\n          [[1.0, -1.0], [10.0, -10.0], [100.0, -100.0]])\n      w0 = ta.unstack(value)\n      r0 = w0.read(0)\n      self.assertAllEqual((2,), r0.get_shape())\n\n      c1 = constant_op.constant([4.0, 5.0])\n      w1 = w0.write(3, c1)\n\n      if not control_flow_util.ENABLE_CONTROL_FLOW_V2:\n        # TensorArray v2 does not support clear_after_read.\n        with self.assertRaisesOpError(\n            r\"Could not read index 0 twice because it was cleared after a \"\n            r\"previous read \\(perhaps try setting clear_after_read = false\\?\\)\"\n        ):\n          with ops.control_dependencies([r0]):\n            self.evaluate(w1.read(0))\n\n      r1 = w1.read(1)\n      self.assertAllEqual(c1.get_shape(), r1.shape)\n\n      c2 = constant_op.constant([4.0, 5.0, 6.0])\n      with self.assertRaises(ValueError):\n        w1.write(4, c2)\n\n  def testUnpackShape(self):\n    self._testUnpackShape()\n\n  @test_util.deprecated_graph_mode_only\n  def testSplitShape(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=0,\n          dynamic_size=True,\n          infer_shape=True)\n      value = constant_op.constant([[1.0, -1.0], [2.0, -2.0], [3.0, -3.0]])\n      w0 = ta.split(value, [1, 1, 1])\n      r0 = w0.read(0)\n      self.assertAllEqual((1, 2), r0.get_shape())\n\n      ta1 = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo1\",\n          size=0,\n          dynamic_size=True,\n          infer_shape=True)\n      w0 = ta1.split(value, [1, 2])\n      r0 = w0.read(0)\n      if context.executing_eagerly():\n        self.assertEqual((1, 2), r0.get_shape())\n        self.assertEqual((2, 2), w0.read(1).get_shape())\n      else:\n        self.assertEqual(r0.get_shape().ndims, None)\n        if not control_flow_util.ENABLE_CONTROL_FLOW_V2:\n          self.assertEqual(\n              tensor_shape.TensorShape(\n                  ta1.handle.op.get_attr(\"element_shape\")).ndims, None)\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerWriteUnknownShape(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=3,\n          infer_shape=True)\n      c0 = array_ops.placeholder(dtypes.float32)\n      w0 = ta.write(0, c0)\n      r0 = w0.read(0)\n      self.assertAllEqual(r0.get_shape(), tensor_shape.unknown_shape())\n\n  def _testGradientWhenNotAllComponentsRead(self):\n    with self.cached_session() as session:\n      ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=2)\n      x = constant_op.constant([2.0, 3.0])\n      w = ta.unstack(x)\n      r0 = w.read(0)\n      # calculate (dr0/dx0, dr0/dx1).  since r0 = x0, gradients are (1, 0).\n      grad_r0 = gradients_impl.gradients(ys=[r0], xs=[x], grad_ys=[1.0])\n      grad_r0_vals = session.run(grad_r0)[0]\n      self.assertAllEqual(grad_r0_vals, [1.0, 0.0])\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerGradientWhenNotAllComponentsRead(self):\n    self._testGradientWhenNotAllComponentsRead()\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerWriteButNotAllComponentsReadGrad(self):\n    with self.cached_session() as session:\n      x0 = constant_op.constant(5.0)\n      x1 = constant_op.constant(10.0)\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, size=2).write(0, x0).write(1, x1)\n      r0 = ta.read(0)\n      # calculate (dr0/dx0, dr0/dx1).  since r0 = x0, gradients are (1, 0).\n      grad_r0_x1 = gradients_impl.gradients(ys=[r0], xs=[x0, x1], grad_ys=[1.0])\n      grad_r0_x1_vals = session.run(grad_r0_x1)\n      self.assertAllEqual(grad_r0_x1_vals, [1.0, 0.0])\n\n  def _testTensorArrayUnpackDynamic(self):\n    with self.cached_session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, size=3, dynamic_size=True)\n      x = constant_op.constant([1.0, 2.0, 3.0])\n      w0 = ta.unstack(x)\n      w1 = w0.write(3, 4.0)\n      r = w1.stack()\n      self.assertAllEqual(np.array([1.0, 2.0, 3.0, 4.0]), self.evaluate(r))\n      grad = gradients_impl.gradients(ys=[r], xs=[x])\n      self.assertAllEqual(np.array([1.0, 1.0, 1.0]), self.evaluate(grad)[0])\n\n  @test_util.run_deprecated_v1\n  def testSkipEagerTensorArrayUnpackDynamic(self):\n    self._testTensorArrayUnpackDynamic()\n\n  @test_util.run_deprecated_v1\n  def testSkipEagerTensorArraySplitDynamic(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, size=3, dynamic_size=True)\n      x = constant_op.constant([1.0, 2.0, 3.0])\n      w0 = ta.split(x, [1, 1, 1])\n      w1 = w0.write(3, [4.0])\n      r = w1.concat()\n      self.assertAllEqual(np.array([1.0, 2.0, 3.0, 4.0]), self.evaluate(r))\n      grad = gradients_impl.gradients(ys=[r], xs=[x])\n      self.assertAllEqual(np.array([1.0, 1.0, 1.0]), self.evaluate(grad)[0])\n\n  def testStackShape(self):\n\n    @def_function.function\n    def ta_stack():\n      ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=3)\n      x = constant_op.constant([1.0, 2.0, 3.0])\n      ta = ta.write(0, x)\n      t = ta.stack()\n      self.assertEqual(t.shape.as_list(), [3, 3])\n      return t\n\n    ta_stack()\n\n  def testReadShape(self):\n\n    @def_function.function\n    def ta_read():\n      ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=3)\n      x = constant_op.constant([1.0, 2.0, 3.0])\n      ta = ta.write(0, x)\n      t = ta.read(0)\n      self.assertEqual(t.shape.as_list(), [3])\n      return t\n\n    ta_read()\n\n  def testGatherShape(self):\n\n    def ta_gather(indices):\n      ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=3)\n      x = constant_op.constant([1.0, 2.0, 3.0])\n      ta = ta.write(0, x)\n      t = ta.gather(indices)\n      self.assertEqual(t.shape.as_list(), [first_dim, 3])\n      return t\n\n    # This propagates shape of `indices` when compiling ta_gather.\n    ta_gather_with_known_indices_shape = def_function.function(ta_gather)\n    first_dim = 1\n    ta_gather_with_known_indices_shape([0])\n\n    # Here were force the shape of `indices` to be [None] during ta_gather's\n    # compilation.\n    ta_gather_with_unknown_indices_shape = def_function.function(\n        ta_gather,\n        input_signature=[\n            tensor_spec.TensorSpec(dtype=dtypes.int32, shape=[None])\n        ])\n    first_dim = None\n    ta_gather_with_unknown_indices_shape([0])\n\n  def _testTensorArrayEvalEmpty(self):\n    with self.cached_session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, size=0, dynamic_size=False, infer_shape=False)\n      v2_msg = (\"Tried to stack elements of an empty list with \"\n                \"non-fully-defined element_shape\")\n      v1_msg = (\n          \"TensorArray has size zero, but element shape <unknown> is not \"\n          \"fully defined. Currently only static shapes are supported when \"\n          \"packing zero-size TensorArrays.\")\n      with self.assertRaisesOpError(\n          v2_msg if control_flow_util.ENABLE_CONTROL_FLOW_V2 else v1_msg):\n        ta.stack().eval()\n\n  @test_util.run_deprecated_v1\n  def testSkipEagerTensorArrayEvalEmpty(self):\n    self._testTensorArrayEvalEmpty()\n\n  # this test is ill-defined for Eager mode --- unpacking an empty tensor\n  # gives an empty list / there is not equivalent of \"mark_used\" in Eager\n  def _testTensorArrayEvalEmptyWithDefault(self):\n    with self.cached_session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, size=0, dynamic_size=False, infer_shape=True)\n      self.assertEqual(0, ta.size().eval())\n      # Don't actually perform the pack.  This stores the static shape.\n      if control_flow_util.ENABLE_CONTROL_FLOW_V2:\n        ta = ta.unstack(array_ops.zeros([0, 3, 5]))\n      else:\n        ta.unstack(array_ops.zeros([0, 3, 5])).mark_used()\n      packed = ta.stack()\n      concatenated = ta.concat()\n      self.assertAllEqual([0, 3, 5], self.evaluate(packed).shape)\n      # Concatenating zero tensors along their first dimension gives a\n      # first dimension of zero\n      self.assertAllEqual([0, 5], self.evaluate(concatenated).shape)\n\n  @test_util.run_deprecated_v1\n  def testSkipEagerTensorArrayEvalEmptyWithDefault(self):\n    self._testTensorArrayEvalEmptyWithDefault()\n\n  @test_util.run_deprecated_v1\n  def testSkipEagerTensorArrayScatterReadAndGradients(self):\n    with self.session() as session:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=0,\n          dynamic_size=True)\n\n      indices = constant_op.constant([1, 8])\n      value = constant_op.constant([[1.0, -1.0], [10.0, -10.0]])\n\n      w = ta.scatter(indices, value)\n      r0 = w.read(1)\n      r1 = w.read(8)\n\n      # Test combined gradients + aggregation of read(0)\n      grad = gradients_impl.gradients(\n          ys=[r0, r1], xs=[value], grad_ys=[[2.0, 3.0], [4.0, 5.0]])\n      read_vals, grad_vals = session.run([[r0, r1], grad])\n\n      self.assertEqual(len(read_vals), 2)\n      self.assertEqual(len(grad_vals), 1)\n      self.assertAllEqual([1.0, -1.0], read_vals[0])\n      self.assertAllEqual([10.0, -10.0], read_vals[1])\n      self.assertAllEqual([[2.0, 3.0], [4.0, 5.0]], grad_vals[0])\n\n  @test_util.run_deprecated_v1\n  def testSkipEagerTensorArrayScatterPartialReadAndGradients(self):\n    with self.session() as session:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=0,\n          dynamic_size=True)\n\n      indices = constant_op.constant([1, 8])\n      value = constant_op.constant([[1.0, -1.0], [10.0, -10.0]])\n\n      w = ta.scatter(indices, value)\n      r0 = w.read(1)\n\n      # Test combined gradients + aggregation of read(0)\n      grad = gradients_impl.gradients(\n          ys=[r0], xs=[value], grad_ys=[[2.0, 3.0]])[0]\n      read_val, grad_val = session.run([r0, grad])\n\n      self.assertAllEqual([1.0, -1.0], read_val)\n      self.assertAllEqual([[2.0, 3.0], [0.0, 0.0]], grad_val)\n\n  def testScatterIntoExistingList(self):\n    ta = tensor_array_ops.TensorArray(\n        dtype=dtypes.float32, tensor_array_name=\"foo\", size=5)\n\n    ta = ta.scatter(indices=[3, 4], value=array_ops.ones([2]))\n    self.assertAllEqual(ta.stack(), [0., 0., 0., 1., 1.])\n\n    ta = ta.scatter(indices=[1], value=array_ops.ones([1]))\n    self.assertAllEqual(ta.stack(), [0., 1., 0., 1., 1.])\n\n    ta = ta.scatter(indices=[0, 2], value=[5., 6.])\n    self.assertAllEqual(ta.stack(), [5., 1., 6., 1., 1.])\n\n  @test_util.run_v1_only(\"b/118890905\")\n  def testTensorArrayWriteGatherAndGradients(self):\n    with self.session() as session:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=0,\n          dynamic_size=True)\n\n      def func(values):\n        indices = constant_op.constant([1, 8])\n        w = ta.unstack(values)\n        g = w.gather(indices)\n        return g\n\n      values = constant_op.constant([[1.0 * x, -1.0 * x] for x in range(10)])\n      g = func(values)\n      grad_ys = [[[2.0, 3.0], [4.0, 5.0]]]\n      # Test combined gradients + aggregation of read(0)\n      if context.executing_eagerly():\n        g_vals = [g]\n        grad_vals = backprop.gradients_function(func)(\n            values, dy=constant_op.constant(grad_ys[0], dtype=dtypes.float32))\n      else:\n        grad = gradients_impl.gradients(ys=[g], xs=[values], grad_ys=grad_ys)\n        g_vals, grad_vals = session.run([[g], grad])\n\n      # Gradients for 8 of the 10 unread components are zero.\n      expected_grad = np.zeros((10, 2))\n      expected_grad[1] = [2.0, 3.0]\n      expected_grad[8] = [4.0, 5.0]\n\n      self.assertEqual(len(g_vals), 1)\n      self.assertEqual(len(grad_vals), 1)\n      self.assertAllEqual([[1.0, -1.0], [8.0, -8.0]], g_vals[0])\n      self.assertAllEqual(expected_grad, grad_vals[0])\n\n  @test_util.disable_control_flow_v2(\"colocate_with not supported in v2.\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testSkipEagerTensorArrayGetsDeviceFromFirstWrite(self):\n    with ops.device(\"/job:worker/task:0/cpu:0\"):\n      # this initial device will be ignored.\n      ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=2)\n    with ops.device(\"/job:worker/task:1/cpu:0\"):\n      # the first write sets the op's device.\n      ta = ta.write(0, 1.0)\n    with ops.device(\"/job:worker/task:2/cpu:0\"):\n      # subsequent writes do not modify the op's device.\n      ta = ta.write(1, 1.0)\n\n    # The gradient TA will sit on the same device as the forward TA.\n    ta_grad = ta.grad(\"grad\")\n    flows = [ta.flow, ta_grad.flow]\n\n    # Similar tests for unpack and split\n    with ops.device(\"/job:worker/task:0/cpu:0\"):\n      ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=3)\n    with ops.device(\"/job:worker/task:1/cpu:0\"):\n      ta = ta.unstack([1.0, 2.0])\n    with ops.device(\"/job:worker/task:2/cpu:0\"):\n      ta = ta.write(2, 3.0)\n    flows.append(ta.flow)\n\n    with ops.device(\"/job:worker/task:0/cpu:0\"):\n      ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=2)\n    with ops.device(\"/job:worker/task:1/cpu:0\"):\n      ta = ta.split([1.0, 2.0], [1, 1])\n    flows.append(ta.flow)\n\n    session = session_lib.Session(self._workers[0].target)\n\n    run_options = config_pb2.RunOptions(\n        trace_level=config_pb2.RunOptions.FULL_TRACE)\n    run_metadata = config_pb2.RunMetadata()\n\n    session.run(flows, options=run_options, run_metadata=run_metadata)\n    self.assertTrue(run_metadata.HasField(\"step_stats\"))\n    dev_stats = {d.device: d.node_stats\n                 for d in run_metadata.step_stats.dev_stats}\n    for d in dev_stats:\n      if \"/task:1/\" in d:\n        self.assertTrue(\n            [s for s in dev_stats[d] if \"/TensorArray\" in s.node_name])\n      elif \"/host:CPU\" not in d:\n        self.assertFalse(\n            [s for s in dev_stats[d] if \"/TensorArray\" in s.node_name])\n\n  @test_util.disable_control_flow_v2(\"colocate_with not supported in v2.\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testSkipEagerTensorArrayGetsDeviceFromFirstWriteInWhileLoop(self):\n    with ops.device(\"/job:worker/task:0/cpu:0\"):\n      ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=2)\n\n    def _body(i, ta_i):\n      with ops.device(\"/job:worker/task:1/cpu:0\"):\n        return i + 1, ta_i.write(i, constant_op.constant(0.0))\n\n    _, ta_out = control_flow_ops.while_loop(\n        lambda i, ta: i < 2, _body, loop_vars=[0, ta])\n\n    session = session_lib.Session(self._workers[0].target)\n\n    run_options = config_pb2.RunOptions(\n        trace_level=config_pb2.RunOptions.FULL_TRACE)\n    run_metadata = config_pb2.RunMetadata()\n\n    session.run(ta_out.flow, options=run_options, run_metadata=run_metadata)\n    self.assertTrue(run_metadata.HasField(\"step_stats\"))\n    dev_stats = {d.device: d.node_stats\n                 for d in run_metadata.step_stats.dev_stats}\n    for d in dev_stats:\n      if \"/task:1/\" in d:\n        self.assertTrue(\n            [s for s in dev_stats[d] if \"TensorArray\" == s.node_name])\n      else:\n        self.assertFalse(\n            [s for s in dev_stats[d] if \"TensorArray\" == s.node_name])\n\n  @test_util.disable_control_flow_v2(\"colocate_with not supported in v2.\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testSkipEagerTensorArrayDisabledColocateWithFirstWriteCall(self):\n    with ops.device(\"/job:worker/task:0/cpu:0\"):\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, size=2, colocate_with_first_write_call=False)\n\n    def _body(i, ta_i):\n      with ops.device(\"/job:worker/task:1/cpu:0\"):\n        return i + 1, ta_i.write(i, constant_op.constant(0.0))\n\n    _, ta_out = control_flow_ops.while_loop(\n        lambda i, ta: i < 2, _body, loop_vars=[0, ta])\n\n    session = session_lib.Session(self._workers[0].target)\n\n    run_options = config_pb2.RunOptions(\n        trace_level=config_pb2.RunOptions.FULL_TRACE)\n    run_metadata = config_pb2.RunMetadata()\n\n    session.run(ta_out.flow, options=run_options, run_metadata=run_metadata)\n    self.assertTrue(run_metadata.HasField(\"step_stats\"))\n    dev_stats = {d.device: list(d.node_stats)\n                 for d in run_metadata.step_stats.dev_stats}\n    for d in dev_stats:\n      if \"/task:0/\" in d and \"CPU\" in d:  # Skip any GPU node stats\n        self.assertTrue(\n            [s for s in dev_stats[d] if \"TensorArray\" == s.node_name])\n      else:\n        self.assertFalse(\n            [s for s in dev_stats[d] if \"TensorArray\" == s.node_name])\n\n  def testTensorArrayIdentity(self):\n    with self.session():\n      ta0 = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=2,\n                                         infer_shape=False)\n      ta1 = tensor_array_ops.TensorArray(dtype=dtypes.int32, size=4,\n                                         infer_shape=True)\n\n      ta0 = ta0.write(0, 0.)\n      ta1 = ta1.write(0, 1)\n\n      v0 = variable_scope.get_variable(\n          \"v0\", shape=(), initializer=init_ops.zeros_initializer())\n      v1 = variable_scope.get_variable(\n          \"v1\", shape=(), initializer=init_ops.zeros_initializer())\n\n      with ops.control_dependencies([v0.assign_add(1)]):\n        ta0 = ta0.identity()\n\n      with ops.control_dependencies([v1.assign_add(1)]):\n        ta1 = ta1.identity()\n\n      read0 = ta0.read(0)\n      read1 = ta1.read(0)\n\n      size0 = ta0.size()\n      size1 = ta1.size()\n\n      # Tests correct properties on new TensorArrays.\n      self.assertEqual(dtypes.float32, ta0.dtype)\n      self.assertEqual(dtypes.int32, ta1.dtype)\n      if context.executing_eagerly():\n        self.assertEqual(tensor_shape.TensorShape([]), read0.get_shape())\n      else:\n        self.assertEqual(tensor_shape.unknown_shape(), read0.get_shape())\n      self.assertEqual(tensor_shape.TensorShape([]), read1.get_shape())\n\n      if not context.executing_eagerly():\n        self.evaluate(variables.global_variables_initializer())\n\n      read0_v, read1_v, size0_v, size1_v = self.evaluate((read0, read1, size0,\n                                                          size1))\n\n      # Tests that the control dependencies was added and executed.\n      self.assertEqual(1, self.evaluate(v0))\n      self.assertEqual(1, self.evaluate(v1))\n\n      # Tests correct TensorArray.\n      self.assertEqual(read0_v, 0)\n      self.assertEqual(read1_v, 1)\n      self.assertEqual(size0_v, 2)\n      self.assertEqual(size1_v, 4)\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerTensorArrayGradYsInCorrectScope(self):\n    n_time = 1\n    n_dim = 1\n    x = constant_op.constant([[1.42]])\n    dy = constant_op.constant([[2.42]])\n\n    ta = tensor_array_ops.TensorArray(\n        dtypes.float32, size=n_time, element_shape=[n_dim])\n    for t in range(n_time):\n      ta = ta.write(index=t, value=x[t])\n      y = ta.stack()\n      # dy is outside of the gradients name scope; tf.gradients must\n      # wrap it in the correct name scope.\n      dx, = gradients_impl.gradients(ys=[y], xs=[x], grad_ys=[dy])\n      with self.cached_session():\n        vdx, vdy = self.evaluate([dx, dy])\n      self.assertAllClose(vdx, vdy)\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerTensorArrayInt64GPU(self):\n    if not test.is_gpu_available():\n      return\n    with self.session(force_gpu=True) as sess:\n      value = array_ops.placeholder(dtypes.int64)\n      ta = tensor_array_ops.TensorArray(dtype=dtypes.int64, size=2)\n      ta = ta.scatter([0, 1], value)\n      r0 = ta.read(0)\n      r1 = ta.read(1)\n      v0, v1 = sess.run([r0, r1], feed_dict={value: [-3, 100]})\n      self.assertAllEqual(v0, -3)\n      self.assertAllEqual(v1, 100)\n\n  @test_util.deprecated_graph_mode_only\n  def testTensorArrayScatterBfloat16GPU(self):\n    if not test.is_gpu_available():\n      return\n    with self.session(force_gpu=True) as sess:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.bfloat16, tensor_array_name=\"foo\", size=5)\n      ta = ta.scatter(\n          indices=[3, 4], value=array_ops.ones([2], dtype=dtypes.bfloat16))\n      self.assertAllEqual(ta.stack(), [0., 0., 0., 1., 1.])\n\n  def testInferShapeFalseValid(self):\n    ta = tensor_array_ops.TensorArray(\n        dtypes.float32, size=3, infer_shape=False, element_shape=[None, 10, 20])\n    ta = ta.write(0, array_ops.ones([50, 10, 20]))\n    ta = ta.write(1, array_ops.ones([50, 10, 20]))\n    ta = ta.write(2, array_ops.ones([1, 10, 20]))\n    ta = ta.concat()\n\n    correct = np.ones([101, 10, 20])\n\n    self.assertAllEqual(ta, correct)\n\n  def testInferShapeFalseInvalid(self):\n    ta = tensor_array_ops.TensorArray(\n        dtypes.float32, size=2, infer_shape=False, element_shape=[None, 10, 20])\n    ta = ta.write(0, array_ops.ones([50, 10, 20]))\n\n    with self.assertRaises(ValueError):\n      ta = ta.write(1, array_ops.ones([1, 20, 20]))\n\n  def testInferShapeTrue(self):\n    ta = tensor_array_ops.TensorArray(\n        dtypes.float32, size=3, infer_shape=True, element_shape=[None, 10, 20])\n    self.assertAllEqual((None, 10, 20), ta.element_shape.as_list())\n    ta = ta.write(0, array_ops.ones([50, 10, 20]))\n    self.assertAllEqual((50, 10, 20), ta.element_shape.as_list())\n    ta = ta.write(1, array_ops.ones([50, 10, 20]))\n    with self.assertRaises(ValueError):\n      ta = ta.write(\n          2, array_ops.ones([1, 10, 20])\n      )  # Inconsistent shapes: saw (1, 10, 20) but expected (50, 10, 20)\n\n  def testStackShapeOnEmpty(self):\n    ta = tensor_array_ops.TensorArray(\n        dtypes.float32, size=0, element_shape=(5, 10), dynamic_size=True)\n    self.assertAllEqual([0, 5, 10], self.evaluate(ta.stack()).shape)\n\n  @test_util.run_deprecated_v1\n  def testSkipEagerStackOnPartiallyDefinedShape(self):\n    ta = tensor_array_ops.TensorArray(\n        dtypes.float32, size=0, element_shape=(5, None), dynamic_size=True)\n    self.assertEqual([None, 5, None], ta.stack().shape.as_list())\n\n  def testStackShapeOnStaticSize(self):\n    ta = tensor_array_ops.TensorArray(dtypes.float32, size=42)\n    ta = ta.write(0, [0])\n    self.assertEqual([42, 1], ta.stack().shape.as_list())\n\n\nclass TensorArrayBenchmark(test.Benchmark):\n\n  def _tensorArrayWriteInWhile(self):\n    size = 10000\n    ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=size)\n    (_, ta) = control_flow_ops.while_loop(\n        lambda i, _: i < size,\n        lambda i, ta: (i + 1, ta.write(i, 0.)), [0, ta],\n        parallel_iterations=1)\n    return ta.stack()\n\n  def _benchmarkWriteInWhile(self):\n    ops.reset_default_graph()\n    op = self._tensorArrayWriteInWhile()\n    self.run_op_benchmark(session_lib.Session(), op)\n\n  def benchmarkWriteInWhile(self):\n    self._benchmarkWriteInWhile()\n\n  @test_util.enable_control_flow_v2\n  def benchmarkWriteInWhileWithControlFlowV2(self):\n    self._benchmarkWriteInWhile()\n\n  def benchmarkWriteInDatasetMapFn(self):\n    ds = dataset_ops.Dataset.from_tensors(array_ops.zeros([10])).repeat()\n    ds = ds.map(lambda _: self._tensorArrayWriteInWhile())\n    op = ds.make_one_shot_iterator().get_next()\n    self.run_op_benchmark(session_lib.Session(), op)\n\n  def benchmarkWriteInDatasetParallelMapFn(self):\n    ds = dataset_ops.Dataset.from_tensors(array_ops.zeros([10])).repeat()\n    ds = ds.map(lambda _: self._tensorArrayWriteInWhile(), num_parallel_calls=2)\n    op = ds.make_one_shot_iterator().get_next()\n    self.run_op_benchmark(session_lib.Session(), op)\n\n\nif __name__ == \"__main__\":\n  test.main()"