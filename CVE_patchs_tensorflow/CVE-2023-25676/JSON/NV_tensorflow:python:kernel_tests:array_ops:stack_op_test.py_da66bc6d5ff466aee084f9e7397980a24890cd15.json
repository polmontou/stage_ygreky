"# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Functional tests for Stack and ParallelStack Ops.\"\"\"\n\nimport numpy as np\n\nfrom tensorflow.python import tf2\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gen_array_ops\nfrom tensorflow.python.ops import gradient_checker_v2\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.platform import test\n\n\ndef np_split_squeeze(array, axis):\n  axis_len = array.shape[axis]\n  return [\n      np.squeeze(\n          arr, axis=(axis,)) for arr in np.split(\n              array, axis_len, axis=axis)\n  ]\n\n\nclass StackOpTest(test.TestCase):\n\n  def randn(self, shape, dtype):\n    data = np.random.randn(*shape)\n    if dtype == np.bool_:\n      return data < 0  # Naive casting yields True with P(1)!\n    else:\n      return data.astype(dtype)\n\n  def testSimple(self):\n    np.random.seed(7)\n    for shape in (2,), (3,), (2, 3), (3, 2), (8, 2, 10):\n      rank = len(shape)\n      for axis in range(-rank, rank):\n        for dtype in [np.bool_, np.float32, np.int32, np.int64]:\n          data = self.randn(shape, dtype)\n          xs = np_split_squeeze(data, axis)\n          # Stack back into a single tensorflow tensor\n          with self.subTest(shape=shape, axis=axis, dtype=dtype):\n            c = array_ops.stack(xs, axis=axis)\n            self.assertAllEqual(c, data)\n\n  def testSimpleParallelCPU(self):\n    # tf.parallel_stack is only supported in graph mode.\n    with ops.Graph().as_default():\n      np.random.seed(7)\n      with test_util.device(use_gpu=False):\n        for shape in (2,), (3,), (2, 3), (3, 2), (4, 3, 2), (100, 24, 24, 3):\n          with self.subTest(shape=shape):\n            data = self.randn(shape, np.float32)\n            xs = list(map(constant_op.constant, data))\n            c = array_ops.parallel_stack(xs)\n            self.assertAllEqual(c, data)\n\n  def testParallelConcatShapeZero(self):\n    if not tf2.enabled():\n      self.skipTest(\"only fails in TF2\")\n\n    @def_function.function\n    def f():\n      y = gen_array_ops.parallel_concat(values=[[\"tf\"]], shape=0)\n      return y\n\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError, r\"0th dimension .* must be greater than\"\n    ):\n      f()\n\n  def testSimpleParallelGPU(self):\n    # tf.parallel_stack is only supported in graph mode.\n    with ops.Graph().as_default():\n      with test_util.device(use_gpu=True):\n        for shape in (2,), (3,), (2, 3), (3, 2), (4, 3, 2), (100, 24, 24, 3):\n          with self.subTest(shape=shape):\n            data = self.randn(shape, np.float32)\n            xs = list(map(constant_op.constant, data))\n            c = array_ops.parallel_stack(xs)\n            self.assertAllEqual(c, data)\n\n  def testConst(self):\n    np.random.seed(7)\n    with test_util.use_gpu():\n      # Verify that shape induction works with shapes produced via const stack\n      a = constant_op.constant([1, 2, 3, 4, 5, 6])\n      b = array_ops.reshape(a, array_ops.stack([2, 3]))\n      self.assertAllEqual(b.get_shape(), [2, 3])\n\n      # Check on a variety of shapes and types\n      for shape in (2,), (3,), (2, 3), (3, 2), (4, 3, 2), (8, 2, 10):\n        for dtype in [np.bool_, np.float32, np.int16, np.int32, np.int64]:\n          with self.subTest(shape=shape, dtype=dtype):\n            data = self.randn(shape, dtype)\n            # Stack back into a single tensorflow tensor directly using np array\n            c = array_ops.stack(data)\n            if not context.executing_eagerly():\n              # This is implemented via a Const:\n              self.assertEqual(c.op.type, \"Const\")\n            self.assertAllEqual(c, data)\n\n            # Python lists also work for 1-D case:\n            if len(shape) == 1:\n              data_list = list(data)\n              cl = array_ops.stack(data_list)\n              if not context.executing_eagerly():\n                self.assertEqual(cl.op.type, \"Const\")\n              self.assertAllEqual(cl, data)\n\n  def testConstParallelCPU(self):\n    # tf.parallel_stack is only supported in graph mode.\n    with ops.Graph().as_default():\n      np.random.seed(7)\n      with test_util.device(use_gpu=False):\n        for shape in (2,), (3,), (2, 3), (3, 2), (4, 3, 2), (8, 2, 10):\n          with self.subTest(shape=shape):\n            data = self.randn(shape, np.float32)\n            if len(shape) == 1:\n              data_list = list(data)\n              cl = array_ops.parallel_stack(data_list)\n              self.assertAllEqual(cl, data)\n\n            data = self.randn(shape, np.float32)\n            c = array_ops.parallel_stack(data)\n            self.assertAllEqual(c, data)\n\n  def testConstParallelGPU(self):\n    # tf.parallel_stack is only supported in graph mode.\n    with ops.Graph().as_default():\n      np.random.seed(7)\n      with test_util.device(use_gpu=True):\n        for shape in (2,), (3,), (2, 3), (3, 2), (4, 3, 2):\n          with self.subTest(shape=shape):\n            data = self.randn(shape, np.float32)\n            if len(shape) == 1:\n              data_list = list(data)\n              cl = array_ops.parallel_stack(data_list)\n              self.assertAllEqual(cl, data)\n\n            data = self.randn(shape, np.float32)\n            c = array_ops.parallel_stack(data)\n            self.assertAllEqual(c, data)\n\n  def testGradientsAxis0(self):\n    np.random.seed(7)\n    for shape in (2,), (3,), (2, 3), (3, 2), (8, 2, 10):\n      data = np.random.randn(*shape)\n      with self.subTest(shape=shape):\n        with self.cached_session():\n\n          def func(*xs):\n            return array_ops.stack(xs)\n          # TODO(irving): Remove list() once we handle maps correctly\n          xs = list(map(constant_op.constant, data))\n          theoretical, numerical = gradient_checker_v2.compute_gradient(\n              func, xs)\n          self.assertAllClose(theoretical, numerical)\n\n  def testGradientsAxis1(self):\n    np.random.seed(7)\n    for shape in (2, 3), (3, 2), (8, 2, 10):\n      data = np.random.randn(*shape)\n      out_shape = list(shape[1:])\n      out_shape.insert(1, shape[0])\n      with self.subTest(shape=shape):\n        with self.cached_session():\n\n          def func(*inp):\n            return array_ops.stack(inp, axis=1)\n          # TODO(irving): Remove list() once we handle maps correctly\n          xs = list(map(constant_op.constant, data))\n          theoretical, numerical = gradient_checker_v2.compute_gradient(\n              func, xs)\n          self.assertAllClose(theoretical, numerical)\n\n  def testZeroSizeCPU(self):\n    # tf.parallel_stack is only supported in graph mode.\n    with ops.Graph().as_default():\n      # Verify that stack doesn't crash for zero size inputs\n      with test_util.device(use_gpu=False):\n        for shape in (0,), (3, 0), (0, 3):\n          with self.subTest(shape=shape):\n            x = np.zeros((2,) + shape).astype(np.int32)\n            p = self.evaluate(array_ops.stack(list(x)))\n            self.assertAllEqual(p, x)\n\n            p = self.evaluate(array_ops.parallel_stack(list(x)))\n            self.assertAllEqual(p, x)\n\n  def testZeroSizeGPU(self):\n    # tf.parallel_stack is only supported in graph mode.\n    with ops.Graph().as_default():\n      # Verify that stack doesn't crash for zero size inputs\n      with test_util.device(use_gpu=True):\n        for shape in (0,), (3, 0), (0, 3):\n          with self.subTest(shape=shape):\n            x = np.zeros((2,) + shape).astype(np.int32)\n            p = self.evaluate(array_ops.stack(list(x)))\n            self.assertAllEqual(p, x)\n\n            p = self.evaluate(array_ops.parallel_stack(list(x)))\n            self.assertAllEqual(p, x)\n\n  def testAxis0DefaultCPU(self):\n    # tf.parallel_stack is only supported in graph mode.\n    with ops.Graph().as_default():\n      with test_util.device(use_gpu=False):\n        t = [constant_op.constant([1, 2, 3]), constant_op.constant([4, 5, 6])]\n        stacked = self.evaluate(array_ops.stack(t))\n        parallel_stacked = self.evaluate(array_ops.parallel_stack(t))\n\n      expected = np.array([[1, 2, 3], [4, 5, 6]])\n      self.assertAllEqual(stacked, expected)\n      self.assertAllEqual(parallel_stacked, expected)\n\n  def testAxis0DefaultGPU(self):\n    # tf.parallel_stack is only supported in graph mode.\n    with ops.Graph().as_default():\n      with test_util.device(use_gpu=True):\n        t = [constant_op.constant([1, 2, 3]), constant_op.constant([4, 5, 6])]\n        stacked = self.evaluate(array_ops.stack(t))\n        parallel_stacked = self.evaluate(array_ops.parallel_stack(t))\n\n      expected = np.array([[1, 2, 3], [4, 5, 6]])\n      self.assertAllEqual(stacked, expected)\n      self.assertAllEqual(parallel_stacked, expected)\n\n  def testAgainstNumpy(self):\n    # For 1 to 5 dimensions.\n    for shape in (3,), (2, 2, 3), (4, 1, 2, 2), (8, 2, 10):\n      rank = len(shape)\n      expected = self.randn(shape, np.float32)\n      for dtype in [np.bool_, np.float32, np.int32, np.int64]:\n        # For all the possible axis to split it, including negative indices.\n        for axis in range(-rank, rank):\n          test_arrays = np_split_squeeze(expected, axis)\n\n          with self.cached_session():\n            with self.subTest(shape=shape, dtype=dtype, axis=axis):\n              actual_pack = array_ops.stack(test_arrays, axis=axis)\n              self.assertEqual(expected.shape, actual_pack.get_shape())\n              actual_pack = self.evaluate(actual_pack)\n\n              actual_stack = array_ops.stack(test_arrays, axis=axis)\n              self.assertEqual(expected.shape, actual_stack.get_shape())\n              actual_stack = self.evaluate(actual_stack)\n\n              self.assertNDArrayNear(expected, actual_stack, 1e-6)\n\n  def testDimOutOfRange(self):\n    t = [constant_op.constant([1, 2, 3]), constant_op.constant([4, 5, 6])]\n    with self.assertRaisesRegex(ValueError,\n                                r\"Argument `axis` = 2 not in range \\[-2, 2\\)\"):\n      array_ops.stack(t, axis=2)\n\n  def testDimOutOfNegativeRange(self):\n    t = [constant_op.constant([1, 2, 3]), constant_op.constant([4, 5, 6])]\n    with self.assertRaisesRegex(ValueError,\n                                r\"Argument `axis` = -3 not in range \\[-2, 2\\)\"):\n      array_ops.stack(t, axis=-3)\n\n  def testComplex(self):\n    np.random.seed(7)\n    with self.session():\n      for shape in (2,), (3,), (2, 3), (3, 2), (8, 2, 10):\n        for dtype in [np.complex64, np.complex128]:\n          with self.subTest(shape=shape, dtype=dtype):\n            data = self.randn(shape, dtype)\n            xs = list(map(constant_op.constant, data))\n            c = array_ops.stack(xs)\n            self.assertAllEqual(self.evaluate(c), data)\n\n  def testZeroDimUnmatch(self):\n    # Test case for GitHub issue 53300.\n    # Error message is `Shapes of all inputs must match` in eager mode,\n    # and `Shapes ...` in graph mode. Below is to capture both:\n    with self.assertRaisesRegex((errors.InvalidArgumentError, ValueError),\n                                r\"Shapes\"):\n      with self.session():\n        t = [array_ops.zeros([0, 3]), array_ops.zeros([1, 3])]\n        self.evaluate(array_ops.stack(t))\n\n  def testQTypes(self):\n    np.random.seed(7)\n    with self.session(use_gpu=True):\n      shape = [2]\n      for dtype in [\n          dtypes.quint8, dtypes.quint16, dtypes.qint8, dtypes.qint16,\n          dtypes.qint32\n      ]:\n        with self.subTest(shape=shape, dtype=dtype):\n          data = self.randn(shape, dtype.as_numpy_dtype)\n          xs = list(map(constant_op.constant, data))\n          c = math_ops.equal(array_ops.stack(xs), data)\n          self.assertAllEqual(self.evaluate(c), [True, True])\n\n\nclass AutomaticStackingTest(test.TestCase):\n\n  def testSimple(self):\n    self.assertAllEqual([1, 0, 2],\n                        ops.convert_to_tensor([1, constant_op.constant(0), 2]))\n    self.assertAllEqual([[0, 0, 0], [0, 1, 0], [0, 0, 0]],\n                        ops.convert_to_tensor([[0, 0, 0],\n                                               [0,\n                                                constant_op.constant(1), 0],\n                                               [0, 0, 0]]))\n    self.assertAllEqual([[0, 0, 0], [0, 1, 0], [0, 0, 0]],\n                        ops.convert_to_tensor([[0, 0, 0],\n                                               constant_op.constant([0, 1, 0]),\n                                               [0, 0, 0]]))\n    self.assertAllEqual([[0, 0, 0], [0, 1, 0], [0, 0, 0]],\n                        ops.convert_to_tensor([\n                            constant_op.constant([0, 0, 0]),\n                            constant_op.constant([0, 1, 0]),\n                            constant_op.constant([0, 0, 0])\n                        ]))\n\n  def testWithNDArray(self):\n    with self.session():\n      result = ops.convert_to_tensor([[[0., 0.],\n                                       constant_op.constant([1., 1.])],\n                                      np.array(\n                                          [[2., 2.], [3., 3.]],\n                                          dtype=np.float32)])\n      self.assertAllEqual([[[0., 0.], [1., 1.]], [[2., 2.], [3., 3.]]],\n                          self.evaluate(result))\n\n  def testDtype(self):\n    t_0 = ops.convert_to_tensor([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]])\n    self.assertEqual(dtypes.float32, t_0.dtype)\n\n    t_1 = ops.convert_to_tensor([[0., 0., 0.], constant_op.constant(\n        [0., 0., 0.], dtype=dtypes.float64), [0., 0., 0.]])\n    self.assertEqual(dtypes.float64, t_1.dtype)\n\n    t_2 = ops.convert_to_tensor(\n        [[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]], dtype=dtypes.float64)\n    self.assertEqual(dtypes.float64, t_2.dtype)\n\n    t_3 = ops.convert_to_tensor(\n        [[0., 0., 0.],\n         constant_op.constant([0., 0., 0.], dtype=dtypes.float64), [0., 0., 0.]\n        ],\n        dtype=dtypes.float32)\n    self.assertEqual(dtypes.float32, t_3.dtype)\n\n    t_4 = ops.convert_to_tensor(\n        [constant_op.constant([0., 0., 0.], dtype=dtypes.float64)],\n        dtype=dtypes.float32)\n    self.assertEqual(dtypes.float32, t_4.dtype)\n\n    with self.assertRaises(TypeError):\n      ops.convert_to_tensor([\n          constant_op.constant(\n              [0., 0., 0.], dtype=dtypes.float32), constant_op.constant(\n                  [0., 0., 0.], dtype=dtypes.float64), [0., 0., 0.]\n      ])\n\n  def testDtypeConversionWhenTensorDtypeMismatch(self):\n    t_0 = ops.convert_to_tensor([0., 0., 0.])\n    self.assertEqual(dtypes.float32, t_0.dtype)\n\n    t_1 = ops.convert_to_tensor([0, 0, 0])\n    self.assertEqual(dtypes.int32, t_1.dtype)\n\n    t_2 = ops.convert_to_tensor([t_0, t_0, t_1], dtype=dtypes.float64)\n    self.assertEqual(dtypes.float64, t_2.dtype)\n\n\nif __name__ == \"__main__\":\n  test.main()"